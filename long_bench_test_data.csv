input,context,answers,length,dataset,language,all_classes,_id,manual_answers,answers_len
"Which film came out first, Una Prostituta Al Servizio Del Pubblico E In Regola Con Le Leggi Dello Stato or The Bag Man?","Passage 1:
The Police Serve the Citizens?
La polizia è al servizio del cittadino? (internationally released as The Police Serve the Citizens?) is a 1973 Italian giallo-poliziottesco film directed by Romolo Guerrieri. The film is set in Genova.

Cast
Enrico Maria Salerno: Commissioner Nicola Sironi
Giuseppe Pambieri: Commissioner Marino
John Steiner: Lambro
Venantino Venantini: Mancinelli
Alessandro Momo: Michele Sironi
Memmo Carotenuto: Baron
Marie Sophie Persson: Cristina
Daniel Gélin: Ing. Pier Paolo Brera
Gabriella Giorgelli: Eros, Prostitute
Enzo Liberti: Greengrocer

Release
The film was released on August 25, 1973. It was distributed by P.I.C. in Italy. The film grossed a total of ₤1.033 billion in Italy.
Passage 2:
Bag-in-box
A bag-in-box or BiB is a container for the storage and transportation of liquids.  It consists of a strong bladder (or plastic bag), usually made of several layers of metallised film or other plastics, seated inside a corrugated fiberboard box.

Features
The bag is supplied to the company which will fill it as an empty pre-made bag. The company filling the bag with its product generally removes the tap, fills the bag (with wine, juice or other liquid) and replaces the tap and then the bag is placed in the box. 
The bags are available as singles for semi-automatic machines or as web bags, where the bags have perforations between each one. These are used on automated filling systems where the bag is separated on line either before the bag is automatically filled or after. Depending on the end use, there are a number of options that can be used on the bag instead of the tap. The bags can be filled from chilled product temperatures up to 85 °C (185 °F).
BiB packaging can be made using form seal fill (FSF) technology, where the bags are manufactured on-line from reels of film, then the FlexTap is inserted then filled on an integral rotary head filler. The BiB is currently used to package wine, soda fountain syrup products, milk, liquid chemicals, and water.

History
The first commercial BiB system was invented by American chemist William R. Scholle in 1955 for the safe transportation and dispensing of battery acid. Scholle's invention inspired a ""packaging revolution."" In 1991, Scholle was inducted into the packaging hall of fame for his invention.

Uses
BiB has many common commercial applications. Among the most common ones are to supply syrup to soft drink fountains and to dispense bulk supplied condiments such as ketchup or mustard in the foodservice industry (especially in fast food outlets). BiB technology is still used for its original application of dispensing sulfuric acid for filling lead-acid batteries in garages and dealerships. As explained further below, BiBs have also been implemented for consumer applications like boxed wine.For commercial syrup applications, the customer opens one end of the box (sometimes via a pre-scored opening) and connects a compatible connector to a built-in port on the bag to pump out its contents. The port itself contains a one-way valve which opens only under pressure from the attached connector and which prevents contamination of the syrup in the bag. For consumer applications like boxed wine, there is a tap already present on the bag which protrudes through a pre-cut hole on the box, so all the consumer has to do is locate the tap on the outside of the box.
Producers like BiB packaging because it is inexpensive. BiB also offers environmental benefits by allowing contents of 1.5–1000 liters, so that less packaging or labeling is required. The material it is made from is lighter than the other plastic alternatives, providing it with a better carbon footprint.

Wine cask
The 'wine cask' was invented by Thomas Angove (1918–2010) of Angove's, a winemaker from Renmark, South Australia, and patented by the company on April 20, 1965. Polyethylene bladders of 1 gallon (4.5 litres) were put into corrugated boxes for sale to consumers. An original design required that the consumer cut the corner off the bladder inside the box, pour out the desired quantity of wine and then reseal it with a special peg.In 1967, Charles Malpas and Penfolds Wines patented a plastic, air-tight tap welded into an aluminised film bladder, making storage much more convenient for consumers. All modern wine casks now utilise some sort of plastic tap, which is exposed by tearing away a perforated panel on the box.
The main advantage of BiB packaging is that it prevents oxidation of the wine during dispensing. Rather than working as a conventional tap, the bladder uses gravity pressure to squeeze the liquid out of the bladder, whereas a conventional barrel tap works by allowing incoming air to displace the contents. After opening wine in a bottle, it is oxidized by air in the bottle which has displaced the wine poured; wine in a bag is not touched by air and thus not subject to oxidation until it is dispensed. Cask wine is not subject to cork taint or spoilage due to slow consumption after opening.Although a promising technology, there have been production and design problems. The impermeable bladders tend to delaminate around the tap and where the two halves are joined. If tap components are deposited in the bladder during assembly, all the bladders must be destroyed to find the components as the bladders are opaque. It has also been difficult to manufacture taps that do not leak air into the bladder since tap parts usually do not join neatly, although there have been significant improvements. Most red wines require breathing before consumption which is not possible with casks, so the wine has air circulated through it before bottling (usually by running through a centrifuge), which reduces shelf life considerably. Most casks will have a best-before date stamped. As a result, it is not intended for cellaring and should be consumed within the prescribed period.

Aseptic packaging
BiB is also used extensively in the packaging of processed fruit and dairy products in aseptic processes. Using aseptic packaging equipment, products can be packed in aseptic packaging. Pasteurized or UHT treated products packed into this format can be ""shelf-stable"", requiring no refrigeration. Some products can have a shelf life of up to 2 years, depending on the type of bag that is used.
The key to this unique system is that the product being filled is not exposed to the external environment at any stage during the process and as such, there is no possibility of a bacterial load being added to the product during the filling process. To ensure there is no contamination from the packaging, the bag is irradiated after the bag manufacturing process.
These packs are typically from 10 to 1200 liters and offer the advantage of cheap, disposable and transport efficient packaging.

See also
Corrugated box design
Tetra Brik

Notes
Passage 3:
Bagman (disambiguation)
A bagman or bag man is a collector of dirty money for organized crime.
Bagman may also refer to:

Bagman (video game), a 1982 French platform arcade game
Bagman, a biochip containing the personality of a fallen comrade in the 2000AD comic Rogue Trooper
Bagman (film), a 2010 film about Jack Abramoff
Sack Man or Bag Man, a bogeyman-figure
Ludo Bagman, a character in J. K. Rowling's Harry Potter series
The Bag Man, a 2014 film
Bag Man (podcast), a 2018 podcast about Spiro Agnew's 1973 bribery and corruption scandal
""Bagman"" (Better Call Saul), an episode of the television series Better Call Saul

See also
Bağban (disambiguation)
Bag boy (disambiguation)
Bag lady (disambiguation)
Passage 4:
Donne con le gonne
Women in Skirts (Italian: Donne con le gonne) is a 1991 Italian romantic comedy film directed by Francesco Nuti. It was the highest-grossing Italian film in Italy in 1992. The film was nominated for two awards, Best Supporting Actress and Best Costume Design.

Cast
Francesco Nuti as Renzo Calabrese
Carole Bouquet as Margherita
Barbara Enrichi as Renzo's mother
Cinzia Leone as Cinzia
Gastone Moschin as lawyer Carabba
Didi Perego as Pubblico Ministero
Daniele Dublino as count Ugolino
Passage 5:
Io piaccio
Io piaccio (also known as La via del successo... con le donne) is a 1955 Italian comedy film directed by Giorgio Bianchi.

Plot
Professor Maldi, a researcher on the company held by Commendatore Tassinetti (Aldo Fabrizi), experiments on various animals, and especially on the capon Gildo, its preparation which should give courage to the men. Pressed by Tassinetti Maldi decides to experiment on himself the latest version of its compound, without waiting to know the reaction of the capon.
Soon, the shy Maldi finds himself desired by every woman he meets: rather than courage, his discovery provides an irresistible fascination for twenty-four hours.

Cast
Walter Chiari: Prof. Roberto Maldi
Aldo Fabrizi: Commendatore Tassinetti
Peppino De Filippo: Nicolino Donati
Dorian Gray: Doriana Paris
Bianca Maria Fusari: Sandra, Maldi's assistant
Tina Pica: Sibilla
Mario Carotenuto: Marassino
Sandra Mondaini:Giovanna
Lina Volonghi: Lucia, Tassinetti's wife
Valeria Fabrizi: Wardrobe supervisor
Enrico Glori: Butler at Caprice nightclub
Riccardo Billi: Husband
Erminio Spalla: Doriana's confidence man
Bruno Corelli: Director
Dina Perbellini: Marassino's wife
Passage 6:
It's in the Bag
It's in the Bag can refer to:

It's in the Bag, a pricing game on The Price Is Right
It's in the Bag (game show), a long-running New Zealand game show
L'affaire est dans le sac (English: It's in the Bag), a 1932 French film
It's in the Bag (1936 film), a British film
It's in the Bag (1944 film), a British film
It's in the Bag! (1945 film), a 1945 American film starring Fred Allen
Passage 7:
The Bag Man
The Bag Man (also known as Motel or The Carrier) is a 2014 neo-noir crime thriller film directed by David Grovic. It is based on an original screenplay by James Russo and a rewrite by David Grovic and Paul Conway and an inspiration of The Cat: A Tale of Feminine Redemption by Marie-Louise von Franz. The film stars John Cusack, Rebecca Da Costa, Crispin Glover, Dominic Purcell, Robert De Niro, and Sticky Fingaz. The film premiered on February 28, 2014, in New York and Los Angeles.

Plot
Brutal gangster Dragna recruits professional killer Jack to pick up a bag and wait for his arrival at a motel.  Dragna stresses that Jack is not to open the bag or allow anyone to view its contents under any circumstances.  Confused as to why Dragna wants him to do such an apparently easy job, Jack presses for more details, but Dragna only reiterates the rules.  When Jack acquires the bag and a henchman of Dragna's shoots him in the hand, Jack kills him and stuffs the body in his car's trunk.  Dragna is unsympathetic when Jack calls him and instructs Jack to stick to the plan.
At the motel, Jack meets several people: Ned, the desk clerk who uses a wheelchair; Rivka, a tall hooker; and Lizard and Guano, a pair of pimps. Jack requests room number thirteen, and Ned cautions him that it is a deathtrap, as it is unconnected.  When two suited men become curious about Jack, he abruptly breaks into their room and kills both. A subsequent search reveals FBI badges and a briefcase. Jack sets the briefcase aside and leaves to dump the corpse stored in his car, but promptly returns upon being spotted, only to discover that Rivka has broken into his room. Fearful that Lizard will kill her, she requests that he allow her to stay.
Jack initially demands that she leave, then detains her when he cannot be sure whether she opened the bag. Rivka points out that the briefcase contains a photo of the bag, and Jack becomes worried that others may attempt to acquire it. When Jack attempts to drive Rivka to a bus station, she spots the corpse in the back of his car. Lizard and Guano first question Jack about Rivka and later, on the road, attack him. On their corpses he finds another photo of the bag. Unsure what to do with Rivka, or of her involvement, he returns to the hotel with her to await Dragna's arrival.
Ned becomes suspicious that Jack has a guest in his room and calls the sheriff when Jack refuses to pay the double occupancy fee. Sheriff Larson briefly questions Jack, and, after Larson leaves, Jack threatens Ned. Increasingly worried about the safety of the bag, Jack buries it near the motel, only to be caught by Ned, who is now out of his wheelchair. Jack kills Ned and returns to the motel, where Larson arrests him under suspicion of Ned's disappearance.
As Larson prepares to torture Jack for information, Rivka shows up. Larson threatens to rape her, but she overpowers a deputy and frees Jack, who then kills Larson. When Rivka demonstrates detailed knowledge of the bag, Jack becomes suspicious of her again, but she points out that she has saved his life.  Somewhat mollified, he retrieves the bag and waits in room fourteen. Dragna finally appears, disappointed in Jack's apparent lack of trust. Nonetheless satisfied that Jack has not looked in the bag, Dragna explains that the whole situation was a test of Jack's skills and character, as he doubted Jack's resolve in the wake of Jack's fiancee's unsolved murder some months earlier. The motel and the local cops are all on Dragna's payroll, and were all (unwittingly) part of the test.
As Dragna prepares to leave, Rivka spontaneously tells Jack that she looked in the bag. Frustrated, Jack points out that Dragna will now kill them both. Jack dutifully reports Rivka's action to Dragna, who orders her killed. Jack instead kills Dragna's bodyguard, who wounds Rivka. Jack hunts down Dragna, and both are wounded. Jack looks in the bag and discovers the head of his fiancee, whom Dragna had killed in order to prevent Jack from quitting the murder-for-hire business. Trying to persuade Jack to surrender, Dragna destroys the hotel (where Rivka supposedly is) with remote-detonated explosives. Moments later, Rivka appears and saves Jack by killing Dragna, but she is shot again. Later, in Dragna's lawyer's office, Rivka reveals herself as Dragna's mistress and personal assassin who was sent to the motel to ensure things went Dragna's way. She collects a five million dollar reward for Jack's assassination, and she and Jack drive off together.

Cast
John Cusack as Jack
Rebecca Da Costa as Rivka
Robert De Niro as Dragna
Crispin Glover as Ned Stensen
Dominic Purcell as Sheriff Larson
Sticky Fingaz as Lizard
Martin Klebba as Guano
Theodus Crane as Goose
David Shumbris as Pike
Mike Mayhall as Deputy Jones
Danny Cosmo as Bishop
David Grovic as Dragna's lawyer

Reception
Rotten Tomatoes, a review aggregator, reports that 10% of 42 surveyed critics gave the film a positive review; the average rating was 3.4/10.  Metacritic rated it 28/100 based on 18 reviews.  Scott Foundas of Variety called it ""a tedious, self-consciously quirky postmodern noir"".  Stephen Farber of The Hollywood Reporter wrote, ""If it weren't for the touches of cruelty, this might have been a passable B-movie, but Bag Man ends up wasting the A-list talent caught up in the lurid exercise.""  Stephen Holden of The New York Times described the plot as ""a protracted and increasingly tedious cat-and-mouse game"" that ""pathetically tries to build up expectations about what might be in the bag"".  Mike D'Angelo of The A.V. Club called it a Quentin Tarantino knock-off made two decades too late.
Passage 8:
Una prostituta al servizio del pubblico e in regola con le leggi dello stato
Una prostituta al servizio del pubblico e in regola con le leggi dello stato (literally ""A prostitute serving the public and complying with the laws of the state"", also known as Prostitution Italian Style) is a 1970  Italian comedy-drama film written and directed by Italo Zingarelli.For her performance Giovanna Ralli won the Grolla d'oro for best actress.

Cast
Giovanna Ralli: Oslavia
Giancarlo Giannini: Walter
Jean-Marc Bory: François Coly
Denise Bataille
Paolo Bonacelli
Roberto Chevalier
Passage 9:
The Man with the Bag
""The Man with the Bag"" may refer to:

The Sack Man, a mythical figure
""(Everybody's Waitin' for) The Man with the Bag"", a Christmas song
""The Man with the Bag"", an episode of Ally McBeal
Passage 10:
The Police Serve the Citizens?
La polizia è al servizio del cittadino? (internationally released as The Police Serve the Citizens?) is a 1973 Italian giallo-poliziottesco film directed by Romolo Guerrieri. The film is set in Genova.

Cast
Enrico Maria Salerno: Commissioner Nicola Sironi
Giuseppe Pambieri: Commissioner Marino
John Steiner: Lambro
Venantino Venantini: Mancinelli
Alessandro Momo: Michele Sironi
Memmo Carotenuto: Baron
Marie Sophie Persson: Cristina
Daniel Gélin: Ing. Pier Paolo Brera
Gabriella Giorgelli: Eros, Prostitute
Enzo Liberti: Greengrocer

Release
The film was released on August 25, 1973. It was distributed by P.I.C. in Italy. The film grossed a total of ₤1.033 billion in Italy.",['Una Prostituta Al Servizio Del Pubblico E In Regola Con Le Leggi Dello Stato'],2809,2wikimqa,en,,cf56ec8180c3c7622a439e68704fc1ff73770f2e8ee2926e,Una Prostituta Al Servizio Del Pubblico E In Regola Con Le Leggi Dello Stato,76
Who directed the film where Susanne Pollatschek voiced Olivia Flaversham?,"Passage 1:
Chaste Susanne (film)
Chaste Susanne (German: Die keusche Susanne) is a 1926 German silent comedy film directed by Richard Eichberg and starring Lilian Harvey, Willy Fritsch and Ruth Weyher. It is based on the 1910 operetta Die keusche Susanne composed by Jean Gilbert with a libretto by Georg Okonkowski.  In Britain it was released under the alternative title The Girl in the Taxi in reference to The Girl in the Taxi in the English version of the operetta. The film's art direction is by Jacek Rotmil. It was filmed at the Johannisthal Studios in Berlin.
It marked the first pairing of Harvey and Fritsch who went on to become the leading screen couple in Weimar and early Nazi cinema. The film premiered at the UFA-Palast am Zoo and was a smash hit on its release.

Cast
Lilian Harvey as Jacqueline
Willy Fritsch as René Boislurette
Ruth Weyher as Susanne
Otto Wallburg as Charency
Hans Junkermann as Baron Aubrais
Lydia Potechina as Baronin Aubrais
Sascha Bragowa as Charencys Frau Rose
Werner Fuetterer as Hubert
Hans Wassmann as Dr. med. Pomarel
Ernst Hofmann as Henry, Renés Freund
Wilhelm Bendow
Albert Paulig
Passage 2:
The Night Manager (miniseries)
The Night Manager is a British television serial directed by Susanne Bier and starring Tom Hiddleston, Hugh Laurie, Olivia Colman, Tom Hollander, David Harewood and Elizabeth Debicki. It is based on the 1993 novel of the same title by John le Carré and adapted to the present day by David Farr. The six-part series began broadcasting on BBC One on 21 February 2016. In the United States, it began on 19 April 2016 on AMC. It has been sold internationally by IMG to over 180 countries.The Night Manager was nominated for thirty-six awards and won eleven, including two Primetime Emmy Awards (for director Bier and music composer Victor Reyes) and three Golden Globe Awards (for Hiddleston, Colman, and Laurie).

Premise
Jonathan Pine, night manager of a luxury hotel in Cairo and former British soldier, is recruited by Angela Burr, the manager of a Foreign Office task force investigating illegal arms sales, to infiltrate the inner circle of arms dealer Richard Roper.

Cast
Main
Tom Hiddleston as Jonathan Pine, a former military officer.
Hugh Laurie as Richard ""Dicky"" Onslow Roper, an arms dealer who is also the father of Danny Roper. Despite his charming personality and also his regular charity work, Roper is regarded as ""the worst man in the world"" due to his ruthless, amoral psychopathic nature.
Olivia Colman as Angela Burr, a woman who is the manager of a Foreign Office task force dedicated to bringing down arms dealer Richard Roper.
Tom Hollander as Major Lance ""Corky"" Corkoran, a man who works for Richard Roper.
Elizabeth Debicki as Jemima ""Jed"" Marshall, the girlfriend/mistress of Richard Roper.
Alistair Petrie as Alexander ""Sandy"" Langbourne, the Lord Langbourne and a man who assists Richard Roper in his arms dealing business.
Natasha Little as Caroline ""Caro"" Langbourne, the Lady Langbourne, Alexander ""Sandy"" Langbourne's wife.
Douglas Hodge as Rex Mayhew
David Harewood as Joel Steadman
Tobias Menzies as Geoffrey Dromgoole
Antonio de la Torre as Juan Apostol
Adeel Akhtar as Rob Singhal
Michael Nardone as Frisky, one of Richard Roper's henchmen.
Hovik Keuchkerian as Tabby, one of Richard Roper's henchmen.

Supporting
Noah Jupe as Danny Roper, a young boy who is the son of arms dealer Richard Roper.
David Avery as Freddie Hamid, a man responsible for killing Sophie ""Samira"" Alekan.
Amir El-Masry as Youssuf
Aure Atika as Sophie (Samira) Alekan
Nasser Memarzia as Omar Barghati
Russell Tovey as Simon Ogilvey
Neil Morrissey as Harry Palfrey
Katherine Kelly as Pamela, the Permanent Secretary
Bijan Daneshmand as Kouyami
Hannah Steele as Marilyn

Production
In January 2015 it was announced that the series would be co-produced by the BBC, AMC and The Ink Factory. Onsite services were provided by Palma Pictures.
Filming began on 19 March 2015 in Zermatt, Switzerland. Production then moved to London, UK. From 13 to 17 April 2015, location filming took place at Blackpool Mill Cottage, Hartland Abbey, and in and around Hartland, Devon.  On 20 April 2015, production moved to Marrakesh, Morocco. The Es Saadi Resort was used as the  location for the fictional Nefertiti Hotel in Cairo. At the end of May, production moved to Majorca, Spain; principal photography wrapped in Majorca on 3 July 2015. Notable places include Port de Sóller, luxury property La Fortaleza in Port de Pollença and several locations in Palma.The author John le Carré makes a cameo appearance as an insulted restaurant diner in episode four.In February 2023, it was reported that the second series was in development with Hiddleston set to return.

Episodes
Broadcast
The first episode of The Night Manager was broadcast on 21 February 2016 on BBC One in the United Kingdom. AMC Spain broadcast the series on 24 February 2016 in Spain. TV3 in New Zealand broadcast the series on 28 February 2016. In the United States, the show premiered on 19 April 2016 on AMC. The serial aired in Australia on BBC First on 20 March 2016. The serial aired in Saudi Arabia on AMC starting on 6 June 2016. In Finland the serial premiered 22 June 2016 on MTV3. In Sweden the serial first aired on 22 August 2016 on TV4, split up into eight episodes not the original release of six episodes. In Germany the serial started airing on 29 August 2016 on ZDF. The series was broadcast on Raidió Teilifís Éireann in Ireland on 29 August 2016. On 24 February 2017, The Night Manager started to air in the Netherlands on public broadcaster NPO 1, being broadcast by AVROTROS. The series was broadcast by BBC Persian from 15 February 2018 in Iran, Afghanistan and Tajikistan.

Critical reception
The series received widespread critical acclaim.
Adam Sisman, le Carré's biographer, wrote in the UK The Daily Telegraph, ""It is more than 20 years since the novel was published, and in that time two film companies have tried and failed to adapt it, concluding that it was impossible to compress into two hours. But this six-hour television adaptation is long enough to give the novel its due."" He added, ""And though Hugh Laurie may seem a surprising choice to play 'the worst man in the world', he dominates the screen as a horribly convincing villain.  Alert viewers may spot a familiar face in the background of one scene, in a restaurant: John le Carré himself makes a cameo, as he did in the films of A Most Wanted Man and Tinker Tailor Soldier Spy. But he is on screen only for an instant: blink and you'll miss him.""Reviewing the first episode for The Guardian, Archie Bland began by noting, ""The Night Manager is as sexed up as television drama comes. In Tom Hiddleston and Hugh Laurie it has bona fide international stars; in John le Carré's source novel it has a pedigree of untouchable grandeur. The palette is as sumptuous as one of our hero Jonathan Pine's beautiful hotels"". He added, ""It's Laurie's vulpine performance that gives The Night Manager its force once the smell of money has worn off. But we barely see him for the first 40 minutes – a delayed gratification trick that's always worked like magic on me, ever since we spent the whole first episode of The West Wing waiting impatiently to meet Josiah Bartlet."" Turning to Hiddleston's performance, Bland wrote, ""And as the embodiment of the show's atmosphere of paralysed establishment glamour, Hiddleston is the business. When the noble beast beneath that accommodating English exterior begins to make itself known, I find the righteous revenge he's intent on wreaking on Roper compelling.""IGN reviewer Jesse Schedeen gave the serial 8.8 out of 10, saying, ""The Night Manager proves that television is the ideal format to bring le Carré's novels to life. This miniseries is tightly paced, suspenseful and boasts strong performances from the likes of Hiddleston, Laurie, Colman and Hollander. With any luck, this series will open the doors for more of le Carré's classic spy tales to make their way to the small screen.""The New Yorker reviewer Emily Nussbaum was unimpressed, calling the miniseries ""elegant but ultimately empty"", with ""overwrought sequences of doomed love"", ""just an old recipe made with artisanal ingredients"". She praised the actors but found the characterisation of Roper ""less Dr. No and more Mr. Magoo"". However, Brian Tallerico called it a ""brilliant adaptation"" on RogerEbert.com, with praise for the performances of Hiddleston and Laurie, and for Susanne Bier's direction: ""Bier brings a cinematic language to The Night Manager, and a deeper understanding of character than we often get in projects that hinge on espionage. She understands that it’s not about the twists and turns of the spy game but the impact it has on those who are playing it.""

Accolades
Notes
Passage 3:
The Great Mouse Detective
The Great Mouse Detective (released as Basil the Great Mouse Detective in some countries and as The Adventures of the Great Mouse Detective for its 1992 American re-release) is a 1986 American animated mystery adventure film produced by Walt Disney Feature Animation and released by Walt Disney Pictures. The 26th Disney animated feature film, the film was directed by John Musker, Ron Clements, Dave Michener, and Burny Mattinson (in their feature directorial debuts). The main characters are all mice and rats living in Victorian London.
Based on the children's book series Basil of Baker Street by Eve Titus and Paul Galdone, it draws heavily on the tradition of Sherlock Holmes with a heroic mouse who consciously emulates the detective. Titus named the main character after actor Basil Rathbone, who is best remembered for playing Holmes in film (and whose voice, sampled from a 1966 reading of ""The Red-Headed League"" was the voice of Holmes in this film, 19 years after his death). Sherlock Holmes also mentions ""Basil"" as one of his aliases in the Arthur Conan Doyle story ""The Adventure of Black Peter"".
The Great Mouse Detective was released to theaters on July 2, 1986, to positive reviews and financial success, in sharp contrast to the box office underperformance of Disney's previous animated feature film, The Black Cauldron (1985). Indeed, the film's timely success has been credited with keeping Walt Disney Animation a going concern after the previous film's failure by renewing upper management's confidence in the department, thus setting the stage for the Disney Renaissance when feature animated films would become the corporation's most lucrative and prestigious product.

Plot
In London in June 1897, a young mouse named Olivia Flaversham is celebrating her birthday with her single  father, toymaker Mr. Flaversham. Suddenly, a bat with a crippled wing and a peg leg bursts into the toyshop, kidnapping Flaversham. Olivia leaves to find Basil of Baker Street, the famous Great Mouse Detective, but gets lost. David Q. Dawson, a retired army surgeon mouse newly returned from Afghanistan, meets Olivia and escorts her to Basil's residence. Being busy already, Basil initially dismisses them. Olivia then mentions the bat that abducted her father, and Basil realizes that Olivia saw Fidget, the assistant of Professor Ratigan, the criminal mastermind whom Basil was working to catch. It is then revealed that Ratigan kidnapped Flaversham to create a clockwork robot replica of the Queen of the Mice, so that Ratigan can usurp her place as ""Supreme Ruler of all Mousedom"". Flaversham initially refuses to participate in the scheme, but capitulates when Ratigan threatens to harm Olivia.
Meanwhile, Fidget appears in Basil's window, then suddenly disappears. Basil, Dawson and Olivia take Toby, Sherlock Holmes' pet Basset Hound, to trail Fidget's scent. They trace Fidget to a human-sized toyshop; while searching the area, Dawson finds Fidget's checklist, and Basil discovers Fidget has been stealing clockwork mechanisms and toy soldiers' uniforms. 
Fidget ambushes and kidnaps Olivia before Basil and Dawson can stop him. Basil does some chemical tests to the checklist, discovering it came from the ""Rat Trap"", a tavern near the junction of the sewer and the Thames. Basil and Dawson disguise themselves as sailors and head to the tavern; they spot Fidget there, and follow him to Ratigan's headquarters, only to be ambushed by Ratigan and his henchmen. Ratigan has the pair tied to a spring-loaded mousetrap connected to a Rube Goldberg machine of various killing devices. Ratigan then sets out for Buckingham Palace, where his henchmen hijack the royal guards' roles and kidnap the Mouse Queen. Inspired by a remark Dawson made, Basil deduces the trap's weakness, freeing himself, Dawson and Olivia.
At Buckingham Palace, Ratigan forces Flaversham to operate the toy Queen, while the real one is taken to be fed to Felicia, Ratigan's pet cat. The toy Queen declares Ratigan the ruler of all Mousedom, and he announces his dictatorial plans for his new ""subjects"". After Basil, Dawson, and Olivia save Flaversham and the real Queen, they restrain Fidget and Ratigan's other henchmen, while Toby chases Felicia until she jumps over a wall, inadvertently into a pack of Royal Guard Dogs. Basil seizes control of the mechanical queen, making it denounce Ratigan as a fraud while breaking it into pieces. Realizing Ratigan's treason, the enraged crowd attacks, but Ratigan escapes on his dirigible with Fidget, holding Olivia hostage. Basil, Dawson, and Flaversham create an airship from a matchbox, balloons, and a Union Jack, and set off in pursuit. Ratigan tosses Fidget overboard to lighten the load; however, Basil jumps onto the dirigible to confront Ratigan, causing it to crash straight into Big Ben's clockface. Inside the clocktower, Basil restrains Ratigan, rescues Olivia, and safely delivers her to Flaversham. Ratigan breaks free and attacks Basil; however, when the clock strikes 10:00, the vibrations cause Ratigan to fall to his death. He attempts to take Basil with him, but Basil grabs a part of Ratigan's dirigible and saves himself. 
Back at Baker Street, the group recounts their adventures. The Flavershams depart for home, and Dawson reluctantly resolves to leave as well. A new client arrives, and Basil introduces Dawson to her as his friend and investigative partner, prompting Dawson to remain and assist in Basil's future cases.

Voice cast
Barrie Ingham as Basil, a brilliant mouse detective.
Ingham also voices Bartholomew, a drunken lackey of Ratigan's.
Vincent Price as Professor Ratigan, Basil's long-established arch-enemy.
Val Bettin as Major Dr. David Q. Dawson, previously of the Queen's 66th Regiment in Afghanistan. He eventually becomes Basil's associate, friend, and personal biographer. Dawson also serves as the film's narrator.
Bettin also voices one of Ratigan's thugs.
Susanne Pollatschek as Olivia Flaversham, a young Scottish mouse who seeks Basil's help in finding her father.
Candy Candido as Fidget, Ratigan's bumbling bat right-hand henchman who has a crippled wing and a peg leg. As a result of his crippled wing, he cannot fly.
Candido also voices a reprobate in the pub.
Alan Young as Mr. Flaversham, Olivia's affectionate Scottish father who owns a toy shop.
Diana Chesney as Mrs. Judson, Basil's housekeeper who is often exasperated by his antics.
Eve Brenner as Queen Mousetoria, the mouse Queen of the United Kingdom, whom Ratigan attempts to depose.
Basil Rathbone as Sherlock Holmes, the famous human detective who lives above Basil. His voice is taken from the 1966 Caedmon Records recording of the Sherlock Holmes story ""The Red-Headed League"".
Laurie Main as Dr. Watson, the medical associate/partner of Sherlock Holmes, who also lives above Basil. Unlike Rathbone, voice samples of Nigel Bruce were not used for the voice of Watson as he had died in 1953.
Wayne Allwine, Tony Anselmo, and Walker Edmiston as Ratigan's Thugs.
Melissa Manchester as Miss Kitty Mouse, who sings ""Let Me Be Good To You"".

Production
The idea of doing an animated film about Sherlock Holmes with animals was first discussed during the production of The Rescuers. Veteran layout artist Joe Hale is credited with suggesting to adapt the children's book series Basil of Baker Street by Eve Titus, but the project fell into development limbo because of the similarities to The Rescuers. In 1982, Ron Clements proposed adapting the children's book series into an animated feature and, along with story artist Pete Young, it was pitched to Disney President Ron Miller who approved the project. Earlier in his career, Clements created a 15-minute Sherlock Holmes animated short recorded on Super 8 film. Because the animators were displeased with the direction The Black Cauldron was heading, Basil of Baker Street was approved as an alternative project.Burny Mattinson and John Musker were assigned as the original directors while Dave Michener was also added as co-director. Miller became the producer for the film. The first idea for the victim was for Olivia—then an older and potential love interest whom Dawson falls for, but Miller suggested the character be ""a little girl, someone they [the audience] can feel sorry for."" One of the dropped characters was a stool pigeon who always hung around Buckingham Palace and tipped Basil off about the skullduggery. The writers dropped the characters deciding for Basil to figure it out for himself.With the departure of Miller in 1984, the board of directors appointed Michael Eisner, who had resigned from Paramount Pictures, to become the new CEO. Eisner recruited former production head Jeffrey Katzenberg to become studio chairman over Disney's film division. Following a story reel screening of Basil, Eisner and Katzenberg complained about the slow pacing of the story and ordered for rewrites before animation would commence. Although the intended release was set for Christmas 1987, Michael Eisner slashed the projected production budget at $24 million in half where it was green-lit at $10 million and moved the release date up to July 1986 giving the production team one year to complete the film. To replace Miller who had been producer, Feature Animation chairman Roy E. Disney assigned Mattinson to serve as director/producer, but finding both tasks much too laborious, Mattinson decided to remain as producer. Musker and Michener remained as directors, but with the shortened production schedule, Clements became an additional director.Following the box office under-performance of the 1985 Paramount/Amblin film Young Sherlock Holmes, Eisner decided to rename Basil of Baker Street into The Great Mouse Detective feeling the name ""Basil"" was ""too English"". The re-titling of the film proved to be unpopular with the filmmakers so much that animator Ed Gombert wrote a satirical interoffice memo, allegedly by studio executive Peter Schneider, which gave preceding Disney films generic titles such as Seven Little Men Help a Girl, The Wonderful Elephant Who Could Really Fly, The Little Deer Who Grew Up, The Girl with the See-through Shoes, Two Dogs Fall in Love, Puppies Taken Away, and A Boy, a Bear and a Big Black Cat. These generic titles would later become a category on Jeopardy!.

Casting
Following a succession of American and British actors who read for the part of Basil, Royal Shakespeare actor Barrie Ingham won the role within six minutes of his audition. Val Bettin was co-director Ron Clements's first choice for Dawson. For Olivia, Susanne Pollatschek was selected over hundreds of other applicants while Alan Young, who had voiced Scrooge McDuck for Mickey's Christmas Carol, was selected to voice her father Hiriam because of his authentic Scottish brogue.When the filmmakers watched the 1950 comedy film Champagne for Caesar to study Ronald Colman's performance as a possible model for Basil, they immediately decided to cast Vincent Price, who also starred in the film, as Ratigan. A veteran actor for fifty-two years, Price was willing to do an audition commenting ""If anybody but Disney had asked me, I would have been offended."" Following a voice test, veteran voice artist Candy Candido recorded his dialogue for Fidget in one hour. To heighten the pitch, the tape recording of his voice was sped up. Candido's natural voice was kept for one character shouting ""Get off, you eight-legged bum.""

Animation
Basil was first modeled on Bing Crosby, but the animators eventually took inspiration from Leslie Howard. Initially, Ratigan had been designed as thin, weasely, and ratlike. Following the screening of Champagne for Caesar, Glen Keane noted that following the casting of Price, ""his expressive voice and attitude inspired us to further redesign the character."" Additionally, during one story meeting, Glen Keane decided to base the stature of Ratigan on then-Disney CEO Ron Miller, who was a 6'6"" former football player for the Los Angeles Rams. Furthermore, Keane lifted his personality as he was thumbing through these ""photographs of people of London in the 1800s, of railroad men, and there was this one guy smoking a cigar—he had a top hat and there was just something about this guy—this Ratigan ... this rat sucking the cigar, completely dressed to the hilt, he was sharp and perfect—he's a sewer rat dressed like a king and he lives as a king!"" The following supervising animators included Mark Henn for Basil, Hendel Butoy for Dawson, Rob Minkoff for Olivia, Andreas Deja for Queen Moustoria, Ruben Aquino for Mrs. Judson, and Mike Gabriel for Toby and Felicia.The original finale was to take place on the hands of Big Ben with Ratigan eventually falling to his demise. However, layout artist Mike Peraza approached Musker with the idea of restaging the final confrontation so the characters would break through the face of Big Ben with the grinding clockwork gears providing added menace, in which Musker agreed. Peraza's inspiration for the scene was a Japanese anime film, The Castle of Cagliostro (1979), the feature film debut of animator Hayao Miyazaki which is part of the Lupin III franchise; The Castle of Cagliostro, which Peraza was a fan of, featured a climactic scene involving characters amidst giant turning gears in a clock tower. Pereza and his team was sent to London for video reference and were granted unprecedented access to the clockworks inside Big Ben. Because the bells would chime at every quarter-hour, the team completed their research in one hour.Back at the Feature Animation building, animators Phil Nibbelink and Tad Gielow spent months designing the interior of Big Ben, with each gear produced as wire-frame graphics on a computer that was printed out and traced onto animation cels onto which the colors and characters were added. The two-minute climax scene thus used computer-generated imagery (CGI), making it the first Disney film to extensively use computer animation, a fact that Disney used to promote the film during marketing.The film was the last work to feature Eric Larson as an animation consultant before his retirement. Larson was the last of Disney's Nine Old Men, the group that had defined much of Disney's theatrical direction since the 1930s. The character of Dr. Dawson was modeled on Larson as a tribute.

Music
Unusually for a Disney animated feature, there was no soundtrack album released alongside the film; it was released in 1992 alongside the film's reissue under its new title by Varèse Sarabande, the only Disney cartoon to have an original soundtrack on the label to date (and the only one not to be issued under a Walt Disney imprint). The album marked the debut of Henry Mancini for score composition of an animated feature aside from the animated opening for The Pink Panther.Initially, Mancini composed a song titled ""Are You the One Who Loves Me?"" to serve as a parody of a Victorian British music hall. Already in rough animation, the song was recorded by Shani Wallis. However, Katzenberg and the new management desired a more contemporary song as they would help make the film more marketable. Michael Jackson was considered by Eisner to voice a character who would enter the saloon, confront Basil, and sing a song at the tavern, but the suggestion was met with uncomfortable silence for which Eisner withdrew the idea; Eisner later proposed for Madonna to perform the song. Eventually, Melissa Manchester was brought in; she wrote and performed ""Let Me Be Good to You"", by which the rough animation had to be re-timed and often re-animated to properly sync with the song. Mancini also co-wrote two of the film's three original songs, ""The World's Greatest Criminal Mind"" and ""Goodbye So Soon"" (both performed by Vincent Price).

Songs
Original songs performed in the film include:

Release
During the film's initial theatrical release, the film was accompanied with the short, Clock Cleaners.

Home media
Following the theatrical re-release in February 1992, the film was released on VHS and Laserdisc in July 1992 as part of the Walt Disney Classics series. It was placed into moratorium on April 30, 1993. It was released again on VHS on August 3, 1999 (with a game sheet inside it as part of a contest) and on DVD in 2002 with a short making-of featurette. In the United Kingdom, it was first released on VHS in 1992 followed by re-releases in 1993 and 1995.
A ""Mystery in the Mist Edition"" of The Great Mouse Detective was released on DVD on April 13, 2010, and on Blu-ray Disc on October 9, 2012. Unlike previous home media releases, which all used the 1992 reissue title print (The Adventures of the Great Mouse Detective), this DVD restored the original 1986 title card, which had previously not been seen since the original 1986 release. The DVD also has the film in its 1.78:1 widescreen aspect ratio, which brings it closer to its original theatrical aspect ratio. The Blu-ray edition is region-free and thus can be played in any region of the world. The Blu-ray was finally released in the UK on November 9, 2015, and released in France on Blu-ray on October 20, 2015.

Reception
Critical reaction
On their syndicated television show, At the Movies, the film received a ""two thumbs up"" rating from critics Gene Siskel and Roger Ebert. In his print review for The Chicago Tribune, Siskel enthusiastically praised the film as the most ""truly memorable animated feature in 25 years"" that ""travels a wide emotional range, taking us from cuddly to scary, from recognition to wonder."" Likewise, in his print review for the Chicago Sun-Times, Ebert gave the film three stars out of four in which he praised the film's animation and compared the film to that of Disney's golden age. He summarized that ""the result is a movie like The Great Mouse Detective, which looks more fully animated than anything in some 30 years.""London's Time Out magazine wrote, ""As usual with film noir [...] it is the villain who steals the heart and one is rooting for in the breathtaking showdown high up in the cogs and ratchets of Big Ben."" Nina Darnton of The New York Times applauded that ""[t]he heroes are appealing, the villains have that special Disney flair – humorous blackguards who really enjoy being evil – and the script is witty and not overly sentimental."" Johanna Steinmetz, also from The Chicago Tribune, graded the film three-and-a-half stars (out of four) writing ""This movie is cute, cute, cute, but it's a higher grade of cute than The Rescuers (1977) and The Fox and the Hound (1981). The key to good Disney animation is character and facial expression, and Detective abounds in both."" Alex Stewart reviewed The Great Mouse Detective for White Dwarf #83, and stated that ""After their dismal fudge of The Black Cauldron, it's good to see the Disney studios taking a step, however cautious, towards the world of animation as it is today. The style is looser and more vigorous, and, in a climactic fight inside Big Ben, effectively amalgamates computer-drawn clockwork with hand-drawn characters.""The review aggregator website Rotten Tomatoes reported that the film received an 80% approval rating with an average rating of 7.1/10 based on 25 reviews. The website's consensus states that ""The Great Mouse Detective may not rank with Disney's classics, but it's an amiable, entertaining picture with some stylishly dark visuals."" Metacritic gave the film a score of 73 based on 13 reviews, indicating ""generally favorable reviews"".Animation critic Charles Solomon listed this as one of the best animated films of the 1980s while singling out Keane's key work on Ratigan.

Box-office
The film grossed around $50 million worldwide against a budget of over $14 million during its initial release. Its inexpensive success after its predecessor's under-performance gave the new management of Disney confidence in the viability of their animation department, though it was surpassed at the box office by An American Tail. Re-titled as The Adventures of the Great Mouse Detective, the film was re-released theatrically on February 14, 1992, where it grossed $13,288,756. The Great Mouse Detective has had a lifetime North American gross of $38.7 million across its original release and reissue.

Legacy
Basil and Professor Ratigan were characters to meet-and-greet at the Disney Parks, until both were retired after 2004.
In the television series Darkwing Duck, a little statue of Basil opened the secret passage to Darkwing's hidden base. Some of the characters from the film have recurring cameo appearances in the television series House of Mouse.
Professor Ratigan is one of the villains with a main focus in the anthology film Once Upon a Halloween. He is also one of the villains present in the board game Disney Villainous.Basil of Baker Street appears as a playable character in the video game Disney Heroes: Battle Mode.Additionally, in honor of Mickey Mouse's 75th anniversary, was planned a film under the title The Search for Mickey Mouse in which Mickey gets kidnapped by unknown forces, forcing Minnie Mouse to enlist Basil of Baker Street to investigate his disappearance. However, the project was cancelled after it suffered script problems.
Passage 4:
The One and Only (1999 film)
The One and Only (Danish: Den Eneste Ene) is a 1999 Danish romantic comedy film directed by Susanne Bier. The film starred Sidse Babett Knudsen, Niels Olsen, Rafael Edholm, and Paprika Steen in story about two unfaithful married couples faced with becoming first-time parents. The film was considered to mark a modern transition in Danish romantic comedies, and became the third biggest box-office success of the 1990s in Denmark. The film earned both the Robert Award and Bodil Award as the Best Film of 1999.

Cast
Sidse Babett Knudsen as Sus
Niels Olsen as Niller
Paprika Steen as Stella
Søs Egelind as Lizzie
Sofie Gråbøl as Mulle
Lars Kaalund as Knud
Rafael Edholm as Andrea aka Sonny
Hella Joof as the adoption lady
Liv Corfixen as beauty clinic customer
Charlotte Munck as beauty clinic customer
Klaus Bondam as Priest
Lars Kjeldgaard as one of the couples with new kitchen
Jacob Thuesen as one of the couples with new kitchen
Vanessa Gouri as Mgala
Jan Hertz as the Doctor
Berrit Kvorning Beauty clinic chief

Box office
The film was the most successful Danish film for 15 years with admissions of 820,000 and a gross of 38.5 million Krone ($5.5 million) in Denmark.
Passage 5:
Brothers (2009 film)
Brothers is a 2009 American psychological drama war film directed by Jim Sheridan and written by David Benioff. A remake of the 2004 Danish film, it follows Captain Sam Cahill (portrayed by Tobey Maguire), a presumed-dead prisoner of the War in Afghanistan who deals with extreme PTSD while reintegrating into society following his release from captivity. The film also stars Jake Gyllenhaal as Cahill's brother and Natalie Portman as his wife. Both films take inspiration from Homer's epic poem the Odyssey.The film received a mixed response and grossed $43 million. Maguire, however, received particular praise for his performance, receiving a Golden Globe nomination for Best Actor in a Motion Picture Drama.

Plot
United States Marine Corps officer Sam Cahill is about to be sent to war in Afghanistan. He is married to his high school friend Grace, and together, they have two young daughters, Isabelle and Maggie. Sam's older brother, Tommy, is a criminal who is just released from prison on parole a few days before Sam's departure. At a family dinner with Tommy and Sam's parents, Hank and Elsie, Maggie reveals to Tommy that Grace dislikes him, and Hank insults Tommy for his lack of success compared to Sam.
During Sam's tour, his helicopter is shot down in and he and Private Joe Willis are the sole survivors. They are taken prisoner by the Taliban, but are declared killed in action by the government. At Sam's funeral in absentia, Hank attempts to drive Elsie and the girls home while drunk, but Tommy intercepts him. Hank once again berates Tommy and he accuses Hank for influencing Sam to join the Marines because of his own Vietnam War service.
Tommy attempts to redeem himself and completes a kitchen remodel for Grace. Hank and Tommy also mend their relationship, and Grace bonds with Tommy, aided by his growing paternal connection with Isabelle and Maggie. Grace and Tommy share a fireside kiss, but do not take their attraction any further. However, Tommy continues to remain close with the family and his nieces grow attached to him. Meanwhile, Sam and Joe are tortured by their captors and Sam is eventually forced to brutally beat Joe to death.
Sometime later, Sam is rescued and returns home, where he struggles to readjust, showing signs of severe post-traumatic stress disorder; his daughters grow fearful and resentful toward him. Sam also lies to Joe's widow about her husband's death. His paranoia also causes him to believe Grace and Tommy fell in love while he went away, causing him to remain aloof. When Grace questions Sam about what happened in Afghanistan, Sam demands to know what happened between her and Tommy. Grace tells Sam that she and Tommy kissed while he was gone, but Sam has trouble believing that is all what happened. When Grace questions Sam again, Sam does not respond and storms off.
During Maggie's birthday party, Isabelle is rude to her sister and jealously complains that Maggie got what she wanted for her birthday, while Isabelle did not. Isabelle chastises Sam for being in Afghanistan during her birthday, but being able to attend Maggie's. Isabelle then begins to play with a balloon loudly, and, despite objections from Sam and Grace, triggers Sam's PTSD, causing him to lose his temper and burst the balloon out of fury in front of the family. Already scared and resentful of Sam's erratic behavior and violent mood swings, a hurt Isabelle falsely claims that Tommy and Grace are having an affair and angrily tells Sam that she wishes he had actually died.
Returning home, Sam, believing his daughter's story, throws a temper tantrum and proceeds to destroy the kitchen out of rage with a crowbar in front of Grace. He suddenly calms down when Tommy shows up and attempts to bring Sam into an embrace, trying to calm his brother's violent breakdown, while Grace isolates her daughters from their father. However, Sam immediately becomes defensive when he realizes that Tommy has already called the police. DOD Police arrive and confront Sam, leading into a standoff. He fires a pistol into the air and demands that the policemen kill him before holding the gun to his own head, contemplating suicide, but reluctantly changes his mind and surrenders after several protests from Tommy and a tearful Grace.
Sam is arrested and admitted to a Veterans' Affairs hospital. Grace visits, giving him an ultimatum that if he does not tell her the truth, Sam will lose her forever. Sam finally confesses that he killed Joe and they embrace, wondering if he will ever be able to live a normal life again.

Cast
Tobey Maguire as Capt. Sam Cahill
Jake Gyllenhaal as Tommy Cahill
Natalie Portman as Grace Cahill
Sam Shepard as Hank Cahill
Mare Winningham as Elsie Cahill
Bailee Madison as Isabelle Cahill
Taylor Geare as Maggie Cahill
Patrick Flueger as Pvt. Joe Willis
Carey Mulligan as Cassie Willis
Clifton Collins Jr. as Maj. Cavazos
Jenny Wade as Tina
Omid Abtahi as Yusuf
Navid Negahban as Murad
Enayat Delawary as Ahmed
Ethan Suplee as Sweeney
Arron Shiver as A. J.
Ray Prewitt as Owen

Reception
Box office
On its opening weekend, the film opened #3 with $9,527,848 behind New Moon and The Blind Side. Since its box office debut the film has grossed $43,318,349 worldwide.

Critical response
Brothers received mixed reviews from film critics. On the review aggregator Rotten Tomatoes, the film has an approval rating of 64% based on 159 reviews, with an average score of 6.2/10. The website's critical consensus reads, ""It plays more like a traditional melodrama than the Susanne Bier film that inspired it, but Jim Sheridan's Brothers benefits from rock-solid performances by its three leads."" On Metacritic, the film has a weighted average score of 58 out of 100, based on 31 reviews, indicating ""mixed or average reviews"".Tobey Maguire received critical acclaim for his dramatic performance; Roger Ebert gave the film three and a half stars and wrote that Brothers is ""Tobey Maguire's film to dominate, and I've never seen these dark depths in him before."" Claudia Puig of USA Today observed the resemblance between Maguire and Gyllenhaal, and praised their onscreen chemistry. Regarding Portman's performance, Puig opined that it was ""subdued and reactive"". Writing for New York magazine, David Edelstein praised the three main actors: ""Sheridan's actors work with their intellects fully engaged—and they engage us on levels we barely knew we had."" He also complimented the cinematography and Sheridan's ability to pull the reader into the plot. Entertainment Weekly's Owen Gleiberman gave the film a rating of C+, writing, ""Brothers isn't badly acted, but as directed by the increasingly impersonal Jim Sheridan, it’s lumbering and heavy-handed, a film that piles on overwrought dramatic twists until it begins to creak under the weight of its presumed significance.""

Accolades
Of his Golden Globe Award nomination, Tobey Maguire said ""I had no expectation about getting a nomination, but I was watching nonetheless. My wife and my son got really excited. I was sort of surprised — I was like, 'Oh, wow.' And I couldn't hear the latter part of my name."" The Edge of U2 described how the band planned to celebrate their nomination. ""I think we might have a pint of Guinness and eat a potato in honor of (director) Jim (Sheridan) and his great piece of work.""

Home media
Brothers was released on DVD and Blu-ray on March 23, 2010.

Opera adaptation
Brothers – The Opera is an opera based on the original 2004 Danish version of the film by Icelandic composer Daníel Bjarnason; it was premiered at the Musikhuset Aarhus on 16 August 2017. It was commissioned by Den Jyske Opera. Kerstin Perski wrote the libretto and the director was Kasper Holten. To celebrate Aarhus as the European Capital of Culture 2017, three stage works – a musical, dance, and an opera all based on films by Susanne Bier – were commissioned and performed in Musikhuset.
Passage 6:
A Boy and His Dog (1975 film)
A Boy and His Dog is a 1975 American black comedy science fiction film directed by actor L.Q. Jones, from a screenplay by Jones based on the 1969 novella of the same title by fantasy author Harlan Ellison. The film stars Don Johnson, Susanne Benton, Alvy Moore, and Jason Robards. It was independently produced and distributed by Jones' company LQ/Jaf Productions.
The film's storyline concerns a teenage boy, Vic, and his telepathic dog, Blood, who work together as a team in order to survive in the dangerous post-apocalyptic wasteland of the Southwestern United States.
Shout! Factory released the film on DVD and Blu-ray in August 2013.

Plot
In the post-nuclear war America of 2024, Vic (Don Johnson) is an 18-year-old boy, born in and scavenging throughout the wasteland of the former southwestern United States. Vic is most concerned with food and sex; having lost his parents, he has no formal education and does not understand ethics or morality. He is accompanied by a well-read, misanthropic, telepathic dog named Blood, who helps him find women to rape, in exchange for which Vic finds food for the dog. Blood cannot forage for himself due to the same genetic engineering that granted him telepathy. The two steal for a living, evading bands of raiders, berserk military androids, and mutants. Blood and Vic have an occasionally antagonistic relationship (Blood frequently annoys Vic by calling him ""Albert"" for reasons never made clear), though they realize that they need each other to survive. Blood wishes to find the legendary promised land of ""Over the Hill"" where above-ground utopias are said to exist, though Vic believes that they must make the best of what they have.
Searching a bunker for a woman for Vic to rape, they find one, but she has already been severely mutilated and is on the verge of death. Vic displays no pity. He is merely angered by the ""wastefulness"" of such an act, as well as disgusted by the thought of satisfying his urges with a woman in such a condition. They move on, only to find slavers excavating another bunker. Vic steals several cans of their food, later using them to barter for goods in a nearby shanty town.
That evening, while watching old vintage stag films at a local outdoor ""cinema"", Blood claims to smell a woman, and the pair track her to a large underground warehouse. There, Vic attempts to rape Quilla June Holmes (Susanne Benton), a scheming and seductive teenage girl from Downunder, a society in a large underground settlement. Unknown to the pair, Quilla June's father, Lou Craddock (Jason Robards), had sent her above ground to ""recruit"" surface dwellers. Blood takes an instant dislike to her, but Vic ignores him. After Vic saves Quilla June from raiders and mutants, they have repeated sex. Eventually, she secretly returns to her underground society. Enticed by the thought of more women and sex, Vic follows her, despite Blood's warnings. Blood remains on the surface at Downunder's portal.
Downunder has an artificial biosphere, complete with forests and a city, which is named Topeka after the ruins of the destroyed city that it lies beneath. The city is ruled by a triumvirate known as the Committee, who have shaped Topeka into a bizarre caricature of pre-nuclear war America, with all residents wearing whiteface and clothes evocative of rural United States prior to World War II. When Vic is told that he has been brought to Topeka to help fertilize the female population, he is elated to learn of his ""stud"" value. His joy is short-lived, when he is informed that Topeka meets its need for exogamous reproduction by electroejaculation and artificial insemination, which will deny him the sexual pleasure that he had envisioned. People who refuse to comply with the Committee are sent off to ""the farm"" and never seen again, as they are violently killed. Vic is informed that when his semen has been used to impregnate 35 women, he will also be sent to ""the farm"".
Quilla June helps Vic escape only because she wants him to kill the Committee members and destroy their android enforcer, Michael (Hal Baylor), so that she can usurp their power. Vic has no interest in politics or remaining underground. He only wants to return to Blood and the wasteland. The rebellion is quashed by Michael, who crushes the heads of Quilla June's co-conspirators before Vic disables him. She proclaims her ""love"" for Vic and wants to escape to the surface with him—now that her rebellion has been quashed, and the Committee has decreed that she will be sent to the farm.
On the surface, Vic and Quilla June discover that Blood is starving and near death. She pleads with Vic to abandon Blood, forcing him to face his true feelings. Vic decides that his loyalties lie with his dog. Off-camera, Vic murders Quilla June and cooks her flesh so that Blood can eat and survive. Blood thanks Vic for the food, and they both comment on Quilla June. Vic says that it was her fault that she followed him, while Blood wryly jokes that she had marvelous judgment but did not have particularly good ""taste"". The boy and his dog continue to talk as they walk off together into the wasteland.

Cast
Don Johnson as Vic
Tim McIntire as ""Blood"" (voice)
Susanne Benton as Quilla June Holmes
Jason Robards as Lou Craddock
Alvy Moore as Dr. Moore
Helene Winston as ""Mez"" Smith
Charles McGraw as Preacher
Hal Baylor as Michael
Ron Feinberg as Fellini
Michael Rupert as Gery
Don Carter as Ken
Michael Hershman as Richard
L.Q. Jones as Actor In Porno Film

Production
Harlan Ellison wrote the original novella A Boy and His Dog and began an adaptation for film. However, after encountering writer's block, actor/director L.Q. Jones came onboard to write the script. Jones' own company, LQ/Jaf Productions (L.Q. Jones & Friends), independently produced the film. Distributors initially were reluctant to finance the production, so Jones raised $400,000 through family and business associates. The film was shot at Pacific Ocean Park in Venice, California, and on location around Barstow, California and Coyote Dry Lake in the Mojave Desert.
In an interview, Harlan Ellison said: ""When he [Blood] calls Vic 'Al' or 'Albert', he is referring to the Albert Payson Terhune dog stories, whereas a traditional boy and his dog relationship is turned upside down in this movie.""
James Cagney's voice was considered as the voice of Blood, but was dropped because it would have been too recognizable and prove to be a distraction. Eventually, after going through approximately 600 auditions, they settled on Tim McIntire, a veteran voice actor who also did most of the music for the film. Ray Manzarek (misspelled in the film credits as ""Manzarec""), formerly of The Doors, was also among those credited for the score.
McIntire sang the main theme. Bolivian composer Jaime Mendoza-Nava provided the music for the Topeka underground segment.

Reception
On Rotten Tomatoes, the film has a 78% approval rating based on 36 reviews, with an average rating of 6.6/10. The site's consensus states: ""An offbeat, eccentric black comedy, A Boy and His Dog features strong dialogue and an oddball vision of the future"". On Metacritic the film has a score of 68% based on reviews from 10 critics, indicating ""generally favorable reviews"".Roger Ebert of the Chicago Sun-Times gave the film 2.5 stars out of a possible 4, writing that Ellison's novella ""seemed almost to defy filming"" but nonetheless Jones managed to offer ""a sort of wacky success"". Richard Eder of The New York Times wrote that the realistic world set up in the beginning and the underground community introduced later ""don't really work together; their contrast, and a ridiculous ending, shatter the picture. And the talking dog chews up the pieces"". Variety called the film ""a turkey"" and ""an amateurish blend of redneck humor, chaotic fight scenes, and dimwitted philosophizing"". Gene Siskel of the Chicago Tribune gave the film 1.5 stars out of 4 and wrote: ""Rather than illuminate the present through a glance at a possible future, 'A Boy and His Dog' is simply a dim-witted collection of tired sex gags and anti-American imagery"". Charles Champlin of the Los Angeles Times praised the film as ""an offbeat delight"" with performances that ""have that comfortable naturalness often detectable when an actor is directing other actors"". Gary Arnold of The Washington Post panned the film as a ""shoddy, puerile science-fiction parable"" that ""mistakes juvenile facetiousness for wit and glorifies a juvenile concept of freedom, which means making it in the wild, away from such unmanly encumbrances as civilization and girls"".The film was not commercially successful at its release. It has since become a cult film and also inspired the video game series Fallout ""on many levels, from underground communities of survivors to glowing mutants"", according to Jesse Heining, a developer of the game. On the film's DVD audio commentary, Jones states that Ellison was generally pleased with the film, with the exception of some lines of dialogue. Ellison particularly objected to the film's final line, which did not originate from his original short story, in which Blood said of Quilla, ""Well, I'd say she certainly had marvelous judgement, Albert, if not particularly good taste"". Ellison referred to it as a ""moronic, hateful chauvinist last line, which I despise"".The 1975 science fiction film directed by L.Q. Jones was controversial for alleged misogyny; the script included lines that were not in Ellison's original stories and that authors such as Joanna Russ, in her essay ""A Boy and his Dog: The final solution,"" found to be objectionable. Ellison did, however, accept that the ending remained popular with fans, saying: ""I would have kept the original last line from the original story, which I think is much more human and beguiling than the sort of punchline that L.Q. Jones used. But L.Q. knew what he was doing in terms of the market, I suppose."" On the other hand, Harlan also loved the movie (as stated in an interview conversation with L.Q. Jones on the Shout Factory Blu Ray); after Jones screened it to him, he said it was exactly what the story was supposed to be on screen. It was a few days after he brought up his problems, mostly concerning the way Blood talked about the girl during the locker room scene when they first meet.

Accolades
The film won the 1976 Hugo Award for Best Dramatic Presentation at MidAmeriCon, the 34th World Science Fiction Convention in Kansas City, Missouri, located not far from the real Topeka, Kansas. Johnson won the Golden Scroll for Best Actor, which was shared with James Caan for his performance in Rollerball. In 2007, it ranked #96 on Rotten Tomatoes' ""Journey Through Sci-Fi"" (100 best-reviewed science fiction films).

Legacy
According to L.Q. Jones, George Miller cited the 1975 film adaptation of A Boy and His Dog as an influence on the Mad Max films, particularly The Road Warrior (1981).

Sequel
There were rumors regarding a sequel, but it never materialized. On the film's DVD audio commentary, L.Q. Jones states that he had started to write a script sequel to the film that would have picked up where the first film ended and featured a female warrior named Spike, and we would have seen this world through the eyes of a female instead of a male (this happens in Ellison's story, Blood's a Rover, when Blood partners with Spike after the ostensible death of Vic). Jones and Ellison reportedly collaborated on this short-lived effort, although Ellison said that such 'collaboration' never went beyond a short ""what if?"" conversation, and that any efforts were solely that of Jones.
According to Cult Movies 2, Jones had a sequel planned called A Girl and Her Dog, but the plan was scrapped when Tiger, the dog who portrayed Blood, died. In a December 2003 interview, Jones claimed that he had been repeatedly approached to make a sequel, but funding was always an issue. In 2018, Ellison's teleplay featuring Spike — the girl in the proposed A Girl and Her Dog film — was finally published. Blood's a Rover by Harlan Ellison (Subterranean Press 2018), a ""fix-up"" novel, consisting of ""Eggsucker"" and ""Run Spot, Run"", two short stories from the 1970s and 1980s, as well as ""A Boy and His Dog"" (Ellison's novella) and an unproduced teleplay from the 1970s, ""Blood's a Rover"", was published in a limited number of hardcovers.

See also
List of American films of 1975
Passage 7:
Susanne Pollatschek
The Great Mouse Detective (released as Basil the Great Mouse Detective in some countries and as The Adventures of the Great Mouse Detective for its 1992 American re-release) is a 1986 American animated mystery adventure film produced by Walt Disney Feature Animation and released by Walt Disney Pictures. The 26th Disney animated feature film, the film was directed by John Musker, Ron Clements, Dave Michener, and Burny Mattinson (in their feature directorial debuts). The main characters are all mice and rats living in Victorian London.
Based on the children's book series Basil of Baker Street by Eve Titus and Paul Galdone, it draws heavily on the tradition of Sherlock Holmes with a heroic mouse who consciously emulates the detective. Titus named the main character after actor Basil Rathbone, who is best remembered for playing Holmes in film (and whose voice, sampled from a 1966 reading of ""The Red-Headed League"" was the voice of Holmes in this film, 19 years after his death). Sherlock Holmes also mentions ""Basil"" as one of his aliases in the Arthur Conan Doyle story ""The Adventure of Black Peter"".
The Great Mouse Detective was released to theaters on July 2, 1986, to positive reviews and financial success, in sharp contrast to the box office underperformance of Disney's previous animated feature film, The Black Cauldron (1985). Indeed, the film's timely success has been credited with keeping Walt Disney Animation a going concern after the previous film's failure by renewing upper management's confidence in the department, thus setting the stage for the Disney Renaissance when feature animated films would become the corporation's most lucrative and prestigious product.

Plot
In London in June 1897, a young mouse named Olivia Flaversham is celebrating her birthday with her single  father, toymaker Mr. Flaversham. Suddenly, a bat with a crippled wing and a peg leg bursts into the toyshop, kidnapping Flaversham. Olivia leaves to find Basil of Baker Street, the famous Great Mouse Detective, but gets lost. David Q. Dawson, a retired army surgeon mouse newly returned from Afghanistan, meets Olivia and escorts her to Basil's residence. Being busy already, Basil initially dismisses them. Olivia then mentions the bat that abducted her father, and Basil realizes that Olivia saw Fidget, the assistant of Professor Ratigan, the criminal mastermind whom Basil was working to catch. It is then revealed that Ratigan kidnapped Flaversham to create a clockwork robot replica of the Queen of the Mice, so that Ratigan can usurp her place as ""Supreme Ruler of all Mousedom"". Flaversham initially refuses to participate in the scheme, but capitulates when Ratigan threatens to harm Olivia.
Meanwhile, Fidget appears in Basil's window, then suddenly disappears. Basil, Dawson and Olivia take Toby, Sherlock Holmes' pet Basset Hound, to trail Fidget's scent. They trace Fidget to a human-sized toyshop; while searching the area, Dawson finds Fidget's checklist, and Basil discovers Fidget has been stealing clockwork mechanisms and toy soldiers' uniforms. 
Fidget ambushes and kidnaps Olivia before Basil and Dawson can stop him. Basil does some chemical tests to the checklist, discovering it came from the ""Rat Trap"", a tavern near the junction of the sewer and the Thames. Basil and Dawson disguise themselves as sailors and head to the tavern; they spot Fidget there, and follow him to Ratigan's headquarters, only to be ambushed by Ratigan and his henchmen. Ratigan has the pair tied to a spring-loaded mousetrap connected to a Rube Goldberg machine of various killing devices. Ratigan then sets out for Buckingham Palace, where his henchmen hijack the royal guards' roles and kidnap the Mouse Queen. Inspired by a remark Dawson made, Basil deduces the trap's weakness, freeing himself, Dawson and Olivia.
At Buckingham Palace, Ratigan forces Flaversham to operate the toy Queen, while the real one is taken to be fed to Felicia, Ratigan's pet cat. The toy Queen declares Ratigan the ruler of all Mousedom, and he announces his dictatorial plans for his new ""subjects"". After Basil, Dawson, and Olivia save Flaversham and the real Queen, they restrain Fidget and Ratigan's other henchmen, while Toby chases Felicia until she jumps over a wall, inadvertently into a pack of Royal Guard Dogs. Basil seizes control of the mechanical queen, making it denounce Ratigan as a fraud while breaking it into pieces. Realizing Ratigan's treason, the enraged crowd attacks, but Ratigan escapes on his dirigible with Fidget, holding Olivia hostage. Basil, Dawson, and Flaversham create an airship from a matchbox, balloons, and a Union Jack, and set off in pursuit. Ratigan tosses Fidget overboard to lighten the load; however, Basil jumps onto the dirigible to confront Ratigan, causing it to crash straight into Big Ben's clockface. Inside the clocktower, Basil restrains Ratigan, rescues Olivia, and safely delivers her to Flaversham. Ratigan breaks free and attacks Basil; however, when the clock strikes 10:00, the vibrations cause Ratigan to fall to his death. He attempts to take Basil with him, but Basil grabs a part of Ratigan's dirigible and saves himself. 
Back at Baker Street, the group recounts their adventures. The Flavershams depart for home, and Dawson reluctantly resolves to leave as well. A new client arrives, and Basil introduces Dawson to her as his friend and investigative partner, prompting Dawson to remain and assist in Basil's future cases.

Voice cast
Barrie Ingham as Basil, a brilliant mouse detective.
Ingham also voices Bartholomew, a drunken lackey of Ratigan's.
Vincent Price as Professor Ratigan, Basil's long-established arch-enemy.
Val Bettin as Major Dr. David Q. Dawson, previously of the Queen's 66th Regiment in Afghanistan. He eventually becomes Basil's associate, friend, and personal biographer. Dawson also serves as the film's narrator.
Bettin also voices one of Ratigan's thugs.
Susanne Pollatschek as Olivia Flaversham, a young Scottish mouse who seeks Basil's help in finding her father.
Candy Candido as Fidget, Ratigan's bumbling bat right-hand henchman who has a crippled wing and a peg leg. As a result of his crippled wing, he cannot fly.
Candido also voices a reprobate in the pub.
Alan Young as Mr. Flaversham, Olivia's affectionate Scottish father who owns a toy shop.
Diana Chesney as Mrs. Judson, Basil's housekeeper who is often exasperated by his antics.
Eve Brenner as Queen Mousetoria, the mouse Queen of the United Kingdom, whom Ratigan attempts to depose.
Basil Rathbone as Sherlock Holmes, the famous human detective who lives above Basil. His voice is taken from the 1966 Caedmon Records recording of the Sherlock Holmes story ""The Red-Headed League"".
Laurie Main as Dr. Watson, the medical associate/partner of Sherlock Holmes, who also lives above Basil. Unlike Rathbone, voice samples of Nigel Bruce were not used for the voice of Watson as he had died in 1953.
Wayne Allwine, Tony Anselmo, and Walker Edmiston as Ratigan's Thugs.
Melissa Manchester as Miss Kitty Mouse, who sings ""Let Me Be Good To You"".

Production
The idea of doing an animated film about Sherlock Holmes with animals was first discussed during the production of The Rescuers. Veteran layout artist Joe Hale is credited with suggesting to adapt the children's book series Basil of Baker Street by Eve Titus, but the project fell into development limbo because of the similarities to The Rescuers. In 1982, Ron Clements proposed adapting the children's book series into an animated feature and, along with story artist Pete Young, it was pitched to Disney President Ron Miller who approved the project. Earlier in his career, Clements created a 15-minute Sherlock Holmes animated short recorded on Super 8 film. Because the animators were displeased with the direction The Black Cauldron was heading, Basil of Baker Street was approved as an alternative project.Burny Mattinson and John Musker were assigned as the original directors while Dave Michener was also added as co-director. Miller became the producer for the film. The first idea for the victim was for Olivia—then an older and potential love interest whom Dawson falls for, but Miller suggested the character be ""a little girl, someone they [the audience] can feel sorry for."" One of the dropped characters was a stool pigeon who always hung around Buckingham Palace and tipped Basil off about the skullduggery. The writers dropped the characters deciding for Basil to figure it out for himself.With the departure of Miller in 1984, the board of directors appointed Michael Eisner, who had resigned from Paramount Pictures, to become the new CEO. Eisner recruited former production head Jeffrey Katzenberg to become studio chairman over Disney's film division. Following a story reel screening of Basil, Eisner and Katzenberg complained about the slow pacing of the story and ordered for rewrites before animation would commence. Although the intended release was set for Christmas 1987, Michael Eisner slashed the projected production budget at $24 million in half where it was green-lit at $10 million and moved the release date up to July 1986 giving the production team one year to complete the film. To replace Miller who had been producer, Feature Animation chairman Roy E. Disney assigned Mattinson to serve as director/producer, but finding both tasks much too laborious, Mattinson decided to remain as producer. Musker and Michener remained as directors, but with the shortened production schedule, Clements became an additional director.Following the box office under-performance of the 1985 Paramount/Amblin film Young Sherlock Holmes, Eisner decided to rename Basil of Baker Street into The Great Mouse Detective feeling the name ""Basil"" was ""too English"". The re-titling of the film proved to be unpopular with the filmmakers so much that animator Ed Gombert wrote a satirical interoffice memo, allegedly by studio executive Peter Schneider, which gave preceding Disney films generic titles such as Seven Little Men Help a Girl, The Wonderful Elephant Who Could Really Fly, The Little Deer Who Grew Up, The Girl with the See-through Shoes, Two Dogs Fall in Love, Puppies Taken Away, and A Boy, a Bear and a Big Black Cat. These generic titles would later become a category on Jeopardy!.

Casting
Following a succession of American and British actors who read for the part of Basil, Royal Shakespeare actor Barrie Ingham won the role within six minutes of his audition. Val Bettin was co-director Ron Clements's first choice for Dawson. For Olivia, Susanne Pollatschek was selected over hundreds of other applicants while Alan Young, who had voiced Scrooge McDuck for Mickey's Christmas Carol, was selected to voice her father Hiriam because of his authentic Scottish brogue.When the filmmakers watched the 1950 comedy film Champagne for Caesar to study Ronald Colman's performance as a possible model for Basil, they immediately decided to cast Vincent Price, who also starred in the film, as Ratigan. A veteran actor for fifty-two years, Price was willing to do an audition commenting ""If anybody but Disney had asked me, I would have been offended."" Following a voice test, veteran voice artist Candy Candido recorded his dialogue for Fidget in one hour. To heighten the pitch, the tape recording of his voice was sped up. Candido's natural voice was kept for one character shouting ""Get off, you eight-legged bum.""

Animation
Basil was first modeled on Bing Crosby, but the animators eventually took inspiration from Leslie Howard. Initially, Ratigan had been designed as thin, weasely, and ratlike. Following the screening of Champagne for Caesar, Glen Keane noted that following the casting of Price, ""his expressive voice and attitude inspired us to further redesign the character."" Additionally, during one story meeting, Glen Keane decided to base the stature of Ratigan on then-Disney CEO Ron Miller, who was a 6'6"" former football player for the Los Angeles Rams. Furthermore, Keane lifted his personality as he was thumbing through these ""photographs of people of London in the 1800s, of railroad men, and there was this one guy smoking a cigar—he had a top hat and there was just something about this guy—this Ratigan ... this rat sucking the cigar, completely dressed to the hilt, he was sharp and perfect—he's a sewer rat dressed like a king and he lives as a king!"" The following supervising animators included Mark Henn for Basil, Hendel Butoy for Dawson, Rob Minkoff for Olivia, Andreas Deja for Queen Moustoria, Ruben Aquino for Mrs. Judson, and Mike Gabriel for Toby and Felicia.The original finale was to take place on the hands of Big Ben with Ratigan eventually falling to his demise. However, layout artist Mike Peraza approached Musker with the idea of restaging the final confrontation so the characters would break through the face of Big Ben with the grinding clockwork gears providing added menace, in which Musker agreed. Peraza's inspiration for the scene was a Japanese anime film, The Castle of Cagliostro (1979), the feature film debut of animator Hayao Miyazaki which is part of the Lupin III franchise; The Castle of Cagliostro, which Peraza was a fan of, featured a climactic scene involving characters amidst giant turning gears in a clock tower. Pereza and his team was sent to London for video reference and were granted unprecedented access to the clockworks inside Big Ben. Because the bells would chime at every quarter-hour, the team completed their research in one hour.Back at the Feature Animation building, animators Phil Nibbelink and Tad Gielow spent months designing the interior of Big Ben, with each gear produced as wire-frame graphics on a computer that was printed out and traced onto animation cels onto which the colors and characters were added. The two-minute climax scene thus used computer-generated imagery (CGI), making it the first Disney film to extensively use computer animation, a fact that Disney used to promote the film during marketing.The film was the last work to feature Eric Larson as an animation consultant before his retirement. Larson was the last of Disney's Nine Old Men, the group that had defined much of Disney's theatrical direction since the 1930s. The character of Dr. Dawson was modeled on Larson as a tribute.

Music
Unusually for a Disney animated feature, there was no soundtrack album released alongside the film; it was released in 1992 alongside the film's reissue under its new title by Varèse Sarabande, the only Disney cartoon to have an original soundtrack on the label to date (and the only one not to be issued under a Walt Disney imprint). The album marked the debut of Henry Mancini for score composition of an animated feature aside from the animated opening for The Pink Panther.Initially, Mancini composed a song titled ""Are You the One Who Loves Me?"" to serve as a parody of a Victorian British music hall. Already in rough animation, the song was recorded by Shani Wallis. However, Katzenberg and the new management desired a more contemporary song as they would help make the film more marketable. Michael Jackson was considered by Eisner to voice a character who would enter the saloon, confront Basil, and sing a song at the tavern, but the suggestion was met with uncomfortable silence for which Eisner withdrew the idea; Eisner later proposed for Madonna to perform the song. Eventually, Melissa Manchester was brought in; she wrote and performed ""Let Me Be Good to You"", by which the rough animation had to be re-timed and often re-animated to properly sync with the song. Mancini also co-wrote two of the film's three original songs, ""The World's Greatest Criminal Mind"" and ""Goodbye So Soon"" (both performed by Vincent Price).

Songs
Original songs performed in the film include:

Release
During the film's initial theatrical release, the film was accompanied with the short, Clock Cleaners.

Home media
Following the theatrical re-release in February 1992, the film was released on VHS and Laserdisc in July 1992 as part of the Walt Disney Classics series. It was placed into moratorium on April 30, 1993. It was released again on VHS on August 3, 1999 (with a game sheet inside it as part of a contest) and on DVD in 2002 with a short making-of featurette. In the United Kingdom, it was first released on VHS in 1992 followed by re-releases in 1993 and 1995.
A ""Mystery in the Mist Edition"" of The Great Mouse Detective was released on DVD on April 13, 2010, and on Blu-ray Disc on October 9, 2012. Unlike previous home media releases, which all used the 1992 reissue title print (The Adventures of the Great Mouse Detective), this DVD restored the original 1986 title card, which had previously not been seen since the original 1986 release. The DVD also has the film in its 1.78:1 widescreen aspect ratio, which brings it closer to its original theatrical aspect ratio. The Blu-ray edition is region-free and thus can be played in any region of the world. The Blu-ray was finally released in the UK on November 9, 2015, and released in France on Blu-ray on October 20, 2015.

Reception
Critical reaction
On their syndicated television show, At the Movies, the film received a ""two thumbs up"" rating from critics Gene Siskel and Roger Ebert. In his print review for The Chicago Tribune, Siskel enthusiastically praised the film as the most ""truly memorable animated feature in 25 years"" that ""travels a wide emotional range, taking us from cuddly to scary, from recognition to wonder."" Likewise, in his print review for the Chicago Sun-Times, Ebert gave the film three stars out of four in which he praised the film's animation and compared the film to that of Disney's golden age. He summarized that ""the result is a movie like The Great Mouse Detective, which looks more fully animated than anything in some 30 years.""London's Time Out magazine wrote, ""As usual with film noir [...] it is the villain who steals the heart and one is rooting for in the breathtaking showdown high up in the cogs and ratchets of Big Ben."" Nina Darnton of The New York Times applauded that ""[t]he heroes are appealing, the villains have that special Disney flair – humorous blackguards who really enjoy being evil – and the script is witty and not overly sentimental."" Johanna Steinmetz, also from The Chicago Tribune, graded the film three-and-a-half stars (out of four) writing ""This movie is cute, cute, cute, but it's a higher grade of cute than The Rescuers (1977) and The Fox and the Hound (1981). The key to good Disney animation is character and facial expression, and Detective abounds in both."" Alex Stewart reviewed The Great Mouse Detective for White Dwarf #83, and stated that ""After their dismal fudge of The Black Cauldron, it's good to see the Disney studios taking a step, however cautious, towards the world of animation as it is today. The style is looser and more vigorous, and, in a climactic fight inside Big Ben, effectively amalgamates computer-drawn clockwork with hand-drawn characters.""The review aggregator website Rotten Tomatoes reported that the film received an 80% approval rating with an average rating of 7.1/10 based on 25 reviews. The website's consensus states that ""The Great Mouse Detective may not rank with Disney's classics, but it's an amiable, entertaining picture with some stylishly dark visuals."" Metacritic gave the film a score of 73 based on 13 reviews, indicating ""generally favorable reviews"".Animation critic Charles Solomon listed this as one of the best animated films of the 1980s while singling out Keane's key work on Ratigan.

Box-office
The film grossed around $50 million worldwide against a budget of over $14 million during its initial release. Its inexpensive success after its predecessor's under-performance gave the new management of Disney confidence in the viability of their animation department, though it was surpassed at the box office by An American Tail. Re-titled as The Adventures of the Great Mouse Detective, the film was re-released theatrically on February 14, 1992, where it grossed $13,288,756. The Great Mouse Detective has had a lifetime North American gross of $38.7 million across its original release and reissue.

Legacy
Basil and Professor Ratigan were characters to meet-and-greet at the Disney Parks, until both were retired after 2004.
In the television series Darkwing Duck, a little statue of Basil opened the secret passage to Darkwing's hidden base. Some of the characters from the film have recurring cameo appearances in the television series House of Mouse.
Professor Ratigan is one of the villains with a main focus in the anthology film Once Upon a Halloween. He is also one of the villains present in the board game Disney Villainous.Basil of Baker Street appears as a playable character in the video game Disney Heroes: Battle Mode.Additionally, in honor of Mickey Mouse's 75th anniversary, was planned a film under the title The Search for Mickey Mouse in which Mickey gets kidnapped by unknown forces, forcing Minnie Mouse to enlist Basil of Baker Street to investigate his disappearance. However, the project was cancelled after it suffered script problems.","['Burny Mattinson, David Michener, and the team of John Musker and Ron Clements']",11970,hotpotqa,en,,e54d1e07a1fc2ce2afdf7e75f79561d8fe0abd22f4d8bd53,"Burny Mattinson, David Michener, and the team of John Musker and Ron Clements",77
For what type of work is the production company for The Year Without a Santa Claus best known?,"Passage 1:
Santa's Workshop (Colorado)
Santa's Workshop is an amusement park that opened on June 16, 1956 in Cascade, Colorado, located on U.S. Route 24 just west of Colorado Springs at the entrance to the Pikes Peak Highway, at the Northern end of Pikes Peak. Modeled after the Santa's Workshop in Wilmington, New York, the park features a charming North Pole village complete with a variety of shops selling toys, candy, and Christmas decorations. The village is also home to Santa's Workshop itself, where children (and adults) can meet with Santa Claus and Mrs. Claus year round. Much of the staff is dressed in Christmas themed attire, especially those at work in stores and admissions.
In addition to the village, Santa's Workshop is a fully operational amusement park best suited for children ages 2 to 12. It is home to 28 rides, many of which are classified specifically as ""kiddie"" rides. Family highlights include a small roller coaster, the highest altitude Ferris wheel in North America, a Giant Slide (Helter skelter), as well as a North Pole made of permanent ice in the center of the park.  Attractions also include a Tilt-A-Whirl, a Scrambler, a chairlift, a narrow gauge railroad, a magic show, and an arcade. In 2014, Santa's Sleigh, a 30 mph, 2-person zip line was added.
The park is generally open from mid-May through Christmas Eve, and is closed from January to May. As of 2018, admission is $24.00 per person for ages 3–59. Under 3 and 60 or older are free. Military and group rates are available. The park is still owned and operated by the Haggard family who opened it in 1956.

External links
Santa's Workshop (Colorado amusement park) at the Roller Coaster DataBase
Passage 2:
Larry Wilson (screenwriter)
Larry Wilson (born January 23, 1948) is an American film producer and screenwriter. He is best known for his screenwriting work on the films Beetlejuice (1988) and The Addams Family (1991). He also co-wrote the films The Little Vampire (2000) and, for television, The Year Without a Santa Claus (2006). He wrote and directed a number of episodes of the Tales from the Crypt television series from 1991 to 1996.
Passage 3:
Rankin/Bass Productions
Rankin/Bass Animated Entertainment (founded and formerly known as Videocraft International, Ltd. and Rankin/Bass Productions, Inc.) was an American production company located in New York City, and known for its seasonal television specials, usually done in stop motion animation. Rankin/Bass' stop-motion productions are recognizable by their visual style of doll-like characters with spheroid body parts and ubiquitous powdery snow using an animation technique called ""Animagic"".
Nearly all of the studio's animation was outsourced to Japanese animation companies such as Toei Doga Entertainment, MOM Production, Mushi Productions and Topcraft. Rankin/Bass was one of the first western studios to outsource their low-budget animated television and film productions to animation studios in foreign countries; the others that already practiced animation outsourcing includes Total Television and King Features Syndicate TV in New York City; and Jay Ward Productions and Hanna-Barbera Productions in Los Angeles, California.

History
The company was founded in New York City by Arthur Rankin Jr. and Jules Bass on September 14, 1960, as Videocraft International, Ltd. The majority of Rankin/Bass' work, including all of their ""Animagic"" stop-motion productions (which they were well known for), were created in Tokyo, Japan. Throughout the 1960s, the Animagic productions were headed by Japanese stop-motion animator Tadahito Mochinaga at his studio, MOM Production. He was credited for his supervision as ""Tad Mochinaga"".
Rankin/Bass' traditional animation output was done by several animation studios such as Toei Animation, Eiken (formerly known as TCJ, short for the Television Corporation of Japan), Mushi Production, and especially Topcraft, which was formed on February 1, 1972 by Toei animator Toru Hara (who was credited as an animation supervisor in some of Rankin/Bass' specials). While several of Topcraft's staff, including Hara and industry legends such as Hayao Miyazaki, would go on to form Studio Ghibli in the wake of Topcraft's death, others formed another studio: Pacific Animation Corporation, which continued working on Rankin/Bass' titles until the latter company shut down.
In addition to the ""name"" talent that provided the narration for the specials, Rankin/Bass had its own company of voice actors. For the studio's early work, this group was based in Toronto, Ontario where recording was supervised by veteran CBC announcer Bernard Cowan. The Canadian group included actors such as Paul Soles, Larry D. Mann, and Carl Banas.
Maury Laws served as musical director for almost all of the animated films and television programs. Romeo Muller was another consistent contributor, serving as screenwriter for many of Rankin/Bass' best-known productions including Rudolph the Red-Nosed Reindeer (1964), The Little Drummer Boy (1968), and Frosty the Snowman (1969).

Output
One of Videocraft's first projects was an independently produced television series in 1960, The New Adventures of Pinocchio, based on the Italian author Carlo Collodi's 1883 novel The Adventures of Pinocchio and featuring ""Animagic"", a stop motion animation process using figurines or puppets (a process already pioneered by George Pal's ""Puppetoons"" and Art Clokey's Gumby and Davey and Goliath), managed by Mochinaga and his MOM Production staffers for Videocraft with Dentsu. This was followed by another independently produced series in 1961, Tales of the Wizard of Oz, Videocraft's adaptation of the 1900 novel The Wonderful Wizard of Oz by L. Frank Baum, as well as their first production to use traditional cel animation. Unlike many of Rankin/Bass' works, Tales of the Wizard of Oz was animated by Crawley Films in Ottawa, headed by F. R. Crawley.

Rudolph era
One of the mainstays of the business was holiday-themed animated specials for airing on American television. In 1964, the company produced a special for NBC and sponsor General Electric, later owner of NBC. It was a stop motion animated adaptation of Robert L. May's 1939 story ""Rudolph the Red-Nosed Reindeer"" and the 1949 song it inspired, ""Rudolph the Red-Nosed Reindeer"", written by May's brother-in-law, Johnny Marks. Almost two decades earlier, in 1948, it had been made into a cartoon by Max Fleischer, brother and former partner of Dave Fleischer, as a traditional cel animated short for the Jam Handy Film Company.  
With the American actor Burl Ives in the role of Sam the snowman, the narrator, Canadian actress Billie Mae Richards as the voice of the main title character, Rudolph, and an original orchestral score composed by Marks himself, Rudolph became one of the most popular, and longest-running Christmas specials in television history: it remained with NBC until around 1972 when it moved to CBS. In 2019, for its 55th anniversary, the special was also aired on Freeform as part of its ""25 Days of Christmas"" franchise, although it will continue to air on CBS under a separate license with Universal.The special contained seven original songs. In 1965, a new song was filmed in ""Animagic"" to replace ""We're a Couple of Misfits"", titled ""Fame and Fortune"".
The success of Rudolph led to numerous other Christmas specials. The first was The Cricket on the Hearth in 1967, with two live-action announcements by Danny Thomas, continuity and character designs by Don Duga and Paul Coker, and animation by Jiro Yanase's TCJ, followed by the 1968 Thanksgiving special The Mouse on the Mayflower, told by Tennessee Ernie Ford and animated by Kenzo Masaoka, Sanae Yamamoto, and Yasuji Murata's Toei Animation. Paul Coker Jr. would go on to design characters and production for more than 40 Rankin-Bass specials and episodes.

Other holiday specials
Many of their other specials, like Rudolph, were based on popular Christmas songs. In 1968, the British-American actress Greer Garson provided dramatic narration for The Little Drummer Boy, based on the traditional song and set during the birth of the baby Jesus Christ, and starring the Puerto Rican actor José Ferrer as the voice of Ben Haramed. During that year, Videocraft International, Ltd. (whose logo dominated the Rankin/Bass logo in the closing credit sequences) changed its name to Rankin/Bass Productions, Inc., and adopted a new logo, retaining a Videocraft byline in their closing credits until 1971 when Tomorrow Entertainment, a unit of the General Electric Company, acquired the production company. The ""Animagic"" process for The Little Drummer Boy took place at MOM Production, which was renamed Video Tokyo Production after Tadahito Mochinaga left Japan for his return trip to China following the completion of the animation for Mad Monster Party?, thus ending his collaboration with Rankin/Bass. Takeo Nakamura, the director of Sanrio's 1979 stop motion feature Nutcracker Fantasy, was among the ""Animagic"" team, but he was never credited as a supervisor.
The following year, in 1969, Jimmy Durante sang and told the story of Frosty the Snowman, with Jackie Vernon voicing Frosty. It was based on Steve Nelson and Jack Rollins' 1950 song of the same name, and also introduced Billy De Wolfe as the voice of Professor Hinkle, a greedy magician who tries to steal away the magic hat that brought Frosty to life to become a billionaire. Mushi Production, an animation studio founded in 1961 and formerly led by the manga artist Osamu Tezuka (creator of Astro Boy, Kimba the White Lion and Ambassador Magma), handled the animation for the special with supervision by Yusaku ""Steve"" Nakagawa, a layout artist and character designer from Hanna-Barbera Productions in Los Angeles, California.
The year 1970 brought another Christmas special, Santa Claus Is Comin' to Town. Rankin/Bass enlisted Fred Astaire as narrator S.D. (Special Delivery) Kluger, a mailman answering children's questions about Santa Claus and telling his origin story. The story involved young Kris Kringle, voiced by Mickey Rooney, and the villainous Burgermeister Meisterburger, voiced by Paul Frees. Kringle later marries the town's schoolteacher, Miss Jessica, voiced by Robie Lester. Kizo Nagashima, the associate director of Rankin/Bass' previous productions, was credited as a production supervisor.
In 1971, Rankin/Bass produced their first Easter television special, Here Comes Peter Cottontail, with the voices of Danny Kaye as the narrator Seymour S. Sassafrass, Vincent Price as the evil rabbit January Q. Irontail, and Casey Kasem from Hanna-Barbera's Scooby-Doo franchise as the title character Peter Cottontail. It was not based upon the title song by Steve Nelson and Jack Rollins, but on a 1957 novel by Priscilla and Otto Friedrich titled The Easter Bunny That Overslept. This was the second and final ""Animagic"" production to be supervised by Kizo Nagashima. Steve Nakagawa was also involved in this special as a continuity designer. In 1977, Fred Astaire returned as S. D. Kluger in The Easter Bunny Is Comin' to Town, telling the tale of the Easter Bunny's origins. From there, Rankin/Bass used Masaki Iizuka as an associate producer, and Akikazu Kono as an ""Animagic"" supervisor. Back in 1973, Iizuka was the production assistant of Marco—a live-action musical film based on the biography of Italian merchant, explorer, and writer Marco Polo, filmed at Toho Company in Tokyo and on location throughout East Asia, and featuring Kono's ""Animagic"" sequence of the Tree People. Previously, he was met by Rankin during the animation production of the Halloween television special Mad, Mad, Mad Monsters at Mushi Production in 1972, and became an integral part of Rankin/Bass for many years.
In 1974, Rankin/Bass Productions was relaunched once again as an independent production company and produced another Christmas special for television, The Year Without a Santa Claus, featuring Shirley Booth, voicing narrator Mrs. Claus; Mickey Rooney, returning as the voice of Santa Claus; and supporting characters Snow Miser (voiced by Dick Shawn) and Heat Miser (voiced by George S. Irving). It was the first Rankin/Bass ""Animagic"" production on which Akikazu Kono and puppet maker Ichiro Komuro share in the production supervision. It was remade as a poorly received live-action/special effects TV movie shown on NBC in 2006 starring Delta Burke and John Goodman as Mrs. Claus and Santa.Throughout the 1970s, Rankin/Bass, with Video Tokyo and the former Toei Animation employee Toru Hara's Topcraft, continued to produce animated sequels to its classic specials, including the teaming of Rudolph and Frosty in 1979's Rudolph and Frosty's Christmas in July, with the voice of Ethel Merman as Lilly Loraine, the ringmistress of a seaside circus, and Rooney again returning as Santa. The special features cameos by characters from several other Rankin/Bass holiday specials, including Big Ben the Clockwork Whale from Rudolph's Shiny New Year and Jack Frost from Frosty's Winter Wonderland. Later that year, Jack appeared in his own special, Jack Frost. Narrated by Buddy Hackett, it tells the story of the winter sprite's love for a mortal woman menaced by the evil Cossack king, Kubla Kraus (Paul Frees, in addition to Kubla, voiced Jack Frost's overlord, Father Winter). In this special, Jack's voice was performed by Robert Morse, who previously voiced Stuffy in 1976's The First Easter Rabbit (loosely based on Margery Williams' The Velveteen Rabbit), and young Ebenezer Scrooge in 1978's The Stingiest Man in Town (based on Charles Dickens' A Christmas Carol).
Among Rankin/Bass' original specials was 1975's The First Christmas: The Story of the First Christmas Snow, featuring the voice of Angela Lansbury (who also starred in the 1982 adaptation of The Last Unicorn) as the narrating and singing nun, Sister Theresa, and Irving Berlin's Christmas classic ""White Christmas"".
Their final stop-motion style Christmas story was The Life and Adventures of Santa Claus, taken from the L. Frank Baum story of the same name and released in 1985. In this story, the Great Ak (voiced by Alfred Drake) summons a council of the Immortals to bestow upon a dying Claus (voiced by Earl Hammond, with J.D. Roth voicing the young Claus) the Mantle of Immortality. To make his case, the Great Ak tells Claus's life story, from his discovery as a foundling in the magical forest and his raising by Immortals, through his education by the Great Ak in the harsh realities of the human world, and his acceptance of his destiny to struggle to bring joy to children. This special has recently been released as part of Warner Bros. Home Entertainment's Warner Archive Collection, on a double-feature disc that also contains Nestor, the Long-Eared Christmas Donkey which is often paired with The First Christmas on holiday broadcasts.
Many of these specials are still shown seasonally on American television, and some have been released on VHS, Betamax, LaserDisc, DVD, Blu-ray, and Digital.

Non-holiday output
Throughout the 1960s, Videocraft produced other stop motion and traditional animation specials and films, some of which were non-holiday stories. 1965 saw the production of Rankin/Bass' first theatrical film, Willy McBean and His Magic Machine, another joint venture between Videocraft and Dentsu. 1966 brought The Daydreamer, the first of three films to be produced in association with executive producer Joseph E. Levine's Embassy Pictures in Los Angeles, California, and the film adaptation of the stories and characters by the Danish author Hans Christian Andersen, which combines live-action, special effects and ""Animagic""; and The Ballad of Smokey the Bear, the story of the famous forest fire-fighting bear seen in numerous public service announcements, narrated by James Cagney.The theatrical feature film Mad Monster Party? saw theatrical release in the spring of 1967, featuring one of the last performances by the British actor Boris Karloff. The film features affectionate send-ups of classic movie monsters and their locales, adding ""Beatle""-wigged skeletons as a send-up of the era's pop bands, and a writing staff borrowed from Mad magazine, including the cartoonist Jack Davis, who designed the characters of this film. It is also the last ""Animagic"" project that Tadahito Mochinaga supervised.
In 1972 and 1973, Rankin/Bass produced four animated TV movies for The ABC Saturday Superstar Movie series: Mad Mad Mad Monsters (with the animation by Mushi), Willie Mays and the Say-Hey Kid, The Red Baron, and That Girl in Wonderland (all featuring the animation by Topcraft).
In 1977, Rankin/Bass produced an animated version of J. R. R. Tolkien's The Hobbit. It was followed in 1980 by an animated version of The Return of the King (the animation rights to the first two volumes were held by Saul Zaentz, producer of Ralph Bakshi's animated adaptation The Lord of the Rings). Other books adapted include The Last Unicorn by Peter S. Beagle, a rare theatrical release that was co-produced with ITC Entertainment in London, England, Peter Dickinson's The Flight of Dragons and Kenneth Grahame's The Wind in the Willows which was animated by the second overseas animation unit of Hanna-Barbera, James Wang's Cuckoo's Nest Studios (now Wang Film Productions) in Taipei, Taiwan.
In addition to their prime time specials, Rankin/Bass produced several regular television shows in traditional animation, including The King Kong Show in 1966, co-produced with Toei Animation, The Tomfoolery Show in 1970, The Jackson 5ive in 1971 (the latter co-produced with Motown Productions), and Kid Power and The Osmonds in 1972. The most successful of these was Ted Wolf's ThunderCats in 1985, an action-adventure series based on his related line of toys. It was followed by two similar TV shows about humanoid animals, SilverHawks in 1986, and TigerSharks, as part of the series The Comic Strip in 1987. Each of those four series was mainly animated by former Topcraft employees' Pacific Animation Corporation, with production management by Masaki Iizuka, just before the studio was bought by Disney and renamed Walt Disney Animation Japan in 1988. Neither one enjoyed the same commercial success as ThunderCats did, however.
Rankin/Bass also attempted live-action productions, such as 1967's King Kong Escapes, a co-production with Toho; 1976's The Last Dinosaur; 1978's The Bermuda Depths; 1980's The Ivory Ape (all co-produced with Tsuburaya Productions, the creators of the Ultra Series); and 1983's The Sins of Dorian Gray. With the exception of King Kong Escapes, all were made-for-television films.

Demise
After its last series output, Rankin/Bass shut down its production company on March 4, 1987.
Arthur Rankin Jr. would split his time between New York City, where the company still has its offices, and his home in Bermuda. Rankin died at Harrington Sound, Bermuda on January 30, 2014, at the age of 89. Bass became a vegetarian; a decade later, he wrote Herb, the Vegetarian Dragon, the first children's book character developed specifically to explore moral issues related to vegetarianism. The original story and a follow-up cookbook became bestsellers for independent publishing house Barefoot Books. Bass died on October 25, 2022 at the age of 87.In 1999, Rankin/Bass joined forces with James G. Robinson's Morgan Creek Productions and Nest Family Entertainment (creators of The Swan Princess franchise) for the first and only animated adaptation of Rodgers and Hammerstein's musical The King and I, based on a treatment by Rankin. Distributed by Warner Bros. Pictures with its Warner Bros. Family Entertainment division, the film flopped at the American box office. Stephen Hunter, among several American film critics, criticized the film's depictions of ""offensive ethnic stereotyping.""In 2001, Fox aired the first new original Christmas special to be produced by both Rankin and Bass in 16 years, Santa, Baby!, which like most of their production company's other specials was based on a popular, similarly-titled Christmas song. Santa, Baby! stood out from its predecessors due to its use of African-American characters and voice performers, such as Patti LaBelle (the narrator), Eartha Kitt, Gregory Hines, Vanessa L. Williams and Tom Joyner. Although Pacific Animation Corporation was responsible for the overseas animation production of the special with the background art provided by Atelier BWCA and the See Throu Studio, some of the animation services were done at Steven Hahn's Hanho Heung-Up in Seoul, South Korea. Santa, Baby! turned out to be the final Rankin/Bass-produced special; the Rankin/Bass partnership was officially dissolved shortly after, with most of its remaining assets acquired by Warner Bros. Entertainment.
Currently, the pre-September 1974 Rankin/Bass library (including works from Videocraft International) is owned by NBCUniversal's Universal Pictures via DreamWorks Animation's DreamWorks Classics subsidiary, while Warner Bros. Discovery through Warner Bros. unit owns the rights to the post-September 1974 library via Telepictures. NBCUniversal also retained the rights to King Kong Escapes and also currently holds the rights to Willy McBean and his Magic Machine, again, via DreamWorks Classics. StudioCanal holds the rights to the films from Rankin/Bass that Embassy Pictures distributed, while ITV Studios currently holds the rights to The Last Unicorn. The rights to the 1999 animated film adaptation of The King and I are currently held by Morgan Creek Entertainment.

Legacy
For over 20 years, most of Rankin/Bass' films were shown on the Family Channel and Freeform during their December ""25 Days of Christmas"" seasonal period. Starting in 2018, the post-1974 specials moved to AMC and air during their ""Best Christmas Ever"" seasonal period, with Freeform retaining the pre-1974 specials' cable rights. The original Rudolph and Frosty specials currently air on CBS under a separate contract with Rankin/Bass and its successors-in-interest, with Santa Claus Is Comin' To Town airing on ABC.
The specials of Rankin/Bass have been parodied by the likes of TV series from Saturday Night Live to South Park, while non-holiday works like The Last Unicorn maintained a cult following. The look and style of the Christmas specials heavily influences more modern holiday classics such as Elf (2003).Beginning in 2013, and for several years thereafter, the animation studio ShadowMachine was hired by the SoCal Honda Dealers group (via Secret Weapon Marketing) to create stop-motion animated commercials in the style of Rankin/Bass's Christmas specials.RiffTrax, consisted of former Mystery Science Theater 3000 alumni Kevin Murphy, Bill Corbett and Michael J. Nelson, spoofed Nestor the Long-Eared Christmas Donkey on December 17, 2006 (this time with just Nelson himself riffing).In 2022, an agreement between Warner Bros. and NBCUniversal (which co-owned Studio Distribution Services, LLC) was made to release The Complete Rankin-Bass Christmas Collection as a nine-disc DVD box set with a 24-page booklet and special features. The box set features eighteen specials, comprising every stand-alone, Rankin-Bass produced Christmas special aside from Santa, Baby!.

Filmography
Franchises
Overseas animation studios used by Rankin/Bass
Japanese studios
MOM Production, Tokyo, Japan
Toei Animation, Tokyo, Japan
TCJ (Television Corporation of Japan) (now Eiken), Tokyo, Japan
Mushi Production, Tokyo, Japan
Topcraft, Tokyo, Japan
Pacific Animation Corporation, Tokyo, Japan
The Anime International Company, Tokyo, Japan
Anime R, Osaka, Japan
Mook Animation, Tokyo, Japan
Atelier BWCA (background studio), Tokyo, Japan
See Throu Studio (background studio), Tokyo, Japan

Other studios
Crawley Films, Ottawa, Ontario, Canada
Halas and Batchelor, London and Stroud, England, United Kingdom
Estudios Moro, Barcelona, Spain
Cuckoo's Nest Studios (Wang Film Productions), Taipei, Taiwan
Hong Ying Animation, Taipei, Taiwan
Hanho Heung-Up, Seoul, South Korea
Passage 4:
Santa Claus: The Movie
Santa Claus: The Movie is a 1985 Christmas film starring Dudley Moore, John Lithgow, and David Huddleston. It depicts the origin of Santa Claus (played by Huddleston), and his modern-day adventure to save one of his elves (Moore) who has been manipulated by an unscrupulous toy company executive (Lithgow). It was directed by Jeannot Szwarc and was the last major fantasy film produced by the Paris-based father-and-son production team of Alexander and Ilya Salkind.
Released in North America by TriStar Pictures on November 27, 1985, Santa Claus: The Movie was a box office bomb and received mostly negative reviews from critics.

Plot
In the Middle Ages, a woodcutter named Claus delivers hand-carved toys to the children of his village each Christmas, accompanied by his wife Anya and their reindeer Donner and Blitzen. Caught in a blizzard, they are saved by elves and taken to their magical workshop at the North Pole. Lead elf Dooley explains that their coming was prophesied; that it is Claus' destiny to deliver the toys made by the elves to the children of the world; and that they, like the elves, will live forever. The following Christmas Eve, the oldest elf dubs Claus ""Santa Claus"" and explains that the night will last as long as it takes for him to deliver toys to every child on Earth. Donner and Blitzen join six other reindeer and are fed hay sprinkled with magical powder that enables them to fly, pulling Santa's sleigh through the air. As the centuries pass, much of the mythology and traditions surrounding Santa Claus develop.
By the late 20th century Santa is exhausted by his ever-growing workload, and Anya suggests that he enlist an assistant. Two elves compete for the position: Puffy, who follows traditional toymaking methods, and Patch, who has many ideas for modernization. Patch wins by designing a machine to increase production through automation, but unbeknownst to him it begins to malfunction and produce shoddy toys. In New York City Santa befriends homeless orphan boy Joe and takes him for a ride in his sleigh. They unsuccessfully attempt the ""Super Duper Looper"", a vertical loop maneuver which always fails due to Donner's acrophobia. They also meet wealthy orphan girl Cornelia, who befriends Joe.
When the toys produced by Patch's machine fall apart on Christmas Day, he resigns in disgrace and leaves the North Pole, winding up in New York City. Meanwhile, the B.Z. Toy Company, run by Cornelia's unscrupulous step-uncle B.Z., is facing shutdown by the government for producing unsafe toys. Seeing the company's toys being pulled from a storefront, Patch mistakenly thinks they are very popular and approaches B.Z. about a job. Hoping to redeem himself in Santa's eyes, he creates lollipops laced with the magic powder that allows the reindeer to fly, and a flying car which he uses to deliver them to the world's children on Christmas Eve. The lollipops allow people to fly, making them an instant sensation and leaving Santa feeling obsolete and disheartened. B.Z. convinces Patch to strengthen the formula and put it in candy canes, planning to launch his own holiday called ""Christmas 2"" in late March.
Cornelia and Joe overhear B.Z. plotting to oust Santa as the figurehead of Christmas, and Joe is captured. Cornelia further overhears B.Z.'s assistant, Towzer, share his discovery that the candy canes explode when exposed to heat. B.Z. plans to take Towzer and their money and flee to Brazil, letting Patch take the fall for their dangerous product. Cornelia writes to Santa, who rushes to help despite two of his reindeer being sidelined by illness. Patch finds Joe tied up in the toy factory and frees him. Seeing a wood carving resembling Patch that Santa made for Joe, Patch realizes that Santa misses him. He and Joe take off for the North Pole in his flying car with the candy canes loaded in the trunk, unaware that they are becoming unstable. Santa and Cornelia pursue in Santa's sleigh; as the car explodes, they successfully perform the Super Duper Looper, saving Joe and Patch. The police, alerted by Cornelia, attempt to arrest B.Z., but he eats several of the magic candy canes and jumps out a window, only to float upward uncontrollably.
Santa agrees to let Joe and Cornelia stay at his workshop until next Christmas. As they celebrate with the elves, B.Z. floats off into space.

Cast
Dudley Moore as Patch
John Lithgow as B.Z.
David Huddleston as Claus/Santa Claus
Burgess Meredith as the Ancient Elf
Judy Cornwell as Anya
Jeffrey Kramer as Towzer
Christian L Fitzpatrick as Joe
Carrie Kei Heim as Cornelia
John Barrard as Dooley
Anthony O'Donnell as Puffy
Aimee Delamain as a storyteller in Claus' village
Dorothea Phillips as Miss Tucker, Cornelia's nanny
John Hallam as Grizzard, B.Z.'s chauffeur
Judith Morse as Miss Abruzzi, who works for B.Z.
Jerry Harte as a Senate Chairman
Ian Wise as Salvation Army Bandsman (Central Park Scene)Additional elves were played by Melvyn Hayes, Don Estelle, Tim Stern, Peter O'Farrell, and Christopher Ryan as Goober, Groot, Boog, Honka, Vout, and Goobler, respectively. Other minor roles were played by Paul Aspland, Sally Granfield, and Michael Drew as reporters; Walter Goodman as a street corner Santa; John Cassady as a wino; and Ronald Fernee and Michael Ross as policemen.

Production
Development
Conceived by Ilya Salkind in the wake of the apparently waning critical and U.S. box office success of 1983's Superman III and its immediate follow-up, 1984's Supergirl, Santa Claus: The Movie was directed by Supergirl director Jeannot Szwarc, from a story by David and Leslie Newman (though David Newman took sole screenplay credit). Pierre Spengler, Ilya's longtime partner and a longtime collaborator of the Salkinds', joined Ilya as the project's producer.
John Carpenter was originally offered the chance to direct, but also wanted a say in the writing, musical score, and final cut of the film. Carpenter's original choice for the role of Santa was Brian Dennehy. Szwarc, however, felt that he needed an actor with more warmth than Dennehy. Lewis Gilbert was another early choice for director but, despite initial interest, he could not agree with the Salkinds over certain aspects of the script. Robert Wise was also offered the chance to direct, but had a different approach to the story. Guy Hamilton, who'd had to withdraw from directing Superman: The Movie in 1976, lobbied hard for the chance to direct the film, but only on the condition that it be shot either in Los Angeles, Vancouver, or Rome. Ultimately, the Salkinds chose Szwarc because of their excellent working relationship on Supergirl.

Casting
Dudley Moore was the Salkinds' top choice to play the lead elf in the film, Ilya Salkind having remembered a scene in Arthur in which Liza Minnelli's character asks Moore if he is Santa's Little Helper. Moore was attached to the project early on, and had a say in both scripting and choice of director. David Newman's first script draft named Moore's character Ollie, but Moore decided that the name should be changed to Patch, which was the nickname of his young son, Patrick. Moore had briefly been considered to play the role of Mister Mxyzptlk in the Salkinds' aborted original script for Superman III, and for the role of Nigel in Supergirl. He turned down that role, but suggested his longtime friend and comic partner Peter Cook for the part.
Ilya Salkind wanted an American actor to portray Santa Claus because he felt that the film focused on a primary piece of Americana in much the same way that Superman: The Movie had. Szwarc screen-tested such actors as David White (who, being in his late 60s, was considered too old for the role) and Moore's Arthur co-star Barney Martin. For a while, Ilya Salkind actively pursued Carroll O'Connor for the role before Szwarc showed him David Huddleston's screen-test, which won Salkind over.
For the role of B.Z., the producers wanted a star with a similar stature to Gene Hackman when he had played Lex Luthor in Superman: The Movie. To this end, they offered the role to Harrison Ford who turned them down. They made offers to Dustin Hoffman, Burt Reynolds and Johnny Carson, each of whom also turned the part down. Eventually, John Lithgow was settled on after Ilya Salkind watched Terms of Endearment and realized that he had a Grinch-type look to him.
The role of the Ancient Elf was written with James Cagney in mind. Though he liked the film's overall idea, Cagney, at 84, turned the role down due to being too weakened by age to perform it. Fred Astaire was considered, but when this eventually came to nothing Dudley Moore suggested his friend Burgess Meredith for the role, which he in the end won. At the time of the film's announcement in mid-1983, the British Press carried reports that diminutive actors such as David Jason, Patrick Troughton and Norman Wisdom would be cast alongside Moore as fellow elves, but none of them were.

Filming
Santa Claus: The Movie was filmed in Buckinghamshire, England at Pinewood Studios, between August and November 1984. The film was photographed by Arthur Ibbetson, whose credits included Willy Wonka & the Chocolate Factory (1971). Santa Claus: The Movie was his final feature film. Serving as film editor was Peter Hollywood. The production was designed by Anthony Pratt, with costume design concepts by Bob Ringwood. The visual effects unit, as well as several of the production staff, were Salkind stalwarts from the Superman films: Derek Meddings, director of visual and miniature effects; Roy Field, optical visual effects supervisor; and David Lane, flying and second unit director.

Reception
Santa Claus: The Movie received negative reviews upon release, with a rating of 20% on Rotten Tomatoes, from the 20 reviews counted. Box Office Mojo lists the film's total United States box office gross as $23,717,291, less than its $30–50 million production budget. It was very popular in the UK, grossing £5,073,000.Chicago Sun-Times critic Roger Ebert noted some positive points to the film, writing that the film ""does an interesting job of visualizing Santa's workshop"" and Santa's elves. He also praised the film's special effects, particularly the New York City fly-over sequence involving Santa. Ebert also had some praise for Lithgow's ""nice, hateful performance"", but wrote that ""the villain is not drawn big enough."" He ceded that young children would probably like most of the film, but that older children and adults are ""likely to find a lot of it a little thin.""Vincent Canby of The New York Times was less positive than Ebert, calling the production ""elaborate and tacky"". He described the film as having ""the manner of a listless musical without any production numbers"". Unlike Ebert, he offered little praise for the film's production design. Canby quipped that ""Santa's workshop must be the world's largest purchaser of low-grade plywood"" and that the flyover sequences with Santa ""aren't great."" The only praise he had for the film's acting was for John Lithgow, who Canby wrote ""(gave) the film's only remotely stylish performance."" A more recent review by William Mager for the BBC echoed Canby and Ebert's comments.In his book Have Yourself a Movie Little Christmas, critic Alonso Duralde lists Santa Claus: The Movie in his chapter of worst Christmas films ever. His reasons include weak plot, garish production design, blatant product placement (particularly for McDonald's, though Coke and Pabst Blue Ribbon are also prominent), and scenery-chewing overacting on the part of Lithgow. Duralde ultimately concludes that the film is ""a train-wreck of a Christmas film that's so very wrong that you won't be able to tear yourself away from it.""John Lithgow, in a 2019 interview, said, ""It's just one of the tackiest movies I've ever been in. It seemed cheesy and it certainly never stuck...except in England. It is huge over there. I wish I had a nickel for every Englishman who's told me [it's their favorite film]. In England, that's half of what I'm known for.""

Soundtrack
The soundtrack score was composed and conducted by Henry Mancini, composer of the themes from The Pink Panther and Peter Gunn, with veteran lyricist and screenwriter Leslie Bricusse contributing five original songs. The song ""It's Christmas (All Over the World)"" was written by Bill House and John Hobbs with Freddie Mercury in mind. While it is known that Mercury recorded a demo for the House/Hobbs song at Pinewood Studios, he was never to make a full commitment to the project, as he and his Queen bandmates had already committed themselves to the Highlander soundtrack. In the end, Mercury turned down the project, stating that he felt that Queen had become overcrowded with requests to work on film soundtracks; as a result, Sheena Easton was ultimately chosen to record the tune. As mentioned on the DVD commentary of the film by Jeannot Szwarc, Paul McCartney was asked to compose songs for the film. It is unknown why he did not do so in the end.

Track listing""Main Title: Every Christmas Eve 1 and Santa's Theme (Giving)"" (Mancini/Bricusse)
""Arrival of the Elves"" (Mancini)
""Making Toys"" (Mancini/Bricusse)2
""Christmas Rhapsody: Deck the Halls/Joy to the World/Hark! The Herald Angels Sing/12 Days of Christmas/O Tannenbaum/The First Noel/Silent Night""
""It's Christmas Again"" (Mancini/Bricusse)2
""March of the Elves"" (Mancini)
""Patch, Natch!"" (Mancini/Bricusse) 3
""It's Christmas (All Over The World)"" (Bill House, John Hobbs)5
""Shouldn't Do That"" (Nick Beggs, Stuart Croxford, Neal Askew, Steve Askew) 4
""Sleigh Ride over Manhattan"" (Mancini)
""Sad Patch"" (Mancini)
""Patch Versus Santa"" (Mancini)
""Thank You, Santa"" (Mancini/Bricusse) 21Sung by Aled Jones2Performed by the Ambrosian Children's Choir. 3Performed by the Ambrosian Singers4Produced by Ken Scott and performed by Kaja5Produced by Keith Olsen for Pogologo Corporation, and performed by Sheena Easton.
The soundtrack was originally released on record and cassette by EMI America Records in 1985. Soon after, it went out of print and remained unavailable until 2009 when it was released on CD as a limited run of 1000 copies which sold out immediately upon release. This production suffered from several issues, most notably a master which had been subjected to heavy noise reduction resulting in a loss of sound quality. Additionally, the left & right channels had been erroneously flipped, a superficial re-edit had been performed on ""It's Christmas (All Over the World)"", and the song ""Shouldn't Do That"" by Kaja (Kajagoogoo) had been omitted due to licensing issues. In 2012 a deluxe three-disc set, including remastered tracks, outtakes and alternate versions and a 32-page booklet, was released.

Comic book adaptation
Marvel Comics published a comic book adaptation of the film by writer Sid Jacobson and artist Frank Springer in Marvel Super Special #39.

See also
Santa Claus in film
List of Christmas films
Passage 5:
Jonathan Meath
Jonathan Meath (born September 16, 1955) is an American television producer and director. He was senior producer of the television game show Where in the World is Carmen Sandiego? He also was a producer of The Wubbulous World of Dr. Seuss and the 1990s' remake of Zoom. In addition, he is notable for having a dual career as a professional Santa Claus. He made numerous appearances in various media as Santa, including on Good Morning America, at Radio City Music Hall with The Rockettes, on the cover of Boston Magazine, and on a Delta Air Lines' pre-flight safety demonstration. He was described by National Public Radio and Time as a ""top Santa"".

Television career
Meath attended Phillips Academy and graduated in 1974 with the school's first co–educational class. He graduated from New York University in 1979. During the 1980s Meath worked at CBS, Business Times, The Creative Establishment, MTV Networks and Greenwood Productions in various capacities. During 1996–1998, he produced shows for the Jim Henson Company called The Wubbulous World of Dr. Seuss. He produced for PBS 295 half-hour episodes of Where in the World is Carmen Sandiego?, as well as 80 episodes of Zoom.

Career
Meath, whose beard and hair went white early in life, noticed that children sometimes called him ""Santa"". He is slightly overweight – he has described himself as an ""organic Santa"" – and his wife bought him a red suit. He attended schools to learn the craft of being a Santa. He appeared in parades. He is a professional vocalist. In 2012, he appeared as Santa at Radio City Music Hall for the Christmas Spectacular show in New York City. Meath uses his real beard but conditions it with a ""shimmer-like shampoo known as Cowboy Magic, and uses hair gel for his mustache. His role as Santa was described in numerous publications. In 2009, he appeared in a thirty-second television commercial spot for the Boston Red Sox baseball team.

Personal life
Meath has one child and lives in Newburyport, Massachusetts. His mother was activist and historian Mary Stewart Hewitt. He is the great great grandson of businessman and sportsman John Malcolm Forbes and the great great great grandson of railroad industrialist John Murray Forbes. Through this, Meath is distantly related to John Kerry. Meath's daughter, Amelia Randall Meath, is a member of the bands Mountain Man and Sylvan Esso.

Awards
Passage 6:
Santa Claus Is a Black Man
""Santa Claus Is a Black Man"" is a Christmas song by record producer and songwriter Teddy Vann, performed by his daughter Akim Vann (billed as Akim) and his Teddy Vann Production Company for a 1973 single. The song, described as ""Vann's take on 'I Saw Mommy Kissing Santa Claus'"", has been called a cult classic, and continues to receive Christmas airplay.The elder Vann wrote the song as a Christmas gift for Akim, then five years old, who performs vocals on the recording. A soul song, the lyrics describe Akim happening upon her mother and Santa Claus dancing, noting Santa's curious resemblance to her father. The elder Vann was active in mentoring children, and intended the song to provide positive imagery and empowerment for young African-Americans; the song also makes reference to the Kwanzaa holiday, which the elder Vann was active in promoting in his native Brooklyn.Vann would later become better known for co-writing ""Power of Love/Love Power"" with Luther Vandross in 1991, for which he won a Grammy Award.Film director John Waters included the song on his 2004 Christmas music collection, A John Waters Christmas, despite the wishes of the elder Vann, who rejected Waters's request because the auteur ""is not considered mainstream"". Vann filed suit against Waters for using the song without permission.
Passage 7:
Santa Claus in film
Motion pictures featuring Santa Claus constitute their own subgenre of the Christmas film genre. Early films of Santa revolve around similar simple plots of Santa's Christmas Eve visit to children. In 1897, in a short film called Santa Claus Filling Stockings, Santa Claus is simply filling stockings from his pack of toys. Another film called Santa Claus and the Children was made in 1898. A year later, a film directed by George Albert Smith titled Santa Claus (or The Visit from Santa Claus in the United Kingdom) was created. In this picture, Santa Claus enters the room from the fireplace and proceeds to trim the tree. He then fills the stockings that were previously hung on the mantle by the children. After walking backward and surveying his work, he suddenly darts at the fireplace and disappears up the chimney. 
Santa Claus' Visit in 1900 featured a scene with two little children kneeling at the feet of their mother and saying their prayers. The mother tucks the children snugly in bed and leaves the room. Santa Claus suddenly appears on the roof, just outside the children's bedroom window, and proceeds to enter the chimney, taking with him his bag of presents and a little hand sled for one of the children. He goes down the chimney and suddenly appears in the children's room through the fireplace. He distributes the presents and mysteriously causes the appearance of a Christmas tree laden with gifts. The scene closes with the children waking up and running to the fireplace just too late to catch him by the legs. A 1909 film by D. W. Griffith titled A Trap for Santa Claus shows children setting a trap to capture Santa Claus as he descends the chimney, but instead capture their father who abandoned them and their mother but tries to burglarize the house after he discovers that she inherited a fortune. A 29-minute 1925 silent film production titled Santa Claus, by explorer/documentarian Frank E. Kleinschmidt, filmed partly in northern Alaska, feature Santa in his workshop, visiting his Eskimo neighbors, and tending his reindeer. A year later, another movie titled Santa Claus was produced with sound on De Forest Phonofilm.Over the years, various actors have donned the red suit (aside from those discussed below), including Leedham Bantock in Santa Claus (1912), Monty Woolley in Life Begins at Eight-thirty (1942), Alberto Rabagliati in The Christmas That Almost Wasn't (1966), Dan Aykroyd in Trading Places (1983), Jan Rubes in One Magic Christmas (1985), David Huddleston in Santa Claus: The Movie (1985), Jonathan Taylor Thomas in I'll Be Home for Christmas (1998), and Ed Asner in The Story of Santa Claus (1996), Olive, the Other Reindeer (1999), Ellen's First Christmas (2001), Elf (2003), Regular Show: The Christmas Special (2012), Elf: Buddy's Musical Christmas (2014), Santa Stole Our Dog: A Merry Doggone Christmas!, and A StoryBots Christmas (both 2017).

Santa's origins
Some films about Santa Claus seek to explore his origins. They explain how his reindeer can fly, where the elves come from, and other questions that children have generally asked about Santa. Two Rankin/Bass stop motion animation television specials addressed this issue: the first, Santa Claus Is Comin' to Town (1970), with Mickey Rooney as the voice of Kris Kringle, reveals how Santa delivered toys to children despite the fact that the evil Burgermeister Meisterburger had forbidden children to play with them; and the second, The Life and Adventures of Santa Claus (1985), based on L. Frank Baum's 1902 children's book of the same name, follows Santa being reared by a collection of mythical creatures who finally grant him immortality. Another animated version of Baum's book was made by Glen Hill in 2000, and the book also served as the basis for an anime series, Shounen Santa no Daibôken (Young Santa's Adventures) in 1994 and The Oz Kids video, Who Stole Santa? (1996). None of these films focus on Santa Claus's saintly origins.
The 1985 feature film Santa Claus: The Movie, inspired by the 1978 Superman: The Movie, stars David Huddleston as Santa Claus and British actress Judy Cornwell as his wife Anya, shows how Santa and Anya are discovered by a clan of elves called the Vendequm. Dudley Moore portrays Patch, the central character and main focus of the story; Burgess Meredith portrays their wise leader, the Ancient One, who reveals that Claus represents the fulfillment of an ancient prophecy, whereby he has been designated as ""the Chosen One"", whose mission it will be to deliver the elves' toys to children all over the world. The film's prologue features Claus and Anya performing Santa-like duties in their home village, and strongly suggests Santa's saintly origins.
The 2019 animated Netflix Christmas comedy Klaus, also features its own telling of the origin of Santa. Where Jesper Johansen, the pampered spoiled son of a postmaster general (played by Jason Schwartzman) is sent to the secluded town of Smeerensburg, as their latest postman. There Jesper learns that due to a local family feud, the town’s people hardly have time for anything else, let alone writing or exchanging letters. There he reluctantly teams up with the titular Santa Claus (played by J. K Simmons) who is portrayed as gruff hermit, with a sad past and a skilled hand for woodcarving and toy making. Together, they not only help end the feud in Smeerensburg, but also create one of the most beloved holiday traditions.

Questioning and believing
Another genre of Santa Claus films seeks to dispel doubts about his existence.

One of the first films of this nature was titled A Little Girl Who Did Not Believe in Santa Claus (1907) and involves a well-to-do boy trying to convince his poorer friend that Santa Claus is real. She doubts because Santa has never visited her family because of their poverty.
Miracle on 34th Street (1947), starring Natalie Wood as Susan Walker, revolves around the disbelief of young Susan, whose mother (Maureen O'Hara) employs a kind old man (Edmund Gwenn, who won an Academy Award for Best Supporting Actor) to play Santa Claus at Macy's; he later convinces Susan that he really is Santa.
One Magic Christmas (1985) depicts Santa as the Dutch Sinterklaas, in charge of Christmas angels (deceased people) in lieu of elves, whom he assigns to restore individuals' Christmas spirit. He assigns Gideon (Harry Dean Stanton) to a woman (Mary Steenburgen) who worries too much about the practical side of life and shows little charity. Gideon shows her a potential Christmas in which her husband is killed by a desperate bank robber whom she has neglected to help, and takes her little girl to Santa, who gives her the last letter her mother sent him as a child and tells her to give it to her. This works to restore her spirit and gives her a do-over of Christmas with her husband and the desperate man, and she greets Santa putting presents under her tree.
The Polar Express (2004), based on the children's book of the same name, also deals with issues and questions of belief as a magical train conducted by Tom Hanks transports a doubting boy to the North Pole to visit Santa Claus.
Yes, Virginia (2009) is an animated holiday TV special based on the true story of a young girl, Virginia O'Hanlon, who writes a letter to the editor of the New York Sun in 1897 after her friends tell her that there is no Santa. The newspaper editor tells her that indeed there is a Santa: ""He lives, and he lives forever."" Francis Pharcellus Church was the real-life editor played by Charles Bronson in the special.

Santa as a hero
Some science fiction B movies feature Santa Claus as a superhero-type figure, such as the 1959 film titled Santa Claus produced in Mexico with José Elías Moreno as Santa Claus. In this movie, Santa allies with Merlin the magician to battle the devil, who is attempting to trap Santa. In the Cold War-era film Santa Claus Conquers the Martians (1964), Santa Claus is captured by Martians and brought to Mars, and ultimately foils a plot to destroy him.In the 1984 film The Night They Saved Christmas, starring Art Carney as Santa, likewise chronicles how Santa Claus and Claudia Baldwin (Jaclyn Smith), the wife of an oil explorer, have to save the North Pole from explosions while Baldwin's husband is searching for oil in the Arctic. Santa Claus: The Movie contains a subplot in which Santa Claus rescues Joe (Christian Fitzpatric) from his best friend Cornelia's (Carrie Kei Heim) evil step-uncle B. Z. (John Lithgow). Santa is a hero in The Nightmare Before Christmas, held captive by Oogie Boogie, although he is spiteful and enraged at Jack when freed. The 2004 film Christmas With the Kranks, based on the 2001 John Grisham novel Skipping Christmas, has Austin Pendleton as Marty, a Santa-dressed umbrella salesman who turns out to be the real Santa, saving Luther Krank's (Tim Allen's) forced Christmas party by capturing a burglar unintentionally brought to the house by the police. After leaving the party, he rides through the sky in his Volkswagen Beetle, pulled by reindeer. In the 2005 film adaptation of The Chronicles of Narnia: The Lion, the Witch and the Wardrobe, Father Christmas (James Cosmo) supplies the Pevensie children with the weapons and tools they need to battle the White Witch (Tilda Swinton). In the 2012 DreamWorks film Rise of the Guardians, which is based on the book series The Guardians of Childhood by William Joyce, Santa (Alec Baldwin) is shown as one of the main characters and leader of the Guardians, who are the heroes of the movie. The 2022 film Violent Night portrays a drunk and disillusioned Santa (played by David Harbour) caught up in a hostage situation and becoming an action hero, parodying the film Die Hard and its protagonist John McClane.In the television show South Park, Santa is often depicted with firearms; in the episode ""Red Sleigh Down"", he battles Iraqis to try to bring Christmas to Iraq. In the episode ""A Woodland Critter Christmas"", he uses a combat shotgun to blast away Satanic animals who try to give birth to the AntiChrist. Santa (played by Nick Frost) made a brief appearance at the end of the Doctor Who episode, ""Death in Heaven"". In the following episode, which served as the show's 2014 Christmas special, ""Last Christmas"", he plays a more prominent role. It is eventually revealed that the scenes with him are the characters experiencing a shared dream, and he is their subconscious trying to help them wake up before they are killed. At the episode's end, he successfully awakens the Doctor and Clara, reuniting the two.

Succession of Santas
One genre of movies suggests that Santa Claus is not historically a single individual but a succession of individuals.
In Ernest Saves Christmas (1988), Ernest P. Worrell (Jim Varney) joins the challenge of Santa Claus, alias Seth Applegate (Douglas Seale), to convince Florida kids' show host Joe Carruthers (Oliver Clark) to become the next Santa.
In The Santa Clause (1994), Tim Allen plays Scott Calvin, who accidentally causes Santa Claus to fall off the roof of his house. After he puts on Santa's suit, he finds himself contractually obligated to become the next Santa. Reluctant at first, his appearance changes over the next year from average to the legendary image: he grows fat, his hair whitens, and his beard grows uncontrollably by magic. He ultimately falls in love with his new role.
A 2001 television special, Call Me Claus, stars Whoopi Goldberg as Lucy Cullins, an African American woman destined to reluctantly become the next Santa Claus.
In The Hebrew Hammer (2003), the role of Santa Claus is traditionally passed down from father to son. The system is disrupted when the reigning Santa is murdered by his son, Damian, who then uses the position to attack the competing holidays of Hanukkah and Kwanzaa.
The 2011 animated film Arthur Christmas portrays being Santa Claus as a dynasty. The first ""Santa"", Saint Nicholas, established the North Pole workshop and passed the title and responsibilities to his son after 70 Christmases, after which his son passed them on to his son, and so on. In the film, the current Santa initially refuses to retire, due to worry about what he will be if he is not Santa.

Impostor Santas
Several films have been created which explore the consequences should an impostor Santa take over.
One of the first films featuring a fake Santa Claus is the 1914 silent film, The Adventure of the Wrong Santa Claus, written by Frederic Arnold Kummer. In this film, a bogus Santa steals all the Christmas presents and amateur detective Octavius (played by Herbert Yost) tries to recover them. 
The most notorious impostor appears in the 1966 cartoon based on Dr. Seuss's 1957 children's book, How the Grinch Stole Christmas!, wherein the Grinch, a hairy and mean-tempered creature, attempts to rob the Whos in Whoville of their Christmas by stealing their presents, food and decorations, but has a change of heart when he sees that he has not stopped them from celebrating after all. This animated feature was made into a live-action movie in 2000, directed by Ron Howard and starring Jim Carrey as the Grinch. A CGI feature film was made in 2018, starring Benedict Cumberbatch as the voice of the Grinch. 
Another less-than-friendly impostor appears in A Christmas Story (1983) as a disgruntled mall Santa at Higbee's Department Store (a real store in downtown Cleveland, Ohio) in the fictional town of Holman, Indiana. Played by Jeff Gillen, Santa is depicted as a larger-than-life figure who terrifies, rather than amuses, children. Gillen's performance lends credence to the theory that the mall Santa is not quite genuine.
Another recent devious mall Santa was played by Billy Bob Thornton in Bad Santa (2003), a film which gained the normally family-friendly Disney ""bad press"".Tim Burton's stop-action animated musical film, The Nightmare Before Christmas (1993), depicts Jack Skellington, the Pumpkin King of Halloween Town, wanting to become Santa Claus after an accidental visit to Christmas Town. After the mostly well-meaning but totally clueless Halloween Town citizens capture Santa, they then try to take over Christmas with disastrous results; children are terrified by Jack's Halloween-themed gifts, and the real Santa is almost killed by Oogie Boogie. Santa is voiced here by Ed Ivory and in the video game spinoffs by Corey Burton.
In 2002's The Santa Clause 2: The Mrs. Clause, Scott Calvin (Tim Allen) must leave the North Pole to find a wife, so his number one elf turns a plastic toy Santa into a life-size robot clone of Calvin to cover for him. The robo-Santa interprets the rules of Christmas literally, convinces himself that all of the world's children are naughty, and dresses and runs the North Pole like a Latin American dictator, constructing an army of giant toy soldiers. When Calvin returns with his wife, he must defeat his clone to regain control of Christmas.
In 2006's The Santa Clause 3: The Escape Clause, Martin Short appears as Jack Frost, who is jealous of Santa Claus. He usurps the role from Scott Calvin, turns Christmas into ""Frostmas"" and the North Pole into a Disneyesque tourist resort, and addicts the worlds' children to toys which their parents must buy from him.
In 2007's Fred Claus, Vince Vaughn plays Santa's jealous older brother who grows up hating Santa and Christmas. He cons his brother (Paul Giamatti) into giving him a large sum of money for a business deal, in return for which he must come to the Pole and help prepare for Christmas. Fred ends up sabotaging his brother by placing all the world's naughty children on the nice list. Meanwhile, another Santa-hater (Kevin Spacey) is auditing Santa, looking for excuses to fire him and the elves. When Santa injures his back, Fred must deliver the gifts in order to save Christmas.
Other, darker impostors have appeared in slasher films such as the first three films of the five-film Silent Night, Deadly Night series, Santa Claws and Santa's Slay, and in the short ""...And All Through the House"", part of the horror anthology film Tales from the Crypt (1972) and later remade as episode 1.2 and directed by Robert Zemeckis for the HBO TV series of the same name. Both versions were inspired by the comic book Tales from the Crypt.

See also
Christmas elf#In films and television
Mrs. Claus#In popular media
List of Christmas films
Passage 8:
The Year Without a Santa Claus
The Year Without a Santa Claus is a 1974 stop motion animated Christmas television special produced by Rankin/Bass Productions. The story is based on Phyllis McGinley's 1956 book of the same name. It is narrated by Shirley Booth (her final acting credit before her retirement from acting) and starring the voices of Mickey Rooney, Dick Shawn, and George S. Irving. It was originally broadcast on December 10, 1974, on ABC.

Plot
Santa Claus wakes up with a cold sometime before Christmas in the early 20th century. His doctor, who thinks nobody cares about Santa anymore, advises him to make some changes to his routine, so Santa decides to take a holiday instead of delivering gifts. Mrs. Claus unsuccessfully tries to convince him otherwise, so she enlists two elves named Jingle and Jangle to find proof that people still believe in Santa.
Jingle and Jangle set out with Santa's youngest reindeer Vixen, but are shot down by crossfire between the conflicting Miser Brothers: Snow Miser, who controls the world's cold weather, and Heat Miser, who controls its warm weather. Vixen saves her guardian elves from falling to their doom and they continue on their uncertain path.
Jingle, Jangle, and Vixen come upon Southtown, a small community in the southern United States. They ask a group of children, including a boy named Iggy, if they believe in Santa, but they are skeptical. To make matters worse, Vixen grows sick due to the warm weather and is sent to the local animal shelter after Jingle and Jangle disguise her as a dog. The town's police officer refers them to the town's mayor, who laughs at their story but agrees to free Vixen if they can prove they are elves by making it snow in Southtown on Christmas.
Jingle and Jangle call Mrs. Claus to pick them up. As she leaves, Santa discovers Vixen is missing and travels to Southtown himself to retrieve her while disguised as a civilian named ""Klaus"". While there, he meets Iggy and his family. Klaus reveals his belief in Santa, and Iggy's father reveals that Santa personally visited him one Christmas, and he still believes as well. When Claus leaves to retrieve Vixen, Iggy realizes his true identity and resolves to help Jingle and Jangle.
Iggy joins Mrs. Claus when she arrives to pick up Jingle and Jangle, and together they visit the Miser Brothers. They ask Snow Miser to send snow to Southtown for a day; he is agreeable but says he cannot as it is part of Heat Miser's territory. They then ask Heat Miser, who says he will only comply if Snow Miser turns the North Pole over to him in exchange. When the brothers begin bickering again, Mrs. Claus goes over their heads and visits their mother, Mother Nature, who convinces her sons to compromise.
As Christmas approaches, the world's children send their own presents to Santa, setting off international headlines. Touched by the outpouring of generosity and appreciation, Santa decides to make his journey after all. On Christmas Eve, he makes a public stop in Southtown during a snowfall. The next day, the children, including Iggy, are delighted to receive their presents. As the special ends, Mrs. Claus narrates that somehow, ""yearly, newly, faithfully and truly"", Santa always comes. Santa is shown getting out of bed to prepare himself, his reindeer, and his gift-loaded sleigh, remarking he could never imagine ""a year without a Santa Claus"".

Voice cast
Shirley Booth as Mrs. Claus
Mickey Rooney as Santa Claus
Dick Shawn as Snow Miser
George S. Irving as Heat Miser
Bob McFadden as Jingle Bells, Elf Doctor
Bradley Bolke as Jangle Bells, Police Officer
Rhoda Mann as Mother Nature, Mrs. Thistlewhite
Ron Marshall as Mr. Thistlewhite, Mayor of Southtown
Colin Duffy as Ignatius ""Iggy"" Thistlewhite
Noelle Magargle as the Blue Christmas Girl
The Wee Winter Singers as the Children Choir

Songs
""Sleigh Ride"" (instrumental)
""The Year Without a Santa Claus""
""I Could Be Santa Claus""
""I Believe in Santa Claus""
""It's Gonna Snow Right Here in Dixie""
""The Snow Miser Song""
""The Heat Miser Song""
""Blue Christmas""
""Sleigh Ride"" (instrumental)
""Here Comes Santa Claus""
""The Year Without a Santa Claus (reprise)""

Television rights
The special premiered in 1974 on ABC and aired annually on Freeform during its 25 Days of Christmas programming block until 2017. As of 2018, AMC: American Movie Classics currently airs the special uncut as part of the Best Christmas Ever block. Warner Bros. Entertainment currently distributes the special through their ownership of the post-1974 Rankin/Bass Productions library.

Home video
The special was first released on VHS by Vestron Video on September 5, 1991 as part of their Christmas Classics Series, which is distributed by Family Home Entertainment. Warner Home Video released the special on VHS on September 2, 1992, and re-released it on VHS on September 28, 1999. The special was then released on DVD on October 31, 2000, and re-released on the Deluxe Edition DVD on October 2, 2007. Warner Home Video released the special on Blu-ray on October 5, 2010, making it the first Rankin/Bass production to be released on that format.

DVD details
Release date: October 31, 2000 (Original DVD), January 17, 2004 (30th Anniversary Edition DVD), October 2, 2007 (Deluxe Edition DVD), October 5, 2010 (Blu-ray)
Full Screen
Region: 1
Aspect Ratios: 1.33:1
Audio tracks: English
Special Features:
Rudolph's Shiny New Year
Nestor, The Long-Eared Christmas Donkey
Stop Motion 101 (Deluxe Edition Exclusive)
We Are Santa's Elves: Profiling Arthur Rankin Jr. & Jules Bass (Deluxe Edition Exclusive)

Live-action remake
A live-action remake of The Year Without a Santa Claus premiered on NBC on December 11, 2006, and was released on DVD the following day. It follows largely the same plot as the original special.
Paul Mavis, for Drunk TV, wrote, ""The Year Without a Santa Claus is a nauseating, angry, joyless little holiday confection sure to poison any child unlucky enough to chance upon it. This hate-filled stocking stuffer has nothing but contempt for its intended audience, promoting the worst possible beliefs about people, while cloaking itself, incredibly, in the fake guise of a meaningful lesson about the holidays: the gall that the cretinous makers of this film have is really quite audacious.”

Cast
John Goodman as Santa Claus
Delta Burke as Mrs. Claus
Michael McKean as Snow Miser
Harvey Fierstein as Heat Miser
Ethan Suplee as Jingle Bells
Eddie Griffin as Jangle Bells
Chris Kattan as Sparky
Dylan Minnette as Ignatius ""Iggy"" Thistlewhite
Billy Slaughter as Nerd ElfCarol Kane as Mother Nature
Carson Kressley as the elf costumer
Laura Schlessinger, ""Dr. Laura"", as herself
Jack LaLanne as Hercules

Sequel
A sequel, titled A Miser Brothers' Christmas, was produced in 2008 by Warner Bros. Animation and Cuppa Coffee Studios, and it also used stop-motion animation. Mickey Rooney, age 88, reprised his role as Santa Claus, and George S. Irving, age 86, reprised his role as Heat Miser. Juan Chioran and Catherine Disher replaced Dick Shawn and Shirley Booth as Snow Miser and Mrs. Claus, respectively, Shawn and Booth having died prior to the film's production.

See also
List of Christmas films
Santa Claus in film
List of animated feature films
List of stop-motion films
List of Rankin/Bass Productions films
Passage 9:
A Miser Brothers' Christmas
A Miser Brothers’ Christmas is a stop motion spin-off special based on some of the characters from the 1974 Rankin-Bass special The Year Without a Santa Claus. Distributed by Warner Bros. Animation under their Warner Premiere label (the rights holders of the post-1974 Rankin-Bass library) and Toronto-based Cuppa Coffee Studios, the one-hour special premiered on ABC Family on Saturday, December 13, 2008, during the network's annual The 25 Days of Christmas programming.Mickey Rooney (at age 88) and George S. Irving (at age 86) reprised their respective roles as Santa Claus and Heat Miser. Snow Miser (originally portrayed by Dick Shawn who died in 1987) was voiced by Juan Chioran, while Mrs. Claus (voiced in the original by Shirley Booth who died in 1992) was portrayed by Catherine Disher. The movie aimed to emulate the Rankin/Bass animation style. This is the last Christmas special to feature Mickey Rooney as Santa Claus, as he died in 2014, as well as the last time George Irving voiced Heat Miser, as he died in 2016.

Plot
The feuding Miser Brothers (Heat and Snow) attend their family reunion with Mother Nature and their fellow siblings including the North Wind, Earthquake, Thunder and Lightning, and the Tides. North Wind passively asks Mother Nature what might happen if Santa would be unable to complete his duties on Christmas. She responds that North Wind would take control instead. Heat then begins to call out Snow for trying to ""give global warming a bad name"". Snow responds by talking about Heat's attempts to scare people with reports of a second Ice Age. Heat then reprimands Snow for claiming Iceland as his own, which barely has any ice. Snow then calls out Heat for claiming Greenland as his own because it's full of ice. The brothers then fight each other. Mother Nature ends the fight.
Despite his dashing appearance and veneer of flattery and devotion toward Mother Nature, the North Wind is far more malevolent than either of his brothers. Self-absorbed and vain, the North Wind is fixated with the idea of replacing Santa Claus as a way to achieve personal glory. Beginning his machinations, He then sends two of his minions to crash Santa's Super-Sleigh designed by his mechanic Tinsel, causing Santa to injure his back after falling in the middle of a fight between the brothers as he unintentionally crosses into their domain.
Despite what she told the North Wind before and having been informed by Mrs. Claus about what happened to Santa, Mother Nature assigns the Miser Brothers the responsibility of running the toy factory. Their fighting continues as they move through several workshop stations. The North Wind hatches a new plan to keep them fighting so it would appear as if they ruined Christmas themselves, but Mrs. Claus convinces the Miser Brothers to put aside their differences and cooperate by showing them the Naughty/Nice list station. The brothers' history is revealed, showing they've always been on Santa's naughty list for mutual bickering. Upon learning the error of their ways, they begin working together and successfully get work back up to speed. However, the North Wind hatches a plan to destroy their truce and get them fighting again, leaving Santa to deliver the toys and giving North Wind the chance to finish him off.
On Christmas Eve, the North Wind's minions surreptitiously attach heating and cooling units to the sleigh, apparently capable of heating or cooling entire regions of the planet. The discovery causes the Miser Brothers to blame each other. With them fighting again, Santa has no choice but to drive the sleigh as North Wind planned. After Santa leaves, Tinsel discovers the super-sleigh has been sabotaged, which stops the Misers' fight as they realize that neither of them was responsible for injuring Santa. Upon finding one of North Wind's Christmas cards with him dressed as Santa, the Misers realize the truth about their brother and comprehend his plan. Meanwhile, the North Wind attacks Santa's sleigh in flight, whipping up a vortex to consume Santa, but the Miser Brothers, with the aid of Tinsel and a team of young reindeer, save Santa in the nick of time.
The North Wind's cover is blown and Mother Nature sentences him to do household chores for the next several thousand years as punishment for trying to finish off Santa and making his brothers fight. With North Wind thwarted, the brothers learn they've finally made the nice list. They deliver the presents for Santa and give gifts to each other in the process, making peace between them and ending their feud.

Cast
Mickey Rooney as Santa Claus
George S. Irving as Heat Miser
Juan Chioran as Snow Miser
Catherine Disher as Mrs. Claus, Reindeer Elf
Brad Adamson as North Wind
Patricia Hamilton as Mother Nature
Peter Oldring as Bob, Elf #1
Susan Roman as Tinsel, Dr. Noel

Reception
The movie had 3.7 million viewers in its first airing, as determined by Nielsen ratings. It received a nomination for ""Best Animated Television Production Produced for Children"" in the 36th Annual Annie Awards.

See also
Santa Claus in film
List of Christmas films","['seasonal television specials, particularly its work in stop motion animation']",11635,hotpotqa,en,,ed6c85899d2a29f6dc8e02ba62bc8f1b1043a8c135fdd132,"seasonal television specials, particularly its work in stop motion animation",76
What conference champions were putted against each other in the first NBA Finals in which the Orlando Magic's played?,"Passage 1:
List of Miami Heat seasons
The Miami Heat are a professional basketball team based in Miami that competes in the National Basketball Association (NBA). The Heat formed in 1988 as an expansion franchise and have since made the playoffs 24 out of 35 seasons, captured 16 division titles, seven conference titles, and three NBA championships. Along with the Orlando Magic, they are one of two NBA teams representing the state of Florida. As of the 2021–22 season, they are one of two franchises formed after 1980 to win the NBA title, along with the Toronto Raptors. They defeated the Dallas Mavericks in the 2006 NBA Finals, the Oklahoma City Thunder in the 2012 NBA Finals, and the San Antonio Spurs in the 2013 NBA Finals.

Table key
Seasons
Note: Statistics are correct as of the 2022–23 season.

All-time records
Note: Statistics are correct as of June 12, 2023.

Notes
Passage 2:
Orlando Magic
The Orlando Magic are an American professional basketball team based in Orlando, Florida. The Magic compete in the National Basketball Association (NBA) as a member of the league's Eastern Conference Southeast Division. The franchise was established in 1989 as an expansion franchise, and such notable NBA stars as Shaquille O'Neal, Penny Hardaway, Grant Hill, Tracy McGrady, Dwight Howard, Jameer Nelson, Rashard Lewis and Nikola Vučević have played for the club throughout its history. As of 2021, the franchise has played in the NBA playoffs 16 times in 32 seasons, and twice went to the NBA Finals, in 1995 and 2009, losing to the Houston Rockets and the Los Angeles Lakers, respectively. Orlando has been the second most successful of the four expansion teams brought into the league in 1988 and 1989 in terms of winning percentage and playoff success, after the Miami Heat.

Franchise history
1985–1986: team creation
In September 1985, Orlando businessman Jim L. Hewitt approached Philadelphia 76ers general manager Pat Williams as they met in Texas on his idea of bringing an NBA team to Orlando. Intrigued by the potential of an Orlando-based NBA team, Williams became the front man of the investment group one year later, after he left the 76ers. On June 19, 1986, the two held a news conference to announce their intention of seeking an NBA franchise.At the same time, Hewitt and Williams decided to hold a contest in the Orlando Sentinel newspaper to get names for their new franchise. Out of a total of 4,296 submitted entries, the names were subsequently narrowed to four, ""Heat"", ""Tropics"", ""Juice"", and ""Magic"". The last one, which had been submitted by 11 people, was picked after Williams brought his 7-year-old daughter Karyn to visit in Orlando. On July 27, 1986, it was announced that the committee chose the Magic to be the new name of the Orlando franchise in the NBA. The name ""Magic"" alludes to the area's biggest tourist attraction and economic engine Walt Disney World, along with its Magic Kingdom, highlighting its corporate theme of magic. Hewitt added that ""You look at all the aspects of Central Florida, and you find it really is an exciting place, a magical place.""Many, including Williams himself at first, thought that Miami or Tampa were better locations in Florida for a franchise. At the time, Orlando was a small city without a major airport or a suitable arena. Hewitt brought investors such as real estate developer William DuPont, Orlando Renegades owner Don Dizney, and Southern Fruit Citrus owners Jim and Steve Caruso, and talked the Orlando city officials into approving an arena project. Meanwhile, Williams gave presentations to NBA commissioner David Stern and the owners of the other teams of the league that the town was viable.In April, the franchise committee recommended expanding by three teams, with two of the slots going to Charlotte and Minneapolis-St. Paul. The recommendation put the Orlando bid in doubt, since it advised that the state of Florida should only be allocated one team as part of the three-team expansion. This feedback put the planned Orlando franchise up against the Miami-based team, originally known as the Florida Heat and eventually named the Miami Heat. When both Miami and Orlando ownership groups made successful pitches, the expansion committee decided to expand by four teams, allowing both to have a franchise.The Magic became the first-ever major-league professional sports franchise in the Orlando area, following an expansion fee of reportedly $32.5 million. They were one of the four new expansion franchises awarded by the NBA in 1987 along with the Charlotte Hornets, Miami Heat and Minnesota Timberwolves. The Magic hired Matt Guokas as the team's first coach, who helped the Magic select 12 players in the NBA Expansion Draft on June 15, 1989. On June 27, 1989, the Magic chose Nick Anderson with the 11th pick in the first round, who became the first draft pick of the franchise.

1989–1992: early years
The Magic, in their debut year, went on to select Nick Anderson (Illinois), who had just won both the Mr. Basketball and College Player of the Year awards, in the first round of the 1989 NBA Draft. As the 10th pick, he was the team's first franchise player and leading scorer for a decade.
The Magic's very first game played was an exhibition game on October 13, 1989, against the then-reigning champion Detroit Pistons, which the Magic won. Anderson was quoted as saying the atmosphere and the people watching the game was ""like Game 7 of the NBA Finals"".On November 4, 1989, the Magic played their first season game at the Orlando Arena (O-Rena) against the visiting New Jersey Nets, who won 111–106 in a hard-fought game. The Magic's first victory came two days later, as the Magic defeated the New York Knicks 118–110. The inaugural team compiled a record of 18–64 with players including the franchise star player Nick Anderson, Reggie Theus, Scott Skiles, Terry Catledge, Sam Vincent, Otis Smith, and Jerry Reynolds.In the 1990 NBA draft, the Orlando Magic selected Dennis Scott with the fourth overall pick. On December 30, 1990, Scott Skiles racked up 30 assists in the 155–116 victory over the Denver Nuggets, breaking Kevin Porter's NBA single-game assists record (29). Skiles was named the NBA's Most Improved Player at the end of the season, as the Magic heralded the NBA's most improved record that season. Forward Dennis Scott set a team mark with 125 three-point field goals for the season, the best long-distance production by a rookie in NBA history. He was named to the NBA All-Rookie First Team.On September 19, 1991, the DeVos family, founders of Amway, purchased the franchise for $85 million. Family patriarch Richard DeVos became the owner of the franchise. The 1991–92 season was disappointing for the Magic as various players missed games with injuries. Dennis Scott played only 18 games, Nick Anderson missed 22 games, Stanley Roberts, Jerry Reynolds, Brian Williams, Sam Vincent and Otis Smith all missed at least 27 games each. With a shortage of healthy players, the team struggled through a 17-game losing streak and finished with a 21–61 record. The Magic still managed to have all 41 home games sold out.

1992–1996: the Shaquille O'Neal and Penny Hardaway era
The Magic history was changed on May 17, 1992, when the franchise won the first pick in the 1992 NBA draft Lottery. The Magic selected big-man Shaquille O'Neal from Louisiana State University, the biggest prize in the draft since the Knicks won Patrick Ewing. O'Neal, a 7'1"" center, made an immediate impact on the Magic, leading the team to a 41–41 record. The Magic again became the NBA's most improved franchise, as they improved by 20 games. O'Neal was the first rookie to be voted an All-Star starter since Michael Jordan in 1985. He also became the 1992–1993 NBA Rookie of the Year. Despite O'Neal's presence, the Magic missed the 1993 NBA playoffs because they were tied with the Indiana Pacers for the eighth and final playoff spot in the Eastern Conference with the Pacers holding the tiebreaker.Despite barely missing the playoffs and receiving the least chance of gaining the top draft pick with only one ball in the lottery, the Magic again won the first pick in the 1993 NBA draft Lottery. Prior to the draft, Guokas stepped down as head coach, and Brian Hill was promoted to become the Magic's second head coach. In the draft, the Magic selected Chris Webber, but traded him to the Golden State Warriors for the number three pick, guard Anfernee ""Penny"" Hardaway and three future first-round draft picks. With the combination of O'Neal and Hardaway, the Magic became a dominant team in the NBA, compiling the first 50 win season in franchise history with a 50–32 record. The Magic were in the playoffs for the first time, ranked the fourth seed in the Eastern Conference; however, the Pacers swept the Magic 3–0 in the first round, thus ending the Magic's season.In the 1994–95 season, the Magic's sixth season, All-Star forward Horace Grant was acquired as a free agent from the Chicago Bulls. The Orlando Magic compiled a 57–25 record, best in the East and winning the Atlantic Division title, becoming the second-fastest team (behind the Milwaukee Bucks in 1971, who were in their third season) to advance to the NBA Finals in league history. In the playoffs, the Magic defeated the Boston Celtics, Bulls, and the Indiana Pacers, advancing to the NBA Finals where O'Neal, Hardaway and the young Magic bowed to a more playoff-experienced Hakeem Olajuwon and the Houston Rockets, winning their second consecutive championship in a 4–0 sweep of Orlando.In the 1995–96 season, the Magic again were near the top of the Eastern Conference and the Atlantic Division with a 60–22 record, led by O'Neal and Hardaway; however, the Magic were seeded number two, behind the NBA's second-best all-time 72–10 record of the Chicago Bulls. In the meantime, general manager Pat Williams was promoted to senior executive vice president and replaced by the vice president of Basketball Operations John Gabriel on April 29, 1996. In the playoffs, after the Magic defeated the Detroit Pistons and the Atlanta Hawks, Orlando met the Bulls in the Eastern Conference Finals. The combination of Jordan, Scottie Pippen and rebounder Dennis Rodman was too much for the Magic, and Orlando was swept 4–0.

1996–1999: the post Shaq/Penny Hardaway era
In the offseason, O'Neal left as a free agent to the Los Angeles Lakers, dealing a huge blow to the Magic franchise. In the middle of the season, urged by player discontent, management fired coach Brian Hill and named Richie Adubato as interim coach for the rest of the season. Under Adubato, the Magic went 21–12 to compile a 45–37 record, led by Penny Hardaway. In the playoffs, the Magic quickly fell 0–2 to the heavily favored Miami Heat in the first round, but Hardaway battled back with consecutive 40-point games to assure a game five (the first player to do so), which the Magic ultimately lost.The Magic then hired Chuck Daly to be head coach for the 1997–98 season. In addition, Hall of Famer Julius Erving joined the Magic's front office, giving Orlando hope for a successful season. The season was hampered by an injury to Hardaway who sat out the majority of the season. Anderson, combined with newly acquired free agent Bo Outlaw, led the team to a 41–41 record, just out of reach of the NBA playoffs. In addition, Seikaly was traded during the season to the New Jersey Nets for three role players and a future draft pick.In 1998–99, with the drafting of Michael Doleac and Matt Harpring with the 12th and 15th picks in the 1998 draft, and a healthy Penny Hardaway and Nick Anderson, the Magic tied for the Eastern Conference's best record with the Miami Heat in the lockout-shortened season, 33–17. At the heart of the team was veteran and future Orlando Magic Hall of Famer Darrell Armstrong, leading from the vanguard (a constant that Orlando fans would enjoy for a decade) and picking up the NBA's Sixth-Man and Most Improved Player awards along the way. Orlando also acquired NBA great Dominique Wilkins, along with brother Gerald, who were past their primes but were both still serviceable NBA players. In the playoffs the Penny Hardaway-led Magic were seeded number 3 because of tiebreakers and faced the Philadelphia 76ers. The 76ers, led by Allen Iverson, upset the Magic 3–1 in the first round. The team also changed their uniforms for the first time ever, changing from pinstripes to stars.

1999–2000: ""Heart and Hustle"" season
In 1999, the Magic, under general manager John Gabriel, who was later named Executive of the Year, hired rookie-coach Doc Rivers. Gabriel dismantled the previous team trading their only remaining superstar Penny Hardaway to the Phoenix Suns for Danny Manning (who never donned a Magic uniform), Pat Garrity, and two future draft picks. The Magic were then a team composed of virtually all no name players and little experience, which included team captain Armstrong, Bo Outlaw and a young Ben Wallace, along with Coach Rivers, who led the Magic to a 41–41 record, barely missing out on the playoffs. At the end of the season Rivers was named Coach of the Year. That year was characterized by the slogan ""Heart and Hustle"", as the team was known for its hard-working style.

2000–2004: the Tracy McGrady era
The following offseason, Gabriel, with millions of cleared salary cap space, attempted to lure three of the NBA's most prized free agents: Tim Duncan, Grant Hill, and Tracy McGrady. While Duncan opted to remain with the San Antonio Spurs, the Magic acquired Hill, a perennial All-Star, and McGrady. With McGrady and Hill together, the Magic were expected to be a force in the East. However, Hill was limited to 4 games because of an ankle injury. McGrady blossomed into a star during the season, becoming one of the NBA's top scorers. With the addition of Mike Miller from the draft, the Magic compiled a 43–39 record, which included a nine-game winning streak, and once again made the playoffs. McGrady made the All-Star Team and All-NBA 2nd Team. Miller won the Rookie of the Year. In the playoffs, they faced the Milwaukee Bucks in the first round. The Bucks won the series 3–1.
In 2001–02, McGrady led the Magic to a winning record of 44–38. Hill was still severely limited by his ankle injury, and did not play for the vast majority of the season. McGrady, combined with Armstrong, Miller, and 3-point sharpshooter Pat Garrity, formed the core of the team. McGrady made the All-NBA for the first time and made his second consecutive All-Star Team. However, the Magic were defeated 3–1 in the first round of the playoffs by the Charlotte Hornets led by Baron Davis.In 2002–03, with the acquisitions of Gordan Giricek and Drew Gooden from the Memphis Grizzlies in exchange for Mike Miller, McGrady once again led the Magic to a 42–40 record. McGrady led the league in scoring with 32.1 ppg, made his second All-NBA 1st Team, and 3rd All-Star Team. Despite still not having Hill due to injury, the Magic entered the playoffs for the third straight year. However, after taking a 3–1 lead in the best-of-seven first-round series, the Magic fell to the Detroit Pistons 4–3 in the now infamous heartbreaker. McGrady was quoted as saying, ""It feels good to get in the second round"" after still needing one more win to advance.The Magic's 15th season in 2003–04 proved to be one of its toughest ever. Even with the acquisition of veteran free agents Tyronn Lue and Juwan Howard, the Magic struggled early. After winning its first game, the Magic lost 19 consecutive games, setting a franchise record. They never recovered, and finished an NBA worst 21–61. Despite this, McGrady led the league in scoring with 28.0 ppg, made the All-NBA 2nd Team and his 4th consecutive All-Star Team. In the middle of the 19-game losing streak, coach Doc Rivers was fired, and assistant Johnny Davis was promoted to head coach. General manager Gabriel was replaced by John Weisbrod.

2004–2012: the Dwight Howard era
In the off-season, Weisbrod completely dismantled the team. Though he kept Davis as coach, he shook up the player roster, only keeping a few players from last season. The most significant trade was Tracy McGrady. McGrady, discontent with the Magic, wished to move on; Weisbrod accused McGrady of ""slacking off"" and not attending practices (McGrady later admitted that he did not give 100% during the 2003–2004 season and wanted the team to bring him some help, but never wanted to leave Orlando). The Magic traded McGrady along with Reece Gaines, Tyronn Lue, and Juwan Howard to the Houston Rockets for Steve Francis, Kelvin Cato, and Cuttino Mobley. In addition, the Magic acquired center Tony Battie and two second-round draft picks from the Cleveland Cavaliers in exchange for Drew Gooden, Steven Hunter, and the draft rights to Anderson Varejão. The Magic then signed free agent Hedo Türkoğlu. With the number one draft pick, the Magic selected high-school phenomenon and future All-Star and franchise cornerstone Dwight Howard, and a draft-day trade with the Denver Nuggets got them point guard Jameer Nelson.After a promising 13–6 start, the Magic began to fall apart. First, Weisbrod traded Mobley for Doug Christie from the Sacramento Kings. Christie, because of his emotional ties to the Kings, at first refused to play for the Magic. Later on, Christie claimed he had bone spurs and was placed on the injured list after playing only a few games for the Magic. Near the end of the season, with a playoff-push faltering, Weisbrod fired Davis after leading Davis to believe he was going to be the team's head coach for the entire 2004–05 NBA season. He then promoted Chris Jent to interim head coach. Throughout the season, bolstered by Hill's return, the Magic played spectacularly, defeating top NBA teams. However, led by the erratic play of Francis, the Magic also lost to league teams with losing records. Howard showed great promise, becoming one of the few players to average a double-double. Howard was a consistent rebounder and scorer, becoming the first rookie to start and play all 82 games in a season. In addition, Nelson, after a slow start, developed into a talented player, taking over the starting point guard position. Hill also returned and averaged 19.7 points a game. Hill was chosen an All-Star starter by NBA fans for the 2005 All-Star Game, and Dwight Howard and Jameer Nelson were named to the All-Rookie first and second teams, respectively. Howard was a unanimous selection.

The Magic finished the season 36–46. Their playoff push was hampered by injuries in the last quarter of the season: a season-ending broken wrist for sixth man Hedo Türkoğlu, a shin injury to Grant Hill, a rib cage injury to Nelson, and a three-game suspension to Francis for kicking a photographer. The Magic ended a few games out of the playoffs. On May 23, 2005, the Magic's plans were disrupted by the abrupt resignation of general manager and Chief Operating Officer John Weisbrod. In addition, the Magic announced the following day that Brian Hill, the coach who led the Magic to the NBA Finals under O'Neal and Hardaway, would return as head coach.The Magic drafted Spanish Fran Vázquez with the 11th pick in the 2005 NBA draft. On July 28, 2005, Vazquez stunned the team after announcing that he would remain in Spain to play for Akasvayu Girona, getting ridiculed by media after he was quoted that the decision to stay was made by his girlfriend. Owner Rich DeVos announced on October 21 that he was transferring ownership to his children, with the official owner role moving to son-in-law and team president Bob Vander Weide. The transfer was supposed to be complete by the end of the year.The 2005–06 season opened with high hopes for the Magic despite not being able to add first-round draft pick Vasquez. Grant Hill was supposedly finally healed from his multiple ankle surgeries. Dwight Howard and Jameer Nelson showed excellent progress during summer-league play. Second-round draft pick Travis Diener showed excellent shooting and decision-making during the summer. And the free-agent signing of Keyon Dooling showed that the club was going to continue making progress. Then trouble began. Hill, despite his ankle apparently being healed, suffered a painful sports hernia injury that would hamper his play throughout the entire season. After playing in three preseason games, he underwent surgery to correct the hernia and would not appear during the regular season until mid-December, to which he lasted a month before attempting to make another comeback in February and early March, however, he only played sporadically. Then a foot injury to Nelson forced him to sit out for over a month.
On February 15, 2006, the Magic announced that they had acquired Darko Miličić and Carlos Arroyo from the Detroit Pistons in exchange for Kelvin Cato and a 2007 top-five protected first-round draft pick. One week later, on February 22, the Magic announced that they had traded Steve Francis to the New York Knicks in exchange for Penny Hardaway (whom they waived two days later) and Trevor Ariza. With a set starting rotation of Battie, Howard, Türkoğlu, DeShawn Stevenson, and Nelson, the Magic mounted a surprising run at the eighth playoff spot in the Eastern Conference, including an 8-game winning streak and 12 consecutive home wins. The streak included wins against NBA powerhouses Detroit, San Antonio, Dallas and Miami, as well as a game against the Philadelphia 76ers in which Howard recorded 28 points and a career-high 26 rebounds. Despite their efforts they did not make the playoffs.

2006–2010: return to the NBA Finals
With the 11th overall pick in the 2006 NBA draft, the Magic took former Duke star JJ Redick. Even with the fan support to get him playing time he averaged just over 11 minutes a game. After beginning the season strong with a 13–4 record, the Orlando Magic began to suffer in the standings as the result of multiple losses, due in large part to the injuries of Tony Battie, Keyon Dooling, and Grant Hill. The Magic were also hampered with the sporadic play of many of their young stars, who on multiple occasions showed their propensity for streaky shooting and the team's lack of a solid scoring two-guard. Despite the team's poor play, Dwight Howard continued to develop and blossom in his third year in the league, culminating in his first selection to the Eastern Conference All-Star team. The final few weeks of the season saw the Magic build momentum and confidence with an impressive late push towards the Playoffs. On April 15, 2007, with an 88–86 victory over the Boston Celtics, the Magic secured its first berth in the NBA Playoffs since 2003 by locking up the 8th seed in the Eastern Conference. This marked the first time that the team had made the playoffs while posting a losing record. Nevertheless, their Playoff run ended on April 28, 2007, after they were swept in the first round by first seeded Detroit Pistons whose experience, veteran leadership and ability to consistently make the clutch basket proved far too much for the undermanned and overwhelmed Magic to overcome. It was announced on May 23, 2007, that Brian Hill had been fired as head coach of the Magic.
On June 1, 2007, the Magic signed Billy Donovan to be their head coach for five years. The next day, Donovan wished to be released from the contract and the Magic agreed several days later. On June 6, 2007, the Magic signed a 4-year contract with Stan Van Gundy. In the free agent market, the Magic signed Rashard Lewis of the Seattle SuperSonics to a six-year league-maximum contract believed to be worth over $110 million. At the NBA China Games, the Magic swept the three games in China, twice against the Cleveland Cavaliers and once against the Chinese national team in games held in Shanghai and in Macau.
On November 15, 2007, Bob Vander Weide, the son-in-law of Richard DeVos, officially took over as owner of the team, although ownership is still split evenly amongst Richard DeVos' other children as well.The Magic started the 2007–08 NBA season with an impressive 16–4 record in their first 20 games, which included wins over the Boston Celtics and Cleveland Cavaliers. Through the next few months, the Magic were not so successful, splitting their next 36 games with 18 wins and 18 losses. At the start of March, the Magic seemed to pick up speed again, finishing the month with 10 wins, the first time since November that they won 10 or more in a month. The Magic clinched the Southeast Division title when the Washington Wizards were routed at Utah 129–87 on March 31, 2008. It was the Magic's third division title, but only their first since 1995–96 season, as well as their first since the Southeast Division was formed. They also earned their 50th win of the season against the Chicago Bulls on April 13, which had not happened since the 1995–96 season. The Magic finished the regular season 52–30, their best season since 1995–96. With the 3rd seed in the Eastern Conference, they were matched up in their first-round playoff series against the Toronto Raptors. The Magic had home-court advantage for the first time since the 1998–99 season.
On April 28, 2008, at Amway Arena, the Magic eliminated the Raptors with a 4–1 series victory in the first round. It was the first playoff series victory for the Magic in 12 years after 6 straight first-round exits. The run of success did not last long as they fell 4–1 to the experienced Detroit Pistons in the second round. With the Magic already down in the series, controversy erupted after the Pistons' Game 2 victory. At the conclusion of the 3rd quarter, Chauncey Billups of the Pistons made a three-point shot giving the Pistons a three-point lead. However, the clock had stopped just as the play began. NBA rules prohibit officials from using instant replay or any timing device to determine how much time has elapsed when a clock malfunctions, nor is a replay allowed to be viewed from the time of the malfunction to when the play ends, when the game clock has not expired. Because of the rule, the officials then estimated that the play took 4.6 seconds, and because there were 5.1 seconds remaining when play began, the field goal was allowed to be counted. The NBA later admitted that the play actually took 5.7 seconds and the basket in question should not have counted. The Pistons went on to win Game 2. The Magic were able to win Game 3, with the Pistons' Chauncey Billups out for most of the game with an injury, but were unable to take advantage of his absence and defeat the Pistons in Games 4 and 5, which ended the Magic's playoff run in 2008.

The first half of the 2008–09 season went very well for the Magic. After 41 games, the Magic were 33–8, leading the Southeast Division, as well as having one of the top four records in the league. At the start of February, Jameer Nelson, their all-star starting point guard, went down with a shoulder injury. He was expected to miss the remainder of the season. After trading for Rafer Alston, the Magic finished the regular season with a 59–23 record, it was the most games the team had won in a season since the 1995–96 season in which they had 60 wins. In the playoffs, the Magic beat the Philadelphia 76ers in the first round of the playoffs and then the defending champions, the Boston Celtics, in the Eastern Conference semifinals, behind assistant coach Patrick Ewing's guarantee that they would win Game 7 of that series. In their first conference finals since 1996, the Magic beat the Cleveland Cavaliers, which were led by the season's MVP, LeBron James. After dropping the first two games in the Finals against the Los Angeles Lakers, the Magic finally won their first-ever game in the Finals in Game 3. Despite Nelson's return to the team for the Finals, the Lakers won the series and the championship by beating the Magic in five games.
In the 2009 off-season, Orlando traded Rafer Alston, Tony Battie, and Courtney Lee to the New Jersey Nets in exchange for eight-time All-Star Vince Carter and Ryan Anderson. Hedo Türkoğlu, as part of a sign-and-trade, was sent to the Toronto Raptors. They then made several free agent signings. On July 10, former Dallas Mavericks power forward Brandon Bass was given a 4-year deal. On July 21, the Magic signed former Phoenix Suns forward Matt Barnes. On August 19, they signed former Miami Heat point guard Jason Williams, who decided to come back after a year in retirement.On September 28, 2009, Orlando extended the contract of head coach Stan Van Gundy by exercising his option for the 2010–11 season. They did the same for general manager Otis Smith, which would keep him in that position through the 2011–12 season.The Magic were without Rashard Lewis for the first 10 games of the 2009–10 season. Lewis tested positive for an elevated testosterone level that was caused by an over-the-counter supplement containing a substance banned by the league. To make matters worse, Vince Carter suffered a left ankle injury in just the second game of the season. Carter's injury turned out to be not too serious, but caused him to miss the next five games. Another setback came in mid-November, when Jameer Nelson injured his left knee, which required arthroscopic surgery to repair. Nelson would be out for five weeks. Despite all of this, the Magic had a 23–8 record at the end of December.
Orlando lost seven of their first ten games in January, but recovered well enough to post a winning record for the month by winning six of their next seven. Following the All-Star break, the Magic went on a roll, winning 23 of their 28 remaining games, clinching their fourth consecutive playoff berth and winning their third consecutive division championship in the process. The Magic finished the regular season with a 59–23 record, matching their record from the 2008–09 season, and finishing with not only the second-best record in the Eastern Conference, but the second-best record in the entire league. The team became one of the only teams in NBA history to beat all of the other 29 teams at least once during the regular season. The Magic swept the Charlotte Bobcats and the Atlanta Hawks in the first two rounds of the playoffs, respectively. They then faced the Boston Celtics in the conference finals. After losing the first three games of the series, Orlando managed to win the next two games, but lost on the road in Game 6, ending their season.

2010–2012: ""Dwightmare"" saga
In anticipation of the team's move to Amway Center, the Magic updated its logo. They retained the streaking ball logo, but changed the wordmark taken from their current uniforms. The Magic hosted the NBA All-Star Game in 2012. The Magic also unveiled black alternate uniforms.
In the summer of 2010 the Orlando Magic signed Chris Duhon, formerly of the New York Knicks, and Quentin Richardson, formerly of the Miami Heat.
On December 18, 2010, having lost five of their last six games, the Magic made a blockbuster trade deal with the Phoenix Suns and the Washington Wizards. They traded Vince Carter, Marcin Gortat and Mickaël Piétrus to Phoenix for Hedo Türkoğlu (who led them into the 2009 NBA Finals when they lost 4–1 against the Los Angeles Lakers), Jason Richardson and Earl Clark. Rashard Lewis was traded to Washington for 3-time All-Star Gilbert Arenas.The Magic finished the season with 52 victories, good for 2nd in the Southeast Division. But they were ousted in six games by the Atlanta Hawks in the first round of the 2011 NBA playoffs, the first time head coach Stan Van Gundy was eliminated early in the playoffs.
In a shortened 2012 season, due to the NBA Lockout, the Magic started the offseason on a rocky note, with their All-Star center, Dwight Howard, requesting a trade to either the New Jersey Nets, Los Angeles Lakers, or Dallas Mavericks. Overlooking the trade request the Magic did a sign and trade with the Boston Celtics for Glen Davis and Von Wafer in exchange for Brandon Bass. The Magic also amnestied Gilbert Arenas and signed Larry Hughes, Justin Harper, and DeAndre Liggins. The Magic started the season on Christmas Day in Oklahoma City against the Thunder. They lost the season opener 97–89. During the month of February, the Magic waived Hughes and signed Ish Smith. On February 26, Orlando hosted the 2012 All-Star Game. The Magic struggled to win games consistently, with concerns about the uncertainty of Dwight Howard's future with the franchise. However, after Dwight rescinded his trade demand and signed a one-year deal in March, the Magic seemed to find their footing again. But then in early April, shortly after it became public that Howard requested coach Van Gundy to be replaced, the center was diagnosed with a herniated disk and forced to have back surgery, thus ending his season. The Magic clinched the sixth seed in the east with a 37–29 record. The Magic were faced with the third-seeded Pacers in the first round. Despite winning the first game of the series the Magic were defeated 4–1.
On May 21, 2012, it was reported that general manager Otis Smith and head coach Stan Van Gundy would part ways with the organization. Stan Van Gundy finished with a 259–135 regular-season record with the team which included making the playoffs in those five years and a conference championship.
CEO Alex Martins announced former Oklahoma City assistant general manager Rob Hennigan as the new general manager for the Orlando Magic on June 20, 2012. Once hired, he became the youngest general manager in the league.In the 2012 NBA draft, the Magic selected Andrew Nicholson and Kyle O'Quinn.
On June 25, 2012, Dwight Howard had face-to-face meeting with general manager Rob Hennigan in Los Angeles and demanded a trade to the Brooklyn Nets.On July 9, 2012, the Magic completed a sign-and-trade deal with the New Orleans Hornets, that sent forward Ryan Anderson to the Hornets. In return the Magic received center Gustavo Ayon.
On July 28, 2012, Jacque Vaughn was named the new head coach. He had been the assistant coach for the San Antonio Spurs for the last two seasons.On August 9, 2012, ESPN reported that a four-team trade would send Dwight Howard to the Los Angeles Lakers. ESPN.com's Marc Stein was told the Lakers were to acquire Howard, Chris Duhon and Earl Clark, the Denver Nuggets were to acquire Andre Iguodala, the Philadelphia 76ers were to acquire Andrew Bynum and Jason Richardson, and the Magic were to acquire Arron Afflalo, Al Harrington, Nikola Vučević, Maurice Harkless, Josh McRoberts, Christian Eyenga, and five total protected future (three 1st round, two 2nd round) picks from each of the other three teams. The deal was officially confirmed and completed on August 10.
Howard left the Magic as their all-time leading scorer, shot blocker, and rebounder.

2012–present: rebuilding
2012–2017: Rob Hennigan era
Following the trade of Dwight Howard, the Magic entered into a state of rebuilding with Maurice Harkless and Nikola Vučević. On August 29, the Magic signed free agent guard E'Twaun Moore. On December 2, 2012, Howard's first game against his former team, the Magic defeated the Lakers 113–103.
On February 21, 2013, the Magic traded JJ Redick, Ish Smith and Gustavo Ayón to the Milwaukee Bucks. In return, the Magic received Beno Udrih, Tobias Harris and rookie Doron Lamb. The Magic also traded Josh McRoberts to the Charlotte Bobcats for Hakim Warrick who was waived 2 days later. The Magic finish the 2012–2013 season 20–62 as the worst record in the NBA, missing the playoffs for the first time since 2006.
On June 27, 2013, the Orlando Magic had the 2nd pick in the 1st round of the 2013 NBA draft. The Magic used their lottery pick to draft Big Ten Defensive Player of the Year, shooting guard Victor Oladipo from Indiana University. The Orlando Magic also had the 51st pick in the 2nd round of the NBA draft. They used this pick to draft 6'8"" forward Romero Osby from the University of Oklahoma. Osby averaged 16 points, 7 rebounds, and 1.3 assists during his senior NCAA season at Oklahoma, but was cut by the Magic before the season opener.
The Magic finished the 2013–2014 season with a 23–59 record, 3rd worst in the NBA. The draft lottery gave them the 4th pick in the 2014 NBA draft. In the draft, they selected Aaron Gordon with the 4th pick and Dario Šarić with the 12th pick. Saric was then swapped for the 10th pick, Elfrid Payton in exchange for a 2017 1st round pick and a future 2nd round pick. Roy Devyn Marble was selected with the 56th pick in the 2nd round. On February 5, 2015, Jacque Vaughn was relieved of his head coaching duties after coaching 2½ seasons for the Magic. His overall record was 58–158. He was replaced by interim head coach James Borrego.
On May 29, 2015, the Magic hired their former point guard Scott Skiles as the franchise's 12th head coach.On June 25, 2015, in the 2015 NBA draft, Orlando selected Mario Hezonja with the fifth overall pick and Tyler Harvey
with the 51st overall pick. On February 16, 2016, the Magic traded Tobias Harris to the Detroit Pistons in exchange for Ersan İlyasova and Brandon Jennings.On May 12, 2016, Skiles stepped down as head coach of the Orlando Magic. On May 19, the Orlando Magic agreed to a deal with former Indiana Pacers coach Frank Vogel to become the next head coach of the team.With Vogel as their new coach, the Magic made many changes to their roster during the offseason. On June 23, 2016, in the 2016 NBA draft the Magic selected Domantas Sabonis 11th overall, but then traded Sabonis and shooting guard Victor Oladipo for defensive power forward Serge Ibaka of the Oklahoma City Thunder. During free agency the Magic re-signed Evan Fournier to a five-year, $85 million contract and also signed Bismack Biyombo, Jeff Green, and D. J. Augustin. On July 15, C. J. Wilcox was acquired, along with cash considerations, from the Clippers in exchange for Devyn Marble and a future second round draft pick.On February 14, 2017, Ibaka was traded to the Toronto Raptors in exchange for Terrence Ross and a future first-round draft pick. The Magic finished the 2016–17 season with the third worst record in their conference, finishing 29–53.

2017–present: Jeff Weltman era
In the summer of 2017, the Magic made various changes, the first being the firing of general manager Rob Hennigan on April 13. On May 23, the Magic named Jeff Weltman, the former general manager of the Toronto Raptors, as president of basketball operations and named John Hammond, the former general manager for the Milwaukee Bucks, as the new general manager. With the sixth overall pick in the 2017 NBA draft, Orlando drafted Florida State forward, Jonathan Isaac. During free agency the Magic signed Jonathon Simmons, Arron Afflalo, Shelvin Mack, Marreese Speights, Khem Birch and Adreian Payne.
On October 6, 2017, the Magic announced that former superstar Tracy McGrady, had rejoined the team as assistant to the CEO.
On February 8, 2018, the Magic traded starting point guard Elfrid Payton to the Phoenix Suns in exchange for a 2018 second-round pick.On April 12, 2018, head coach Frank Vogel was fired by the Magic after the conclusion of the 2017–18 regular season. On May 30, the Magic named Steve Clifford as their new head coach.On June 21, 2018, the Magic drafted center Mohamed Bamba with the sixth overall pick. and Melvin Frazier in the second round. Other player acquisitions included trading for Timofey Mozgov and Jerian Grant in a three-team trade for Bismack Biyombo, Jarell Martin in a trade from the Memphis Grizzlies, and free agent Isaiah Briscoe.
On September 6, 2018, team owner Richard DeVos died aged 92 from complications from an infection.During the 2018–19 season, the Magic won their sixth division title and finished in seventh place in the Eastern Conference to clinch a playoff berth for the first time since the 2011–12 season. Mohamed Bamba was diagnosed with stress fracture on February 5, 2019, and eventually missed the remainder of the season. At the NBA trade deadline, the Magic traded Jonathon Simmons and two draft picks to the Philadelphia 76ers for Markelle Fultz. Nikola Vučević was selected as a reserve for the 2019 Eastern Conference All Star Team, being the first Orlando Magic All Star player since Dwight Howard in 2012. In March 2019, the Magic signed former NBA Rookie of the Year Michael Carter-Williams to two ten-day contracts before signing him to a one-year deal. The Magic were matched up against the Toronto Raptors in the first round of the 2019 NBA playoffs. While the Magic won their first playoff game in seven years, the Toronto Raptors won the series 4–1.
In the 2019 NBA draft the Magic selected Chuma Okeke with the 16th overall pick, and traded its second-round pick to the Los Angeles Lakers in exchange for $2.2 million and a future draft pick. With the prospect of limited playing time due to injury, the Magic and Okeke agree to sign a one-year deal with the Magic's G League affiliate in Lakeland and sign Okeke's rookie contract in the summer of 2020. During the 2019 free agency, the Magic re-signed with Vučević, Terrence Ross, Michael Carter-Williams, and Birch, signed free agent Al-Farouq Aminu, and waived Mozgov. The Magic compiled a 30–35 record before the league suspended its season on March 11, 2020, due to the COVID-19 pandemic. Following the suspension of the season, the Magic were one of the 22 teams invited to the NBA Bubble to participate in the final eight seeding games of the regular season. Season ending injuries to Jonathan Isaac and Mo Bamba, along with significant injuries to Terrence Ross and Evan Fournier hampered their performance and they went 3–5 in the seeding games, but the team earned the 8th playoff spot in the Eastern Conference to face the Milwaukee Bucks. While the Magic won the opening game, Milwaukee won the next four ending the Magic's season in round one for the second year in a row.
In the 2020 NBA draft the Magic selected Cole Anthony with the 15th overall pick. The shortened free agency period followed where the Magic signed their 2019 draft pick Chuma Okeke, re-signed James Ennis, Michael Carter-Williams, and Gary Clark, while adding Dwayne Bacon. The injury Jonathan Isaac suffered during the previous season was significant enough that he was ruled out for the entire 2020–21 season. In the 8th game 2020-2021 NBA Season, starting point guard, Markelle Fultz went down with an ACL tear in a game against the Cleveland Cavaliers. After the season-ending injury, rookie point guard, Cole Anthony would fill the starting point guard role. On January 20, 2021, Fultz underwent surgery. At the 2021 NBA trade deadline the Magic traded away Nikola Vučević, Aaron Gordon, and Evan Fournier, each to different teams, in what has been viewed as the beginning of another rebuilding period. At the end of the season, the team and Clifford mutually agreed to part ways.On July 11, 2021, Jamahl Mosley was named head coach after spending the previous season as an assistant coach with the Dallas Mavericks.On July 29, 2021, the Magic selected Jalen Suggs with the 5th pick and Franz Wagner with the 8th pick (Acquired via the Chicago Bulls in the Nikola Vučević trade) in the 2021 NBA draft. The Magic also selected Orlando native Jason Preston in the second round and promptly traded him on draft night to the Los Angeles Clippers.
On June 23, 2022, the Magic selected Paolo Banchero with the 1st pick and Caleb Houstan with the 32nd pick in the 2022 NBA draft.

Home arenas
Amway Center
The team's current home arena, the Amway Center, officially opened on October 1, 2010. The Orlando Magic hosted their first preseason game at Amway Center on October 10 against the New Orleans Hornets. The 2010–11 regular-season home opener was on October 28 against the Washington Wizards, and the Magic won both games. In 2012, the Amway Center hosted the All-Star Weekend.At the time it opened, the new Amway Center was home to the largest Jumbotron in the NBA. The arena also features approximately 2,100 feet (640 m) of digital ribbon boards, and outside the building a 46 feet (14 m) by 53 feet (16 m) video display is visible to motorists traveling on Interstate 4.The Amway Center is also the home of the minor league hockey team, Orlando Solar Bears and the Orlando Predators of the National Arena League (NAL).

Amway Arena (former arena)
Amway Arena opened in 1989 and served as home to the Orlando Magic since their inception until the 2009–2010 season. It was originally known as the Orlando Arena, or the ""O-Rena"", during its first 10 years. In 1999, TD Waterhouse purchased the naming rights and named the venue the TD Waterhouse Centre. In December 2006, the naming rights were purchased by Amway for four years. It is also home of the Arena Football League's Orlando Predators, the Orlando Sharks of the Major Indoor Soccer League, and various sporting and entertainment events. Amway Arena was one of ""The Orlando Venues"" owned and operated by the City of Orlando. The other facilities include the Bob Carr Performing Arts Centre, Tinker Field, Camping World Stadium, Harry P. Leu Gardens, and Mennello Museum.

Team identity
Logos and uniforms
Orlando advertising agency The Advertising Works, led by its president Doug Minear was responsible for the original Magic uniforms. The logo, featuring a basketball crowded by stars and the wordmark ""Magic"" with a star replacing the A, was created following meetings with Walt Disney World artists and over 5000 suggestions sent from around the country. Stars would remain a primary feature of the logo once it was redesigned in 2000 to feature a comet-like basketball. Pat Williams first suggested the colors black and gold of his alma mater Wake Forest, but this was eschewed for various factors, including the local college UCF using the same scheme. Black would still be the primary color in the scheme used by Minear, a trait shared by 16 other NBA teams. Other colors were an electric blue specially made by sporting goods manufacturer MacGregor, and silver. The home uniforms were white with black pinstripes, featuring black numbers with blue trim, and the road jerseys reversed the scheme while featuring ""Orlando"" instead of the Magic logo. Given the standard mesh nylon worn across the NBA did not allow for pinstripes, the jerseys were made out of durene, a material with cotton on the underside and polyester bonded on the outside. The road uniform was changed to blue with white pinstripes in 1994–95, although the black uniforms remained in use as alternates.For the Magic's 10th anniversary in 1998–99, a new look designed by fashion designer Jhane Barnes was unveiled. The pinstripes were dropped and the uniforms now featured stars as the background. Both jerseys, made out of the dazzle that was used in the Women's National Basketball Association uniforms, had the Magic logo, with the home jersey in white and the away in blue.The Magic's 15th anniversary in 2003–04 inspired another uniform revamp, opting for a cleaner look without stripes or stars. The home jerseys were white and the Magic logo was blue with silver and black trim. The away jersey reverted to the city name, and was blue. The logo and numbers are white with black trim. During this time, the team's original pinstriped jerseys were worn several times per year as alternates: black in 2003–04 and 2006–07, blue in 2004–05, and white in 2005–06.
For the 2008–09 season, the Magic have once again introduced new uniforms. The Magic returned to the pinstriped uniforms to commemorate the team's 20th anniversary. The current design combines the elements of the previous three uniform designs the Magic used in its 20-year history. The home jerseys are white with silver pinstripes, while the away jerseys are blue with white pinstripes. The font used for the number and player/team name has also been updated to a more modern look. Magic alternate logos are on the shorts and the back of the jersey. This is the fourth model in franchise history.As the Magic moved to the Amway Center in 2010, they unveiled a new logo that for the first time fully spelled ""Magic"", without the star instead of the A. They also unveiled a black alternate uniform, with silver pinstripes, mirroring the regular blue road uniform. They were usually worn as throwback uniforms as part of the NBA Hardwood Classics program. A variation of the uniform is also used for Noche Latina every March, with 'Orlando' substituted for 'El Magic', with 'El' in black and silver trim and 'Magic' in blue and silver trim. This was unveiled in the 2011–12 season.In 2014, Magic unveiled a silver uniform for the first time in their history. It was sleeved, and featuring white pinstripes along with blue, black and white trim for the letters and numbers. Unlike the three other uniforms, a different striping pattern will be used on the sides. In 2016, The Magic unveiled a third alternate uniform, featuring carbon as the primary color and without pinstripes. Named “Stars”, it featured the team's secondary logo and a blue, white and blue tricolor stripe in front, along with white lettering.The Magic made only a few slight tweaks to their uniforms when Nike became the league's uniform provider in 2017. From 2017 to 2019, the Magic wore white ""Association"" uniforms, blue ""Icon"" uniforms and black ""Statement"" uniforms. With the exception of the black uniforms eliminating the blue side stripes and changing pinstripe colors from silver to blue, the set remained almost identical to the previous Adidas set. Beginning with the 2019–20 season, the black uniform became the ""Icon"" uniform while a new blue uniform served as the ""Statement"" uniform. The uniform was inspired from the different sets the Magic wore through the years, such as the silver star and black stripes with white pinstripes from the 1989–98 set, and the cleaner look of the 2003–08 set. This uniform was then tweaked prior to the 2022–23 season, adding black pinstripes (a nod to the 1989–98 uniforms) and black stripes with subtle star patterns (a nod to the 1998–2003 uniforms). Starting in 2020–21, the ""Statement"" uniforms would feature Jordan Brand's jumpman logo. Their jersey's sponsor is Disney.An annual ""City"" edition is also utilized by Nike to honor either local culture or team tradition. The Magic's 2017–18 ""City"" uniform featured a printed pattern of stars in the sky along with the team's alternate logo in front. For the 2018–19 season, the Magic only made slight changes to their ""City"" uniform, with the printed pattern of stars relegated to the sides and a predominantly black base. The 2019–20 City uniforms featured an ""anthracite"" (grey) base with orange lettering. The orange color pays homage to Florida's orange-growing industry. The Magic kept the orange theme for their 2020–21 ""City"" uniform, this time with a white base and orange letters and numerals with anthracite trim. The uniform also paid homage to the original uniforms worn from 1989 to 1998, featuring ""Orl"" beside an anthracite star in its original typeface along with orange pinstripes. When the jersey's colors are inverted it shows the exact color scheme used in Orlando's original uniforms. In the 2021–22 season, the Magic again reprised the orange theme for its ""City"" uniform, this time featuring a few elements from previous uniforms. For their 2022–23 ""City"" uniform, the Magic went with a black base, dark gray pinstripes, and gothic-inspired white letters with blue trim to represent the team and the city as a kingdom on the rise.Having qualified for the 2020 NBA playoffs the previous year, the Magic were also given an ""Earned"" uniform. The design, which has a white base, featured the alternate logo in front (an homage to the 2016–17 ""Stars"" alternate and 2017–19 ""City"" uniform) and blue numbers with black and silver trim. Side panels featured the stars pattern as an homage to the 1998–2003 blue uniforms.The Magic are one of seven teams to wear ""Classic"" uniforms for the 2018–19 season. As part of their 30th anniversary, the team unveiled the uniform similar to those worn from 1994 to 1998.

Mascot
Stuff the Magic Dragon has been the Magic's mascot since 1989. A dragon designed by Wade Harrison and Bonnie Erickson of Acme Mascots, Inc, his name is a pun on Puff the Magic Dragon, and how a slam dunk is also known as ""stuffing"".

Players
Current roster
Retained draft rights
The Magic hold the draft rights to the following unsigned draft picks who have been playing outside the NBA. A drafted player, either an international draftee or a college draftee who is not signed by the team that drafted him, is allowed to sign with any non-NBA teams. In this case, the team retains the player's draft rights in the NBA until one year after the player's contract with the non-NBA team ends. This list includes draft rights that were acquired from trades with other teams.

Retired numbers
Notes:

1 The number was unretired in the 2001–02 season for Patrick Ewing.
The NBA retired Bill Russell's No. 6 for all its member teams on August 11, 2022.

Basketball Hall of Famers
Notes:

1 In total, Ewing was inducted into the Hall of Fame twice – as player and as a member of the 1992 Olympic team. Also served as assistant coach in 2007–2012.
2 In total, Daly was inducted into the Hall of Fame twice – as coach and as a member of the 1992 Olympic team

FIBA Hall of Famers
Notes:

1 In total, Daly was inducted into the FIBA Hall of Fame twice – as coach and as a member of the 1992 Olympic team.

Orlando Magic Hall of Fame
In 2014, the Orlando Magic launched the team's Hall of Fame, which honors players, coaches and executives who have had a major impact for the team and in the community.

Head coaches
Season-by-season record
List of the last five seasons completed by the Magic. For the full season-by-season history, see List of Orlando Magic seasons.
Note: GP = Games played, W = Wins, L = Losses, W–L% = Winning percentage

Rivalries
Miami Heat
The Orlando Magic and the Miami Heat had a rivalry because both teams are located in Florida, thus the rivalry was known as the Sunshine State rivalry. Another ingredient to the rivalry was the high-caliber players on both teams such as Orlando's Shaquille O'Neal and Penny Hardaway to Miami's Alonzo Mourning and Tim Hardaway. The two had met each other in the NBA playoffs for the first time in 1997, with Miami beating Orlando 3–2, they have not met in the playoffs since.
The rivalry intensified with the rising stardom of Miami's and Orlando's Dwyane Wade and Dwight Howard, along with Miami's acquiring high-caliber stars such LeBron James from the Cleveland Cavaliers and Chris Bosh from the Toronto Raptors and in 2010, resulting in fierce competition between the two.
When Dwight Howard departed from the Magic to the Los Angeles Lakers in August 2012, the rivalry softened. The Orlando Magic have been going through a process of rebuilding ever since then.

Atlanta Hawks
The Atlanta Hawks and the Orlando Magic had an intense rivalry, mostly stemming from playoff competitions and the rising stardom of Dwight Howard and Josh Smith, both from the 2004 NBA draft and who were both raised in Georgia.
The two teams faced each other three times in the 1996, 2010, and 2011 NBA playoffs. The Magic defeated the Hawks in the second round of the 1996 playoffs 4–1, and swept the second-round series 4–0 in the 2010 playoffs, while the Hawks eliminated the Magic 4–2 in the first round of the corresponding 2011 playoffs.

Media
Television
The current television announcing team for the Orlando Magic is play-by-play announcer David Steele and color analyst Jeff Turner. Turner played for the Magic from its inaugural 1989 season to 1996. Paul Kennedy and Dante Marchetelli serve as courtside reporters, while Marchetelli, former coach Brian Hill, and former Magic player Nick Anderson host the pre-game, halftime and post-game shows. Television broadcasts were split in 2007–08 between Fox Sports Florida and Sun Sports (later Fox Sports Sun, currently Bally Sports Sun). In the 18 years before then, broadcasts were split between Sun Sports (formerly known as the Sunshine Network) and local television stations, originally WKCF and, later, WRBW.
There was a controversy with moving broadcasts to Fox Sports Florida since Orlando's largest cable TV provider, Bright House Networks, did not carry the network. Pressure increased for the cable provider to pick up FS Florida in time for the 2007–08 NBA season but this did not happen. The Magic persisted with airing games on FS Florida into the 2008–09 season despite Bright House's refusal to pick up the channel in all of its affiliates. Bright House and FS Florida came to an agreement on January 1, 2009, and began airing the channel as part of its standard cable package. However, Bright House airs the channel using a digital signal that only allows customers who own the Digital cable box to receive the channel. The customers do not have to pay any additional costs to get the channel with their Digital cable box.
As of the 2020–21 season, all Magic games are now on Bally Sports Florida.

Radio
After the end of the 2019–20 season, the Orlando Magic decided to not to renew its contract with play-by-play announcer Dennis Neumann and color analyst Richie Adubato, another former Magic head coach. Games are produced by Magic Radio Network flagship AM 580 WDBO in Orlando, and also broadcast on AM 1380 WELE in Daytona Beach, 99.5 FM WGMW ""The Star"" in Gainesville and Ocala, AM 1290 WPCF in Panama City, AM 1590 WPSL in Port St. Lucie and AM 1450 WSTU in Stuart. The affiliate in Tallahassee is AM 1270 ""My 94.3"" WTLY. The immediate Tampa Bay area has no affiliate although AM 1340 in Clearwater WTAN is listed on the team's website. The Magic will continue to have a radio presence using the audio from its television broadcasts.The flagship broadcast was simulcast on WDBO-FM during the 2011–12 NBA season while that station moved from AM to FM. When WDBO re-formatted from talk radio to sports radio, it retained the flagship status. However, WOEX (the former WDBO-FM) still simulcasts Magic games in Central Florida.

Podcasts
The official Orlando Magic website features a collection of podcasts available on iTunes, including ""Magic Overtime with Dante and Galante"".

Leaders
Franchise leaders
Bold denotes still active with team.
Italic denotes still active but not with team.
Points scored (regular season) (as of the 2022–23 NBA season)

Other statistics (regular season) (as of the 2022–23 NBA season)

Individual records
Most points in one game with 62 (Tracy McGrady on March 10, 2004, vs. Washington Wizards)
Most points in one half with 37 in the first half (Tracy McGrady on March 9, 2003, vs. Denver Nuggets)
Most points in one quarter with 25 in the second quarter (Tracy McGrady on March 9, 2003, vs. Denver Nuggets)
Most free throws made in one game with 21 (Dwight Howard on January 12, 2012, vs. Golden State Warriors
Most free throws attempted in one game with 39 (Dwight Howard on January 12, 2012, vs. Golden State Warriors) ***NBA Record
Most points in a playoff game with 46 (Dwight Howard in Game 1 of 2011 Eastern Conference playoffs, First round vs. Atlanta Hawks and Tracy McGrady in Game 2 of the 2003 Eastern Conference playoffs, First round vs. Detroit Pistons)
Most assists made in one game with 30 (Scott Skiles on December 30, 1990, vs. Denver Nuggets) ***NBA Record
Most rebounds in one game with 29 (Nikola Vučević on December 31, 2012, vs. Miami Heat)

Franchises accomplishments and awards
Individual awards
NBA All-Star Weekend
NBA All-Star Team

Shaquille O'Neal – 1993–1996
Penny Hardaway – 1995–1998
Tracy McGrady – 2001–2004
Grant Hill – 2001, 2005
Dwight Howard – 2007–2012
Rashard Lewis – 2009
Jameer Nelson – 2009
Nikola Vučević – 2019, 2021NBA All-Star head coaches

Brian Hill – 1995
Stan Van Gundy – 2010

Notes
Passage 3:
1995 NBA Finals
The 1995 NBA Finals was the championship round of the 1994–95 National Basketball Association (NBA) season. The series pitted the Eastern Conference champion Orlando Magic against the defending NBA champion and  Western Conference champion Houston Rockets. The pre-series hype and buildup of the Finals was centered on the meeting of the two centers opposing each other: Shaquille O'Neal of the Magic and Hakeem Olajuwon of the Rockets. Going into the series the matchup was compared to the Bill Russell–Wilt Chamberlain matchup of the 1960s.
The Rockets became the first team in NBA history to beat four 50-win teams in a single postseason en route to the championship. The Rockets would win a playoff-record nine road games in the 1995 playoffs. It was the second NBA Finals sweep in the 2–3–2 Finals format (after the Detroit Pistons did so against the Los Angeles Lakers in 1989). The Rockets also became the first repeat NBA Champion in history to keep the title with a sweep. In addition, the Rockets became the first team in NBA history to win the title without having home-court advantage in any of the four playoff rounds since the playoffs was expanded to a 16 team format in 1984. Coincidentally, this feat would also be achieved in the NHL by the New Jersey Devils that same year, when they won the Stanley Cup over the Detroit Red Wings.
The Orlando Magic, making their first NBA Finals appearance, began the series at home, hosting the defending champion Houston Rockets. With the Magic up 110–107 late in Game 1, Nick Anderson missed four consecutive free throws in the closing seconds of the game, and Kenny Smith hit a three-pointer, tying the game and sending it to overtime as well as setting a new record at the time, with the most three-pointers in an NBA Finals game with seven. The more experienced Rockets went on to win in overtime and eventually swept the Magic, winning their second consecutive NBA Championship. In achieving this, they earned the distinction of being the only team to win both championships during Michael Jordan's first retirement (although Jordan did return in the closing months of the 1994–95 season), in addition to being the only team other than the Chicago Bulls to win multiple championships in the 1990s. This was also the second consecutive 90s championship series not featuring the Chicago Bulls, a streak the Bulls would end in 1996. 
The season-ending documentary Double Clutch by Hal Douglas, was released by NBA Entertainment to coincide with the Rockets' championship season.

Background
Houston Rockets
The Rockets entered the 1994–95 season as defending champions. They had won their first eight games of the season, the first defending champions to have won their first eight games of their season since the 1987-88 Lakers. However, they struggled to maintain last season's form due to injuries and off-court-distractions. On February 14, the Rockets acquired Clyde Drexler from the Portland Trail Blazers, but the trade of a hometown hero (Drexler was a teammate of Olajuwon at the University of Houston) did not improve matters, and the Rockets settled for the sixth seed with a 47–35 record.
However, Houston once again lived up to its Clutch City reputation come playoff time. En route to the Finals, the Rockets defeated three teams with 55 or more victories. They began by ousting the Utah Jazz in six games (the Rockets trailed 2–1 after three games), then repeating last season's comeback effort over the Phoenix Suns (wherein the Rockets trailed 3–1 after four games). In Game 7 of that series, Phoenix led Houston 51-42 after the first half before Houston mounted a comeback to get the series win, 115-114. After dispatching the Suns, the Rockets outclassed the top-seeded San Antonio Spurs in six games of the conference finals. They also became the first team in NBA History to have lost all their home games of the series but won all road games of that certain series thus advancing to the next round.

Orlando Magic
The Magic were only in their sixth season of existence, but they were a team on the rise. Led by All-Stars Shaquille O'Neal and Penny Hardaway, new acquisition Horace Grant, and franchise cornerstones Nick Anderson and Dennis Scott, the Magic rolled through the Eastern Conference, winding up with a then-franchise best 57–25 mark.
Orlando's road to the Finals began with a convincing 3–1 series win over the Boston Celtics. They followed it up with a six-game ouster of Michael Jordan (returning from an 18-month retirement) and the Chicago Bulls in the second round, and in the conference finals, they vanquished the Indiana Pacers in a tough seven-game series.

Road to the Finals
Regular season series
The Orlando Magic won both games in the regular season series:

1995 NBA Finals rosters
Houston Rockets
Orlando Magic
Series summary
This was one of only two NBA Finals in which the team who did not have home court advantage swept the series, (the other being the 1975 Finals, in which the Golden State Warriors swept the Washington Bullets).

All times are in Eastern Daylight Time (UTC−4).

Game 1
Orlando led 110-107 with 10.5 seconds left, when Nick Anderson was intentionally fouled to send him to the free-throw line. Normally a respectable free-throw shooter (70.4 percent in the regular season), Anderson missed both of his free throws, but was able to grab the offensive rebound after the second miss and was fouled again. Anderson shockingly missed the next two free throws, and Houston grabbed the rebound, and would tie the game with 1.6 seconds left on Kenny Smith's 3-point shot. The shot was one of Smith's seven made 3-point shots, setting a then-Finals record. In overtime, Hakeem Olajuwon tipped in a missed finger roll by Clyde Drexler with three-tenths of a second left to win the game. Hakeem Olajuwon finished the game with 31 points, 6 rebounds, 7 assists and 4 blocks while Kenny Smith recorded 23 points and 9 assists. The four consecutive missed free-throws by Nick Anderson would haunt him for the rest of his career. After the Finals, Anderson would shoot only 60.5% on free-throws for the rest of his career.

Game 2
Hakeem Olajuwon recorded a double-double with 34 points and 11 rebounds to lead the Rockets to a 117-106 victory and a 2-0 series lead. The Magic, on the other hand, became the 2nd team in NBA Finals history to lose the first two of their four home games.

Game 3
Robert Horry hit a three-pointer to give Houston a 104-100 lead with 14.1 seconds left.  Orlando's Anfernee Hardaway then missed a three-pointer, and the rebound deflected off Dennis Scott and out of bounds with 6.8 seconds left, turning the ball over to Houston.  Clyde Drexler was immediately fouled.  He missed his first free throw and made the second for a 105-100 Rockets lead with 5.9 seconds left.  Nick Anderson hit a three-pointer with 2.7 seconds left to bring the Magic within two points, and then Sam Cassell was immediately fouled.  He needed to make both free throws to likely seal it, but missed the first. He made the second to give the Rockets a three-point lead with 2.2 seconds left.  After a timeout to advance the ball to midcourt, the Magic had one last chance to tie the game and force overtime, but Hardaway missed a three-pointer as the buzzer sounded.  The Rockets held on for a 106-103 win in Game 3 to take a 3-0 series lead and were one win away from their second consecutive NBA title.

Game 4
At the end of the first half, the Magic had a 4-point advantage over the home team, Rockets. However, the Rockets had another notable comeback as they outscored the Magic 66-50 in the second half, thus winning their second consecutive NBA championship. Olajuwon outscored O'Neal by 10 points and capped off the sweep by hitting a memorable yet uncharacteristic 3-pointer in front of O'Neal. When accepting the Larry O'Brien Trophy on the floor of The Summit, Rockets head coach Rudy Tomjanovich said ""Don't ever underestimate the heart of a champion!"" Olajuwon, with his 35 point and 15 rebound performance, was named Finals MVP for the second straight year.

Olajuwon v. O'Neal
Although both centers played well, Olajuwon outscored O'Neal in every game of the series and became one of the few players in NBA history to score at least 30 points in every game of an NBA Finals series:
By winning his second straight NBA Finals MVP award, Hakeem Olajuwon became the sixth player to win the award on multiple occasions, joining Willis Reed, Kareem Abdul-Jabbar, Magic Johnson, Larry Bird, and Michael Jordan. Jordan and Olajuwon at the time were the only players to win the award consecutively.

Player statistics
Houston RocketsOrlando Magic

Media coverage
In the United States, the NBA Finals was broadcast on NBC television, with Marv Albert, Matt Guokas and Bill Walton calling the action. Ahmad Rashad, Hannah Storm, and Jim Gray served as sideline reporters, and studio coverage was handled by Bob Costas, Julius Erving and Peter Vecsey.
National radio coverage was provided by the NBA Radio Network, with Joe McConnell and Wes Unseld on the call. After the season, NBA Radio was dissolved and subsequent national radio broadcasts of the NBA Finals would be handled by ESPN Radio.

Aftermath
The Rockets' title reign ended in 1996, when they were swept by the Seattle SuperSonics in the second round. The Sonics were also the last team to beat the Rockets in the playoffs prior to their championship run, having eliminated them in the second round in 1993. That year, the Rockets won 48 games and achieved the fourth seed. The following offseason, they acquired former NBA MVP Charles Barkley in a trade, but age and injuries would take its toll and the closest the Rockets would achieve after their two-year championship reign was a six-game loss to the Utah Jazz in the 1997 Western Conference Finals.
The Magic won a franchise-record 60 games in the 1995–96 NBA season, but were swept by the Chicago Bulls in the 1996 Eastern Conference Finals. During the offseason, Shaquille O'Neal signed with the Los Angeles Lakers, with whom he went on to win three championships; he later added a fourth title to his resume with the Miami Heat in 2006. The Magic would not return to the Finals until 2009, which they lost to the Lakers in five games.",['Eastern Conference champion Orlando Magic against the Western Conference champion Houston Rockets.'],11880,hotpotqa_e,en,,726f8ef3f7163c31b7a13fd8cbd5f6d2a2a8f1f39dea3669,Eastern Conference champion Orlando Magic against the Western Conference champion Houston Rockets.,98
Which two features were played up the person who had the biggest net worth in 2017?,"Passage 1:
Red Bull
Red Bull is a brand of energy drinks created and owned by the Austrian company Red Bull GmbH. With a market share of 43%, it is the most popular energy drink brand as of 2020, and the third most valuable soft drink brand behind Coca-Cola and Pepsi. Since its launch in 1987, more than 100 billion cans of Red Bull have been sold worldwide, including over 11.5 billion in 2022.Originally available only in a single nondescript flavor sold in a tall and slim silver-blue can, called Red Bull Energy Drink, numerous variants of the drink were added over the course of time. Its slogan, ""Red Bull Gives You Wings"", is considered one of the most popular and memorable advertising slogans in the United States, ranking at 16 out of 25 with a 59.3% slogan recognition rate according to a study by advertising and market research firm Survata. Rather than following a traditional marketing approach, Red Bull has generated awareness and created a ""brand myth"" through proprietary extreme sport event series such as Red Bull Cliff Diving World Series, Red Bull Air Race, Red Bull Crashed Ice and standout stunts such as the Stratos space diving project. In addition to sport series, its marketing includes multiple sports team ownerships; celebrity endorsements; and music, through its Red Bull Records label.Red Bull was first derived from a similar drink called Krating Daeng that originated in Thailand and was introduced by the pharmacist Chaleo Yoovidhya. While doing business in Thailand, Dietrich Mateschitz purchased a can of Krating Daeng and claimed it cured his jet lag. Mateschitz sought to create a partnership with Yoovidhya and formulated a product that would suit the tastes of Westerners, such as by carbonating the drink. In 1984, the two founded Red Bull GmbH in Fuschl am See, Salzburg, Austria. When branding their new product, Mateschitz referenced Krating Daeng's name: in Thai, daeng means red, and a krating (known in English as a gaur or Indian bison) is a large species of wild bovine native to the Indian subcontinent. In 1987, the company sold its first can of Red Bull in Austria. In 1996, Red Bull began operation in the United States, and has seen steady growth ever since. Both Red Bull and Kraeting Daeng use the same red bull on yellow sun logo while continuing to market their drinks separately in the Thai and Western markets.

History
In 1976, Chaleo Yoovidhya introduced a drink called Krating Daeng in Thailand, which means ""red gaur"" in English. It was popular among Thai truck drivers and labourers. While working for German manufacturer Blendax (later acquired by Procter & Gamble) in 1982, Dietrich Mateschitz travelled to Thailand and met Chaleo, owner of T.C. Pharmaceutical. During his visit, Mateschitz discovered that Krating Daeng helped cure his jet lag. In 1984, Mateschitz co-founded Red Bull GmbH with Yoovidhya and turned it into an international brand. Each partner invested US$500,000 of savings to fund the company. Yoovidhya and Mateschitz each held a 49 percent share of the new company. They gave the remaining two percent to Yoovidhya's son, Chalerm, but it was agreed that Mateschitz would run the company. The product was first launched in Austria on 1 April 1987.In Thailand, energy drinks are most popular with blue-collar workers. Red Bull re-positioned the drink as a trendy, upscale drink, first introducing it at Austrian ski resorts. Pricing was a key differentiator, with Red Bull positioned as a premium drink and Krating Daeng as a lower cost item. In many countries, both drinks are available, dominating both ends of the price spectrum. The flavouring used for Red Bull is still produced in Bangkok and exported worldwide. Gary Smith is one of the co-CEOs of Red Bull. As a senior board member and corporate secretary between 2000 and 2007, Mr. Smith was also responsible for all day-to-day operations of the company as the COO, including sales, trade marketing, motorsports marketing, finance, information systems, legal, supply chain, operations, and human resources.During the 1990s, the product expanded into Hungary, Slovenia, Germany, the United Kingdom, and the United States. It entered Germany and the UK in 1994, the United States (via California) in 1996 and the Middle East in 2000. In 2008, Forbes magazine listed both Chaleo and Mateschitz as the 250th richest people in the world with an estimated net worth of US$4 billion.Mateschitz died on 22 October 2022 aged 78, following a long illness.Red Bull GmbH is headquartered in Fuschl am See, an Austrian village of about 1,500 inhabitants near Salzburg. The company is 51 percent controlled by the Yoovidhya family who, for technical reasons, own the trademark in Europe and the US.

Ingredients
Depending on the country, Red Bull contains different amounts of caffeine, taurine, B vitamins (B2, B3, B5, B6, B12), glucuronolactone and simple sugars (sucrose and glucose) in a buffer solution of carbonated water, sodium bicarbonate and magnesium carbonate (substituted in some flavours with a trisodium citrate/citric acid buffer, each solution providing electrolytes). To produce Red Bull Sugarfree, sucrose and glucose have been replaced by artificial sweeteners acesulfame K and aspartame or sucralose.

Health effects
Claims about the drink's effects and performance have been challenged on various occasions, with the UK's Advertising Standards Authority imposing advertising restrictions in 2001 in response to complaints recorded as early as 1997.Energy drinks have the effects that caffeine and sugar provide, but experts still argue about the possible effects of the other ingredients. Most of the effects of energy drinks on cognitive performance, such as increased attention and reaction speed, are primarily due to the presence of caffeine. There is evidence that energy drinks can increase mental and athletic performance. A study funded by Red Bull GmbH, which did not include a caffeine-only control group, found that performance during prolonged driving is increased after consumption of Red Bull. Other tests for physical performance showed results such as increased endurance and power. Red Bull energy drink increased upper body muscle endurance during repeated Wingate tests in young healthy adults. Excessive or repeated consumption of energy drinks can lead to cardiac and psychiatric conditions.The European Food Safety Authority (EFSA) concluded that exposure to taurine and glucuronolactone at the levels presently used in energy drinks is not a safety concern. In a separate analysis, they also concluded that there is insufficient evidence to support a number of commercial health claims about taurine. A review published in 2008 found no documented reports of negative or positive health effects associated with the amount of taurine used in energy drinks, including Red Bull.

Caffeine content
The caffeine content of a single 250ml can of Red Bull is approximately 40–80 mg / 250 ml (15–32 mg / 100 ml). The caffeine level in Red Bull varies depending on country, as some countries have legal restrictions on how much caffeine is allowed in drinks. As is the case with other caffeinated beverages, Red Bull drinkers may experience adverse effects as a result of overuse. Excessive consumption may induce mild to moderate euphoria primarily caused by stimulant properties of caffeine and may also induce agitation, anxiety, irritability and insomnia.The general population of healthy adults is not at risk for potential adverse effects from caffeine if they limit their consumption to 400 mg per day, which is provided by 5 standard 250 ml cans. Consumption of a single energy drink will not lead to excessive caffeine intake. Adverse effects associated with caffeine consumption in amounts greater than 400 mg include nervousness, irritability, sleeplessness, increased urination, abnormal heart rhythms (arrhythmia), and dyspepsia. Consumption also has been known to cause pupil dilation when taken with certain antidepressants or SSRIs. Caffeine dosage is not required to be on the product label for food in the United States, unlike drugs, but some advocates are urging the Food and Drug Administration (FDA) to change this practice. (Red Bull voluntarily lists the caffeine content in each can along with the ingredient list.)

Variants
Over the years, Red Bull has offered many variations of its drink, all based on the same formula but differing in taste and colour.Red Bull began offering variations on its drinks in 2003 with a sugar-free version of the drink with a different flavor from the original, called Red Bull Sugarfree. In 2012, Red Bull released Red Bull Total Zero, a variant with zero calories. In 2018, the company released Red Bull Zero, a different sugar-free formulation designed to taste more like the original flavor.In 2009, Red Bull unveiled a highly concentrated variant of its drink called Red Bull Energy Shot, supplied in 2 oz (60 ml) cans.The company began expanding its flavor offerings in 2013 with the launch of Red Bull Editions. Initially available in cranberry, lime, and blueberry, the Editions line has expanded to include a variety of flavours, including some available only during specific seasons or in certain regions.

Other products
Red Bull released a cola drink, called Simply Cola, in 2008. A new version of the cola was released in 2019, as part of Red Bull's Organics line.
In 2018, the company launched Organics by Red Bull, a line of organic sodas with four flavours; bitter lemon, ginger ale, tonic water, and a new version of Red Bull Simply Cola.

Regulatory approval and legal status
Authorities in France, Denmark, and Norway initially did not permit the sale of Red Bull. However, as of 2021, it is on sale in all 27 member states of the European Union and in 171 countries around the world.The French food safety agency was concerned about taurine; a Red Bull drink that did not contain taurine was introduced. The French refusal of market approval was challenged by the European Commission, and partially upheld by the European Court of Justice in 2004. The French food safety agency relented in 2008, because it was unable to prove a definite health risk, taurine-related or not.

Litigation
In 2013, Red Bull told the Redwell Brewery, a Norfolk micro brewery, to change its name or face legal action, because it sounded too similar to Red Bull. The eight-man brewery in Norwich was told its name could ""confuse"" customers and ""tarnish"" its trademark. The two companies reached a settlement permitting Redwell to continue using its name.In 2014, Red Bull entered into a US$13 million settlement to resolve two consumer class action lawsuits in the United States District Court for the Southern District of New York. Named as plaintiffs were Benjamin Careathers, David Wolf, and Miguel Almarez, who had sued the company claiming breach of express warranty and unjust enrichment, saying that Red Bull falsely asserted performance-enhancing benefits from the drink's ingredients that were unsubstantiated by scientific studies. On 1 May 2015 the Court approved the settlement, giving customers who had submitted claims the opportunity to receive a US$10 cash reimbursement or US$15 in Red Bull products within 150 days of affirmance on any appeal. Contrary to reports from some news outlets, the plaintiffs had not alleged that the drink did not give consumers actual wings.

Advertising, sports team ownership, and sponsorships
Since 1997, Red Bull has been making commercials bearing its slogan ""Red Bull gives you wings."" Commercials usually were crudely animated and featured characters with constant squints.
Red Bull's international marketing campaign is largely linked to extreme sports. These range from motorcycle racing, such as MotoGP, Dakar Rally, motorcycle speedway, mountain biking, aerobatics, BMX, motocross, windsurfing, snowboarding, skateboarding, kayaking, rowing, wakeboarding, cliff-diving, parkour, surfing, skating, freestyle motocross, rallycross, Formula 1 motor racing, NASCAR racing, to breakdancing. Red Bull uses music and video games, and has enlisted celebrities, such as Eminem (sponsoring the Red Bull ""EmSee Battle Rap championships""). It hosts events such as art shows and the ""Red Bull Flugtag"" (German for ""flight day"" or ""flying day"").Red Bull owns football teams, with clubs in Austria, Germany, the United States, and Brazil featuring the Red Bull trademark in their names. By associating the drink's image with these activities, the company seeks to promote a ""cool"" public image and raise brand power. The energy drink has created a market for over 150 related types of products.In the PlayStation 3's social gaming platform, PlayStation Home, Red Bull developed its own in-game island, specifically advertising its energy drink and the Red Bull Air Race event (for which the space is named) released in January 2009. In late November 2009, Red Bull produced two new spaces, the Red Bull Illume space, and the Red Bull Beach space featuring the Red Bull Flugtag, both released on the same day. In January 2012, Red Bull released its first personal space called the ""Red Bull House of Skate"" featuring an indoor skate park.In 2010, the company enlisted Adrian Newey to design a prototype racing car, the Red Bull X2010, for the video game Gran Turismo 5.In 2022, Red Bull announced a full-on production of a hypercar called RB17, also designed by Newey.

Red Bull Arts
Red Bull Arts is an art fellowship program launched by Red Bull in 2013 under the name Red Bull House of Arts. The program has multiple locations, including Detroit, Michigan; São Paulo, Brazil; and formerly New York City. The program typically consists of a three-month period during which six to eight participants create new artwork to be displayed at a final exhibition. During the fellowship, artists receive unlimited access to the galleries and a stipend for art supplies. Some of the artwork has been used in Red Bull advertising campaigns.

Sports and esports sponsorships
Red Bull has used sports sponsorships as an advertising vehicle for most of its existence. The company first started sponsoring athletes in 1989, initially focusing on Formula One racing and extreme sports such as windsurfing and hang gliding, and later growing to include more mainstream sports such as basketball and soccer. As of 2016, the company sponsored more than 750 individual athletes and more than a dozen teams in various disciplines, including motorsports, soccer, and esports.

Athlete sponsorships
Austrian Formula One driver Gerhard Berger was the first athlete to be sponsored by Red Bull in 1989. Many of the company's early sponsorships were in lesser-known or extreme sports, including Olympic rower Xeno Müller, who won a gold medal at the 1996 Atlanta Olympics in the single scull race and BASE jumpers Frank ""Gambler"" Gambalie, Miles Dashier, and Shane McConkey. In the 2010s, Red Bull began expanding its athlete base to include athletes from more mainstream sports, including Austrian tennis player Dominic Thiem, Brazilian skateboarder Letícia Bufoni, American skier Lindsey Vonn, and American Major League Baseball player Kris Bryant. The company also started sponsoring video game players and esports athletes, including American Fortnite player Richard ""Ninja"" Blevins, Spanish League of Legends player Enrique Cedeño ""xPeke"" Martinez, and Swedish Super Smash Bros. player William ""Leffen"" Hjelte.

Team ownership and sponsorships
The first team sponsored by Red Bull was ice hockey's EC Salzburg during the 1987–88 season. Red Bull acquired the club outright in 2000. Since 2014, Salzburg has also hosted the company's joint ice hockey and soccer academy. Red Bull became the title sponsor of DEL team EHC München in 2012, and took full ownership the following year. It also financed the team's new arena, SAP Garden.In 1995, Red Bull sponsored its first motorsports team, the Swiss Formula One team Sauber and in 1999 started sponsoring the Flying Bulls, a Czech aerobatics team.In the 2000s, the company expanded its sporting team ownership to include several soccer teams, including the Austrian Bundesliga team SV Austria Salzburg (rebranded as Red Bull Salzburg), the Major League Soccer team the New York MetroStars (rebranded as the New York Red Bulls) in 2006, and the fifth-tier German team SSV Markranstadt (rebranded as RasenBallsport Leipzig) in 2009, which the company sought to move to the top of the German Bundesliga. RB Leipzig has been divisive and the subject of protests by some fans but has also experienced rapid success, climbing through the German soccer divisions to get a place in the top-flight German Bundesliga and earning berths in the UEFA Champions League in 2017–2018 and 2019–2020, the latter trip ending with a semi-final loss to Paris St. Germain. The company also sponsors the Los Angeles Clippers NBA team and Red Bull 3X, a series of men's and women's 3x3 basketball tournaments.In the 2010s, Red Bull began sponsoring gamers and esports organizations, including OG, G2 Esports and Cloud9, and founded the Red Bulls League of Legends team.In 2021, Red Bull sponsored Hoang Anh Gia Lai from V.League 1.
 Red Bull Salzburg
 FC Liefering
 Red Bull Bragantino
 Red Bull Brasil
 RB Leipzig
 RB Leipzig II
 Red Bull New York
 Red Bull New York II

Events
Current and former Red Bull events include ACF Nationals (2009), Air Race World Championship (2003–2019), Argentine motorcycle Grand Prix, Art of Motion, BC One, Big Wave Africa, Cape Fear, Cliff Diving World Series, Crashed Ice, Dolomitenmann, Drifting World Championship, Flugtag, Frozen Rush, Indianapolis motorcycle Grand Prix, King of the Air, King of the Rock Tournament, Last Man Standing, MotoGP Rookies Cup, Motorcycle Grand Prix of the Americas, New Year No Limits, Paper Wings, Rampage, Red Bull 400, Red Bull Joyride, Road Rage, Romaniacs Hard Enduro Rallye, Soapbox Race, Spanish motorcycle Grand Prix, Stratos, Street Freestyle World Champions (2019), Trolley Grand Prix, Unleashed (2015), Wings for Life World Run, X-Alps, Xcbusa, and X-Fighters.

See also
Red Bull Stratos – 2012 stratospheric parachute jump
Passage 2:
Ural Rakhimov
Ural Murtazovich Rakhimov (Russian: Урал Муртазович Рахимов; Bashkir: Рәхимов Урал Мортаза улы, Räximov Ural Mortaza ulı; born 13 December 1961) is a Russian businessman of Bashkir ethnicity. Rakhimov is the 191st richest man in Russia with a net worth of US$500 million as of 2011.

Biography
Rakhimov was born in Ufa. Worldwide, Rakhimov is known to be the only son of Murtaza Rakhimov, the former president of Bashkortostan. Throughout most of Rakhimov's life, he maintained a deep interest in science, eventually studying at Ufa State Petroleum Technological University and graduating in 1984.He then studied at the French Petroleum Institute, receiving a Master's of Science, and then he studied in the United States for another Master's degree. For most of his business career, he was a member of Bashneft. In 2003, he initiated the creation of Bashkir capital. During 2009 till early 2010, he was the owner of Salavat Yulaev Ufa.
Passage 3:
The World's Billionaires
The World's Billionaires is an annual ranking of people who are considered to have a net worth of $1 billion or more, by the American business magazine Forbes. The list was first published in March 1987. The total net worth of each individual on the list is estimated and is cited in United States dollars, based on their documented assets and accounting for debt and other factors. Royalty and dictators whose wealth comes from their positions are excluded from these lists. This ranking is an index of the wealthiest documented individuals, excluding any ranking of those with wealth that is not able to be completely ascertained.In 2018, Amazon founder Jeff Bezos was ranked at the top for the first time and became the first centibillionaire included in the ranking, surpassing Microsoft founder Bill Gates, who had topped the list 18 of the previous 24 years. In 2022, after topping the list for four years, Bezos was surpassed by Elon Musk. In 2023, Musk was in turn surpassed by French businessman Bernard Arnault, after topping the list for just a year. Arnault became the first French person to top the list.

Methodology
Each year, Forbes employs a team of over 50 reporters from a variety of countries to track the activity of the world's wealthiest individuals and sometimes groups or families – who share wealth. Preliminary surveys are sent to those who may qualify for the list. According to Forbes, they received three types of responses – some people try to inflate their wealth, others cooperate but leave out details, and some refuse to answer any questions. Business deals are then scrutinized and estimates of valuable assets – land, homes, vehicles, artwork, etc. – are made. Interviews are conducted to vet the figures and improve the estimate of an individual's holdings. Finally, positions in a publicly traded stock are priced to market on a date roughly a month before publication. Privately held companies are priced by the prevailing price-to-sales or price-to-earnings ratios. Known debt is subtracted from assets to get a final estimate of an individual's estimated worth in United States dollars. Since stock prices fluctuate rapidly, an individual's true wealth and ranking at the time of publication may vary from their situation when the list was compiled.When a living individual has dispersed his or her wealth to immediate family members it is included under a single listing (as a single ""family fortune"") provided that individual (the grantor) is still living. However, if a deceased billionaire's fortune has been dispersed, it will not appear as a single listing, and each recipient will only appear if his or her own total net worth is over a $Billion (his or her net worth will not be combined with family members') Royal families and dictators that have their wealth contingent on a position are always excluded from these lists.

Annual rankings
The rankings are published annually in March, so the net worths listed are snapshots taken at that time. These lists only show the top 10 wealthiest billionaires for each year.

Legend
2023
In the 37th annual Forbes list of the world's billionaires, the list included 2,640 billionaires with a total net wealth of $12.2 trillion, down 28 members and $500 billion from 2022. Nearly half the list is poorer than the previous year, including Elon Musk, who fell from No. 1 to No. 2. The list also marks for the first time a French citizen was in the top position as well as a non-American for the first time since 2013 when the Mexican Carlos Slim Helu was the world's richest person. The list, like in 2022, counted 15 under 30 billionaires with the richest of them being Red Bull heir Mark Mateschitz with a net worth of $34.7 billion. The youngest of the lot were Clemente Del Vecchio, heir to the Luxottica fortune shared with his six siblings and stepmother and Kim Jung-yang, whose fortune lies in Japanese-South Korean gaming giant Nexon, both under-20s.

2022
In the 36th annual Forbes list of the world's billionaires, the list included 2,668 billionaires with a total net wealth of $12.7 trillion, down 97 members from 2021.

2021
In the 35th annual Forbes list of the world's billionaires, the list included 2,755 billionaires with a total net wealth of $13.1 trillion, up 660 members from 2020; 86% of these billionaires had more wealth than they possessed last year.

2020
In the 34th annual Forbes list of the world's billionaires, the list included 2,095 billionaires with a total net wealth of $8 trillion, down 58 members and $700 billion from 2019; 51% of these billionaires had less wealth than they possessed last year. The list was finalized as of 18 March, thus was already partially influenced by the COVID-19 pandemic.

2019
In the 33rd annual Forbes list of the world's billionaires, the list included 2,153 billionaires with a total net wealth of $8.7 trillion, down 55 members and $400 billion from 2018.  The U.S. continued to have the most billionaires in the world, with a record of 609, while China dropped to 324 (when not including Hong Kong, Macau and Taiwan).

2018
In the 32nd annual Forbes list of the world's billionaires, the aggregate wealth of the top 20 richest people on Earth amounted to about 13 percent of all billionaires' fortunes combined. A record of 2,208 billionaires were in the ranking and the total wealth was $9.1 trillion, up 18% since 2017. For the first time, Jeff Bezos was listed as the top billionaire due to Amazon's rising stock price that resulted in one person's biggest one-year gain in wealth ($35 billion) since Forbes started tracking in 1987. The U.S. had the most billionaires in the world, with 585, while China was catching up with 476 when including Hong Kong, Macau and Taiwan; it had 372 when excluding those three places. Forbes excluded Al-Walid bin Talal and all other Saudi billionaires due to the absence of accurate wealth estimations as a result of the 2017–19 Saudi Arabian purge.

2017
On the 30th anniversary of the Forbes list of the world's billionaires, for the fourth year in a row, Bill Gates was named the richest man in the world. In 2017, there was a record of 2,043 people on the list, which is the first time over 2,000 people were listed. This included 195 newcomers of whom 76 were from China and 25 from the U.S.; there were 56 people under 40 and it had a record of 227 women. The number of billionaires increased 13% to 2,043 from 1,810 in 2016; this was the biggest change in over 30 years of tracking billionaires globally. Added together, the total net worth for 2017's billionaires was US$7.67 trillion, up from US$7.1 trillion in 2015. This was the first time after 12 years that Carlos Slim was not within the top five. The U.S. had the most billionaires in the world, with a record of 565. China had 319 (not including Hong Kong, Taiwan or Macau), Germany had 114, and India had the fourth most with 101; India reached over 100 billionaires for its first time.

2016
For the third year in a row, Bill Gates was named the richest man in the world by Forbes' 2016 list of the world's billionaires. This is the 17th time that the founder of Microsoft had claimed the top spot. Amancio Ortega rose from last year's position of number four to second. Warren Buffett of Berkshire Hathaway came in third for the second consecutive time, while Mexican telecommunication mogul Carlos Slim slipped  from last year's second position to fourth. Jeff Bezos of Amazon, Mark Zuckerberg of Facebook and Michael Bloomberg of Bloomberg L.P., appeared for the first time on the Forbes top 10 billionaires list, coming at fifth, sixth and eighth positions, respectively. Zuckerberg became the youngest top 10 billionaire this year at the age of 31. Larry Ellison, Charles Koch and David Koch also slipped from their previous year's positions, with Ellison dropping to seventh from fifth, and the Kochs falling to ninth position from sixth.

2015
In the 29th annual Forbes list of global billionaires, a record 1,826 billionaires were named with an aggregated net worth of $7.1 trillion compared to $6.4 trillion in the previous year. 46 of the billionaires in this list were under the age of 40. A record number of 290 people joined the list for the first time, of whom 25 percent hailed from China, which produced a world-leading 71 newcomers. The United States came in second, with 57; followed by India, with 28; and Germany, with 23. The United States had the largest number of billionaires with 526. Russia went down to 88 from 111 in 2014. Russia was placed behind China, Germany and India by the number of billionaires. Self-made billionaires made up the largest number of people on the list with 1,191 positions (over 65 percent), while just 230 (under 13 percent) had wealth through inheritance. The number of billionaires who inherited a portion but were still working to increase their fortunes is 405.Bill Gates was named the richest man in the world by Forbes' annual list of the world's billionaires. This was the 16th time that the founder of Microsoft claimed the top spot. Carlos Slim came in second for the second consecutive time. Warren Buffett of Berkshire Hathaway placed third, while Amancio Ortega of Spain, slipped down a position from the previous year to number four. Larry Ellison, the founder of Oracle, rounded off the top five. Christy Walton was the highest-ranking female at number eight. America's Evan Spiegel, co-founder of photo messaging app Snapchat, became the youngest billionaire this year at age 24. At age 99, David Rockefeller maintained his position as the oldest billionaire included in the list. Mark Zuckerberg, the founder of Facebook, rose to number 16 with $33.4 billion. Iceland had a billionaire, Thor Bjorgolfsson, in the list after a gap of five years. Guatemala had a billionaire, Mario Lopez Estrada, for the first time in its history.

2014
Gates added $9 billion to his fortune since 2013 and topped the Forbes 2014 billionaire list. He had topped the list in 15 of the previous 20 years, but his previous number one ranking was in 2009. Mexican telecommunication mogul Carlos Slim came in second place after being number one the previous four years. Zara founder Amancio Ortega placed third for the second consecutive year. American investor Warren Buffett was in the top five for the 20th consecutive year, placing fourth. America's Christy Walton was the highest ranking woman, placing ninth overall. Aliko Dangote of Nigeria became the first African to enter the top 25, with an estimated net worth of $25 billion.A total of 1,645 people made the 2014 billionaire list, representing a combined wealth of $6.4 trillion. Of those, a record 268 were newcomers, surpassing 2008's 226 newcomers. 100 people listed in 2013 failed to make the list. The number of women on the list rose to a record 172 in 2014. Approximately 66 percent of the list were self-made, 13 percent achieved their wealth through inheritance alone, and 21 percent through a mixture of the two.The United States had 492 billionaires on the list, the most of any country. It also had the most newcomers with 50, and women with 54. China had the second most billionaires with 152, while Russia was third with 111. Algeria, Lithuania, Tanzania, and Uganda were all represented on the list for the first time. Turkey saw the most people drop off the list, 19, due to a period of high inflation in the country.

2013
Carlos Slim topped the 2013 billionaire list, marking his fourth consecutive year at the top. Gates remained in second, while Amancio Ortega moved up to third. Ortega's gain of $19.5 billion was the largest of anyone on the list. Warren Buffett failed to make the top three for the first time since 2000, placing fourth. Diesel founder Renzo Rosso was among the top newcomers, debuting with an estimate net worth of $3 billion.A global rise in asset prices led Forbes editor Randall Lane to declare ""It [was] a very good year to be a billionaire"". However, it was not a good year to be Eike Batista, who fell from seventh to 100th, suffering the largest net loss of anyone on the list. Overall, net gainers outnumbered net losers by 4:1.A record total of 1,426 people made the 2013 list, representing $5.4 trillion of assets. Of those, 442 billionaires hailed from the United States. The Asian-Pacific region had 386 billionaires and Europe 366. The list also featured a record number of newcomers, 210, representing 42 countries. 60 people from the 2012 list fell below a billion dollars of assets in 2013, and eight others from the 2012 list died. The Asia-Pacific region had the most drop-offs, with 29, followed by the United States with 16. The 2013 list featured 138 women, of which 50 came from the United States. A majority of the list (961 individuals, 67 percent) were entirely self-made; 184 (13 percent) inherited their wealth, and 281 (20 percent) achieved their fortune through a combination of inheritance and business acumen. Vietnam's Phạm Nhật Vượng was the first person from that country to be included in this list.

2012
Carlos Slim topped the 2012 list, marking this third consecutive year at the top. Gates placed second but narrowed the gap from 2011 as Slim's fortune fell $5 billion while Gates' rose $5 billion. Warren Buffett remained in third place. Bernard Arnault of France was the top-ranking European on the list, placing fourth. Ricardo Salinas Pliego was the greatest gainer in terms of dollars, adding $9.2 billion to his fortune and moving up to number 37 overall. Making her debut on the list at age 27, Spanx founder Sara Blakely became the youngest self-made female billionaire ever. Colombia's Alejandro Santo Domingo was the highest-ranked newcomer, inheriting a $9.5 billion stake in Santo Domingo Group from his father. India's Lakshmi Mittal was the largest loser as his fortune dropped from $31.1 billion to $20.7 billion as the price of steelmaker ArcelorMittal fell sharply. As a result, he failed to make the top 10 for the first time since 2004 and lost his title of richest Asian to Hong Kong's Li Ka-shing.A record total of 1,226 people made the 2012 list, representing 58 countries. Of those, 126 were newcomers to the list and 104 were women. The United States had the greatest number of billionaires with 425. Russia had 96 people on the list, while China had 95. Georgia, Morocco, and Peru were newly represented on the list. Falling stock prices in Asia contributed to 117 former billionaires falling from the list worldwide. Twelve others listed in 2011 died. Overall, net gainers (460) barely outnumbered net losers (441).To coincide with the release of the 2012 list, Forbes announced a then-new ""Billionaire Real-Time Ticker"" updating the wealth of the world's top 50 billionaires in real time.

2011
In the 25th annual Forbes list of global billionaires, Slim added $20.5 billion to his fortune, the most of anyone, and retained his number one ranking with a total fortune of $74 billion. Gates remained in second place with $56 billion, while Warren Buffett was third with $50 billion. The top 10 had a combined wealth of $406 billion, up from $342 billion in 2010. According to Forbes editor Kerry Dolan, ""media and technology billionaires definitely benefited from a stronger stock market and a growing enthusiasm for all things social"" since the 2010 list. However, Nigerian commodity mogul Aliko Dangote was the greatest gainer on a percentage basis as his fortune increased 557 percent to $13.5 billion. Mark Zuckerberg was one of seven Facebook-related billionaires on the list, as he added $9.5 billion to his net worth to move up to 52nd. Facebook co-founder Dustin Moskovitz was the youngest person on the list. Aged 26, eight days younger than Zuckerberg, he debuted at number 420 with an estimated fortune of $2.7 billion. IKEA founder Ingvar Kamprad was the largest loser as he saw his fortune plummet from $23 billion to $6 billion, dropping him from 11th to 162nd overall.A record 1,210 billionaires made the 2011 list, representing a combined wealth of $4.5 trillion, up from $3.6 trillion the previous year. One third of the world's billionaires, 413, came from the United States. China had the second most billionaires with 115, while Russia was third with 101. Asia moved up to 332 billionaires, passing Europe as a region for the first time since the 1990s. The 2011 list included 214 newcomers and the average net worth of those on it increased to $3.7 billion.

2010
Slim narrowly eclipsed Gates to top the billionaire list for the first time. Slim saw his estimated worth surge $18.5 billion to $53.5 billion as shares of America Movil rose 35 percent. Gates' estimated wealth rose $13 billion to $53 billion, placing him second. Warren Buffett was third with $47 billion. Christy Walton was the highest-ranking woman, placing 12th overall, with an inherited fortune of $22.5 billion. At age 25, Mark Zuckerberg continued to be the world's youngest self-made billionaire. American Isaac Perlmutter was among the newcomers with an estimated fortune of $4 billion largely acquired in his sale of Marvel Entertainment to Disney.A total of 1,011 people made the 2010 list. The United States accounted for 403 billionaires, followed by China with 89 and Russia with 62. It was the first time China, while including Hong Kong, placed second. A total of 55 countries were represented on the 2010 list, including Finland and Pakistan which claimed their first billionaires. Eighty-nine women made the list, but only 14 of them were self-made. The combined net worth of the list was $3.6 trillion, up 50 percent from 2009's $2.4 trillion, while the average net worth was $3.5 billion.The 2010 list featured 164 re-entries and 97 true newcomers. Asia accounted for more than 100 of the new entrants. Overall, just 12 percent of the list lost wealth since 2009, and 30 people fell off the list. 13 others died. Of the 89 women, 12 were newcomers in 2010. Steve Forbes said the growing number of billionaires was a clear sign that the world's economy was recovering from 2009's global financial crisis.In June 2010, Gates and Buffett announced the Giving Pledge, a promise to give the majority of their wealth to philanthropic causes. As of 2017, the pledge had 158 signatories, but some of the signatories have since died. Most of the signers of the pledge are billionaires, and their pledges total over $365 billion.

2009
In the wake of the Financial crisis of 2007–2008, the world's billionaires lost $2 trillion in net worth and the list became 30% smaller than the previous year's list.

2008
Facebook founder Mark Zuckerberg, four years after starting the company, joined the list at 23 to become the youngest self-made billionaire.

2007
Forbes recorded a then record of 946 billionaires. There were 178 newcomers, as well as the first billionaires from Cyprus, Oman, Romania and Serbia. Over 66% of the previous year's billionaires became richer. The billionaires' net worth increased in 2007 by $900 billion to $3.5 trillion.

2006
Free cash used by consumers from home equity extraction, known as the real estate bubble created a total of nearly $5 trillion in 2005, contributing to economic growth worldwide.

2005
The net worth of 2005's 691 billionaires was $2.2 trillion. More than half of them had self-made fortunes.

2004
The founders of Google, Sergey Brin and Larry Page, became billionaires at age 30.

* Each hold an essentially equal share in Walmart.

2003
Oprah Winfrey became the first female African-American billionaire.

* Each hold an essentially equal share in Walmart.

2002
As a result of the market crash caused by the Dot-com bubble, 83 billionaires dropped off the list from the previous year.

* Each hold an essentially equal share in Walmart.

2001
In 2001, BET founder Robert L. Johnson became the first ever African-American billionaire.
* Each hold an essentially equal share in Wal-Mart. Had he been alive in 2001, Sam Walton would have been the world's wealthiest person.

2000
Gates became the first American to take the top spot of the world's billionaires in 1995 with a net worth of $12.5 billion, and he remained there during the dot-com bubble's height in 1999 when his fortune peaked at $90 billion. After the dot-com bubble started to collapse in 2000, his wealth dropped to $60 billion, although he remained at the top of the list.

1999
1998
1997
1996
1995
1994
1993
1992
1991
1990
1989
1988
1987
Statistics
The dot-com bubble created the most paper wealth for some billionaires. However, once the dotcom bubble burst the new rich saw their fortunes disappear. Billionaires' fortunes were hit even harder by the global financial crisis; 2009 was the first time in five years that the world had a net loss in the number of billionaires. The strong performance of the financial markets and global economic recovery have erased financial assets losses. Most of the richest people in the world saw their fortunes soar in the early 2010s.

See also
Passage 4:
Windows 98
Windows 98 is a consumer-oriented operating system developed by Microsoft as part of its Windows 9x family of Microsoft Windows operating systems. The second operating system in the 9x line, it is the successor to Windows 95, and was released to manufacturing on May 15, 1998, and generally to retail on June 25, 1998. Like its predecessor, it is a hybrid 16-bit and 32-bit monolithic product with the boot stage based on MS-DOS.Windows 98 is a web-integrated operating system that bears numerous similarities to its predecessor. Most of its improvements were cosmetic or designed to improve the user experience, but there were also a handful of features introduced to enhance system functionality and capabilities, including improved USB support and accessibility, as well as support for hardware advancements such as DVD players. Windows 98 was the first edition of Windows to adopt the Windows Driver Model, and introduced features that would become standard in future generations of Windows, such as Disk Cleanup, Windows Update, multi-monitor support, and Internet Connection Sharing.
Microsoft had marketed Windows 98 as a ""tune-up"" to Windows 95, rather than an entirely improved next generation of Windows. Upon release, it was generally well-received for its web-integrated interface and ease of use, as well as its addressing of issues present in Windows 95, although some pointed out that it was not significantly more stable than its predecessor. Windows 98 sold an estimated 58 million licenses and saw one major update, known as Windows 98 Second Edition (SE), released on May 5, 1999. After the release of its successor, Windows Me in 2000, mainstream support for Windows 98 and 98 SE ended on June 30, 2002, followed by extended support on July 11, 2006.

Development
Following the success of Windows 95, the development of Windows 98 began, initially under the development codename ""Memphis."" The first test version, Windows Memphis Developer Release, was released in January 1997.Memphis first entered beta as Windows Memphis Beta 1, released on June 30, 1997. It was followed by Windows 98 Beta 2, which dropped the Memphis name and was released in July. Microsoft had planned a full release of Windows 98 for the first quarter of 1998, along with a Windows 98 upgrade pack for Windows 95, but it also had a similar upgrade for Windows 3.x operating systems planned for the second quarter. Stacey Breyfogle, a product manager for Microsoft, explained that the later release of the upgrade for Windows 3 was because the upgrade required more testing than that for Windows 95 due to the presence of more compatibility issues, and without user objections, Microsoft merged the two upgrade packs into one and set all of their release dates to the second quarter.On December 15, Microsoft released Windows 98 Beta 3. It was the first build to be able to upgrade from Windows 3.1x, and introduced new startup and shutdown sounds.Near its completion, Windows 98 was released as Windows 98 Release Candidate on April 3, 1998, which expired on December 31. This coincided with a notable press demonstration at COMDEX that month. Microsoft CEO Bill Gates was highlighting the operating system's ease of use and enhanced support for Plug and Play (PnP). However, when presentation assistant Chris Capossela plugged a USB scanner in, the operating system crashed, displaying a Blue Screen of Death. Bill Gates remarked after derisive applause and cheering from the audience, ""That must be why we're not shipping Windows 98 yet."" Video footage of this event became a popular Internet phenomenon.Microsoft had quietly marketed the operating system as a ""tune-up"" to Windows 95. It was compiled as Windows 98 on May 11, 1998, before being fully released to manufacturing on May 15. The company was facing pending legal action for allowing free downloads of, and planning to ship Windows licenses with, Internet Explorer 4.0 in an alleged effort to expand its software monopoly. Microsoft's critics believed the lawsuit would further delay Windows 98's public release; it did not, and the operating system was released on June 25, 1998.A second major version of the operating system called Windows 98 Second Edition was later unveiled in March 1999. Microsoft compiled the final build on April 23, 1999, before publicly releasing it on May 5, 1999. Windows 98 was to be the final product in the Windows 9x line until Microsoft briefly revived the line to release Windows Me in 2000 as the final Windows 9x product before the introduction of Windows XP in 2001, which was based on the Windows NT architecture and kernel used in Windows 2000.

New and updated features
Web integration and shell enhancements
The first release of Windows 98 included Internet Explorer 4.01. This was updated to 5.0 in the Second Edition. Besides Internet Explorer, many other Internet companion applications are included such as Outlook Express, Windows Address Book, FrontPage Express, Microsoft Chat, Personal Web Server and a Web Publishing Wizard, and NetShow. NetMeeting allows multiple users to hold conference calls and work with each other on a document.The Windows 98 shell is web-integrated; it contains deskbands, Active Desktop, Channels, ability to minimize foreground windows by clicking their button on the taskbar, single-click launching, Back and Forward navigation buttons, favorites, and address bar in Windows Explorer, image thumbnails, folder infotips and Web view in folders, and folder customization through HTML-based templates. The taskbar supports customizable toolbars designed to speed up access to the Web or the user's desktop; these toolbars include an Address Bar and Quick Launch. With the Address Bar, the user accesses the Web by typing in a URL, and Quick Launch contains shortcuts or buttons that perform system functions such as switching between windows and the desktop with the Show Desktop button. Another feature of this new shell is that dialog boxes show up in the Alt-Tab sequence.
Windows 98 also integrates shell enhancements, themes and other features from Microsoft Plus! for Windows 95 such as DriveSpace 3, Compression Agent, Dial-Up Networking Server, Dial-Up Scripting Tool and Task Scheduler. 3D Pinball Space Cadet is included on the CD-ROM, but not installed by default. Windows 98 had its own separately purchasable Plus! pack, called Plus! 98.Title bars of windows and dialog boxes support two-color gradients, a feature ported from and refined from Microsoft Office 95. Windows menus and tooltips support slide animation. Windows Explorer in Windows 98, as in Windows 95, converts all-uppercase filenames to sentence case for readability purposes; however, it also provides an option Allow all uppercase names to display them in their original case. Windows Explorer includes support for compressed CAB files. The Quick Res and Telephony Location Manager Windows 95 PowerToys are integrated into the core operating system.

Improvements to hardware support
Windows Driver Model
Windows 98 was the first operating system to use the Windows Driver Model (WDM). This fact was not well publicized when Windows 98 was released, and most hardware producers continued to develop drivers for the older VxD driver standard, which Windows 98 supported for compatibility's sake. The WDM standard only achieved widespread adoption years later, mostly through Windows 2000 and Windows XP, as they were not compatible with the older VxD standard. With the Windows Driver Model, developers could write drivers that were compatible with other versions of Windows. Device driver access in WDM is implemented through a VxD device driver, NTKERN.VXD, which implements several Windows NT-specific kernel support functions.Support for WDM audio enables digital mixing, routing and processing of simultaneous audio streams and kernel streaming with high quality sample rate conversion on Windows 98. WDM Audio allows for software emulation of legacy hardware to support MS-DOS games, DirectSound support and MIDI wavetable synthesis. The Windows 95 11-device limitation for MIDI devices is eliminated. A Microsoft GS Wavetable Synthesizer licensed from Roland shipped with Windows 98 for WDM audio drivers. Windows 98 supports digital playback of audio CDs, and the Second Edition improves WDM audio support by adding DirectSound hardware mixing and DirectSound 3D hardware abstraction, DirectMusic kernel support, KMixer sample-rate conversion for capture streams and multichannel audio support. All audio is sampled by the Kernel Mixer to a fixed sampling rate which may result in some audio getting upsampled or downsampled and having a high latency, except when using Kernel Streaming or third-party audio paths like ASIO which allow unmixed audio streams and lower latency. Windows 98 also includes a WDM streaming class driver (Stream.sys) to address real time multimedia data stream processing requirements and a WDM kernel-mode video transport for enhanced video playback and capture.
Windows Driver Model also includes Broadcast Driver Architecture, the backbone for TV technologies support in Windows. WebTV for Windows utilized BDA to allow viewing television on the computer if a compatible TV tuner card is installed. TV listings could be updated from the Internet and WaveTop Data Broadcasting allowed extra data about broadcasts to be received via regular television signals using an antenna or cable, by embedding data streams into the vertical blanking interval portion of existing broadcast television signals.

Other device support improvements
Windows 98 had more robust USB support than Windows 95, which only had support in OEM versions OSR2.1 and later. Windows 98 supports USB hubs, USB scanners and imaging class devices. Windows 98 also introduced built-in support for some USB Human Interface Device class (USB HID) and PID class devices such as USB mice, keyboards, force feedback joysticks etc. including additional keyboard functions through a certain number of Consumer Page HID controls.Windows 98 introduced ACPI 1.0 support which enabled Standby and Hibernate states. However, hibernation support was extremely limited and vendor-specific. Hibernation was only available if compatible (PnP) hardware and BIOS are present, and the hardware manufacturer or OEM supplied compatible WDM drivers, non-VxD drivers. However, there are hibernation issues with the FAT32 file system, making hibernation problematic and unreliable.
Windows 98, in general, provides improved — and a broader range of — support for IDE and SCSI drives and drive controllers, floppy drive controllers and all other classes of hardware as compared to Windows 95. There is integrated Accelerated Graphics Port (AGP) support (although the USB Supplement to Windows 95 OSR2 and later releases of Windows 95 did have AGP support). Windows 98 has built-in DVD support and UDF 1.02 read support. The Still imaging architecture (STI) with TWAIN support was introduced for scanners and cameras and Image Color Management 2.0 for devices to perform color space transformations. Multiple monitor support allows using up to nine multiple monitors on a single PC, with the feature requiring one PCI graphics adapter per monitor. Windows 98 shipped with DirectX 5.2, which notably included DirectShow. Windows 98 Second Edition would later ship with DirectX 6.1.

Networking enhancements
Windows 98 networking enhancements to TCP/IP include built-in support for Winsock 2, SMB signing, a new IP Helper API, Automatic Private IP Addressing (also known as link-local addressing), IP multicasting, and performance enhancements for high-speed high bandwidth networks. Multihoming support with TCP/IP is improved and includes RIP listener support.
The DHCP client has been enhanced to include address assignment conflict detection and longer timeout intervals. NetBT configuration in the WINS client has been improved to continue persistently querying multiple WINS servers if it failed to establish the initial session until all of the WINS servers specified have been queried or a connection is established.
Network Driver Interface Specification 5 support means Windows 98 can support a wide range of network media, including Ethernet, Fiber Distributed Data Interface (FDDI), Token Ring, Asynchronous Transfer Mode (ATM), ISDN, wide area networks, X.25, and Frame Relay. Additional features include NDIS power management, support for quality of service, Windows Management Instrumentation (WMI) and support for a single INF file format across all Windows versions.Windows 98 Dial-Up Networking supports PPTP tunneling, support for ISDN adapters, multilink support, and connection-time scripting to automate non-standard login connections. Multilink channel aggregation enables users to combine all available dial-up lines to achieve higher transfer speeds. PPP connection logs can show actual packets being passed and Windows 98 allows PPP logging per connection. The Dial-Up Networking improvements are also available in Windows 95 OSR2 and are downloadable for earlier Windows 95 releases.
For networked computers that have user profiles enabled, Windows 98 introduces Microsoft Family Logon which lists all users that have been configured for that computer, enabling users to simply select their names from a list rather than having to type them in.Windows 98 supports IrDA 3.0 which specifies both Serial Infrared Devices and Fast Infrared devices, which are capable of sending and receiving data at 4 Mbit/s. Infrared Recipient, a new application for transferring files through an infrared connection is included. The IrDA stack in Windows 98 supports networking profiles over the IrCOMM kernel-mode driver. Windows 98 also has built-in support for browsing Distributed File System trees on Server Message Block shares such as Windows NT servers.UPnP and NAT traversal APIs can be installed on Windows 98 by installing the Windows XP Network Setup Wizard. An L2TP/IPsec VPN client can also be downloaded. By installing Active Directory Client Extensions, Windows 98 can take advantage of several Windows 2000 Active Directory features.

Improvements to the system and built-in utilities
Performance improvements
Windows 95 introduced the 32-bit, protected-mode cache driver VCACHE (replacing SMARTDrv) to cache the most recently accessed information from the hard drive in memory, divided into chunks. However, the cache parameters needed manual tuning as it degraded performance by consuming too much memory and not releasing it quickly enough, forcing paging to occur far too early. The Windows 98 VCACHE cache size management for disk and network access, CD-ROM access and paging is more dynamic compared to Windows 95, resulting in no tuning being required for cache parameters. On the FAT32 file system, Windows 98 has a performance feature called MapCache that can run applications from the disk cache itself if the code pages of executable files are aligned/mapped on 4K boundaries, instead of copying them to virtual memory. This results in more memory being available to run applications, and lesser usage of the swap file.
Windows 98 registry handling is more robust than Windows 95 to avoid corruption and there are several enhancements to eliminate limitations and improve registry performance. The Windows 95 registry key size limitation of 64 KB is gone. The registry uses less memory and has better caching.Disk Defragmenter has been improved to rearrange program files that are frequently used to a hard disk region optimized for program start. However, as with Windows 95, the message ""Drive contents changed....restarting."" still exists in this version (i.e. if the contents of the hard drive changed, then the entire drive is then rescanned and then progress resumed where it had left off). If it gets stuck on the same area too many times, it will ask the user if it should keep trying or give up. The version of Disk Defragmenter from Windows Me does not have this problem and will function on Windows 98 or Windows 95 if the user simply copies it over.Windows 98 also supports a Fast Shutdown feature that initiates shutdown without uninitializing device drivers. However, this can cause Windows 98 to hang instead of shutting down the computer if a buggy driver is active, so Microsoft supplied instructions for disabling the feature. Windows 98 supports write-behind caching for removable disk drives. A utility for converting FAT16 partitions to FAT32 without formatting the partition is also included.

Other system tools
A number of improvements are made to various other system tools and accessories in Windows 98. Microsoft Backup supports differential backup and SCSI tape devices in Windows 98. Disk Cleanup, a new tool, enables users to clear their disks of unnecessary files. Cleanup locations are extensible through Disk Cleanup handlers. Disk Cleanup can be automated for regular silent cleanups.Scanreg (DOS) and ScanRegW are Registry Checker tools used to back up, restore or optimize the Windows registry. ScanRegW tests the registry's integrity and saves a backup copy each time Windows successfully boots. The maximum number of copies could be customized by the user through ""scanreg.ini"" file. The restoration of a registry that causes Windows to fail to boot can only be done from DOS mode using ScanReg.System Configuration Utility is a new system utility used to disable programs and services that are not required to run the computer. A Maintenance Wizard is included that schedules and automates ScanDisk, Disk Defragmenter and Disk Cleanup. Windows Script Host, with VBScript and JScript engines is built-in and upgradeable to version 5.6. System File Checker checks installed versions of system files to ensure they were the same version as the one installed with Windows 98 or newer. Corrupt or older versions are replaced by the correct versions. This tool was introduced to resolve the DLL hell issue and was replaced in Windows Me by System File Protection.
Windows 98 Setup simplifies installation, reducing the bulk of user input required. The Windows 98 Startup Disk contains generic, real-mode ATAPI and SCSI CD-ROM drivers that can be used instead in the event that the specific driver for a CD-ROM is unavailable.The system could be updated using Windows Update. A utility to automatically notify the user of critical updates was later released.Windows 98 includes an improved version of the Dr. Watson utility that collects and lists comprehensive information such as running tasks, startup programs with their command line switches, system patches, kernel driver, user drivers, DOS drivers and 16-bit modules. With Dr. Watson loaded in the system tray, whenever a software fault occurs (general protection fault, hang, etc.), Dr. Watson will intercept it and indicate what software crashed and its cause.Windows Report Tool takes a snapshot of system configuration and lets users submit a manual problem report along with system information to technicians. It has e-mail confirmation for submitted reports.

Accessories
Windows 98 includes Microsoft Magnifier, Accessibility Wizard and Microsoft Active Accessibility 1.1 API (upgradeable to MSAA 2.0.) A new HTML Help system with 15 Troubleshooting Wizards was introduced to replace WinHelp.
Users can configure the font in Notepad. Microsoft Paint supports GIF transparency. HyperTerminal supports a TCP/IP connection method, which allows it to be used as a Telnet client. Imaging for Windows is updated. System Monitor—used to track the performance of hardware and software—supports output to a log file.

Miscellaneous improvements
Telephony API (TAPI) 2.1
DCOM version 1.2
Ability to list fonts by similarity determined using PANOSE information.
Tools to automate setup, such as Batch 98 and INFInst.exe, support error-checking, gathering information automatically to create an INF file directly from a machine's registry, customizing IE4, shell and desktop settings and adding custom drivers.
Several other Resource Kit tools are included on the Windows 98 CD.
Windows 98 has new system event sounds for Low Battery Alarm and Critical Battery Alarm.
Windows 98 also introduced new and updated system sounds. The new startup sound for Windows 98 was composed by Microsoft sound engineer Ken Kato, who considered it to be a ""tough act to follow"".
Windows 98 shipped with Flash Player and Shockwave Player preinstalled.

Windows 98 Second Edition
Windows 98 Second Edition (often shortened to Windows 98 SE and sometimes to Win98 SE) is an updated version of Windows 98 released on May 5, 1999, nine months before the release of Windows 2000. It includes many bug fixes, improved WDM audio and modem support, improved USB support, the replacement of Internet Explorer 4.0 with Internet Explorer 5.0, Web Folders (WebDAV namespace extension for Windows Explorer), and related shell updates. Also included is basic OHCI-compliant FireWire DV camcorder support (MSDV class driver) and SBP-2 support for mass storage class devices. Wake-On-LAN reenables suspended networked computers due to network activity, and Internet Connection Sharing allows multiple networked client computers to share an Internet connection via a single host computer.Other features in the update include DirectX 6.1 which introduced major improvements to DirectSound and the introduction of DirectMusic, improvements to Asynchronous Transfer Mode support (IP/ATM, PPP/ATM and WinSock 2/ATM support), Windows Media Player 6.1 replacing the older Media Player, Microsoft NetMeeting 3.0, MDAC 2.1 and WMI. A memory overflow issue was resolved in which earlier versions of Windows 98 would crash most systems if left running for 49.7 days (equal to 232 milliseconds). Windows 98 SE could be obtained as retail upgrade and full version packages, as well as OEM and a Second Edition Updates Disc for existing Windows 98 users. USB audio device class support is present from Windows 98 SE onwards. Windows 98 Second Edition improved WDM support in general for all devices, and it introduced support for WDM for modems (and therefore USB modems and virtual COM ports). However, Microsoft driver support for both USB printers and USB mass-storage device class is not available for Windows 98.

Removed features
Windows 98 Second Edition did not ship with the WinG API or RealPlayer 4.0, unlike the original release of Windows 98, due to both of these having been superseded by DirectX and Windows Media Player, respectively.

Upgradeability
Several components of both Windows 98 and Windows 98 Second Edition can be updated to newer versions. These include:

Internet Explorer 6 SP1 and Outlook Express 6 SP1
Windows Media Format Runtime and Windows Media Player 9 Series on Windows 98 Second Edition (Windows Media Player 7.1 on Windows 98 original release)
Windows Media Encoder 7.1 and Windows Media 8 Encoding Utility
DirectX 9.0c (the latest compatible runtime is from October 2007.)
MSN Messenger 7.0
Significant features from newer Microsoft operating systems can be installed on Windows 98. Chief among them are .NET Framework versions 1.0, 1.1 and 2.0, the Visual C++ 2005 runtime, Windows Installer 2.0, the GDI+ redistributable library, Remote Desktop Connection client 5.2 and the Text Services Framework.
Several other components such as MSXML 3.0 SP7, Microsoft Agent 2.0, NetMeeting 3.01, MSAA 2.0, ActiveSync 3.8, WSH 5.6, Microsoft Data Access Components 2.81 SP1, WMI 1.5 and Speech API 4.0.
Office XP is the last version of Microsoft Office that is compatible with Windows 98.
Although Windows 98 does not fully support Unicode, certain Unicode applications can run if the Microsoft Layer for Unicode is installed.

System requirements
The two major versions of Windows 98 have minimum requirements needed to be run.

Users can bypass processor requirement checks with the undocumented /NM setup switch. This allows installation on computers with processors as old as the Intel 80386.

Limitations
The original release of Windows 98 may fail to boot on computers with a processor faster than 2.1 GHz. Windows 98 is only designed to handle up to 512 MB of RAM without changes. The maximum amount of RAM the operating system is designed to use is up to 1 GB of RAM. Systems with more than 1.5 GB of RAM may continuously reboot during startup. Both Windows 98 and Windows 98 Second Edition have problems running on hard drives of capacities larger than 32 GB in systems with certain Phoenix BIOS configurations. A software update fixed this shortcoming.

Support lifecycle
All computers running Windows NT 4.0 Workstation, Windows 2000 Professional, and Windows 98 can be directly upgraded to Windows XP Professional. Support for Windows 98 under Microsoft's consumer product life cycle policy was originally planned to end on June 30, 2003, however, in December 2002, Microsoft extended the support window to January 16, 2004. This date would then be extended again to June 30, 2006 on January 13, 2004 up to a final end of support date of July 11, 2006, citing support volumes in emerging markets as the reason for the extension.Retail availability for Windows 98 ended on June 30, 2002, and later became completely unavailable from Microsoft in any form (through MSDN or otherwise) due to the terms of Java-related settlements Microsoft made with Sun Microsystems.The Windows Update website continued to be available after Windows 98's end of support date, however, in 2011, Microsoft retired the Windows Update v4 website and removed the updates for Windows 98 and Windows 98 SE from its servers.

Reception
Windows 98 was released to generally favorable reviews, with praise directed to its improved graphical user interface and customizability, ease of use,: 30–31  and the degree to which it addressed complaints that users and critics had with Windows 95. Michael Sweet of Smart Computing characterized it as heavily integrating features of the Internet browser, and found file and folder navigation easier.: 30–31  Ed Bott of PC Computing lauded the bug fixes, easier troubleshooting, and support for hardware advances such as DVD players and USB. However, he also found that the operating system crashed only slightly less frequently, and criticized the high upgrade price and system requirements. He rated it four stars out of five.

Sales
Windows 98 sold 530,000 licenses in its first four days of availability, overtaking Windows 95's 510,000. It later sold a total of 580,000 and 350,000 licenses in the first and second months of availability, respectively.In the first year of its release, Windows 98 sold a total of 15 million licenses – 2 million more than its predecessor. However, International Data Corporation estimated that of the roughly 89 million shipped computers in the desktop market, the operating system had a market share of 17.2 percent, compared to Windows 95's 57.4 percent. Meanwhile, the two operating systems continued to observe a trend whereby Windows 98 improved in sales performance, whereas Windows 95 dwindled. After a legal dispute and subsequent settlement with Sun Microsystems over the former's Java Virtual Machine, Microsoft ceased distributing the operating system on December 15, 2003, and IDC estimated that a total of 58 million copies were installed worldwide by then.
Passage 5:
Susanne Klatten
Susanne Hanna Ursula Klatten (née Quandt, born 28 April 1962) is a German billionaire heiress, the daughter of Herbert and Johanna Quandt. As of January 2022, her net worth was estimated at US$23.4 billion, and the richest woman in Germany and the 50th richest person in the world according to the Bloomberg Billionaires Index.

Education
Klatten was born in Bad Homburg, West Germany. After gaining a degree in business finance, she worked for the advertising agency Young & Rubicam in Frankfurt from 1981 to 1983. This was followed by a course in marketing and management at the University of Buckingham, and an MBA from IMD Business School in Lausanne specialising in advertising.She gained further business experience in London with Dresdner Bank, the Munich branch of management consultants McKinsey and the bank Bankhaus Reuschel & Co.
She has often worked under the name Susanne Kant.

Investments
On her father's death she inherited his 50.1% stake in pharmaceutical and chemicals manufacturer Altana. She sits on Altana's supervisory board and helped transform it into a world-class corporation in the German DAX list of 30 top companies. In 2006 Altana AG sold its pharmaceutical activities to Nycomed for €4.5 billion, leaving only its speciality chemicals business. The €4.5 billion was distributed to shareholders as a dividend. Altana maintained its stock exchange listing and Klatten remained its majority shareholder. 
In 2009, she bought almost all shares she did not already own in Altana. Altana and SKion, which are both wholly owned by Susanne Klatten, are shareholder of Landa Digital Printing with together 46% since 2018. Landa Digital Printing is a company of the Israeli entrepreneur and inventor Benny Landa in the field of digital printing and nanotechnology.Her father also left her a 12.50% stake in BMW, but following the death of her mother in 2015, her stake in BMW is now 19.2%. She was appointed to the supervisory board of BMW with her brother Stefan Quandt in 1997.
German graphite maker SGL Carbon said on 16 March 2009 that Klatten owns options to raise her stake in SGL from 8% to almost a quarter of the shares but no more than that.

Quandt family activities during WWII
The Hanns Joachim Friedrichs Award winning documentary film The Silence of the Quandts by the German public broadcaster ARD described in October 2007 the role of the Quandt family businesses during the Second World War. The family's Nazi past was not well known, but the documentary film revealed this to a wide audience and confronted the Quandts about the use of slave labourers in the family's factories during World War II. As a result, five days after the showing, four family members announced, on behalf of the entire Quandt family, their intention to fund a research project in which a historian would examine the family's activities during Adolf Hitler's dictatorship. The independent 1,200-page study researched and compiled by Bonn historian, Joachim Scholtyseck, that was released in 2011 concluded: ""The Quandts were linked inseparably with the crimes of the Nazis"". As of 2008, no compensation, apology or even memorial at the site of one of their factories, have been permitted. BMW was not implicated in the report.

Personal life
Police prevented an attempt to kidnap her and her mother Johanna Quandt in 1978.Susanne met Jan Klatten while she was doing an internship with BMW in Regensburg, where he worked as an engineer. It is reported that during this time, she called herself Kant and did not tell him who she was until they were sure about each other, but Klatten himself denies the story. They married in 1990 in Kitzbühel and live in Munich. They have three children. The couple separated in 2018. She has been a member of the University Council of the Technical University of Munich since 2005. In 2007 she was awarded the Bayerischer Verdienstorden, the Bavarian Order of Merit. She is one of the biggest donors of the centre-right political party, the Christian Democratic Union.In 2007 Klatten was blackmailed by Helg ""Russak"" Sgarbi, a 44-year-old Swiss national who threatened to release materials depicting the two having an affair. Sgarbi, who was charged with similar blackmail schemes against multiple women, was arrested in January 2009 and brought to court in Germany, where he was sentenced to six years in jail. His accomplice, the Italian hotel owner Ernano Barretta who allegedly filmed Sgarbi and Klatten with hidden cameras, was also arrested and was sentenced in 2012 to seven years in prison.

See also
List of female billionaires
Passage 6:
List of countries by steel production
This article summarizes the world steel production by country.
In 2020, total world crude steel production was 1877.5 million tonnes (Mt).  The biggest steel producing country is currently China, which accounted for 57% of world steel production in 2020. In 2020, China became the first country to produce over one billion tons of steel. In 2008, 2009, 2015 and 2016 output fell in the majority of steel-producing countries as a result of the global recession. In 2010 and 2017, it started to rise again. Crude steel production contracted in all regions in 2019 except in Asia and the Middle East.

List of countries by steel production
This is a list of countries by steel production in 1967, 1980, 1990, 2000 and from 2007 to 2021, based on data provided by the World Steel Association. All countries with annual production of crude steel at least 2 million metric tons.

Exports
net: exports - imports

Imports
Net: imports − exports

See also
Steel industry
Global steel industry trends
List of steel producers
List of countries by iron ore production",['ease of use and enhanced support for Plug and Play'],11574,musique,en,,b4a033f133679fb6a0a2351544933c5537546d947ad50c8f,ease of use and enhanced support for Plug and Play,50
What did Madam de Merret and her husband do after walling off the closet?,"Produced by John Bickers, and Dagny





LA GRANDE BRETECHE

(Sequel to ""Another Study of Woman."")


By Honore De Balzac


Translated by Ellen Marriage and Clara Bell





LA GRANDE BRETECHE


""Ah! madame,"" replied the doctor, ""I have some appalling stories in my
collection. But each one has its proper hour in a conversation--you know
the pretty jest recorded by Chamfort, and said to the Duc de Fronsac:
'Between your sally and the present moment lie ten bottles of
champagne.'""

""But it is two in the morning, and the story of Rosina has prepared us,""
said the mistress of the house.

""Tell us, Monsieur Bianchon!"" was the cry on every side.

The obliging doctor bowed, and silence reigned.

""At about a hundred paces from Vendome, on the banks of the Loir,"" said
he, ""stands an old brown house, crowned with very high roofs, and so
completely isolated that there is nothing near it, not even a fetid
tannery or a squalid tavern, such as are commonly seen outside small
towns. In front of this house is a garden down to the river, where the
box shrubs, formerly clipped close to edge the walks, now straggle
at their own will. A few willows, rooted in the stream, have grown
up quickly like an enclosing fence, and half hide the house. The
wild plants we call weeds have clothed the bank with their beautiful
luxuriance. The fruit-trees, neglected for these ten years past,
no longer bear a crop, and their suckers have formed a thicket. The
espaliers are like a copse. The paths, once graveled, are overgrown with
purslane; but, to be accurate there is no trace of a path.

""Looking down from the hilltop, to which cling the ruins of the old
castle of the Dukes of Vendome, the only spot whence the eye can
see into this enclosure, we think that at a time, difficult now to
determine, this spot of earth must have been the joy of some country
gentleman devoted to roses and tulips, in a word, to horticulture, but
above all a lover of choice fruit. An arbor is visible, or rather
the wreck of an arbor, and under it a table still stands not entirely
destroyed by time. At the aspect of this garden that is no more, the
negative joys of the peaceful life of the provinces may be divined as we
divine the history of a worthy tradesman when we read the epitaph on his
tomb. To complete the mournful and tender impressions which seize the
soul, on one of the walls there is a sundial graced with this homely
Christian motto, '_Ultimam cogita_.'

""The roof of this house is dreadfully dilapidated; the outside shutters
are always closed; the balconies are hung with swallows' nests; the
doors are for ever shut. Straggling grasses have outlined the flagstones
of the steps with green; the ironwork is rusty. Moon and sun, winter,
summer, and snow have eaten into the wood, warped the boards, peeled
off the paint. The dreary silence is broken only by birds and cats,
polecats, rats, and mice, free to scamper round, and fight, and eat each
other. An invisible hand has written over it all: 'Mystery.'

""If, prompted by curiosity, you go to look at this house from the
street, you will see a large gate, with a round-arched top; the children
have made many holes in it. I learned later that this door had been
blocked for ten years. Through these irregular breaches you will see
that the side towards the courtyard is in perfect harmony with the side
towards the garden. The same ruin prevails. Tufts of weeds outline
the paving-stones; the walls are scored by enormous cracks, and the
blackened coping is laced with a thousand festoons of pellitory. The
stone steps are disjointed; the bell-cord is rotten; the gutter-spouts
broken. What fire from heaven could have fallen there? By what decree
has salt been sown on this dwelling? Has God been mocked here? Or was
France betrayed? These are the questions we ask ourselves. Reptiles
crawl over it, but give no reply. This empty and deserted house is a
vast enigma of which the answer is known to none.

""It was formerly a little domain, held in fief, and is known as La
Grande Breteche. During my stay at Vendome, where Despleins had left me
in charge of a rich patient, the sight of this strange dwelling became
one of my keenest pleasures. Was it not far better than a ruin? Certain
memories of indisputable authenticity attach themselves to a ruin; but
this house, still standing, though being slowly destroyed by an avenging
hand, contained a secret, an unrevealed thought. At the very least,
it testified to a caprice. More than once in the evening I boarded the
hedge, run wild, which surrounded the enclosure. I braved scratches, I
got into this ownerless garden, this plot which was no longer public or
private; I lingered there for hours gazing at the disorder. I would not,
as the price of the story to which this strange scene no doubt was due,
have asked a single question of any gossiping native. On that spot I
wove delightful romances, and abandoned myself to little debauches of
melancholy which enchanted me. If I had known the reason--perhaps quite
commonplace--of this neglect, I should have lost the unwritten poetry
which intoxicated me. To me this refuge represented the most various
phases of human life, shadowed by misfortune; sometimes the peace of the
graveyard without the dead, who speak in the language of epitaphs; one
day I saw in it the home of lepers; another, the house of the Atridae;
but, above all, I found there provincial life, with its contemplative
ideas, its hour-glass existence. I often wept there, I never laughed.

""More than once I felt involuntary terrors as I heard overhead the dull
hum of the wings of some hurrying wood-pigeon. The earth is dank; you
must be on the watch for lizards, vipers, and frogs, wandering about
with the wild freedom of nature; above all, you must have no fear
of cold, for in a few moments you feel an icy cloak settle on your
shoulders, like the Commendatore's hand on Don Giovanni's neck.

""One evening I felt a shudder; the wind had turned an old rusty
weathercock, and the creaking sounded like a cry from the house, at
the very moment when I was finishing a gloomy drama to account for
this monumental embodiment of woe. I returned to my inn, lost in gloomy
thoughts. When I had supped, the hostess came into my room with an air
of mystery, and said, 'Monsieur, here is Monsieur Regnault.'

""'Who is Monsieur Regnault?'

""'What, sir, do you not know Monsieur Regnault?--Well, that's odd,' said
she, leaving the room.

""On a sudden I saw a man appear, tall, slim, dressed in black, hat
in hand, who came in like a ram ready to butt his opponent, showing a
receding forehead, a small pointed head, and a colorless face of the hue
of a glass of dirty water. You would have taken him for an usher. The
stranger wore an old coat, much worn at the seams; but he had a diamond
in his shirt frill, and gold rings in his ears.

""'Monsieur,' said I, 'whom have I the honor of addressing?'--He took a
chair, placed himself in front of my fire, put his hat on my table,
and answered while he rubbed his hands: 'Dear me, it is very
cold.--Monsieur, I am Monsieur Regnault.'

""I was encouraging myself by saying to myself, '_Il bondo cani!_ Seek!'

""'I am,' he went on, 'notary at Vendome.'

""'I am delighted to hear it, monsieur,' I exclaimed. 'But I am not in a
position to make a will for reasons best known to myself.'

""'One moment!' said he, holding up his hand as though to gain silence.
'Allow me, monsieur, allow me! I am informed that you sometimes go to
walk in the garden of la Grande Breteche.'

""'Yes, monsieur.'

""'One moment!' said he, repeating his gesture. 'That constitutes a
misdemeanor. Monsieur, as executor under the will of the late Comtesse
de Merret, I come in her name to beg you to discontinue the practice.
One moment! I am not a Turk, and do not wish to make a crime of it. And
besides, you are free to be ignorant of the circumstances which
compel me to leave the finest mansion in Vendome to fall into ruin.
Nevertheless, monsieur, you must be a man of education, and you should
know that the laws forbid, under heavy penalties, any trespass on
enclosed property. A hedge is the same as a wall. But, the state in
which the place is left may be an excuse for your curiosity. For my
part, I should be quite content to make you free to come and go in the
house; but being bound to respect the will of the testatrix, I have
the honor, monsieur, to beg that you will go into the garden no more.
I myself, monsieur, since the will was read, have never set foot in the
house, which, as I had the honor of informing you, is part of the estate
of the late Madame de Merret. We have done nothing there but verify the
number of doors and windows to assess the taxes I have to pay annually
out of the funds left for that purpose by the late Madame de Merret. Ah!
my dear sir, her will made a great commotion in the town.'

""The good man paused to blow his nose. I respected his volubility,
perfectly understanding that the administration of Madame de Merret's
estate had been the most important event of his life, his reputation,
his glory, his Restoration. As I was forced to bid farewell to my
beautiful reveries and romances, I was to reject learning the truth on
official authority.

""'Monsieur,' said I, 'would it be indiscreet if I were to ask you the
reasons for such eccentricity?'

""At these words an expression, which revealed all the pleasure which
men feel who are accustomed to ride a hobby, overspread the lawyer's
countenance. He pulled up the collar of his shirt with an air, took out
his snuffbox, opened it, and offered me a pinch; on my refusing, he took
a large one. He was happy! A man who has no hobby does not know all
the good to be got out of life. A hobby is the happy medium between a
passion and a monomania. At this moment I understood the whole bearing
of Sterne's charming passion, and had a perfect idea of the delight with
which my uncle Toby, encouraged by Trim, bestrode his hobby-horse.

""'Monsieur,' said Monsieur Regnault, 'I was head-clerk in Monsieur
Roguin's office, in Paris. A first-rate house, which you may have heard
mentioned? No! An unfortunate bankruptcy made it famous.--Not having
money enough to purchase a practice in Paris at the price to which they
were run up in 1816, I came here and bought my predecessor's business.
I had relations in Vendome; among others, a wealthy aunt, who allowed
me to marry her daughter.--Monsieur,' he went on after a little pause,
'three months after being licensed by the Keeper of the Seals, one
evening, as I was going to bed--it was before my marriage--I was sent
for by Madame la Comtesse de Merret, to her Chateau of Merret. Her maid,
a good girl, who is now a servant in this inn, was waiting at my door
with the Countess' own carriage. Ah! one moment! I ought to tell you
that Monsieur le Comte de Merret had gone to Paris to die two months
before I came here. He came to a miserable end, flinging himself into
every kind of dissipation. You understand?

""'On the day when he left, Madame la Comtesse had quitted la Grand
Breteche, having dismantled it. Some people even say that she had
burnt all the furniture, the hangings--in short, all the chattels and
furniture whatever used in furnishing the premises now let by the
said M.--(Dear, what am I saying? I beg your pardon, I thought I was
dictating a lease.)--In short, that she burnt everything in the meadow
at Merret. Have you been to Merret, monsieur?--No,' said he, answering
himself, 'Ah, it is a very fine place.'

""'For about three months previously,' he went on, with a jerk of his
head, 'the Count and Countess had lived in a very eccentric way; they
admitted no visitors; Madame lived on the ground-floor, and Monsieur on
the first floor. When the Countess was left alone, she was never seen
excepting at church. Subsequently, at home, at the chateau, she refused
to see the friends, whether gentlemen or ladies, who went to call on
her. She was already very much altered when she left la Grande Breteche
to go to Merret. That dear lady--I say dear lady, for it was she who
gave me this diamond, but indeed I saw her but once--that kind lady was
very ill; she had, no doubt, given up all hope, for she died without
choosing to send for a doctor; indeed, many of our ladies fancied she
was not quite right in her head. Well, sir, my curiosity was strangely
excited by hearing that Madame de Merret had need of my services. Nor
was I the only person who took an interest in the affair. That very
night, though it was already late, all the town knew that I was going to
Merret.

""'The waiting-woman replied but vaguely to the questions I asked her on
the way; nevertheless, she told me that her mistress had received the
Sacrament in the course of the day at the hands of the Cure of Merret,
and seemed unlikely to live through the night. It was about eleven when
I reached the chateau. I went up the great staircase. After crossing
some large, lofty, dark rooms, diabolically cold and damp, I reached the
state bedroom where the Countess lay. From the rumors that were current
concerning this lady (monsieur, I should never end if I were to repeat
all the tales that were told about her), I had imagined her a coquette.
Imagine, then, that I had great difficulty in seeing her in the great
bed where she was lying. To be sure, to light this enormous room, with
old-fashioned heavy cornices, and so thick with dust that merely to see
it was enough to make you sneeze, she had only an old Argand lamp. Ah!
but you have not been to Merret. Well, the bed is one of those old world
beds, with a high tester hung with flowered chintz. A small table stood
by the bed, on which I saw an ""Imitation of Christ,"" which, by the
way, I bought for my wife, as well as the lamp. There were also a deep
armchair for her confidential maid, and two small chairs. There was no
fire. That was all the furniture, not enough to fill ten lines in an
inventory.

""'My dear sir, if you had seen, as I then saw, that vast room, papered
and hung with brown, you would have felt yourself transported into a
scene of a romance. It was icy, nay more, funereal,' and he lifted his
hand with a theatrical gesture and paused.

""'By dint of seeking, as I approached the bed, at last I saw Madame de
Merret, under the glimmer of the lamp, which fell on the pillows.
Her face was as yellow as wax, and as narrow as two folded hands. The
Countess had a lace cap showing her abundant hair, but as white as linen
thread. She was sitting up in bed, and seemed to keep upright with
great difficulty. Her large black eyes, dimmed by fever, no doubt,
and half-dead already, hardly moved under the bony arch of her
eyebrows.--There,' he added, pointing to his own brow. 'Her forehead was
clammy; her fleshless hands were like bones covered with soft skin;
the veins and muscles were perfectly visible. She must have been very
handsome; but at this moment I was startled into an indescribable
emotion at the sight. Never, said those who wrapped her in her shroud,
had any living creature been so emaciated and lived. In short, it was
awful to behold! Sickness so consumed that woman, that she was no more
than a phantom. Her lips, which were pale violet, seemed to me not to
move when she spoke to me.

""'Though my profession has familiarized me with such spectacles, by
calling me not infrequently to the bedside of the dying to record their
last wishes, I confess that families in tears and the agonies I have
seen were as nothing in comparison with this lonely and silent woman in
her vast chateau. I heard not the least sound, I did not perceive the
movement which the sufferer's breathing ought to have given to the
sheets that covered her, and I stood motionless, absorbed in looking at
her in a sort of stupor. In fancy I am there still. At last her large
eyes moved; she tried to raise her right hand, but it fell back on the
bed, and she uttered these words, which came like a breath, for her
voice was no longer a voice: ""I have waited for you with the greatest
impatience."" A bright flush rose to her cheeks. It was a great effort to
her to speak.

""'""Madame,"" I began. She signed to me to be silent. At that moment
the old housekeeper rose and said in my ear, ""Do not speak; Madame la
Comtesse is not in a state to bear the slightest noise, and what you say
might agitate her.""

""'I sat down. A few instants after, Madame de Merret collected all her
remaining strength to move her right hand, and slipped it, not without
infinite difficulty, under the bolster; she then paused a moment. With
a last effort she withdrew her hand; and when she brought out a sealed
paper, drops of perspiration rolled from her brow. ""I place my will in
your hands--Oh! God! Oh!"" and that was all. She clutched a crucifix that
lay on the bed, lifted it hastily to her lips, and died.

""'The expression of her eyes still makes me shudder as I think of it.
She must have suffered much! There was joy in her last glance, and it
remained stamped on her dead eyes.

""'I brought away the will, and when it was opened I found that Madame de
Merret had appointed me her executor. She left the whole of her property
to the hospital at Vendome excepting a few legacies. But these were her
instructions as relating to la Grande Breteche: She ordered me to leave
the place, for fifty years counting from the day of her death, in the
state in which it might be at the time of her death, forbidding any one,
whoever he might be, to enter the apartments, prohibiting any repairs
whatever, and even settling a salary to pay watchmen if it were needful
to secure the absolute fulfilment of her intentions. At the expiration
of that term, if the will of the testatrix has been duly carried out,
the house is to become the property of my heirs, for, as you know, a
notary cannot take a bequest. Otherwise la Grande Breteche reverts to
the heirs-at-law, but on condition of fulfilling certain conditions
set forth in a codicil to the will, which is not to be opened till
the expiration of the said term of fifty years. The will has not been
disputed, so----' And without finishing his sentence, the lanky notary
looked at me with an air of triumph; I made him quite happy by offering
him my congratulations.

""'Monsieur,' I said in conclusion, 'you have so vividly impressed
me that I fancy I see the dying woman whiter than her sheets; her
glittering eyes frighten me; I shall dream of her to-night.--But you
must have formed some idea as to the instructions contained in that
extraordinary will.'

""'Monsieur,' said he, with comical reticence, 'I never allow myself
to criticise the conduct of a person who honors me with the gift of a
diamond.'

""However, I soon loosened the tongue of the discreet notary of Vendome,
who communicated to me, not without long digressions, the opinions of
the deep politicians of both sexes whose judgments are law in Vendome.
But these opinions were so contradictory, so diffuse, that I was
near falling asleep in spite of the interest I felt in this authentic
history. The notary's ponderous voice and monotonous accent, accustomed
no doubt to listen to himself and to make himself listened to by his
clients or fellow-townsmen, were too much for my curiosity. Happily, he
soon went away.

""'Ah, ha, monsieur,' said he on the stairs, 'a good many persons would
be glad to live five-and-forty years longer; but--one moment!' and he
laid the first finger of his right hand to his nostril with a cunning
look, as much as to say, 'Mark my words!--To last as long as that--as
long as that,' said he, 'you must not be past sixty now.'

""I closed my door, having been roused from my apathy by this last
speech, which the notary thought very funny; then I sat down in my
armchair, with my feet on the fire-dogs. I had lost myself in a romance
_a la_ Radcliffe, constructed on the juridical base given me by Monsieur
Regnault, when the door, opened by a woman's cautious hand, turned on
the hinges. I saw my landlady come in, a buxom, florid dame, always
good-humored, who had missed her calling in life. She was a Fleming, who
ought to have seen the light in a picture by Teniers.

""'Well, monsieur,' said she, 'Monsieur Regnault has no doubt been giving
you his history of la Grande Breteche?'

""'Yes, Madame Lepas.'

""'And what did he tell you?'

""I repeated in a few words the creepy and sinister story of Madame de
Merret. At each sentence my hostess put her head forward, looking at
me with an innkeeper's keen scrutiny, a happy compromise between the
instinct of a police constable, the astuteness of a spy, and the cunning
of a dealer.

""'My good Madame Lepas,' said I as I ended, 'you seem to know more about
it. Heh? If not, why have you come up to me?'

""'On my word, as an honest woman----'

""'Do not swear; your eyes are big with a secret. You knew Monsieur de
Merret; what sort of man was he?'

""'Monsieur de Merret--well, you see he was a man you never could see
the top of, he was so tall! A very good gentleman, from Picardy, and who
had, as we say, his head close to his cap. He paid for everything down,
so as never to have difficulties with any one. He was hot-tempered, you
see! All our ladies liked him very much.'

""'Because he was hot-tempered?' I asked her.

""'Well, may be,' said she; 'and you may suppose, sir, that a man had to
have something to show for a figurehead before he could marry Madame de
Merret, who, without any reflection on others, was the handsomest and
richest heiress in our parts. She had about twenty thousand francs
a year. All the town was at the wedding; the bride was pretty and
sweet-looking, quite a gem of a woman. Oh, they were a handsome couple
in their day!'

""'And were they happy together?'

""'Hm, hm! so-so--so far as can be guessed, for, as you may suppose, we
of the common sort were not hail-fellow-well-met with them.--Madame de
Merret was a kind woman and very pleasant, who had no doubt sometimes to
put up with her husband's tantrums. But though he was rather haughty, we
were fond of him. After all, it was his place to behave so. When a man
is a born nobleman, you see----'

""'Still, there must have been some catastrophe for Monsieur and Madame
de Merret to part so violently?'

""'I did not say there was any catastrophe, sir. I know nothing about
it.'

""'Indeed. Well, now, I am sure you know everything.'

""'Well, sir, I will tell you the whole story.--When I saw Monsieur
Regnault go up to see you, it struck me that he would speak to you about
Madame de Merret as having to do with la Grande Breteche. That put it
into my head to ask your advice, sir, seeming to me that you are a
man of good judgment and incapable of playing a poor woman like me
false--for I never did any one a wrong, and yet I am tormented by my
conscience. Up to now I have never dared to say a word to the people of
these parts; they are all chatter-mags, with tongues like knives. And
never till now, sir, have I had any traveler here who stayed so long in
the inn as you have, and to whom I could tell the history of the fifteen
thousand francs----'

""'My dear Madame Lepas, if there is anything in your story of a nature
to compromise me,' I said, interrupting the flow of her words, 'I would
not hear it for all the world.'

""'You need have no fears,' said she; 'you will see.'

""Her eagerness made me suspect that I was not the only person to whom
my worthy landlady had communicated the secret of which I was to be the
sole possessor, but I listened.

""'Monsieur,' said she, 'when the Emperor sent the Spaniards here,
prisoners of war and others, I was required to lodge at the charge
of the Government a young Spaniard sent to Vendome on parole.
Notwithstanding his parole, he had to show himself every day to the
sub-prefect. He was a Spanish grandee--neither more nor less. He had
a name in _os_ and _dia_, something like Bagos de Feredia. I wrote his
name down in my books, and you may see it if you like. Ah! he was a
handsome young fellow for a Spaniard, who are all ugly they say. He was
not more than five feet two or three in height, but so well made; and he
had little hands that he kept so beautifully! Ah! you should have
seen them. He had as many brushes for his hands as a woman has for her
toilet. He had thick, black hair, a flame in his eye, a somewhat coppery
complexion, but which I admired all the same. He wore the finest linen
I have ever seen, though I have had princesses to lodge here, and, among
others, General Bertrand, the Duc and Duchesse d'Abrantes, Monsieur
Descazes, and the King of Spain. He did not eat much, but he had such
polite and amiable ways that it was impossible to owe him a grudge for
that. Oh! I was very fond of him, though he did not say four words to me
in a day, and it was impossible to have the least bit of talk with him;
if he was spoken to, he did not answer; it is a way, a mania they all
have, it would seem.

""'He read his breviary like a priest, and went to mass and all the
services quite regularly. And where did he post himself?--we found this
out later.--Within two yards of Madame de Merret's chapel. As he took
that place the very first time he entered the church, no one imagined
that there was any purpose in it. Besides, he never raised his nose
above his book, poor young man! And then, monsieur, of an evening he
went for a walk on the hill among the ruins of the old castle. It was
his only amusement, poor man; it reminded him of his native land. They
say that Spain is all hills!

""'One evening, a few days after he was sent here, he was out very late.
I was rather uneasy when he did not come in till just on the stroke of
midnight; but we all got used to his whims; he took the key of the door,
and we never sat up for him. He lived in a house belonging to us in the
Rue des Casernes. Well, then, one of our stable-boys told us one evening
that, going down to wash the horses in the river, he fancied he had seen
the Spanish Grandee swimming some little way off, just like a fish. When
he came in, I told him to be careful of the weeds, and he seemed put out
at having been seen in the water.

""'At last, monsieur, one day, or rather one morning, we did not find
him in his room; he had not come back. By hunting through his things, I
found a written paper in the drawer of his table, with fifty pieces of
Spanish gold of the kind they call doubloons, worth about five thousand
francs; and in a little sealed box ten thousand francs worth of
diamonds. The paper said that in case he should not return, he left us
this money and these diamonds in trust to found masses to thank God for
his escape and for his salvation.

""'At that time I still had my husband, who ran off in search of him.
And this is the queer part of the story: he brought back the Spaniard's
clothes, which he had found under a big stone on a sort of breakwater
along the river bank, nearly opposite la Grande Breteche. My husband
went so early that no one saw him. After reading the letter, he burnt
the clothes, and, in obedience to Count Feredia's wish, we announced
that he had escaped.

""'The sub-prefect set all the constabulary at his heels; but, pshaw! he
was never caught. Lepas believed that the Spaniard had drowned himself.
I, sir, have never thought so; I believe, on the contrary, that he had
something to do with the business about Madame de Merret, seeing that
Rosalie told me that the crucifix her mistress was so fond of that she
had it buried with her, was made of ebony and silver; now in the early
days of his stay here, Monsieur Feredia had one of ebony and silver
which I never saw later.--And now, monsieur, do not you say that I need
have no remorse about the Spaniard's fifteen thousand francs? Are they
not really and truly mine?'

""'Certainly.--But have you never tried to question Rosalie?' said I.

""'Oh, to be sure I have, sir. But what is to be done? That girl is like
a wall. She knows something, but it is impossible to make her talk.'

""After chatting with me for a few minutes, my hostess left me a prey
to vague and sinister thoughts, to romantic curiosity, and a religious
dread, not unlike the deep emotion which comes upon us when we go into a
dark church at night and discern a feeble light glimmering under a lofty
vault--a dim figure glides across--the sweep of a gown or of a priest's
cassock is audible--and we shiver! La Grande Breteche, with its rank
grasses, its shuttered windows, its rusty iron-work, its locked doors,
its deserted rooms, suddenly rose before me in fantastic vividness. I
tried to get into the mysterious dwelling to search out the heart of
this solemn story, this drama which had killed three persons.

""Rosalie became in my eyes the most interesting being in Vendome. As
I studied her, I detected signs of an inmost thought, in spite of the
blooming health that glowed in her dimpled face. There was in her soul
some element of ruth or of hope; her manner suggested a secret, like
the expression of devout souls who pray in excess, or of a girl who has
killed her child and for ever hears its last cry. Nevertheless, she was
simple and clumsy in her ways; her vacant smile had nothing criminal
in it, and you would have pronounced her innocent only from seeing the
large red and blue checked kerchief that covered her stalwart bust,
tucked into the tight-laced bodice of a lilac- and white-striped gown.
'No,' said I to myself, 'I will not quit Vendome without knowing the
whole history of la Grande Breteche. To achieve this end, I will make
love to Rosalie if it proves necessary.'

""'Rosalie!' said I one evening.

""'Your servant, sir?'

""'You are not married?' She started a little.

""'Oh! there is no lack of men if ever I take a fancy to be miserable!'
she replied, laughing. She got over her agitation at once; for every
woman, from the highest lady to the inn-servant inclusive, has a native
presence of mind.

""'Yes; you are fresh and good-looking enough never to lack lovers! But
tell me, Rosalie, why did you become an inn-servant on leaving Madame de
Merret? Did she not leave you some little annuity?'

""'Oh yes, sir. But my place here is the best in all the town of
Vendome.'

""This reply was such an one as judges and attorneys call evasive.
Rosalie, as it seemed to me, held in this romantic affair the place of
the middle square of the chess-board: she was at the very centre of the
interest and of the truth; she appeared to me to be tied into the knot
of it. It was not a case for ordinary love-making; this girl contained
the last chapter of a romance, and from that moment all my attentions
were devoted to Rosalie. By dint of studying the girl, I observed in
her, as in every woman whom we make our ruling thought, a variety of
good qualities; she was clean and neat; she was handsome, I need not
say; she soon was possessed of every charm that desire can lend to a
woman in whatever rank of life. A fortnight after the notary's visit,
one evening, or rather one morning, in the small hours, I said to
Rosalie:

""'Come, tell me all you know about Madame de Merret.'

""'Oh!' she said, 'I will tell you; but keep the secret carefully.'

""'All right, my child; I will keep all your secrets with a thief's
honor, which is the most loyal known.'

""'If it is all the same to you,' said she, 'I would rather it should be
with your own.'

""Thereupon she set her head-kerchief straight, and settled herself to
tell the tale; for there is no doubt a particular attitude of confidence
and security is necessary to the telling of a narrative. The best tales
are told at a certain hour--just as we are all here at table. No one
ever told a story well standing up, or fasting.

""If I were to reproduce exactly Rosalie's diffuse eloquence, a whole
volume would scarcely contain it. Now, as the event of which she gave me
a confused account stands exactly midway between the notary's gossip and
that of Madame Lepas, as precisely as the middle term of a rule-of-three
sum stands between the first and third, I have only to relate it in as
few words as may be. I shall therefore be brief.

""The room at la Grande Breteche in which Madame de Merret slept was on
the ground floor; a little cupboard in the wall, about four feet deep,
served her to hang her dresses in. Three months before the evening of
which I have to relate the events, Madame de Merret had been seriously
ailing, so much so that her husband had left her to herself, and had his
own bedroom on the first floor. By one of those accidents which it is
impossible to foresee, he came in that evening two hours later than
usual from the club, where he went to read the papers and talk politics
with the residents in the neighborhood. His wife supposed him to have
come in, to be in bed and asleep. But the invasion of France had been
the subject of a very animated discussion; the game of billiards had
waxed vehement; he had lost forty francs, an enormous sum at Vendome,
where everybody is thrifty, and where social habits are restrained
within the bounds of a simplicity worthy of all praise, and the
foundation perhaps of a form of true happiness which no Parisian would
care for.

""For some time past Monsieur de Merret had been satisfied to ask Rosalie
whether his wife was in bed; on the girl's replying always in the
affirmative, he at once went to his own room, with the good faith that
comes of habit and confidence. But this evening, on coming in, he took
it into his head to go to see Madame de Merret, to tell her of his
ill-luck, and perhaps to find consolation. During dinner he had observed
that his wife was very becomingly dressed; he reflected as he came
home from the club that his wife was certainly much better, that
convalescence had improved her beauty, discovering it, as husbands
discover everything, a little too late. Instead of calling Rosalie,
who was in the kitchen at the moment watching the cook and the coachman
playing a puzzling hand at cards, Monsieur de Merret made his way to his
wife's room by the light of his lantern, which he set down at the lowest
step of the stairs. His step, easy to recognize, rang under the vaulted
passage.

""At the instant when the gentleman turned the key to enter his wife's
room, he fancied he heard the door shut of the closet of which I have
spoken; but when he went in, Madame de Merret was alone, standing in
front of the fireplace. The unsuspecting husband fancied that Rosalie
was in the cupboard; nevertheless, a doubt, ringing in his ears like a
peal of bells, put him on his guard; he looked at his wife, and read in
her eyes an indescribably anxious and haunted expression.

""'You are very late,' said she.--Her voice, usually so clear and sweet,
struck him as being slightly husky.

""Monsieur de Merret made no reply, for at this moment Rosalie came in.
This was like a thunder-clap. He walked up and down the room, going from
one window to another at a regular pace, his arms folded.

""'Have you had bad news, or are you ill?' his wife asked him timidly,
while Rosalie helped her to undress. He made no reply.

""'You can go, Rosalie,' said Madame de Merret to her maid; 'I can put in
my curl-papers myself.'--She scented disaster at the mere aspect of her
husband's face, and wished to be alone with him. As soon as Rosalie
was gone, or supposed to be gone, for she lingered a few minutes in the
passage, Monsieur de Merret came and stood facing his wife, and said
coldly, 'Madame, there is some one in your cupboard!' She looked at her
husband calmly, and replied quite simply, 'No, monsieur.'

""This 'No' wrung Monsieur de Merret's heart; he did not believe it; and
yet his wife had never appeared purer or more saintly than she seemed
to be at this moment. He rose to go and open the closet door. Madame de
Merret took his hand, stopped him, looked at him sadly, and said in a
voice of strange emotion, 'Remember, if you should find no one there,
everything must be at an end between you and me.'

""The extraordinary dignity of his wife's attitude filled him with deep
esteem for her, and inspired him with one of those resolves which need
only a grander stage to become immortal.

""'No, Josephine,' he said, 'I will not open it. In either event we
should be parted for ever. Listen; I know all the purity of your soul, I
know you lead a saintly life, and would not commit a deadly sin to save
your life.'--At these words Madame de Merret looked at her husband with
a haggard stare.--'See, here is your crucifix,' he went on. 'Swear to
me before God that there is no one in there; I will believe you--I will
never open that door.'

""Madame de Merret took up the crucifix and said, 'I swear it.'

""'Louder,' said her husband; 'and repeat: ""I swear before God that there
is nobody in that closet.""' She repeated the words without flinching.

""'That will do,' said Monsieur de Merret coldly. After a moment's
silence: 'You have there a fine piece of work which I never saw before,'
said he, examining the crucifix of ebony and silver, very artistically
wrought.

""'I found it at Duvivier's; last year when that troop of Spanish
prisoners came through Vendome, he bought it of a Spanish monk.'

""'Indeed,' said Monsieur de Merret, hanging the crucifix on its nail;
and he rang the bell.

""He had to wait for Rosalie. Monsieur de Merret went forward quickly
to meet her, led her into the bay of the window that looked on to the
garden, and said to her in an undertone:

""'I know that Gorenflot wants to marry you, that poverty alone prevents
your setting up house, and that you told him you would not be his wife
till he found means to become a master mason.--Well, go and fetch him;
tell him to come here with his trowel and tools. Contrive to wake no one
in his house but himself. His reward will be beyond your wishes. Above
all, go out without saying a word--or else!' and he frowned.

""Rosalie was going, and he called her back. 'Here, take my latch-key,'
said he.

""'Jean!' Monsieur de Merret called in a voice of thunder down the
passage. Jean, who was both coachman and confidential servant, left his
cards and came.

""'Go to bed, all of you,' said his master, beckoning him to come close;
and the gentleman added in a whisper, 'When they are all asleep--mind,
_asleep_--you understand?--come down and tell me.'

""Monsieur de Merret, who had never lost sight of his wife while giving
his orders, quietly came back to her at the fireside, and began to tell
her the details of the game of billiards and the discussion at the club.
When Rosalie returned she found Monsieur and Madame de Merret conversing
amiably.

""Not long before this Monsieur de Merret had had new ceilings made to
all the reception-rooms on the ground floor. Plaster is very scarce at
Vendome; the price is enhanced by the cost of carriage; the gentleman
had therefore had a considerable quantity delivered to him, knowing
that he could always find purchasers for what might be left. It was this
circumstance which suggested the plan he carried out.

""'Gorenflot is here, sir,' said Rosalie in a whisper.

""'Tell him to come in,' said her master aloud.

""Madame de Merret turned paler when she saw the mason.

""'Gorenflot,' said her husband, 'go and fetch some bricks from the
coach-house; bring enough to wall up the door of this cupboard; you can
use the plaster that is left for cement.' Then, dragging Rosalie and the
workman close to him--'Listen, Gorenflot,' said he, in a low voice,
'you are to sleep here to-night; but to-morrow morning you shall have a
passport to take you abroad to a place I will tell you of. I will give
you six thousand francs for your journey. You must live in that town for
ten years; if you find you do not like it, you may settle in another,
but it must be in the same country. Go through Paris and wait there till
I join you. I will there give you an agreement for six thousand francs
more, to be paid to you on your return, provided you have carried out
the conditions of the bargain. For that price you are to keep perfect
silence as to what you have to do this night. To you, Rosalie, I will
secure ten thousand francs, which will not be paid to you till your
wedding day, and on condition of your marrying Gorenflot; but, to get
married, you must hold your tongue. If not, no wedding gift!'

""'Rosalie,' said Madame de Merret, 'come and brush my hair.'

""Her husband quietly walked up and down the room, keeping an eye on the
door, on the mason, and on his wife, but without any insulting display
of suspicion. Gorenflot could not help making some noise. Madame de
Merret seized a moment when he was unloading some bricks, and when her
husband was at the other end of the room to say to Rosalie: 'My dear
child, I will give you a thousand francs a year if only you will tell
Gorenflot to leave a crack at the bottom.' Then she added aloud quite
coolly: 'You had better help him.'

""Monsieur and Madame de Merret were silent all the time while Gorenflot
was walling up the door. This silence was intentional on the husband's
part; he did not wish to give his wife the opportunity of saying
anything with a double meaning. On Madame de Merret's side it was pride
or prudence. When the wall was half built up the cunning mason took
advantage of his master's back being turned to break one of the two
panes in the top of the door with a blow of his pick. By this Madame de
Merret understood that Rosalie had spoken to Gorenflot. They all three
then saw the face of a dark, gloomy-looking man, with black hair and
flaming eyes.

""Before her husband turned round again the poor woman had nodded to the
stranger, to whom the signal was meant to convey, 'Hope.'

""At four o'clock, as the day was dawning, for it was the month of
September, the work was done. The mason was placed in charge of Jean,
and Monsieur de Merret slept in his wife's room.

""Next morning when he got up he said with apparent carelessness, 'Oh,
by the way, I must go to the Maire for the passport.' He put on his hat,
took two or three steps towards the door, paused, and took the crucifix.
His wife was trembling with joy.

""'He will go to Duvivier's,' thought she.

""As soon as he had left, Madame de Merret rang for Rosalie, and then in
a terrible voice she cried: 'The pick! Bring the pick! and set to work.
I saw how Gorenflot did it yesterday; we shall have time to make a gap
and build it up again.'

""In an instant Rosalie had brought her mistress a sort of cleaver; she,
with a vehemence of which no words can give an idea, set to work to
demolish the wall. She had already got out a few bricks, when, turning
to deal a stronger blow than before, she saw behind her Monsieur de
Merret. She fainted away.

""'Lay madame on her bed,' said he coldly.

""Foreseeing what would certainly happen in his absence, he had laid
this trap for his wife; he had merely written to the Maire and sent for
Duvivier. The jeweler arrived just as the disorder in the room had been
repaired.

""'Duvivier,' asked Monsieur de Merret, 'did not you buy some crucifixes
of the Spaniards who passed through the town?'

""'No, monsieur.'

""'Very good; thank you,' said he, flashing a tiger's glare at his wife.
'Jean,' he added, turning to his confidential valet, 'you can serve my
meals here in Madame de Merret's room. She is ill, and I shall not leave
her till she recovers.'

""The cruel man remained in his wife's room for twenty days. During
the earlier time, when there was some little noise in the closet,
and Josephine wanted to intercede for the dying man, he said, without
allowing her to utter a word, 'You swore on the Cross that there was no
one there.'""


After this story all the ladies rose from table, and thus the spell
under which Bianchon had held them was broken. But there were some among
them who had almost shivered at the last words.




ADDENDUM

The following personage appears in other stories of the Human Comedy.

     Bianchon, Horace
       Father Goriot
       The Atheist's Mass
       Cesar Birotteau
       The Commission in Lunacy
       Lost Illusions
       A Distinguished Provincial at Paris
       A Bachelor's Establishment
       The Secrets of a Princess
       The Government Clerks
       Pierrette
       A Study of Woman
       Scenes from a Courtesan's Life
       Honorine
       The Seamy Side of History
       The Magic Skin
       A Second Home
       A Prince of Bohemia
       Letters of Two Brides
       The Muse of the Department
       The Imaginary Mistress
       The Middle Classes
       Cousin Betty
       The Country Parson

     In addition, M. Bianchon narrated the following:
       Another Study of Woman






End of the Project Gutenberg EBook of La Grande Breteche, by Honore de Balzac",['They stayed in the bedroom for a few days with the sounds of her lover trapped. '],8148,narrativeqa,en,,8341f7a6590de673c41b1847546ba9a2b7bef46906443a29,They stayed in the bedroom for a few days with the sounds of her lover trapped. ,80
Why was Mortimer Trefinnis once estranged from his siblings?,"Produced by David Brannan.  HTML version by Al Haines.









The Adventure of the Devil's Foot


By

Sir Arthur Conan Doyle




In recording from time to time some of the curious experiences and
interesting recollections which I associate with my long and intimate
friendship with Mr. Sherlock Holmes, I have continually been faced by
difficulties caused by his own aversion to publicity.  To his sombre
and cynical spirit all popular applause was always abhorrent, and
nothing amused him more at the end of a successful case than to hand
over the actual exposure to some orthodox official, and to listen with
a mocking smile to the general chorus of misplaced congratulation.  It
was indeed this attitude upon the part of my friend and certainly not
any lack of interesting material which has caused me of late years to
lay very few of my records before the public.  My participation in some
of his adventures was always a privilege which entailed discretion and
reticence upon me.

It was, then, with considerable surprise that I received a telegram
from Holmes last Tuesday--he has never been known to write where a
telegram would serve--in the following terms:

Why not tell them of the Cornish horror--strangest case I have handled.

I have no idea what backward sweep of memory had brought the matter
fresh to his mind, or what freak had caused him to desire that I should
recount it; but I hasten, before another cancelling telegram may
arrive, to hunt out the notes which give me the exact details of the
case and to lay the narrative before my readers.

It was, then, in the spring of the year 1897 that Holmes's iron
constitution showed some symptoms of giving way in the face of constant
hard work of a most exacting kind, aggravated, perhaps, by occasional
indiscretions of his own.  In March of that year Dr. Moore Agar, of
Harley Street, whose dramatic introduction to Holmes I may some day
recount, gave positive injunctions that the famous private agent lay
aside all his cases and surrender himself to complete rest if he wished
to avert an absolute breakdown.  The state of his health was not a
matter in which he himself took the faintest interest, for his mental
detachment was absolute, but he was induced at last, on the threat of
being permanently disqualified from work, to give himself a complete
change of scene and air.  Thus it was that in the early spring of that
year we found ourselves together in a small cottage near Poldhu Bay, at
the further extremity of the Cornish peninsula.

It was a singular spot, and one peculiarly well suited to the grim
humour of my patient.  From the windows of our little whitewashed
house, which stood high upon a grassy headland, we looked down upon the
whole sinister semicircle of Mounts Bay, that old death trap of sailing
vessels, with its fringe of black cliffs and surge-swept reefs on which
innumerable seamen have met their end.  With a northerly breeze it lies
placid and sheltered, inviting the storm-tossed craft to tack into it
for rest and protection.

Then come the sudden swirl round of the wind, the blistering gale from
the south-west, the dragging anchor, the lee shore, and the last battle
in the creaming breakers.  The wise mariner stands far out from that
evil place.

On the land side our surroundings were as sombre as on the sea. It was
a country of rolling moors, lonely and dun-colored, with an occasional
church tower to mark the site of some old-world village.  In every
direction upon these moors there were traces of some vanished race
which had passed utterly away, and left as its sole record strange
monuments of stone, irregular mounds which contained the burned ashes
of the dead, and curious earthworks which hinted at prehistoric strife.
The glamour and mystery of the place, with its sinister atmosphere of
forgotten nations, appealed to the imagination of my friend, and he
spent much of his time in long walks and solitary meditations upon the
moor. The ancient Cornish language had also arrested his attention, and
he had, I remember, conceived the idea that it was akin to the
Chaldean, and had been largely derived from the Phoenician traders in
tin.  He had received a consignment of books upon philology and was
settling down to develop this thesis when suddenly, to my sorrow and to
his unfeigned delight, we found ourselves, even in that land of dreams,
plunged into a problem at our very doors which was more intense, more
engrossing, and infinitely more mysterious than any of those which had
driven us from London.  Our simple life and peaceful, healthy routine
were violently interrupted, and we were precipitated into the midst of
a series of events which caused the utmost excitement not only in
Cornwall but throughout the whole west of England. Many of my readers
may retain some recollection of what was called at the time ""The
Cornish Horror,"" though a most imperfect account of the matter reached
the London press. Now, after thirteen years, I will give the true
details of this inconceivable affair to the public.

I have said that scattered towers marked the villages which dotted this
part of Cornwall.  The nearest of these was the hamlet of Tredannick
Wollas, where the cottages of a couple of hundred inhabitants clustered
round an ancient, moss-grown church.  The vicar of the parish, Mr.
Roundhay, was something of an archaeologist, and as such Holmes had
made his acquaintance. He was a middle-aged man, portly and affable,
with a considerable fund of local lore.  At his invitation we had taken
tea at the vicarage and had come to know, also, Mr. Mortimer Tregennis,
an independent gentleman, who increased the clergyman's scanty
resources by taking rooms in his large, straggling house.  The vicar,
being a bachelor, was glad to come to such an arrangement, though he
had little in common with his lodger, who was a thin, dark, spectacled
man, with a stoop which gave the impression of actual, physical
deformity.  I remember that during our short visit we found the vicar
garrulous, but his lodger strangely reticent, a sad-faced,
introspective man, sitting with averted eyes, brooding apparently upon
his own affairs.

These were the two men who entered abruptly into our little
sitting-room on Tuesday, March the 16th, shortly after our breakfast
hour, as we were smoking together, preparatory to our daily excursion
upon the moors.

""Mr. Holmes,"" said the vicar in an agitated voice, ""the most
extraordinary and tragic affair has occurred during the night. It is
the most unheard-of business.  We can only regard it as a special
Providence that you should chance to be here at the time, for in all
England you are the one man we need.""

I glared at the intrusive vicar with no very friendly eyes; but Holmes
took his pipe from his lips and sat up in his chair like an old hound
who hears the view-halloa.  He waved his hand to the sofa, and our
palpitating visitor with his agitated companion sat side by side upon
it.  Mr. Mortimer Tregennis was more self-contained than the clergyman,
but the twitching of his thin hands and the brightness of his dark eyes
showed that they shared a common emotion.

""Shall I speak or you?"" he asked of the vicar.

""Well, as you seem to have made the discovery, whatever it may be, and
the vicar to have had it second-hand, perhaps you had better do the
speaking,"" said Holmes.

I glanced at the hastily clad clergyman, with the formally dressed
lodger seated beside him, and was amused at the surprise which Holmes's
simple deduction had brought to their faces.

""Perhaps I had best say a few words first,"" said the vicar, ""and then
you can judge if you will listen to the details from Mr. Tregennis, or
whether we should not hasten at once to the scene of this mysterious
affair.  I may explain, then, that our friend here spent last evening
in the company of his two brothers, Owen and George, and of his sister
Brenda, at their house of Tredannick Wartha, which is near the old
stone cross upon the moor.  He left them shortly after ten o'clock,
playing cards round the dining-room table, in excellent health and
spirits. This morning, being an early riser, he walked in that
direction before breakfast and was overtaken by the carriage of Dr.
Richards, who explained that he had just been sent for on a most urgent
call to Tredannick Wartha.  Mr. Mortimer Tregennis naturally went with
him.  When he arrived at Tredannick Wartha he found an extraordinary
state of things.  His two brothers and his sister were seated round the
table exactly as he had left them, the cards still spread in front of
them and the candles burned down to their sockets.  The sister lay back
stone-dead in her chair, while the two brothers sat on each side of her
laughing, shouting, and singing, the senses stricken clean out of them.
All three of them, the dead woman and the two demented men, retained
upon their faces an expression of the utmost horror--a convulsion of
terror which was dreadful to look upon.  There was no sign of the
presence of anyone in the house, except Mrs. Porter, the old cook and
housekeeper, who declared that she had slept deeply and heard no sound
during the night.  Nothing had been stolen or disarranged, and there is
absolutely no explanation of what the horror can be which has
frightened a woman to death and two strong men out of their senses.
There is the situation, Mr. Holmes, in a nutshell, and if you can help
us to clear it up you will have done a great work.""

I had hoped that in some way I could coax my companion back into the
quiet which had been the object of our journey; but one glance at his
intense face and contracted eyebrows told me how vain was now the
expectation.  He sat for some little time in silence, absorbed in the
strange drama which had broken in upon our peace.

""I will look into this matter,"" he said at last.  ""On the face of it,
it would appear to be a case of a very exceptional nature. Have you
been there yourself, Mr. Roundhay?""

""No, Mr. Holmes.  Mr. Tregennis brought back the account to the
vicarage, and I at once hurried over with him to consult you.""

""How far is it to the house where this singular tragedy occurred?""

""About a mile inland.""

""Then we shall walk over together.  But before we start I must ask you
a few questions, Mr. Mortimer Tregennis.""

The other had been silent all this time, but I had observed that his
more controlled excitement was even greater than the obtrusive emotion
of the clergyman.  He sat with a pale, drawn face, his anxious gaze
fixed upon Holmes, and his thin hands clasped convulsively together.
His pale lips quivered as he listened to the dreadful experience which
had befallen his family, and his dark eyes seemed to reflect something
of the horror of the scene.

""Ask what you like, Mr. Holmes,"" said he eagerly.  ""It is a bad thing
to speak of, but I will answer you the truth.""

""Tell me about last night.""

""Well, Mr. Holmes, I supped there, as the vicar has said, and my elder
brother George proposed a game of whist afterwards.  We sat down about
nine o'clock.  It was a quarter-past ten when I moved to go.  I left
them all round the table, as merry as could be.""

""Who let you out?""

""Mrs. Porter had gone to bed, so I let myself out.  I shut the hall
door behind me.  The window of the room in which they sat was closed,
but the blind was not drawn down.  There was no change in door or
window this morning, or any reason to think that any stranger had been
to the house.  Yet there they sat, driven clean mad with terror, and
Brenda lying dead of fright, with her head hanging over the arm of the
chair.  I'll never get the sight of that room out of my mind so long as
I live.""

""The facts, as you state them, are certainly most remarkable,"" said
Holmes.  ""I take it that you have no theory yourself which can in any
way account for them?""

""It's devilish, Mr. Holmes, devilish!"" cried Mortimer Tregennis. ""It is
not of this world.  Something has come into that room which has dashed
the light of reason from their minds.  What human contrivance could do
that?""

""I fear,"" said Holmes, ""that if the matter is beyond humanity it is
certainly beyond me.  Yet we must exhaust all natural explanations
before we fall back upon such a theory as this.  As to yourself, Mr.
Tregennis, I take it you were divided in some way from your family,
since they lived together and you had rooms apart?""

""That is so, Mr. Holmes, though the matter is past and done with. We
were a family of tin-miners at Redruth, but we sold our venture to a
company, and so retired with enough to keep us.  I won't deny that
there was some feeling about the division of the money and it stood
between us for a time, but it was all forgiven and forgotten, and we
were the best of friends together.""

""Looking back at the evening which you spent together, does anything
stand out in your memory as throwing any possible light upon the
tragedy?  Think carefully, Mr. Tregennis, for any clue which can help
me.""

""There is nothing at all, sir.""

""Your people were in their usual spirits?""

""Never better.""

""Were they nervous people?  Did they ever show any apprehension of
coming danger?""

""Nothing of the kind.""

""You have nothing to add then, which could assist me?""

Mortimer Tregennis considered earnestly for a moment.

""There is one thing occurs to me,"" said he at last.  ""As we sat at the
table my back was to the window, and my brother George, he being my
partner at cards, was facing it.  I saw him once look hard over my
shoulder, so I turned round and looked also.  The blind was up and the
window shut, but I could just make out the bushes on the lawn, and it
seemed to me for a moment that I saw something moving among them.  I
couldn't even say if it was man or animal, but I just thought there was
something there.  When I asked him what he was looking at, he told me
that he had the same feeling.  That is all that I can say.""

""Did you not investigate?""

""No; the matter passed as unimportant.""

""You left them, then, without any premonition of evil?""

""None at all.""

""I am not clear how you came to hear the news so early this morning.""

""I am an early riser and generally take a walk before breakfast. This
morning I had hardly started when the doctor in his carriage overtook
me.  He told me that old Mrs. Porter had sent a boy down with an urgent
message.  I sprang in beside him and we drove on. When we got there we
looked into that dreadful room.  The candles and the fire must have
burned out hours before, and they had been sitting there in the dark
until dawn had broken.  The doctor said Brenda must have been dead at
least six hours.  There were no signs of violence.  She just lay across
the arm of the chair with that look on her face.  George and Owen were
singing snatches of songs and gibbering like two great apes.  Oh, it
was awful to see!  I couldn't stand it, and the doctor was as white as
a sheet.  Indeed, he fell into a chair in a sort of faint, and we
nearly had him on our hands as well.""

""Remarkable--most remarkable!"" said Holmes, rising and taking his hat.
""I think, perhaps, we had better go down to Tredannick Wartha without
further delay.  I confess that I have seldom known a case which at
first sight presented a more singular problem.""


Our proceedings of that first morning did little to advance the
investigation.  It was marked, however, at the outset by an incident
which left the most sinister impression upon my mind. The approach to
the spot at which the tragedy occurred is down a narrow, winding,
country lane.  While we made our way along it we heard the rattle of a
carriage coming towards us and stood aside to let it pass.  As it drove
by us I caught a glimpse through the closed window of a horribly
contorted, grinning face glaring out at us.  Those staring eyes and
gnashing teeth flashed past us like a dreadful vision.

""My brothers!"" cried Mortimer Tregennis, white to his lips. ""They are
taking them to Helston.""

We looked with horror after the black carriage, lumbering upon its way.
Then we turned our steps towards this ill-omened house in which they
had met their strange fate.

It was a large and bright dwelling, rather a villa than a cottage, with
a considerable garden which was already, in that Cornish air, well
filled with spring flowers.  Towards this garden the window of the
sitting-room fronted, and from it, according to Mortimer Tregennis,
must have come that thing of evil which had by sheer horror in a single
instant blasted their minds.  Holmes walked slowly and thoughtfully
among the flower-plots and along the path before we entered the porch.
So absorbed was he in his thoughts, I remember, that he stumbled over
the watering-pot, upset its contents, and deluged both our feet and the
garden path.  Inside the house we were met by the elderly Cornish
housekeeper, Mrs. Porter, who, with the aid of a young girl, looked
after the wants of the family.  She readily answered all Holmes's
questions.  She had heard nothing in the night.  Her employers had all
been in excellent spirits lately, and she had never known them more
cheerful and prosperous.  She had fainted with horror upon entering the
room in the morning and seeing that dreadful company round the table.
She had, when she recovered, thrown open the window to let the morning
air in, and had run down to the lane, whence she sent a farm-lad for
the doctor.  The lady was on her bed upstairs if we cared to see her.
It took four strong men to get the brothers into the asylum carriage.
She would not herself stay in the house another day and was starting
that very afternoon to rejoin her family at St. Ives.

We ascended the stairs and viewed the body.  Miss Brenda Tregennis had
been a very beautiful girl, though now verging upon middle age.  Her
dark, clear-cut face was handsome, even in death, but there still
lingered upon it something of that convulsion of horror which had been
her last human emotion.  From her bedroom we descended to the
sitting-room, where this strange tragedy had actually occurred.  The
charred ashes of the overnight fire lay in the grate.  On the table
were the four guttered and burned-out candles, with the cards scattered
over its surface.  The chairs had been moved back against the walls,
but all else was as it had been the night before.  Holmes paced with
light, swift steps about the room; he sat in the various chairs,
drawing them up and reconstructing their positions.  He tested how much
of the garden was visible; he examined the floor, the ceiling, and the
fireplace; but never once did I see that sudden brightening of his eyes
and tightening of his lips which would have told me that he saw some
gleam of light in this utter darkness.

""Why a fire?"" he asked once.  ""Had they always a fire in this small
room on a spring evening?""

Mortimer Tregennis explained that the night was cold and damp. For that
reason, after his arrival, the fire was lit.  ""What are you going to do
now, Mr. Holmes?"" he asked.

My friend smiled and laid his hand upon my arm.  ""I think, Watson, that
I shall resume that course of tobacco-poisoning which you have so often
and so justly condemned,"" said he.  ""With your permission, gentlemen,
we will now return to our cottage, for I am not aware that any new
factor is likely to come to our notice here.  I will turn the facts
over in my mind, Mr. Tregennis, and should anything occur to me I will
certainly communicate with you and the vicar.  In the meantime I wish
you both good-morning.""

It was not until long after we were back in Poldhu Cottage that Holmes
broke his complete and absorbed silence.  He sat coiled in his
armchair, his haggard and ascetic face hardly visible amid the blue
swirl of his tobacco smoke, his black brows drawn down, his forehead
contracted, his eyes vacant and far away.  Finally he laid down his
pipe and sprang to his feet.

""It won't do, Watson!"" said he with a laugh.  ""Let us walk along the
cliffs together and search for flint arrows.  We are more likely to
find them than clues to this problem.  To let the brain work without
sufficient material is like racing an engine.  It racks itself to
pieces.  The sea air, sunshine, and patience, Watson--all else will
come.

""Now, let us calmly define our position, Watson,"" he continued as we
skirted the cliffs together.  ""Let us get a firm grip of the very
little which we DO know, so that when fresh facts arise we may be ready
to fit them into their places.  I take it, in the first place, that
neither of us is prepared to admit diabolical intrusions into the
affairs of men.  Let us begin by ruling that entirely out of our minds.
Very good.  There remain three persons who have been grievously
stricken by some conscious or unconscious human agency.  That is firm
ground.  Now, when did this occur?  Evidently, assuming his narrative
to be true, it was immediately after Mr. Mortimer Tregennis had left
the room.  That is a very important point.  The presumption is that it
was within a few minutes afterwards.  The cards still lay upon the
table. It was already past their usual hour for bed.  Yet they had not
changed their position or pushed back their chairs.  I repeat, then,
that the occurrence was immediately after his departure, and not later
than eleven o'clock last night.

""Our next obvious step is to check, so far as we can, the movements of
Mortimer Tregennis after he left the room.  In this there is no
difficulty, and they seem to be above suspicion. Knowing my methods as
you do, you were, of course, conscious of the somewhat clumsy water-pot
expedient by which I obtained a clearer impress of his foot than might
otherwise have been possible.  The wet, sandy path took it admirably.
Last night was also wet, you will remember, and it was not
difficult--having obtained a sample print--to pick out his track among
others and to follow his movements.  He appears to have walked away
swiftly in the direction of the vicarage.

""If, then, Mortimer Tregennis disappeared from the scene, and yet some
outside person affected the card-players, how can we reconstruct that
person, and how was such an impression of horror conveyed?  Mrs. Porter
may be eliminated.  She is evidently harmless.  Is there any evidence
that someone crept up to the garden window and in some manner produced
so terrific an effect that he drove those who saw it out of their
senses?  The only suggestion in this direction comes from Mortimer
Tregennis himself, who says that his brother spoke about some movement
in the garden.  That is certainly remarkable, as the night was rainy,
cloudy, and dark.  Anyone who had the design to alarm these people
would be compelled to place his very face against the glass before he
could be seen.  There is a three-foot flower-border outside this
window, but no indication of a footmark.  It is difficult to imagine,
then, how an outsider could have made so terrible an impression upon
the company, nor have we found any possible motive for so strange and
elaborate an attempt.  You perceive our difficulties, Watson?""

""They are only too clear,"" I answered with conviction.

""And yet, with a little more material, we may prove that they are not
insurmountable,"" said Holmes.  ""I fancy that among your extensive
archives, Watson, you may find some which were nearly as obscure.
Meanwhile, we shall put the case aside until more accurate data are
available, and devote the rest of our morning to the pursuit of
neolithic man.""

I may have commented upon my friend's power of mental detachment, but
never have I wondered at it more than upon that spring morning in
Cornwall when for two hours he discoursed upon celts, arrowheads, and
shards, as lightly as if no sinister mystery were waiting for his
solution.  It was not until we had returned in the afternoon to our
cottage that we found a visitor awaiting us, who soon brought our minds
back to the matter in hand.  Neither of us needed to be told who that
visitor was.  The huge body, the craggy and deeply seamed face with the
fierce eyes and hawk-like nose, the grizzled hair which nearly brushed
our cottage ceiling, the beard--golden at the fringes and white near
the lips, save for the nicotine stain from his perpetual cigar--all
these were as well known in London as in Africa, and could only be
associated with the tremendous personality of Dr. Leon Sterndale, the
great lion-hunter and explorer.

We had heard of his presence in the district and had once or twice
caught sight of his tall figure upon the moorland paths. He made no
advances to us, however, nor would we have dreamed of doing so to him,
as it was well known that it was his love of seclusion which caused him
to spend the greater part of the intervals between his journeys in a
small bungalow buried in the lonely wood of Beauchamp Arriance.  Here,
amid his books and his maps, he lived an absolutely lonely life,
attending to his own simple wants and paying little apparent heed to
the affairs of his neighbours.  It was a surprise to me, therefore, to
hear him asking Holmes in an eager voice whether he had made any
advance in his reconstruction of this mysterious episode.  ""The county
police are utterly at fault,"" said he, ""but perhaps your wider
experience has suggested some conceivable explanation.  My only claim
to being taken into your confidence is that during my many residences
here I have come to know this family of Tregennis very well--indeed,
upon my Cornish mother's side I could call them cousins--and their
strange fate has naturally been a great shock to me.  I may tell you
that I had got as far as Plymouth upon my way to Africa, but the news
reached me this morning, and I came straight back again to help in the
inquiry.""

Holmes raised his eyebrows.

""Did you lose your boat through it?""

""I will take the next.""

""Dear me! that is friendship indeed.""

""I tell you they were relatives.""

""Quite so--cousins of your mother.  Was your baggage aboard the ship?""

""Some of it, but the main part at the hotel.""

""I see.  But surely this event could not have found its way into the
Plymouth morning papers.""

""No, sir; I had a telegram.""

""Might I ask from whom?""

A shadow passed over the gaunt face of the explorer.

""You are very inquisitive, Mr. Holmes.""

""It is my business.""

With an effort Dr. Sterndale recovered his ruffled composure.

""I have no objection to telling you,"" he said.  ""It was Mr. Roundhay,
the vicar, who sent me the telegram which recalled me.""

""Thank you,"" said Holmes.  ""I may say in answer to your original
question that I have not cleared my mind entirely on the subject of
this case, but that I have every hope of reaching some conclusion.  It
would be premature to say more.""

""Perhaps you would not mind telling me if your suspicions point in any
particular direction?""

""No, I can hardly answer that.""

""Then I have wasted my time and need not prolong my visit.""  The famous
doctor strode out of our cottage in considerable ill-humour, and within
five minutes Holmes had followed him.  I saw him no more until the
evening, when he returned with a slow step and haggard face which
assured me that he had made no great progress with his investigation.
He glanced at a telegram which awaited him and threw it into the grate.

""From the Plymouth hotel, Watson,"" he said.  ""I learned the name of it
from the vicar, and I wired to make certain that Dr. Leon Sterndale's
account was true.  It appears that he did indeed spend last night
there, and that he has actually allowed some of his baggage to go on to
Africa, while he returned to be present at this investigation.  What do
you make of that, Watson?""

""He is deeply interested.""

""Deeply interested--yes.  There is a thread here which we had not yet
grasped and which might lead us through the tangle.  Cheer up, Watson,
for I am very sure that our material has not yet all come to hand.
When it does we may soon leave our difficulties behind us.""

Little did I think how soon the words of Holmes would be realized, or
how strange and sinister would be that new development which opened up
an entirely fresh line of investigation.  I was shaving at my window in
the morning when I heard the rattle of hoofs and, looking up, saw a
dog-cart coming at a gallop down the road.  It pulled up at our door,
and our friend, the vicar, sprang from it and rushed up our garden
path. Holmes was already dressed, and we hastened down to meet him.

Our visitor was so excited that he could hardly articulate, but at last
in gasps and bursts his tragic story came out of him.

""We are devil-ridden, Mr. Holmes!  My poor parish is devil-ridden!"" he
cried.  ""Satan himself is loose in it!  We are given over into his
hands!""  He danced about in his agitation, a ludicrous object if it
were not for his ashy face and startled eyes.  Finally he shot out his
terrible news.

""Mr. Mortimer Tregennis died during the night, and with exactly the
same symptoms as the rest of his family.""

Holmes sprang to his feet, all energy in an instant.

""Can you fit us both into your dog-cart?""

""Yes, I can.""

""Then, Watson, we will postpone our breakfast.  Mr. Roundhay, we are
entirely at your disposal.  Hurry--hurry, before things get
disarranged.""

The lodger occupied two rooms at the vicarage, which were in an angle
by themselves, the one above the other.  Below was a large
sitting-room; above, his bedroom.  They looked out upon a croquet lawn
which came up to the windows.  We had arrived before the doctor or the
police, so that everything was absolutely undisturbed.  Let me describe
exactly the scene as we saw it upon that misty March morning.  It has
left an impression which can never be effaced from my mind.

The atmosphere of the room was of a horrible and depressing stuffiness.
The servant who had first entered had thrown up the window, or it would
have been even more intolerable.  This might partly be due to the fact
that a lamp stood flaring and smoking on the centre table.  Beside it
sat the dead man, leaning back in his chair, his thin beard projecting,
his spectacles pushed up on to his forehead, and his lean dark face
turned towards the window and twisted into the same distortion of
terror which had marked the features of his dead sister.  His limbs
were convulsed and his fingers contorted as though he had died in a
very paroxysm of fear.  He was fully clothed, though there were signs
that his dressing had been done in a hurry.  We had already learned
that his bed had been slept in, and that the tragic end had come to him
in the early morning.

One realized the red-hot energy which underlay Holmes's phlegmatic
exterior when one saw the sudden change which came over him from the
moment that he entered the fatal apartment.  In an instant he was tense
and alert, his eyes shining, his face set, his limbs quivering with
eager activity.  He was out on the lawn, in through the window, round
the room, and up into the bedroom, for all the world like a dashing
foxhound drawing a cover.  In the bedroom he made a rapid cast around
and ended by throwing open the window, which appeared to give him some
fresh cause for excitement, for he leaned out of it with loud
ejaculations of interest and delight.  Then he rushed down the stair,
out through the open window, threw himself upon his face on the lawn,
sprang up and into the room once more, all with the energy of the
hunter who is at the very heels of his quarry.  The lamp, which was an
ordinary standard, he examined with minute care, making certain
measurements upon its bowl.  He carefully scrutinized with his lens the
talc shield which covered the top of the chimney and scraped off some
ashes which adhered to its upper surface, putting some of them into an
envelope, which he placed in his pocketbook.  Finally, just as the
doctor and the official police put in an appearance, he beckoned to the
vicar and we all three went out upon the lawn.

""I am glad to say that my investigation has not been entirely barren,""
he remarked.  ""I cannot remain to discuss the matter with the police,
but I should be exceedingly obliged, Mr. Roundhay, if you would give
the inspector my compliments and direct his attention to the bedroom
window and to the sitting-room lamp.  Each is suggestive, and together
they are almost conclusive.  If the police would desire further
information I shall be happy to see any of them at the cottage.  And
now, Watson, I think that, perhaps, we shall be better employed
elsewhere.""

It may be that the police resented the intrusion of an amateur, or that
they imagined themselves to be upon some hopeful line of investigation;
but it is certain that we heard nothing from them for the next two
days.  During this time Holmes spent some of his time smoking and
dreaming in the cottage; but a greater portion in country walks which
he undertook alone, returning after many hours without remark as to
where he had been.  One experiment served to show me the line of his
investigation.  He had bought a lamp which was the duplicate of the one
which had burned in the room of Mortimer Tregennis on the morning of
the tragedy.  This he filled with the same oil as that used at the
vicarage, and he carefully timed the period which it would take to be
exhausted. Another experiment which he made was of a more unpleasant
nature, and one which I am not likely ever to forget.

""You will remember, Watson,"" he remarked one afternoon, ""that there is
a single common point of resemblance in the varying reports which have
reached us.  This concerns the effect of the atmosphere of the room in
each case upon those who had first entered it.  You will recollect that
Mortimer Tregennis, in describing the episode of his last visit to his
brother's house, remarked that the doctor on entering the room fell
into a chair? You had forgotten?  Well I can answer for it that it was
so. Now, you will remember also that Mrs. Porter, the housekeeper, told
us that she herself fainted upon entering the room and had afterwards
opened the window.  In the second case--that of Mortimer Tregennis
himself--you cannot have forgotten the horrible stuffiness of the room
when we arrived, though the servant had thrown open the window.  That
servant, I found upon inquiry, was so ill that she had gone to her bed.
You will admit, Watson, that these facts are very suggestive.  In each
case there is evidence of a poisonous atmosphere.  In each case, also,
there is combustion going on in the room--in the one case a fire, in
the other a lamp.  The fire was needed, but the lamp was lit--as a
comparison of the oil consumed will show--long after it was broad
daylight.  Why?  Surely because there is some connection between three
things--the burning, the stuffy atmosphere, and, finally, the madness
or death of those unfortunate people.  That is clear, is it not?""

""It would appear so.""

""At least we may accept it as a working hypothesis.  We will suppose,
then, that something was burned in each case which produced an
atmosphere causing strange toxic effects.  Very good. In the first
instance--that of the Tregennis family--this substance was placed in
the fire.  Now the window was shut, but the fire would naturally carry
fumes to some extent up the chimney.  Hence one would expect the
effects of the poison to be less than in the second case, where there
was less escape for the vapour.  The result seems to indicate that it
was so, since in the first case only the woman, who had presumably the
more sensitive organism, was killed, the others exhibiting that
temporary or permanent lunacy which is evidently the first effect of
the drug.  In the second case the result was complete.  The facts,
therefore, seem to bear out the theory of a poison which worked by
combustion.

""With this train of reasoning in my head I naturally looked about in
Mortimer Tregennis's room to find some remains of this substance.  The
obvious place to look was the talc shelf or smoke-guard of the lamp.
There, sure enough, I perceived a number of flaky ashes, and round the
edges a fringe of brownish powder, which had not yet been consumed.
Half of this I took, as you saw, and I placed it in an envelope.""

""Why half, Holmes?""

""It is not for me, my dear Watson, to stand in the way of the official
police force.  I leave them all the evidence which I found.  The poison
still remained upon the talc had they the wit to find it.  Now, Watson,
we will light our lamp; we will, however, take the precaution to open
our window to avoid the premature decease of two deserving members of
society, and you will seat yourself near that open window in an
armchair unless, like a sensible man, you determine to have nothing to
do with the affair.  Oh, you will see it out, will you?  I thought I
knew my Watson.  This chair I will place opposite yours, so that we may
be the same distance from the poison and face to face.  The door we
will leave ajar.  Each is now in a position to watch the other and to
bring the experiment to an end should the symptoms seem alarming.  Is
that all clear?  Well, then, I take our powder--or what remains of
it--from the envelope, and I lay it above the burning lamp.  So!  Now,
Watson, let us sit down and await developments.""

They were not long in coming.  I had hardly settled in my chair before
I was conscious of a thick, musky odour, subtle and nauseous.  At the
very first whiff of it my brain and my imagination were beyond all
control.  A thick, black cloud swirled before my eyes, and my mind told
me that in this cloud, unseen as yet, but about to spring out upon my
appalled senses, lurked all that was vaguely horrible, all that was
monstrous and inconceivably wicked in the universe.  Vague shapes
swirled and swam amid the dark cloud-bank, each a menace and a warning
of something coming, the advent of some unspeakable dweller upon the
threshold, whose very shadow would blast my soul.  A freezing horror
took possession of me.  I felt that my hair was rising, that my eyes
were protruding, that my mouth was opened, and my tongue like leather.
The turmoil within my brain was such that something must surely snap.
I tried to scream and was vaguely aware of some hoarse croak which was
my own voice, but distant and detached from myself.  At the same moment,
in some effort of escape, I broke through that cloud of despair and had
a glimpse of Holmes's face, white, rigid, and drawn with horror--the
very look which I had seen upon the features of the dead.  It was that
vision which gave me an instant of sanity and of strength.  I dashed
from my chair, threw my arms round Holmes, and together we lurched
through the door, and an instant afterwards had thrown ourselves down
upon the grass plot and were lying side by side, conscious only of the
glorious sunshine which was bursting its way through the hellish cloud
of terror which had girt us in. Slowly it rose from our souls like the
mists from a landscape until peace and reason had returned, and we were
sitting upon the grass, wiping our clammy foreheads, and looking with
apprehension at each other to mark the last traces of that terrific
experience which we had undergone.

""Upon my word, Watson!"" said Holmes at last with an unsteady voice, ""I
owe you both my thanks and an apology.  It was an unjustifiable
experiment even for one's self, and doubly so for a friend.  I am
really very sorry.""

""You know,"" I answered with some emotion, for I have never seen so much
of Holmes's heart before, ""that it is my greatest joy and privilege to
help you.""

He relapsed at once into the half-humorous, half-cynical vein which was
his habitual attitude to those about him.  ""It would be superfluous to
drive us mad, my dear Watson,"" said he.  ""A candid observer would
certainly declare that we were so already before we embarked upon so
wild an experiment.  I confess that I never imagined that the effect
could be so sudden and so severe.""  He dashed into the cottage, and,
reappearing with the burning lamp held at full arm's length, he threw
it among a bank of brambles. ""We must give the room a little time to
clear.  I take it, Watson, that you have no longer a shadow of a doubt
as to how these tragedies were produced?""

""None whatever.""

""But the cause remains as obscure as before.  Come into the arbour here
and let us discuss it together.  That villainous stuff seems still to
linger round my throat.  I think we must admit that all the evidence
points to this man, Mortimer Tregennis, having been the criminal in the
first tragedy, though he was the victim in the second one.  We must
remember, in the first place, that there is some story of a family
quarrel, followed by a reconciliation.  How bitter that quarrel may
have been, or how hollow the reconciliation we cannot tell.   When I
think of Mortimer Tregennis, with the foxy face and the small shrewd,
beady eyes behind the spectacles, he is not a man whom I should judge
to be of a particularly forgiving disposition. Well, in the next place,
you will remember that this idea of someone moving in the garden, which
took our attention for a moment from the real cause of the tragedy,
emanated from him.  He had a motive in misleading us.  Finally, if he
did not throw the substance into the fire at the moment of leaving the
room, who did do so?  The affair happened immediately after his
departure. Had anyone else come in, the family would certainly have
risen from the table.  Besides, in peaceful Cornwall, visitors did not
arrive after ten o'clock at night.  We may take it, then, that all the
evidence points to Mortimer Tregennis as the culprit.""

""Then his own death was suicide!""

""Well, Watson, it is on the face of it a not impossible supposition.
The man who had the guilt upon his soul of having brought such a fate
upon his own family might well be driven by remorse to inflict it upon
himself.  There are, however, some cogent reasons against it.
Fortunately, there is one man in England who knows all about it, and I
have made arrangements by which we shall hear the facts this afternoon
from his own lips. Ah! he is a little before his time.  Perhaps you
would kindly step this way, Dr. Leon Sterndale. We have been conducing
a chemical experiment indoors which has left our little room hardly fit
for the reception of so distinguished a visitor.""

I had heard the click of the garden gate, and now the majestic figure
of the great African explorer appeared upon the path.  He turned in
some surprise towards the rustic arbour in which we sat.

""You sent for me, Mr. Holmes.  I had your note about an hour ago, and I
have come, though I really do not know why I should obey your summons.""

""Perhaps we can clear the point up before we separate,"" said Holmes.
""Meanwhile, I am much obliged to you for your courteous acquiescence.
You will excuse this informal reception in the open air, but my friend
Watson and I have nearly furnished an additional chapter to what the
papers call the Cornish Horror, and we prefer a clear atmosphere for
the present.  Perhaps, since the matters which we have to discuss will
affect you personally in a very intimate fashion, it is as well that we
should talk where there can be no eavesdropping.""

The explorer took his cigar from his lips and gazed sternly at my
companion.

""I am at a loss to know, sir,"" he said, ""what you can have to speak
about which affects me personally in a very intimate fashion.""

""The killing of Mortimer Tregennis,"" said Holmes.

For a moment I wished that I were armed.  Sterndale's fierce face
turned to a dusky red, his eyes glared, and the knotted, passionate
veins started out in his forehead, while he sprang forward with
clenched hands towards my companion.  Then he stopped, and with a
violent effort he resumed a cold, rigid calmness, which was, perhaps,
more suggestive of danger than his hot-headed outburst.

""I have lived so long among savages and beyond the law,"" said he, ""that
I have got into the way of being a law to myself.  You would do well,
Mr. Holmes, not to forget it, for I have no desire to do you an injury.""

""Nor have I any desire to do you an injury, Dr. Sterndale. Surely the
clearest proof of it is that, knowing what I know, I have sent for you
and not for the police.""

Sterndale sat down with a gasp, overawed for, perhaps, the first time
in his adventurous life.  There was a calm assurance of power in
Holmes's manner which could not be withstood.  Our visitor stammered
for a moment, his great hands opening and shutting in his agitation.

""What do you mean?"" he asked at last.  ""If this is bluff upon your
part, Mr. Holmes, you have chosen a bad man for your experiment. Let us
have no more beating about the bush.  What DO you mean?""

""I will tell you,"" said Holmes, ""and the reason why I tell you is that
I hope frankness may beget frankness.  What my next step may be will
depend entirely upon the nature of your own defence.""

""My defence?""

""Yes, sir.""

""My defence against what?""

""Against the charge of killing Mortimer Tregennis.""

Sterndale mopped his forehead with his handkerchief.  ""Upon my word,
you are getting on,"" said he.  ""Do all your successes depend upon this
prodigious power of bluff?""

""The bluff,"" said Holmes sternly, ""is upon your side, Dr. Leon
Sterndale, and not upon mine.  As a proof I will tell you some of the
facts upon which my conclusions are based.  Of your return from
Plymouth, allowing much of your property to go on to Africa, I will say
nothing save that it first informed me that you were one of the factors
which had to be taken into account in reconstructing this drama--""

""I came back--""

""I have heard your reasons and regard them as unconvincing and
inadequate.  We will pass that.  You came down here to ask me whom I
suspected.  I refused to answer you. You then went to the vicarage,
waited outside it for some time, and finally returned to your cottage.""

""How do you know that?""

""I followed you.""

""I saw no one.""

""That is what you may expect to see when I follow you.  You spent a
restless night at your cottage, and you formed certain plans, which in
the early morning you proceeded to put into execution. Leaving your
door just as day was breaking, you filled your pocket with some reddish
gravel that was lying heaped beside your gate.""

Sterndale gave a violent start and looked at Holmes in amazement.

""You then walked swiftly for the mile which separated you from the
vicarage.  You were wearing, I may remark, the same pair of ribbed
tennis shoes which are at the present moment upon your feet.  At the
vicarage you passed through the orchard and the side hedge, coming out
under the window of the lodger Tregennis. It was now daylight, but the
household was not yet stirring.  You drew some of the gravel from your
pocket, and you threw it up at the window above you.""

Sterndale sprang to his feet.

""I believe that you are the devil himself!"" he cried.

Holmes smiled at the compliment.  ""It took two, or possibly three,
handfuls before the lodger came to the window.  You beckoned him to
come down.  He dressed hurriedly and descended to his sitting-room.
You entered by the window.  There was an interview--a short one--during
which you walked up and down the room.  Then you passed out and closed
the window, standing on the lawn outside smoking a cigar and watching
what occurred. Finally, after the death of Tregennis, you withdrew as
you had come.  Now, Dr. Sterndale, how do you justify such conduct, and
what were the motives for your actions?  If you prevaricate or trifle
with me, I give you my assurance that the matter will pass out of my
hands forever.""

Our visitor's face had turned ashen gray as he listened to the words of
his accuser.  Now he sat for some time in thought with his face sunk in
his hands.  Then with a sudden impulsive gesture he plucked a
photograph from his breast-pocket and threw it on the rustic table
before us.

""That is why I have done it,"" said he.

It showed the bust and face of a very beautiful woman.  Holmes stooped
over it.

""Brenda Tregennis,"" said he.

""Yes, Brenda Tregennis,"" repeated our visitor.  ""For years I have loved
her.  For years she has loved me.  There is the secret of that Cornish
seclusion which people have marvelled at.  It has brought me close to
the one thing on earth that was dear to me. I could not marry her, for
I have a wife who has left me for years and yet whom, by the deplorable
laws of England, I could not divorce.  For years Brenda waited.  For
years I waited.  And this is what we have waited for.""  A terrible sob
shook his great frame, and he clutched his throat under his brindled
beard.  Then with an effort he mastered himself and spoke on:

""The vicar knew.  He was in our confidence.  He would tell you that she
was an angel upon earth.  That was why he telegraphed to me and I
returned.  What was my baggage or Africa to me when I learned that such
a fate had come upon my darling?  There you have the missing clue to my
action, Mr. Holmes.""

""Proceed,"" said my friend.

Dr. Sterndale drew from his pocket a paper packet and laid it upon the
table.  On the outside was written ""Radix pedis diaboli"" with a red
poison label beneath it.  He pushed it towards me.  ""I understand that
you are a doctor, sir.  Have you ever heard of this preparation?""

""Devil's-foot root!  No, I have never heard of it.""

""It is no reflection upon your professional knowledge,"" said he, ""for I
believe that, save for one sample in a laboratory at Buda, there is no
other specimen in Europe.  It has not yet found its way either into the
pharmacopoeia or into the literature of toxicology.  The root is shaped
like a foot, half human, half goatlike; hence the fanciful name given
by a botanical missionary.  It is used as an ordeal poison by the
medicine-men in certain districts of West Africa and is kept as a
secret among them.  This particular specimen I obtained under very
extraordinary circumstances in the Ubangi country.""  He opened the
paper as he spoke and disclosed a heap of reddish-brown, snuff-like
powder.

""Well, sir?"" asked Holmes sternly.

""I am about to tell you, Mr. Holmes, all that actually occurred, for
you already know so much that it is clearly to my interest that you
should know all.  I have already explained the relationship in which I
stood to the Tregennis family. For the sake of the sister I was
friendly with the brothers.  There was a family quarrel about money
which estranged this man Mortimer, but it was supposed to be made up,
and I afterwards met him as I did the others.  He was a sly, subtle,
scheming man, and several things arose which gave me a suspicion of
him, but I had no cause for any positive quarrel.

""One day, only a couple of weeks ago, he came down to my cottage and I
showed him some of my African curiosities.  Among other things I
exhibited this powder, and I told him of its strange properties, how it
stimulates those brain centres which control the emotion of fear, and
how either madness or death is the fate of the unhappy native who is
subjected to the ordeal by the priest of his tribe.  I told him also
how powerless European science would be to detect it.  How he took it I
cannot say, for I never left the room, but there is no doubt that it
was then, while I was opening cabinets and stooping to boxes, that he
managed to abstract some of the devil's-foot root.  I well remember how
he plied me with questions as to the amount and the time that was
needed for its effect, but I little dreamed that he could have a
personal reason for asking.

""I thought no more of the matter until the vicar's telegram reached me
at Plymouth.  This villain had thought that I would be at sea before
the news could reach me, and that I should be lost for years in Africa.
But I returned at once.  Of course, I could not listen to the details
without feeling assured that my poison had been used.  I came round to
see you on the chance that some other explanation had suggested itself
to you.  But there could be none.  I was convinced that Mortimer
Tregennis was the murderer; that for the sake of money, and with the
idea, perhaps, that if the other members of his family were all insane
he would be the sole guardian of their joint property, he had used the
devil's-foot powder upon them, driven two of them out of their senses,
and killed his sister Brenda, the one human being whom I have ever
loved or who has ever loved me.  There was his crime; what was to be
his punishment?

""Should I appeal to the law?  Where were my proofs?  I knew that the
facts were true, but could I help to make a jury of countrymen believe
so fantastic a story?  I might or I might not. But I could not afford
to fail.  My soul cried out for revenge. I have said to you once
before, Mr. Holmes, that I have spent much of my life outside the law,
and that I have come at last to be a law to myself.  So it was even
now.  I determined that the fate which he had given to others should be
shared by himself. Either that or I would do justice upon him with my
own hand.  In all England there can be no man who sets less value upon
his own life than I do at the present moment.

""Now I have told you all.  You have yourself supplied the rest. I did,
as you say, after a restless night, set off early from my cottage.  I
foresaw the difficulty of arousing him, so I gathered some gravel from
the pile which you have mentioned, and I used it to throw up to his
window.  He came down and admitted me through the window of the
sitting-room.  I laid his offence before him. I told him that I had
come both as judge and executioner.  The wretch sank into a chair,
paralyzed at the sight of my revolver. I lit the lamp, put the powder
above it, and stood outside the window, ready to carry out my threat to
shoot him should he try to leave the room.  In five minutes he died.
My God! how he died!  But my heart was flint, for he endured nothing
which my innocent darling had not felt before him.  There is my story,
Mr. Holmes.  Perhaps, if you loved a woman, you would have done as much
yourself.  At any rate, I am in your hands.  You can take what steps
you like.  As I have already said, there is no man living who can fear
death less than I do.""

Holmes sat for some little time in silence.

""What were your plans?"" he asked at last.

""I had intended to bury myself in central Africa.  My work there is but
half finished.""

""Go and do the other half,"" said Holmes.  ""I, at least, am not prepared
to prevent you.""

Dr. Sterndale raised his giant figure, bowed gravely, and walked from
the arbour.  Holmes lit his pipe and handed me his pouch.

""Some fumes which are not poisonous would be a welcome change,"" said
he.  ""I think you must agree, Watson, that it is not a case in which we
are called upon to interfere.  Our investigation has been independent,
and our action shall be so also.  You would not denounce the man?""

""Certainly not,"" I answered.

""I have never loved, Watson, but if I did and if the woman I loved had
met such an end, I might act even as our lawless lion-hunter has done.
Who knows?  Well, Watson, I will not offend your intelligence by
explaining what is obvious.  The gravel upon the window-sill was, of
course, the starting-point of my research.  It was unlike anything in
the vicarage garden.  Only when my attention had been drawn to Dr.
Sterndale and his cottage did I find its counterpart.  The lamp shining
in broad daylight and the remains of powder upon the shield were
successive links in a fairly obvious chain.  And now, my dear Watson, I
think we may dismiss the matter from our mind and go back with a clear
conscience to the study of those Chaldean roots which are surely to be
traced in the Cornish branch of the great Celtic speech.""









End of the Project Gutenberg EBook of The Adventure of the Devil's Foot, by 
Arthur Conan Doyle",['The matter of the division of the proceeds from selling the family business.'],10015,narrativeqa,en,,0431126a12d83a266d88603f505ac4f1014bcb3d8cc02c10,The matter of the division of the proceeds from selling the family business.,76
What is the group's request to the Connecticut DEEP Commissioner?,"Outdoors	February 19, 2017
You are here: Home / Archives for Departments / OutdoorsActor Sam Waterson Hosts PBS Documentary on Lyme Land Trust January 14, 2017 by admin Leave a Comment Jack Tiffany, owner of Tiffany Farms on Rte. 156 and an earlier pioneer in Lyme land preservation, is interviewed by PBS “Visionaries” documentary producers.
Filed Under: Lyme, Outdoors Application Deadline for Environmental Leadership Scholarship is Feb. 1 January 8, 2017 by admin Leave a Comment Applications are now being accepted for the Virginia R. Rollefson Environmental Leadership Scholarship, a $1,000 award to recognize a high school student who has demonstrated leadership and initiative in promoting conservation, preservation, restoration, or environmental education.
Filed Under: Lyme, News, Old Lyme, Outdoors, Top Story Preserves in Lyme Now Closed for Hunting During Weekdays November 17, 2016 by admin Leave a Comment Starting yesterday, Wednesday, Nov. 16, the following Preserves in Lyme will be closed Monday through Friday until Tuesday, Dec. 20, 2016, except to licensed hunters with valid consent forms from the Town of Lyme Open Space Coordinator:
Banningwood Preserve
Beebe Preserve
Chestnut Hill Preserve
Eno Preserve
Hand Smith
Honey Hill Preserve
Jewett Preserve
Mount Archer Woods
Pickwick’s Preserve
Plimpton Preserve
Slawson Preserve
These preserves, owned by the Town of Lyme or the Lyme Land Conservation Trust, will be open on Saturdays and Sundays during this hunting period as no hunting is allowed on weekends.
The hunting program is fully subscribed.
For more information on the hunting program in Lyme, visit http://www.lymelandtrust.org/stewardship/hunting-program/
Filed Under: Lyme, Outdoors, Top Story Town of Old Lyme Offers Part-time Land Steward Opportunity October 11, 2016 by admin Leave a Comment The Town of Old Lyme is seeking a part-time individual to maintain and manage the trail systems on its major preserves. Keeping trails cleared, maintaining markers, kiosks, entrances, parking areas, and managing for wildlife and other natural resources are the priorities.
For more information, visit the job posting on the home page of the Town’s web page at http://www.oldlyme-ct.gov/Pages/index.
To learn about the Open Space Commission and the properties it manages, visit http://www.oldlyme-ct.gov/Pages/OldLymeCT_Bcomm/open_space
Filed Under: Old Lyme, Outdoors, Top Story CT Fund for the Environment Annual Meeting to be Held Sunday in Hartford September 22, 2016 by admin Leave a Comment Engaging and educating communities for preservation of the Long Island Sound tidal estuary
Save the Sound is celebrating National Estuaries Week Sept. 17 – 24 with a series of interactive and educational events throughout the Long Island Sound region. This annual celebration of estuaries—the vital coastal zones where freshwater rivers meet salty seas—is sponsored by Restore America’s Estuaries and its member organizations including Save the Sound. This year’s events call attention to the many benefits of thriving coastal ecosystems, including how estuary conservation efforts support our quality of life and economic well-being.
“The Long Island Sound estuary is not only where freshwater rivers meet the saltwater Atlantic, but where wildlife habitat meets beaches and boating, and where modern industry meets traditional oystering,” said Curt Johnson, executive director of Save the Sound, which is a bi-state program of Connecticut Fund for the Environment (CFE).
Johnson continued, “All over the country, estuaries are the lifeblood of coastal economies. From serving as natural buffers to protect our coastlines from storms to providing unique habitat for countless birds, fish, and wildlife, estuaries deserve our protection and our thanks.”
Save the Sound is celebrating estuaries with a number of events this week, including the release of a new video, a presentation on Plum Island at the Old Lyme-Phoebe Griffin Noyes Library and the CFE/Save the Sound annual meeting:
Aerial view of Plum Island lighthouse. (From Preserve Plum Island website)
Chris Cryder, Special Projects Coordinator for Save the Sound and Outreach Coordinator for the Preserve Plum Island Coalition, will host Preserving Plum Island for Future Generations, a special presentation on the importance of conserving the wildlife habitats and historic buildings of Plum Island, New York.
Plum Island flanks Plum Gut in the Long Island Sound estuary’s eastern end, where fast-moving tides create highly productive fishing grounds. The talk is part of a multi-week series featuring photographs and paintings of Plum Island, and lectures on its ecology, geology, and history.
Old Lyme-Phoebe Griffin Noyes Library, 2 Library Lane, Old Lyme, Connecticut
Register by calling the library at 860-434-1684.
The Annual Meeting of Connecticut Fund for the Environment and its bi-state program Save the Sound will take place in the Planet Earth exhibit at the Connecticut Science Center. The event is open to the public with registration, and will feature a keynote address from Curt Spalding, administrator of EPA’s New England Region. Spalding is a leader in combatting nitrogen pollution and in climate change resilience planning efforts for New England.
Connecticut Science Center, 250 Columbus Blvd, Hartford, Connecticut
4 – 7 p.m
RSVP to mlemere@ctenvironment.org
To celebrate the contributions of volunteers to restoring the Long Island Sound estuary, Save the Sound has released a new video of a habitat restoration planting at Hyde Pond in Mystic. Following removal of the old Hyde Pond dam and opening 4.1 miles of stream habitat for migratory fish last winter (see time lapse video here), in May about 30 volunteers planted native vegetation along the Whitford Brook stream bank, under the direction of U.S. Fish and Wildlife Service, CT DEEP’s Fisheries division, and Save the Sound staff.
Find more information on the project’s benefits and funders here.
Look for the planting video on Save the Sound’s website, YouTube, Facebook, and Twitter accounts.
Filed Under: Old Lyme, Outdoors 750+ Volunteers Clean Beaches from Norwalk to New London Including Griswold Point in Old Lyme September 17, 2016 by admin Leave a Comment Kendall Perkins displays a skull she found during Save The Sound‘s Coastal Clean-up Day held yesterday at White Sand Beach.
Save the Sound, a bi-state program of Connecticut Fund for the environment, organized 31 cleanups across Connecticut’s shoreline this weekend. The efforts are part of International Coastal Cleanup, which brings together hundreds of thousands of people each year to remove plastic bags, broken glass, cigarette butts, and other trash from the world’s shores and waterways. One of the areas included in the cleanup effort was from White Sand Beach to the tip of Griswold Point in Old Lyme.
The event was founded by Ocean Conservancy in 1985, and Save the Sound has served as the official Connecticut coordinator for the last 14 years.
“We didn’t plan it this way, but I can’t imagine a better way to celebrate the 31st anniversary of International Coastal Cleanup Day than with 31 cleanups!” said Chris Cryder, special projects coordinator for Save the Sound. “The cleanup just keeps growing, in Connecticut and worldwide. We have some terrific new and returning partners this year, including the SECONN Divers, folks from the U.S. District Court, multiple National Charity League chapters, and many more.”
Cryder continued, “The diversity of the groups involved really reflects the truth that ocean health affects all of us. Clean beaches and oceans are safer for beachgoers and boaters, they’re healthier for wildlife that aren’t eating plastic or getting tangled up in trash, and they’re economic powerhouses for the fishing and tourism industries.”
The cleanups are co-hosted by a wide array of local partners including high schools, youth groups, and scout troops; churches; boaters and divers; watershed associations, park stewards, and land trusts. Twenty-eight cleanups will be held Saturday, with three more on Sunday and others through mid-October, for a total of 70 cleanups statewide.
Based on the estimates of cleanup captains, between 750 and 900 volunteers were expected to pitch in on Saturday alone. Last year, a total of 1,512 volunteers participated in Save the Sound cleanups throughout the fall. They collected more than three tons of litter and debris from 58 sites on Connecticut beaches, marshes, and riverbanks.
Over the event’s three-decade history, 11.5 million volunteers have collected 210 million pounds of trash worldwide. Every piece of trash volunteers find is tracked, reported to Save the Sound, and included in Ocean Conservancy’s annual index of global marine debris. The data is used to track trends in litter and devise policies to stop it at its source.
Filed Under: Old Lyme, Outdoors, Top Story Stonewell Farm Hosts Two-Day Workshop on Dry Stone Wall Building, Sept. 24, 25 September 13, 2016 by admin Leave a Comment Andrew Pighill’s work includes outdoor kitchens, wine cellars, fire-pits, fireplaces and garden features that include follies and other whimsical structures in stone.
KILLINGWORTH — On Sept. 24 and 25, from 9 a.m. to 4 p.m. daily, Andrew Pighills, master stone mason, will teach a two-day, weekend long workshop on the art of dry stone wall building at Stonewell Farm in Killingworth, CT.
Participants will learn the basic principles of wall building, from establishing foundations, to the methods of dry laid (sometimes called dry-stacked) construction and ‘hearting’ the wall. This hands-on workshop will address not only the structure and principles behind wall building but also the aesthetic considerations of balance and proportion.
This workshop expresses Pighill’s commitment to preserve New England’s heritage and promote and cultivate the dry stone wall building skills that will ensure the preservation of our vernacular landscape.
This workshop is open to participants, 18 years of age or older, of all levels of experience. Note the workshop is limited to 16 participants, and spaces fill up quickly.
You must pre-register to attend the workshop. The price for the workshop is $350 per person. Stonewell Farm is located at 39 Beckwith Rd., Killingworth CT 06419
If you have any questions or to register for the workshop, contact the Workshop Administrator Michelle Becker at 860-322-0060 or mb@mbeckerco.com
At the end of the day on Saturday you’ll be hungry, tired and ready for some rest and relaxation, so the wood-fired Stone pizza oven will be fired up and beer, wine and Pizza Rustica will be served.
About the instructor: Born in Yorkshire, England, Andrew Pighills is an accomplished stone artisan, gardener and horticulturist. He received his formal horticulture training with The Royal Horticultural Society and has spent 40+ years creating gardens and building dry stone walls in his native England in and around the spectacular Yorkshire Dales and the English Lake District. Today, Pighills is one of a small, but dedicated group of US-based, certified, professional members of The Dry Stone Walling Association (DSWA) of Great Britain. Having moved to the United States more than 10 years ago, he now continues this venerable craft here in the US, building dry stone walls, stone structures and creating gardens throughout New England and beyond.
His particular technique of building walls adheres to the ancient methods of generations of dry stone wallers in his native Yorkshire Dales. Pighills’ commitment to preserving the integrity and endurance of this traditional building art has earned him a devoted list of private and public clients here and abroad including the English National Trust, the English National Parks, and the Duke of Devonshire estates. His stone work has been featured on British and American television, in Charles McCraven’s book The Stone Primer, and Jeffrey Matz’s Midcentury Houses Today, A study of residential modernism in New Canaan Connecticut. He has featured in the N Y Times, on Martha Stewart Living radio, and in the Graham Deneen film short “Dry Stone”, as well as various media outlets both here and in the UK, including an article in the Jan/Feb 2015 issue of Yankee Magazine.
Pighills is a DSWA fully qualified dry stone walling instructor. In addition to building in stone and creating gardens, Pighills teaches dry stone wall building workshops in and around New England. He is a frequent lecturer on the art of dry stone walling, and how traditional UK walling styles compare to those found in New England. His blog, Heave and Hoe; A Day in the Life of a Dry Stone Waller and Gardener, provides more information about Pighills.
For more information, visit www.englishgardensandlandscaping.com
Filed Under: Outdoors CT Port Authority Chair Tells Lower CT River Local Officials, “We’re All on One Team” August 27, 2016 by Olwen Logan 2 Comments Enjoying a boat ride on the Connecticut River, but still finding time for discussions, are (from left to right) Chester First Selectwoman Lauren Gister, Old Lyme First Selectwoman and Connecticut Port Authority (CPA) board member Bonnie Reemsnyder, Essex First Selectman Norm Needleman, CPA Chairman Scott Bates and Deep River First Selectman Angus McDonald, Jr.
Filed Under: Chester, Deep River, Essex, News, Old Lyme, Outdoors, Politics, Top Story House Approves Courtney-Sponsored Amendment Restricting Sale of Plum Island July 10, 2016 by admin 2 Comments Representative Joe Courtney
Local Congressional Representative Joe Courtney (CT-02) announced Thursday (July 7) that a bipartisan amendment he had led, along with Representatives Rosa DeLauro (CT-03), Lee Zeldin (R-NY) and Peter King (R-NY), to prohibit the sale of Plum Island was passed by the House of Representatives.
The amendment, which will prohibit the General Services Administration (GSA) from using any of its operational funding to process or complete a sale of Plum Island, was made to the Financial Services and General Government Appropriations Act of 2017..
In a joint statement, the Representatives said, “Our amendment passed today is a big step toward permanently protecting Plum Island as a natural area. Plum Island is a scenic and biological treasure located right in the middle of Long Island Sound. It is home to a rich assortment of rare plant and animal species that need to be walled off from human interference.”
The statement continued, “Nearly everyone involved in this issue agrees that it should be preserved as a natural sanctuary – not sold off to the highest bidder for development.” Presumptive Republican Presidential nominee Donald Trump had shown interest in the property at one time.
In 2008, the federal government announced plans to close the research facility on Plum Island and relocate to Manhattan, Kansas. Current law states that Plum Island must be sold publicly to help finance the new research facility.
Aerial view of Plum Island.
The lawmakers joint statement explained, “The amendment will prevent the federal agency in charge of the island from moving forward with a sale by prohibiting it from using any of its operational funding provided by Congress for that purpose,” concluding, ” This will not be the end of the fight to preserve Plum Island, but this will provide us with more time to find a permanent solution for protecting the Island for generations to come.”
For several years, members from both sides of Long Island Sound have been working in a bipartisan manner to delay and, ultimately, repeal the mandated sale of this ecological treasure. Earlier this year, the representatives, along with the whole Connecticut delegation, cosponsored legislation that passed the House unanimously to delay the sale of Plum Island.
Filed Under: Outdoors July 1 Update: Aquatic Treatment Planned for Rogers Lake, July 5 July 1, 2016 by admin Leave a Comment We received this updated information from the Old Lyme Selectman’s office at 11:05 a.m. this morning:
Filed Under: Lyme, Old Lyme, Outdoors, Town Hall They’re Everywhere! All About Gypsy Moth Caterpillars — Advice from CT Agricultural Experiment Station June 2, 2016 by Adina Ripin Leave a Comment Gypsy moth caterpillars – photo by Peter Trenchard, CAES.
The potential for gypsy moth outbreak exists every year in our community.
Dr. Kirby Stafford III, head of the Department of Entomology at the Connecticut Agricultural Experiment Station, has written a fact sheet on the gypsy moth available on the CAES website. The following information is from this fact sheet.
The gypsy moth, Lymantria dispar, was introduced into the US (Massachusetts) by Etienne Leopold Trouvelot in about 1860. The escaped larvae led to small outbreaks in the area in 1882, increasing rapidly. It was first detected in Connecticut in 1905. By 1952, it had spread to 169 towns. In 1981, 1.5 million acres were defoliated in Connecticut. During the outbreak of 1989, CAES scientists discovered that an entomopathogenic fungus, Entomophaga maimaiga, was killing the caterpillars. Since then the fungus has been the most important agent suppressing gypsy moth activity.
The fungus, however, cannot prevent all outbreaks and hotspots have been reported in some areas, in 2005-06 and again in 2015.
The life cycle of the gypsy moth is one generation a year. Caterpillars hatch from buff-colored egg masses in late April to early May. An egg mass may contain 100 to more than 1000 eggs and are laid in several layers. The caterpillars (larvae) hatch a few days later and ascend the host trees and begin to feed on new leaves. The young caterpillars, buff to black-colored, lay down silk safety lines as they crawl and, as they drop from branches on these threads, they may be picked up on the wind and spread.
There are four or five larval stages (instars) each lasting 4-10 days. Instars 1-3 remain in the trees. The fourth instar caterpillars, with distinctive double rows of blue and red spots, crawl up and down the tree trunks feeding mainly at night. They seek cool, shaded protective sites during the day, often on the ground. If the outbreak is dense, caterpillars may feed continuously and crawl at any time.
With the feeding completed late June to early July, caterpillars seek a protected place to pupate and transform into a moth in about 10-14 days. Male moths are brown and fly. Female moths are white and cannot fly despite having wings. They do not feed and live for only 6-10 days. After mating, the female will lay a single egg mass and die. The egg masses can be laid anywhere: trees, fence posts, brick/rock walls, outdoor furniture, cars, recreational vehicles, firewood. The egg masses are hard. The eggs will survive the winter and larvae hatch the following spring during late April through early May.
The impact of the gypsy moth can be extensive since the caterpillar will feed on a wide diversity of trees and shrubs. Oak trees are their preferred food. Other favored tree species include apple, birch, poplar and willow. If the infestation is heavy, they will also attack certain conifers and other less favored species. The feeding causes extensive defoliation.
Healthy trees can generally withstand one or two partial to one complete defoliation. Trees will regrow leaves before the end of the summer. Nonetheless, there can be die-back of branches. Older trees may become more vulnerable to stress after defoliation. Weakened trees can also be attacked by other organisms or lack energy reserves for winter dormancy and growth during the following spring. Three years of heavy defoliation may result in high oak mortality.
The gypsy moth caterpillars drop leaf fragments and frass (droppings) while feeding creating a mess for decks, patios, outdoor furniture, cars and driveways. Crawling caterpillars can be a nuisance and their hairs irritating. The egg masses can be transported by vehicles to areas where the moth is not yet established. Under state quarantine laws, the CAES inspects certain plant shipments destined to areas free of the gypsy moth, particularly for egg masses.
There are several ways to manage the gypsy moth: biological, physical and chemical.
Biologically, the major gypsy moth control agent has been the fungus E. maimaiga. This fungus can provide complete control of the gypsy moth but is dependent on early season moisture from rains in May and June to achieve effective infection rates and propagation of the fungus to other caterpillars. The dry spring of 2015 resulted in little or no apparent fungal inoculation or spread until it killed late-stage caterpillars in some areas of the state, after most defoliation.
Infected caterpillars hang vertically from the tree trunk, head down. Some die in an upside down “V” position, a characteristic of caterpillars killed by the less common gypsy moth nucleopolyhedrosis virus (NPV). This was not detected in caterpillars examined in 2015.
Physical controls include removing and destroying egg masses, which can be drowned in a soapy water and disposed of. Another method is to use burlap refuge/barrier bands wrapped around tree trunks so that migrating caterpillars will crawl into or under the folded burlap or be trapped by the sticky band.
There are a number of crop protection chemicals labeled for the control of gypsy moth on ornamental trees and shrubs. There are treatments for egg masses, larvae and adult moths. Detailed information about these chemical treatments is available in the CAES factsheet.
For complete information about the gypsy moth and its management, visit the CAES website and look for the fact sheet on gypsy moth.
Filed Under: News, Outdoors East Lyme Public Trust Invites Community to Celebrate Boardwalk Re-dedication May 25, 2016 by admin Leave a Comment On Saturday, May 28, at 11 a.m., the East Lyme Public Trust Foundation, in co-operation with East Lyme Parks and Recreation Department, will sponsor A Dream Fulfilled, the official re-dedication of the East Lyme Boardwalk. The re-dedication ceremony, which will be held on the Boardwalk, will feature keynote speaker, Sen. Paul Formica, former First Selectman of East Lyme.
Other speakers will include East Lyme First Selectman Mark Nickerson, Public Trust President Joe Legg, Public Trust Past-President Bob DeSanto, Public Trust Vice-President John Hoye, and Parks and Recreation Director Dave Putnam; all the speakers will recognize the many people who have helped made this dream a reality.
The East Lyme Public Trust Foundation would like to invite the general public to witness this historic occasion. In addition, the members would especially like to encourage the participation of the 200 people who dedicated benches and the innumerable people who sponsored plaques. They would also love to welcome all members of the Trust – past and present – and all those who originally helped make the Boardwalk a reality.
Participants should enter the Boardwalk at Hole-in-the Wall on Baptist Lane, Niantic. Then, there will be a short walk to the area of the monument where the ceremony will take place. At the entrance to Hole-in-the Wall, the Public Trust will have a display of historical information and memorabilia related to the construction and re-construction of the Boardwalk. Public Trust members, Pat and Jack Lewis will be on hand to host the exhibit titled Before and After and to welcome participants. After the ceremony, participants will have the opportunity to visit “their bench” and re-visit “their plaque.” During and after the dedication, music will be provided by Trust member, Bill Rinoski, who is a “D.J. for all occasions.” Rinoski will feature “Boardwalk-related” music and Oldies plus Top 40 selections. This historic occasion will be videotaped as a public service by Mike Rydene of Media Potions of East Lyme. High school volunteers will be on hand to greet participants and help with directions.
The organizing committee is chaired by Michelle Maitland. Her committee consists of Joe Legg, President of the East Lyme Public Trust, Carol Marelli, Bob and Polly DeSanto, June Hoye, and Kathie Cassidy.
Visit Facebook – East Lyme Public Trust Foundation – for more information on the re-dedication ceremony. For more information on the Boardwalk, explore this website.
Filed Under: Outdoors Lyme Land Trust Seeks to Preserve Whalebone Cove Headwaters May 8, 2016 by admin Leave a Comment Lyme Land Trust Preservation Vice President Don Gerber stands with Chairman Anthony Irving (kneeling) next to Whalebone Creek in the proposed Hawthorne Preserve in Hadlyme.
The Lyme Land Conservation Trust has announced a fund raising drive to protect 82 acres of ecologically strategic upland forest and swamp wildlife habitat in Hadlyme on the headwaters of Whalebone Cove, one of the freshwater tidal wetlands that comprises the internationally celebrated Connecticut River estuary complex.
The new proposed preserve is part of a forested landscape just south of Hadlyme Four Corners and Ferry Road (Rt. 148), and forms a large part of the watershed for Whalebone Creek, a key tributary feeding Whalebone Cove, most of which is a national wildlife refuge under the management of the US Fish & Wildlife Service.
The Land Trust said it hopes to name the new nature refuge in honor of William Hawthorne of Hadlyme, whose family has owned the property for several generations and who has agreed to sell the property to the Land Trust at a discount from its market value if the rest of the money necessary for the purchase can be raised by the Land Trust.
“This new wildlife preserve will represent a triple play for habitat conservation,” said Anthony Irving, chairman of the Land Trust’s Preservation Committee.
“First, it helps to protect the watershed feeding the fragile Whalebone Cove eco-system, which is listed as one of North America’s important freshwater tidal marshes in international treaties that cite the Connecticut River estuary as a wetland complex of global importance. Whalebone Creek, one of the primary streams feeding Whalebone Cove, originates from vernal pools and upland swamps just south of the Hawthorne tract on the Land Trust’s Ravine Trail Preserve and adjacent conservation easements and flows through the proposed preserve. Virtually all of the Hawthorne property comprises much of the watershed for Whalebone Creek.
“Second, the 82 acres we are hoping to acquire with this fund raising effort represents a large block of wetlands and forested wildlife habitat between Brush Hill and Joshuatown roads, which in itself is home to a kaleidoscope of animals from amphibians and reptiles that thrive in several vernal pools and swamp land, to turkey, coyote, bobcat and fisher. It also serves as seasonal nesting and migratory stops for several species of deep woods birds, which are losing habitat all over Connecticut due to forest fragmentation.
“Third, this particular preserve will also conserve a key link in the wildlife corridors that connects more than 1,000 acres of protected woodland and swamp habitat in the Hadlyme area.” Irving explained that the preserve is at the center of a landscape-scale wildlife habitat greenway that includes Selden Island State Park, property of the US Fish & Wild Life’s Silvio O Conte Wildlife Refuge, The Nature Conservancy’s Selden Preserve, and several other properties protected by the Lyme Land Conservation Trust.
“Because of its central location as a hub between these protected habitat refuges,” said Irving, “this preserve will protect forever the uninterrupted access that wildlife throughout the Hadlyme landscape now has for migration and breeding between otherwise isolated communities and families of many terrestrial species that are important to the continued robust bio-diversity of southeastern Connecticut and the Connecticut River estuary.”
Irving noted that the Hawthorne property is the largest parcel targeted for conservation in the Whalebone Cove watershed by the recently developed US Fish & Wildlife Service Silvio O Conte Wildlife Refuge Comprehensive Conservation Plan. Irving said the Land Trust hopes to create a network of hiking trails on the property with access from both Brush Hill Road on the east and Joshuatown Road on the west and connection to the Land Trust’s Ravine Trail to the south and the network of trails on the Nature Conservancy’s Selden Preserve.
Irving said there is strong support for the Land Trust’s proposal to preserve the property both within the Hadlyme and Lyme communities and among regional and state conservation groups. He noted letters of support have come from the Hadlyme Garden Club, the Hadlyme Public Hall Association, the Lyme Inland Wetlands & Watercourses Agency, the Lyme Planning and Zoning Commission, the Lyme Open Space Committee, the Lower Connecticut River Valley Council of Governments, the Lyme Garden Club, the Lyme Public Hall, The Nature Conservancy, The Silvio O Conte Refuge, the Connecticut River Watershed Council, and the Friends of Whalebone Cove, Inc.
He reported that between Hawthorne’s gift and several other pledges the Land Trust has already received commitments of 25 percent of the cost of the property.
Filed Under: Lyme, Outdoors, Top Story, vnn Old Lyme Tree Commission Celebrates Arbor Day April 29, 2016 by admin Leave a Comment Members of the three groups gather around the new oak tree. From left to right are Kathy Burton, Joanne DiCamillo, Joan Flynn. Anne Bing, Emily Griswold and Barbara Rayel.
Filed Under: Old Lyme, Outdoors, Top Story, Town Hall Enjoy a Tour of Private Gardens in Essex, June 4 April 28, 2016 by Adina Ripin Leave a Comment See this beautiful private garden in Essex on June 4.
ESSEX – On Saturday, June 4, from 10 a.m. to 3 p.m., plan to stroll through eight of the loveliest and most unusual private gardens in Essex. Some are in the heart of Essex Village while others are hidden along lanes most visitors never see. While exploring, you will find both formal and informal settings, lovely sweeping lawns and panoramic views of the Connecticut River or its coves. One garden you will visit is considered to be a ‘laboratory’ for cultivation of native plants. Master Gardeners will be available to point out specific features, offer gardening tips, and answer questions.
The garden tour is sponsored by the Friends of the Essex Library. Tickets are $25 in advance and $30 at the Essex Library the day of the event. Cash, checks, Visa or Master Card will be accepted. Tickets can be reserved by visiting the library or by completing the form included in flyers available at the library and throughout Essex beginning May 2. Completed forms can be mailed to the library. Confirmations will be sent to the email addresses on the completed forms.
Your ticket will be a booklet containing a brief description of each garden along with a map of the tour and designated parking. Tickets must be picked up at the library beginning at 9:45 a.m. the day of the event.
Richard Conroy, library director, has said, “The Essex Library receives only about half of its operating revenue from the Town. The financial assistance we receive each year from the Friends is critical. It enables us to provide important resources such as Ancestry.com and museum passes, as well as practical improvements like the automatic front doors that were recently installed. I urge you to help your Library by helping our Friends make this event a success! Thank you for your support.”
The tour will take place rain or shine. For more information, call 860-767-1560. All proceeds will benefit Friends of the Essex Library.
Filed Under: Outdoors Potapaug Presents Plum Island Program April 7, 2016 by admin Leave a Comment Potapaug Audubon presents “Preserving Plum Island” on Thursday, April 7, at 7 p.m. at Old Lyme Town Hall, 52 Lyme St., Old Lyme, with guest speaker Chris Cryder, from the Preserve Plum Island Coalition.
Cryder will discuss the efforts to protect the island, which provides vital habitat for threatened and endangered birds.
This is a free program and all are welcome.
Filed Under: Old Lyme, Outdoors CT Legislators Support Study to Preserve Plum Island From Commercial Development March 28, 2016 by Jerome Wilson 1 Comment Aerial view of Plum Island lighthouse. (From Preserve Plum Island website)
Last Thursday, March 24, at a press conference in Old Saybrook, a triumvirate of Congressional legislators from Connecticut, State Senator Richard Blumenthal and US Representatives Joe Courtney (D-2nd District) and Rosa DeLauro (D-3rd District) confirmed their support for a study to determine the future of Plum Island located in Long Island Sound.
Members of the Plum Island Coalition — which has some 65 member organizations all dedicated to preserving the island — were in attendance to hear the good news.
The island still houses a high-security, federal animal disease research facility, but the decision has already been taken to move the facility to a new location in Kansas with an opening slated for 2022. The current facility takes up only a small percentage of the land on the island and significantly for environmentalists, the remainder of the island has for years been left to nature in the wild.
In supporting a federal study on the future of Plum Island, Sen. Blumenthal said, “This study is a step towards saving a precious, irreplaceable national treasure from developers and polluters. It will provide the science and fact-based evidence to make our case for stopping the current Congressional plan to sell Plum Island to the highest bidder.” He continued, “The stark truth is the sale of Plum Island is no longer necessary to build a new bioresearch facility because Congress has fully appropriated the funds. There is no need for this sale – and in fact, Congress needs to rescind the sale.” Congress, however, still has a law on the books that authorizes the sale of Plum Island land to the highest bidder. Therefore, opponents of the sale will have the burden of convincing Congress to change a law that is currently in place.
Filed Under: Old Lyme, Outdoors, Top Story, vnn Land Trusts’ Photo Contest Winners Announced March 24, 2016 by admin Leave a Comment Winner of the top prize, the John G. Mitchell Environmental Conservation Award – Hank Golet
The 10th Annual Land Trust’s Photo Contest winners were announced at a March 11 reception highlighting the winning photos and displaying all entered photos. Land trusts in Lyme, Old Lyme, Salem, Essex and East Haddam jointly sponsor the annual amateur photo contest to celebrate the scenic countryside and diverse wildlife and plants in these towns. The ages of the photographers ranged from children to senior citizens.
Hank Golet won the top prize, the John G. Mitchell Environmental Conservation Award, with his beautiful photograph of a juvenile yellow crowned night heron in the Black Hall River in Old Lyme. Alison Mitchell personally presented the award, created in memory of her late husband John G. Mitchell, an editor at National Geographic, who championed the cause of the environment.
William Burt, a naturalist and acclaimed wildlife photographer, who has been a contest judge for ten years, received a special mention. Judges Burt; Amy Kurtz Lansing, an accomplished art historian and curator at the Florence Griswold Museum; and Skip Broom, a respected, award-winning local photographer and antique house restoration housewright, chose the winning photographs from 219 entries.
The sponsoring land trusts – Lyme Land Conservation Trust, Essex Land Trust, the Old Lyme Land Trust, Salem Land Trust, and East Haddam Land Trust – thank the judges as well as generous supporters RiverQuest/ CT River Expeditions, Lorensen Auto Group, the Oakley Wing Group at Morgan Stanley, Evan Griswold at Coldwell Banker, Ballek’s Garden Center, Essex Savings Bank, Chelsea Groton Bank, and Alison Mitchell in honor of her late husband John G. Mitchell. Big Y and Fromage Fine Foods & Coffee provided support for the reception.
The winning photographers are:
John G. Mitchell Environmental Award, Hank Golet, Old Lyme
1st: Patrick Burns, East Haddam
2nd: Judah Waldo, Old Lyme
3rd: James Beckman, Ivoryton
Honorable Mention Gabriel Waldo, Old Lyme
Honorable Mention Sarah Gada, East Haddam
Honorable Mention Shawn Parent, East Haddam
Cultural/Historic
1st: Marcus Maronne, Mystic
2nd: Normand L. Charlette, Manchester
3rd: Tammy Marseli, Rocky Hill
Honorable Mention Jud Perkins, Salem
Honorable Mention Pat Duncan, Norwalk
Honorable Mention John Kolb, Essex
Landscapes/Waterscapes
1st: Cheryl Philopena, Salem
2nd: Marian Morrissette, New London
3rd: Harcourt Davis, Old Lyme
Honorable Mention Cynthia Kovak, Old Lyme
Honorable Mention Bopha Smith, Salem
1st: Mary Waldron, Old Lyme
2nd: Courtney Briggs, Old Saybrook
3rd: Linda Waters, Salem
Honorable Mention Pete Govert, East Haddam
Honorable Mention Marcus Maronne, Mystic
Honorable Mention Marian Morrissette, New London
First place winner of the Wildlife category – Chris Pimley
1st: Chris Pimley, Essex
2nd: Harcourt Davis, Old Lyme
Honorable Mention Thomas Nemeth, Salem
Honorable Mention Jeri Duefrene, Niantic
Honorable Mention Elizabeth Gentile, Old Lyme
The winning photos will be on display at the Lymes’ Senior Center for the month of March and Lyme Public Library in April. For more information go to lymelandtrust.org.
Filed Under: Outdoors Old Lyme’s Open Space Commission Hosts Talk on Sea Level Rise, Salt Marsh Advance March 11, 2016 by admin 1 Comment The Town of Old Lyme’s Open Space Commission invites all interested parties to a workshop by Adam Whelchel, PhD, Director of Science at The Nature Conservancy’s Connecticut Chapter. The workshop will be held on Friday, March 11, at 9 a.m. in the Old Lyme Town Hall.
Filed Under: Outdoors, Town Hall Inaugural Meeting of ‘Friends of Whalebone Cove’ Held, Group Plans to Protect Famous Tidal Wetland March 7, 2016 by admin Leave a Comment The newly formed ‘Friends of Whalebone Cove’ are working to preserve and protect the Cove’s fragile ecosystem.
A new community conservation group to protect Whalebone Cove, a freshwater tidal marsh along the Connecticut river in Hadlyme recognized internationally for its wildlife habitat, will hold its first organizational meeting this coming Sunday, March 6, at 4 p.m.
Calling the group “Friends of Whalebone Cove” (FOWC), the organizers say their purpose is to “create a proactive, community-based constituency whose mission is to preserve and protect the habitat and fragile eco-systems of Whalebone Cove.”
Much of Whalebone Cove is a nature preserve that is part of the Silvio O. Conte National Wildlife Refuge (www.fws.gov/refuge/silvio_o_conte) under the jurisdiction of the U.S. Fish & Wildlife Service (USFW). The Refuge owns and manages 116 acres of marshland in Whalebone Cove and upland along its shores.
Prior to being taken over by USFW, the Whalebone Cove preserve was under the protection of The Nature Conservancy.
As part of the Connecticut River estuary, the Cove is listed in the Ramsar Convention on International Wetlands (www.ramsar.org) as tidal marshlands on the Connecticut River that constitute a “wetlands complex of international importance.”
The Ramsar citation specifically notes that Whalebone Cove has one of the largest stands of wild rice in the state. Except at high tide, most of the Cove is open marshland covered by wild rice stands with relatively narrow channels where Whalebone Creek winds its way through the Cove to the main stem of the Connecticut River.
Brian Slater, one of the group’s leaders who is filing the incorporation documents creating FOWC, said the creation of the organization was conceived by many of those living around the Cove and others in the Hadlyme area because of increased speeding motor boat and jet ski traffic in the Cove in recent years, damaging wetland plants and disrupting birds and other wildlife that make the Cove their home.
Slater said “Our goal is to develop a master plan for protection of the Cove through a collaborative effort involving all those who have a stake in Whalebone Cove – homeowners along its shores and those living nearby, the Silvio O. Conte Refuge, the Connecticut Department of Energy & Environmental Protection (DEEP), hunters, fishing enthusiasts, canoeing and kayaking groups, Audubon groups, the Towns of Lyme and East Haddam, The Nature Conservancy, the Connecticut River Watershed Council, the Lyme Land Conservation Trust, the Connecticut River Gateway Commission, and others who want to protect the Cove.”
“Such a plan”, said Slater, “should carefully evaluate the habitat, plants, wildlife and eco-systems of the Cove and the surrounding uplands and watershed and propose an environmental management plan that can be both implemented and enforced by those entrusted with stewarding the Cove and its fragile ecosystems for the public trust.”
FOWC has written a letter to Connecticut DEEP Commissioner Rob Klee asking that he appoint a blue ribbon commission to conduct the research and develop the management plan. FOWC also asked that Commissioner Klee either deny or defer approval on any applications for new docks in the Cove until the management plan can be developed and implemented. Currently there are no docks in the Cove.
“We are very concerned that the installation of docks permitted for motor boat use will greatly increase the amount of motorized watercraft in the Cove,” said Slater. “There’s already too much jet ski and speeding motorboat traffic in the Cove. Those living on the Cove have even seen boats towing water skiers crisscrossing the wild rice plants at high tide. Something has to be done to protect the birds and marine life that give birth and raise their young in the Cove.”
Slater urged all those “who treasure Whalebone Cove and the many species of birds, turtles, fish, reptiles, amphibians, beaver, and rare flora and fauna that make their home in it to attend the meeting, whether they live in the Hadlyme area or beyond.”
Expected to be at the meeting will be representatives from USFW, DEEP, the Connecticut River Watershed Council, and several other conservation organizations.
The meeting will be held at Hadlyme Public Hall, 1 Day Hill Rd., in Lyme, which is at the intersection of Ferry Rd. (Rte. 148), Joshuatown Rd., and Day Hill Rd. Representatives from the Silvio O. Conte Refuge will make a short presentation on the history and mission of the Conte Refuge system, which includes nature preserves throughout the Connecticut River Valley in four states.
For more information, call 860-322-4021 or email fowchadlyme@gmail.com
Filed Under: Lyme, News, Outdoors Next Page »",['Appointing a blue ribbon commission to conduct the research and develop the management plan and denying or defering approval on any applications for new docks in the Cove until the management plan can be developed and implemented.'],6819,multifieldqa_en,en,,7e9ba2f426e81faad004a9aba93c714af689ed705e544d22,Appointing a blue ribbon commission to conduct the research and develop the management plan and denying or defering approval on any applications for new docks in the Cove until the management plan can be developed and implemented.,230
What did the London Directory proclaim to contain?,"A Brief History of Benjamin Franklin's Residences on Craven Street, London: 1757 - 1775 - Journal of the American Revolution
Benjamin Franklin House, 36 Craven St, London. (Photo by Elliott Brown | Wikimedia Commons)
If one looked into Benjamin Franklin’s time on Craven Street, they might initially believe he lived at 36 Craven Street the entirety of his two stays in London based on the plethora of articles on the internet that say so. If they dug a little deeper they might read that he lived at No. 27 Craven Street, previously numbered 7, but now numbered 36; or that he lived exclusively at No. 7 Craven Street; or that he lived in multiple residences on Craven Street; or that he moved out of No. 36 to another house on Craven Street and then moved back into No. 36 the last year of his residence. What is one to believe with all of the conflicting accounts? What does the historical record have to say about Franklin’s time on Craven Street?
Figure 1. Spur Alley 1685. “A map of the parish of St Martins in the Fields, taken from ye last survey, with additions (1685)”. (© The British Library Board, Shelfmark: Maps Crace Port. 13.2, Item number: 2)
Before Craven Street existed there was Spur Alley, a narrow passageway sandwiched between the Hungerford Market to the north (now Charing Cross Station) and Scotland Yard and the Northumberland House and Garden to the south. It was flanked on both ends by major thoroughfares, the Strand on the west, connecting Westminster to London by road, and the River Thames on the east, not only connecting the two cities to each other and to Southwark on the south side of the Thames, but connecting the entire metropolis to the rest of the world. Being located in the City of Westminster, Spur Alley had escaped the devastation of the Great Fire of London in 1666 leaving its wooden structures, built in the early part of seventeenth century, intact, but also in dire need of restoration or demolition. “The ratebooks show that during the last thirty years or so of their existence the houses in Spur Alley were in a very bad condition. Few of them were rated at more than a few shillings and many of them were unoccupied.”[1] The landowner, William, 5th Baron Craven, desiring to increase the profitability of his assets, tore down the derelict structures on Spur Alley around 1730 and leased the newly established lots to builders. By 1735, twenty brick houses in the Georgian style had been built on the west side and sixteen on the east side of the way now called Craven Street.[2]
Figure 2. Craven Street 1746. (John Rocque London, Westminster and Southwark, First Edition 1746, Motco Enterprises Limited, motco.com)
Letters to Franklin during his residence with Mrs. Margaret Stevenson, his landlady on Craven Street, were addressed rather vaguely; “Craven Street/Strand”, “Mrs. Stevensons in Craven Street”, or “Benjamin Franklin Esqr.” are but a few examples. Letters from Franklin referenced “London,” or sometimes “Cravenstreet,” but never included a number. Despite the absence of numbered addresses in Franklin’s correspondence, there was a sense of one’s place in the neighborhood based on entries in the Westminster Rate Books (tax assessments). The Rate Books did not list house numbers during Franklin’s time there, but they did list the residents of Craven Street in a particular order that became the default numbering system for the street. Number one was associated with the first resident listed under “Craven Street” in the Rate Books and was the northernmost house on the west side of the street. The numbers increased counter-clockwise down the west side and up the east side in accordance with the list of residents. In 1748, the first year of Margaret Stevenson’s (Stevens in the Rate Books for that year) residence on Craven Street, she is listed as the twenty-seventh resident, the second house north of Court Street (later Craven Court, now Craven Passage) on the east side of the street.[3]
In 1766, Parliament passed the London Paving and Lighting Act (6 Geo. 3 c. 26), “An act for the better paving, cleansing, and enlightening, the city of London, and the liberties thereof; and for preventing obstructions and annoyances within the same; and for other purposes therein mentioned.”[4] One of the other purposes therein mentioned was the numbering of houses. With an aim to bring order to the chaotic numbering systems or lack thereof on London streets the Act provided that “… the said commissioners … may also cause every house, shop, or warehouse, in each of the said streets, lanes, squares, yards, courts, alleys, passages, and places, to be marked or numbered, in such manner as they shall judge most proper for distinguishing the same.”[5] This was quite an undertaking that took years to accomplish. It was a decade later before numbered addresses on Craven Street in the City of Westminster appeared in The London Directory (1776). The London Directory and its competitors were published primarily by booksellers or printers to supplement their income and were highly profitable. To say they were competitive is an understatement. “Some of the most hotly disputed struggles over copyright in the century concerned guidebooks. Many were optimistically emblazoned with a royal license and a notice that the work had been entered at Stationers’ Hall. Various struggles between rival guides intensified as the potential for profits became clear.”[6] The London Directory boldly proclaimed to contain “An ALPHABETICAL LIST OF THE NAMES and PLACES of ABODE of the MERCHANTS and PRINCIPAL TRADERS of the Cities of LONDON and WESTMINSTER, the Borough of SOUTHWARK, and their Environs, with the Number affixed to each House.”[7] Kent’s Directory made a similar proclamation: “An Alphabetical LIST OF THE Names and Places of Abode OF THE DIRECTORS of COMPANIES, Persons in Public Business, MERCHANTS, and other eminent TRADERS in the Cities of London and Westminster, and Borough of Southwark WITH THE NUMBERS as they are affixed to their Houses agreeable to the late Acts of Parliament.”[8] Mrs. Stevenson wasn’t included in the directories because she didn’t meet the criteria of being a merchant or trader, not because she was a woman. Although it is rare to see women listed in the directories, some examples do exist.[9] If Mrs. Stevenson had appeared in the directories in 1776 it would not have been on Craven Street as she had moved to Northumberland Court, a stone’s throw away, the previous year.[10] A comparison of Craven Street residents whose names and addresses do appear in the directories with the same residents as they appear in the Westminster Rate Books determines if the numbering systems were congruent. For the most part they were. For example, Joseph Bond at No. 30, William Rowles at No. 31, Samuel Sneyd at No. 32, and Jonathan Michie at No. 35 in The London Directory coincide with their places of residence in the Westminster Rate Books; however, errors did occur. The 1776 edition of The London Directory lists Brown & Whiteford, wine merchants, at No. 9 Craven Street while the Westminster Rate Books list them as the twenty-ninth residents. Obviously, it makes no sense to have Brown & Whiteford at No. 9 in The London Directory and their next-door neighbor, Joseph Bond, at No. 30. The same error appears in Baldwin’s The New Complete Guide for 1783. The New Complete Guide may have “borrowed” the error from The London Directory. It was not uncommon for the owner of one directory to copy entries from another to save both time and money. Beginning in 1778 and contrary to The London Directory, Kent’s Directory faithfully followed the numbering system of the Westminster Rate Books in all of its editions and listed Brown & Whiteford at No. 29 as did Bailey’s Northern Directory in 1781. Perhaps realizing their error, The London Directory changed their listing of Brown & Whiteford from No. 9 to No. 29 in their 1783 edition and maintained that listing thereafter.
Sometime prior to 1792, the embankment on the Thames at the south end of Craven Street had been sufficiently extended allowing for the construction of ten new houses below the original houses: “ … four houses, Nos. 21–24, were built on the west side, and six houses, Nos. 25–30, on the east side of the way.”[11] In a note in the same report, the new numbering system is explained. “The houses in the street, which had previously been numbered consecutively down the west side and up the east side, were then renumbered on the same system to include the additional houses.”[12] Because the new houses (21-24) on the west side were built below the existing houses (1-20), houses 1-20 retained their original numbering.
Figure 4. Craven Street 1799. (Richard Horwood’s Map of London, Westminster and the Borough of Southwark 1799, Motco Enterprises Limited, motco.com)
One would think that the numbers of the sixteen original houses on the east side, Nos. 21 – 36, would simply increase by ten with the addition of the ten new houses, but such was not the case; they increased by nine. How could that be? The only possible explanation is that No. 21 of the original houses was demolished to make way for the construction of the northernmost of the six new houses on the east side (No. 30). Evidence of No. 21’s demolition appears in the lease granted to Charles Owen by William, 7th Baron Craven, in 1792, which describes No. 22 as: “All that messuage in Craven Street late in the occupation of Francis Deschamps undertaker … being the Southernmost house in the Old Buildings on the East Side of the said Street numbered with the No. 22.”[13] The lease describes No. 22 as being the southernmost house in the old buildings on the east side of Craven Street. Clearly the house previously at No. 21 did not exist when the lease granted to Charles Owen was written in 1792 as it used to be the southernmost house. It is also worth noting that in 1790, The London Directory listed Jacob Life at No. 21 (original numbering). In 1791-2, it listed him at No. 6. With No. 21 vacated, it would allow for its demolition and the construction of the tenth new house. By utilizing lot No. 21 for the new construction, only nine additional lots were needed to build the ten houses, hence, Margaret Stevenson’s former residence at 27 became 36 (27 + 9) in the renumbering and not 37.
For nearly a century and a half after Franklin departed London for America in March of 1775 the scales were tipped heavily in favor of his residence having been No. 7 Craven Street. As early as 1807 in London; Being An Accurate History And Description Of The British Metropolis And Its Neighborhood, Volume 4, one would have read: “In Craven Street is a house, No. 7, remarkable for having been the residence of Dr. Benjamin Franklin.[14] In 1815, the identical phrase appeared in The Beauties of England and Wales.[15] After 23 editions of not mentioning Franklin, his name finally appeared in the 24th edition of The Picture of London in 1826: “The house, No. 7, Craven Street, in the Strand, was once the residence of Dr. Benjamin Franklin.”[16] In 1840, Jared Sparks referred to Franklin’s Craven Street residence appearing in London guide books in his voluminous The Works of Benjamin Franklin: “In the London Guide Books, ‘No. 7, Craven Street,’ is still indicated as the house in which Dr. Franklin resided.”[17] In 1846, George Gulliver F.R.S., in his book, The Works of William Hewson, wrote: “She [Polly] had been upon terms of the warmest friendship with Dr. Franklin
Figure 5. No. 7 Craven Street with Memorial Tablet. (Photo courtesy of British History Online, and the Survey of London)
since she was eighteen years of age. That eminent philosopher resided with her mother, Mrs. Margaret Stevenson, at No. 7, Craven Street, Strand, during the fifteen years of his abode in London.”[18] Guide books mentioning Franklin at No. 7 continued to proliferate throughout the century: Handbook for London; Past and Present, Volume I (1849);”[19] Handbook for Modern London (1851);”[20] The Town; Its Memorable Characters and Events (1859);”[21] London and Its Environs (1879).[22] There was an anomaly when London In 1880 Illustrated With Bird’s-Eye Views of the Principal Streets, Sixth Edition (1880) placed Franklin at 27 Craven street.[23] The anomaly lasted for six years until his place of residence was changed to No. 7 in the revised edition, London. Illustrated by Eighteen Bird’s-Eye Views of the Principal Streets (1886).[24] London Past and Present; Its History, Associations, and Traditions, Volume 1 (1891), copied the 1849 Handbook for London almost word-for-word and included, “The house is on the right from the Strand.”[25] In October of 1867, The Society of Arts in London declared that: “In order to show how rich the metropolis is in the memory of important personages and events, which it would be desirable to mark by means of tablets on houses, the Council have caused an alphabetical list to be prepared, … ”[26] Franklin had been elected a corresponding member to the Society in 1756 and was a popular choice among Council members deciding who they were to memorialize.[27] By January of 1870, a tablet honoring him was affixed to the house they believed to have been his residence while in London, No. 7 Craven Street in the Strand on the west side of the street.[28] A majority of historians writing about Franklin in the nineteenth and early twentieth century placed him at No. 7: O. L. Holley, The Life of Benjamin Franklin (1848); E. M. Tomkinson, Benjamin Franklin (1885); John Torrey Morse, Benjamin Franklin (1891); Paul Elmer More, Benjamin Franklin (1900); John S. C. Abbot, Benjamin Franklin (1903); Sydney George Fisher, The True Benjamin Franklin (1903). A notable exception is D. H. Montgomery’s His Life Written by Himself published in 1896. He has Franklin at No. 27 Craven Street. It seems then that depending upon the source, Franklin was thought to have lived at either No. 7 or No. 27, but not both, the overwhelming majority favoring No. 7. As late as 2011, Franklin is still mentioned as living at No. 7.[29]
In 1913, No. 7 was scheduled to be torn down. An article in the March 1914 edition of The Book News Monthly, describes the situation:
As is well known to informed American pilgrims, it has been possible for all admirers of the famous philosopher and statesman to pay their respects to his memory before that house, No. 7 Craven Street, just off the Strand, which was his chief home during his two sojourns in the British capital, but even as these lines are being written the London newspapers are recording that that interesting shrine is soon to be pulled down to make room for a restaurant. It is some mitigation of this misfortune to remember that at the most the Craven Street house was nothing more than a reproduction of the one in which Franklin had his suite of four rooms, for the structure has been rebuilt since Franklin’s time. When, then, some one makes a piteous plea that at least the philosopher’s bedroom shall be preserved, the soothing answer is that the apartment in question is only a replica of that in which the illustrious American enjoyed his well-earned slumbers in 1757-62 and 1764-75. The restaurant-builder, however, with an eye doubtless to possible American patronage, has assured the world that every effort will be made to preserve as much as possible of the entire structure.[30]
Concerned with the possible demolition of Franklin’s residence, the Royal Society of Arts (formerly the Society of Arts[31]) initiated an inquiry into the matter.[32] The London County Council, having taken over the responsibility of placing memorial tablets on notable houses from the Royal Society, was charged with the investigation. It ultimately fell to Sir George Laurence Gomme, a clerk to the Council, to come up with a response. A few years earlier Sir George had discovered Margaret Stevenson residing at No. 27 Craven Street in the Westminster Rate Books. He must have wondered why No. 7 on the west side of Craven Street was being celebrated as Franklin’s residence when the evidence clearly showed otherwise.
Sir George and his staff examined the various London directories discussed earlier and came up with a novel explanation for the discrepancy. They concluded that there had been two numbering systems on Craven Street. An anonymous author echoes Sir George’s conclusion about the two numbering systems in an article in The Journal of the Royal Society of Arts:
…an inspection of the directories of that time proves that there were at least two systems of numbering in Craven Street before the erection of the additional houses. According to one of these the numbers started from the top (Strand end) on the west side of the street, and ran down to the bottom to No. 20, then crossed over and went back to the Strand along the east side – 21 to 36. According to the other system, the east side of the street was numbered from the bottom upwards, starting at No 1. This was not apparently in general use, but there is evidence that this numbering was at all events occasionally used.
The evidence of these two systems of numbering, and for believing that Mrs. Stevenson’s house was first No. 7 under the oldest system, next No. 27 under the second system, and finally No. 36 under the latest and existing system, is to be found in the various directories and the Westminster rate-books.[33]
The “evidence” mentioned above consisted of The London Directory’s listing of Brown & Whiteford at No. 9: “The rate-books for 1781 and 1786 show the house next but one to the north of Mrs. Stevenson’s house as in the occupation of Brown and ‘Whiteford,’ while the old directories mention the business of the firm as wine merchants, and give their address as 9, Craven Street – then a little later, down to 1791, as 29, Craven Street. Curiously enough, in the years 1778 to 1780, or 1781, Lowndes gives it as No. 9, and Kent as 29.”[34] Ignoring Kent’s Directory having Brown and Whiteford as 29 and The London Directory (Lowndes) having Brown and Whiteford “a little later” as 29, and knowing that Mrs. Stevenson lived two doors south of them, Sir George concluded that her house must have been numbered 7, even though there is no listing in any of the directories of her residence ever being No. 7. He surmised that the No. 7 on the west side of Craven Street with the memorial tablet thought to have been Franklin’s residence had simply been confused with number 7 (27) on the east side. Again from The Journal of the Royal Society of Arts:
Taking all the evidence together, there cannot be any doubt whatever that Mrs. Stevenson’s house, in which Franklin lodged, was the house two doors north from Craven Court, first numbered 7, afterwards 27, and finally 36, and consequently that the house in which Franklin lived was that now numbered 36, not the one now numbered 7, on which the tablet is placed.[35]
A response to The Royal Society of Arts was issued: “… the London County Council … informed the Society that it had made a mistake and that No. 36 Craven street was the building that deserved commemoration.”[36] The Society accepted the Council’s conclusion, and despite assurances of preservation by the restaurant builder, No. 7 was torn down the following year.
Sir George’s assertion “that Mrs. Stevenson’s house, in which Franklin lodged, was the house two doors north from Craven Court” was correct, however, his assertion that it was “first numbered 7, afterwards 27”, was not. It was only by association with the errant entry of Brown & Whiteford at No. 9 from 1776-1782 in The London Directory that Mrs. Stevenson’s address was conjured to be No. 7. The problem with associating her address exclusively with that of Brown & Whiteford at No. 9 during those years is that, as previously demonstrated, The London Directory also listed four other Craven Street residents, Bond, Rowles, Sneyd, and Michie, who’s addresses did conform to the numbering system in The Westminster Rate Books. If Brown & Whiteford at No. 9 was indicative of a numbering system different from The Westminster Rate Books, Bond, Rowles, Sneyd, and Michie would have been listed as Nos. 10, 11, 12, and 15, respectively. So on one hand Sir George was relying on the Westminster Rate Books to establish Mrs. Stevenson at No. 27 and on the other hand he was dismissing the Westminster Rate Books to establish her at No. 7. Instead of using the anomalous listing of Brown & Whiteford at No. 9, he could have just as easily, and more logically, used the Bond et al. listings, or the post-1782 Brown & Whiteford listing in the London Directory at No. 29 to establish Mrs. Stevenson at No. 27. Even if there had been two numbering systems, his assertion that No. 27 was first numbered 7 would still be false. The earliest numbering system was the Westminster Rate Books dating from the early 1730s when the houses were constructed. Brown & Whiteford at No. 9 didn’t appear until 46 years later and then only for a brief period.
There is ample evidence in Franklin’s correspondence and in a memoir by Polly Hewson (Mrs. Stevenson’s daughter) that Benjamin and Mrs. Stevenson lived in not one, but two houses on Craven Street. On July 6, 1772, Polly wrote to Benjamin from her house at Broad Street North in London: “My Mother I must tell you went off last friday week, took our little Boy with her and left Mr. Hewson [Polly’s husband, William] the care of her House [27 Craven Street]. The first thing he did was pulling down a part of it in order to turn it to his own purpose, and advantage we hope. This Demolition cannot affect you, who at present are not even a Lodger [Benjamin was traveling at the time], your litterary apartment remains untouch’d, the Door is lock’d …”[37] In a memoir about her husband written after his death Polly writes: “He [William Hewson] began his Lectures Sept. 30, 1772, in Craven-street, where he had built a Theatre adjoining a house which he intended for the future residence of his family.”[38] On October 7, 1772, Benjamin wrote to his son William: “I am very well. But we [Mrs. Stevenson and I] are moving to another House in the same street; and I go down tomorrow to Lord LeDespencer’s to [stay a] Week till things are settled.”[39] To his son-in-law, Richard Bache, on the same day he wrote: “We are moving to another House in the [street] leaving this to Mr. Hewson.”[40] Writing to a friend on October 30, 1772 he explained: “I should sooner have answered your Questions but that in the Confusion of my Papers, occasioned by removing to another House, I could not readily find the Memorandums …”[41] On November 4, 1772 Benjamin informed his wife Deborah of the move. “We are removed to a more convenient House in the same street, Mrs. Stevenson having accommodated her Son-in-Law with that we lived in. The Removing has been a troublesome Affair, but is now over.”[42]
An agreement had been struck between the parties. Margaret and Benjamin would move to another house on Craven Street and allow Polly and William to move into No. 27, the large yard behind the house being spacious enough to accommodate the anatomy school William wished to build.[43] Perhaps the idea was inspired by Margaret’s next-door neighbor at No. 26, Dr. John Leake, a man-midwife and founder of the Westminster Lying-in Hospital, who had built a theater adjoining his residence in which he practiced anatomy and taught midwifery.[44]
After Margaret and Benjamin vacated No. 27, Polly, William, their son William Jr., and William’s younger sister, Dorothy Hewson, took up residence there.[45] In the 1773 Westminster Rate Books for Craven Street, Mrs. Stevenson’s (Stephenson in the Rate Books) name has been crossed out and replaced with “William Hewson.”[46] Further proof that the Hewsons had indeed moved into 27 Craven Street has been confirmed by the discovery of human and animal remains buried in the basement of No. 36 (formerly No. 27 and now the Benjamin Franklin House), a by-product of the dissections that took place at William’s anatomy school.[47]
So what house on Craven Street did Mrs. Stevenson and Benjamin move into after vacating No. 27? An examination of the Westminster Rate Books for the years 1774 and 1775 reveal them living not at No. 7 on the west side of Craven Street as one might expect from the overwhelming consensus of nineteenth century guidebooks and biographies, but surprisingly at No. 1.[48] The controversy of No. 7 being torn down was all for naught as it had never been Franklin’s residence. Sir George was correct on that point. Unfortunately, No. 1 was torn down as well in the early part of the twentieth century. The first time No. 1 is mentioned as Franklin’s second residence is in the Survey of London: Volume 18, St Martin-in-The-Fields II: the Strand published by the London County Council in 1937, ironically the same County Council that had declared No. 36 as Franklin’s only residence twenty-four years earlier.
From 1748 until 1772 Margaret ‘Stephenson’ occupied this house [No. 27 (36)], and it was there that Benjamin Franklin settled after his arrival in London in 1757 as Agent to the General Assembly of Pennsylvania … In October, 1772, Mrs. Stevenson and Franklin removed to No. 1, Craven Street (now demolished), and No. 36 was for the next two years occupied by William Hewson, surgeon, who had married Mary Stevenson.[49]
In the spring of 1774, William Hewson died unexpectedly of septicemia two weeks after cutting himself while dissecting a cadaver. Polly was left to care for their two young sons and was pregnant with a daughter she would give birth to in August of the same year. Is it possible that Margaret and Benjamin moved back into No. 27 to assist Polly after the death of her husband as suggested in The Americanization of Benjamin Franklin?[50]
If the Westminster Rate Books are to be believed, the answer is no. For the year 1774, the Rate Books list Margaret Stevenson at No. 1 and William Hewson at No. 27. For the year 1775, they list Margaret Stevenson at No. 1 and Magnus Falkner (Falconer/Falconar) at No. 27. Magnus was William’s assistant at the anatomy school and fiancé to William’s sister, Dorothy. On his death bed, William instructed Polly, “let Mr. Falconar be my successor.”[51] Magnus would immediately take over the running of the anatomy school and continue William’s unfinished research. Four months later, he and Dorothy would marry.[52] Essentially only two things changed at 27 Craven Street after William’s death: Polly gave birth to her daughter, and Magnus replaced William as the lease holder, so even if Margaret and Benjamin had wished to move back into No. 27, there would have been no room for them. It is also interesting to note that considering the multiple times Benjamin wrote of his move out of No. 27 (and complained of it), he never once mentioned moving back into No. 27 in any of his correspondence after Mr. Hewson’s death.
Figure 6. No. 36 Craven Street. (Photo courtesy of David Ross, britainexpress.com)
In sum, based on the Westminster Rate Books[53] and Franklin’s correspondence, Mrs. Stevenson is known to have resided at No. 27 (36) Craven Street from 1748 to 1772. It follows that, aside from the two years Franklin spent in Philadelphia from 1762 to 1764, he resided there from 1757 to 1772. Franklin’s correspondence also reveals that in the autumn of 1772, he and Mrs. Stevenson moved to another house on Craven Street. The 1773 Westminster Rate Books show her name crossed off at No. 27 and William Hewson’s inserted. The following year the Rate Books list her at No. 1 Craven Street. Evidence for Mrs. Stevenson and Benjamin remaining at No. 1 after William’s death appears in the Westminster Rate Books for 1775 which have Mrs. Stevenson still residing at No. 1 and Magnus Falkner residing at No. 27. Further evidence can be construed from the lack of any mention of a move back into No. 27 in Franklin’s correspondence. Despite the many theories one could devise as to why Franklin was thought to have lived at No. 7 Craven Street by so many guide books and Franklin biographers of the nineteenth century, one thing is certain; at some point after Franklin’s departure to America in March of 1775, and no later than 1807, someone mistakenly associated him with No. 7 on the west side of Craven Street, and it soon became his de facto residence. Credit must go to D. H. Montgomery in 1896 and Sir George in 1913 for setting the record partially straight by placing Franklin at No. 27(36). In 1937, the London County Council gave us the first accurate account of Franklin’s residences on Craven Street in the Survey of London at No. 27(36) and No. 1. It has been shown conclusively that No. 27 was never previously numbered 7. It was, however, renumbered 36 in 1792 after ten additional houses were built at the southern end of the street and remains No. 36 to this day.
[1] “Craven Street and Hungerford Lane”, in Survey of London: Volume 18, St Martin-in-the-Fields II: the Strand, ed. G H Gater and E P Wheeler (London, 1937), 27-39, Early History of the Site.
http://www.british-history.ac.uk/survey-london/vol18/pt2/pp27-39
[2] “England, Westminster Rate Books, 1634-1900,” from database with images, Craven Street – 1735, FamilySearch from database by FindMyPast and images digitized by FamilySearch; citing Westminster City Archives, London.
[3] Ibid., Craven Street – 1748.
[4] The Statutes at Large, From Magna Charta to the End of the Eleventh Parliament of Great Britain. Anno 1761 Continued, Vol. XXVII, ed. Danby Pickering, (Cambridge, John Archdeacon, 1767), 96.
[6] James Raven, Publishing Business in Eighteenth-Century England, (Woodbridge: The Boydell Press, 2014), 201.
[7] The London Directory For the Year 1776, Ninth Edition, (London: T. Lowndes, 1776), title page.
[8] Kent’s Directory For the Year 1778, Forty-Sixth Edition, (London: Richard and Henry Causton, 1778), title page.
[9] A listing in Kent’s Directory for the Year 1882 on p. 28 reveals, “Brown Sarah, Leather-seller, 1, Westmoreland-buildings, Aldersgate-street”, and in Kent’s Directory for the Year 1883 on p. 175, “Whiteland Mary, Wine & Brandy Mercht. Jermyn-str. St. James.”
[10] “The Papers of Benjamin Franklin,” Sponsored by The American Philosophical Society and Yale University, Digital Edition by The Packard Humanities Institute, 22:263a.
http://franklinpapers.org/franklin
Mrs. Stevenson wrote to Benjamin Franklin a letter from her new home at 75 Northumberland Court on November 16, 1775: “In this Court I have a kind friend, Mr. Lechmoen he comes and seats with me and talks of you with a hiy regard and friendship.”
[11] Survey of London, Early History of the Site.
[12] Survey of London, Footnotes/n 10.
[13] Survey of London, Historical Notes/No. 31.
[14] David Hughson, LL.D., London; Being An Accurate History And Description Of The British Metropolis And Its Neighbourhood, To Thirty Miles Extent, From An Actual Perambulation, Vol. IV, (London: W. Stratford, 1807), 227.
[15] The Reverend Joseph Nightingale, The Beauties of England and Wales: Or, Original Delineations, Topographical, Historical, and Descriptive, of Each County, Vol. X, Part III, Vol. II (London: J. Harris; Longman and Co.; J. Walker; R. Baldwin; Sherwood and Co.; J. and J. Cundee; B. and R. Crosby and Co.; J Cuthell; J. and J. Richardson; Cadell and Davies; C. and J. Rivington; and G. Cowie and Co., 1815), 245.
[16] John Britton, F.S.A. & Co., ed., The Original Picture of London, Enlarged and Improved: Being A Correct Guide For The Stranger, As Well As For the Inhabitant, To The Metropolis Of The British Empire Together With A Description Of The Environs, The Twenty-Fourth Edition (London: Longman, Rees, Orme, Brown, and Green, 1826), 479.
[17] Jared Sparks, The Works of Benjamin Franklin, Vol. VII, (Philadelphia: Childs & Peterson, 1840), 151.
[18] George Gulliver, F.R.S., The Works of William Hewson, F. R. S., (London: Printed for the Sydenham Society, MDCCCXLVI), xx.
[19] Peter Cunningham, Handbook for London; Past and Present, Vol. I, (London: John Murray, 1849), 245.
[20] F. Saunders, Memories of the Great Metropolis: or, London, from the Tower to the Crystal Palace, (New York: G.P. Putnam, MDCCCLII), 138.
[21] Leigh Hunt, The Town; Its Memorable Characters and Events, (London: Smith, Elder and Co., 1859), 185.
[22] K. Baedeker, London and Its Environs, Including Excursions To Brighton, The Isle of Wight, Etc.: Handbook For Travelers, Second Edition, (London: Dulau and Co., 1879), 133.
[23] Herbert Fry, London In 1880 Illustrated With Bird’s-Eye Views of the Principal Streets, Sixth Edition, (New York: Scribner, Welford, & Co., 1880), 50.
[24] Herbert Fry, London. Illustrated By Eighteen Bird’s-Eye Views of the Principal Streets, (London: W. H. Allen and Co., 1886), 40.
[25] Henry B. Wheatley, F.S.A., London Past and Present; Its History, Associations, and Traditions, Vol. 1, (London: John Murray, New York: Scribner & Welford, 1891), 473.
[26] The Journal of the Society of Arts, Vol. XV, No. 778, (October 18, 1867): 717.
[27] D. G. C. Allen, “Dear and Serviceable to Each Other: Benjamin Franklin and the Royal Society of Arts,” American Philosophical Society, Vol. 144, No. 3, (September 2000): 248-249.
Franklin was a corresponding member in 1756 because he was still residing in Philadelphia. He became an active member the following year when he moved to London.
[28] The Journal of the Society of Arts, Vol. XVIII, No. 894, (Jan. 7, 1870): 137.
“Since the last announcement, the following tablets have been affixed on houses formerly occupied by – Benjamin Franklin, 7 Craven-street, Strand, W.C.”
[29] Franklin in His Own Time, eds. Kevin J. Haytes and Isabelle Bour, (Iowa City, University of Iowa Press, 2011), xxxvii.
“Takes lodgings with Margaret Stevenson at No. 7 Craven Street.” It is unknown if the editors are referring to No. 7 on the west side of Craven Street or No. 36 on the east side using Sir George’s explanation of No. 36 being previously numbered 7.
[30] Henry C. Shelly, “American Shrines on English Soil, III. In the Footprints of Benjamin Franklin,” in The Book News Monthly, September, 1913 to August, 1914, (Philadelphia: John Wanamaker, 1914), 325.
[31] The Journal of the Royal Society of Arts, Vol. LVI, No. 2,880, (Jan. 31, 1908): 245.
http://babel.hathitrust.org/cgi/pt?id=mdp.39015058423073;view=1up;seq=251
“His Majesty the King, who is Patron of the Society, has granted permission to the Society to prefix to its title the term ‘Royal,’ and the Society will consequently be known in future as the ‘Royal Society of Arts.’”
[32] Nineteenth Annual Report, 1914, of the American Scenic and Historic Preservation Society, (Albany: J. B. Lyon Company, 1914), 293.
http://babel.hathitrust.org/cgi/pt?id=wu.89072985302;view=1up;seq=4;size=150
[33] The Journal of the Society of Arts, Vol. LXII, No. 3,183, (Nov. 21, 1913): 18.
http://babel.hathitrust.org/cgi/pt?id=mdp.39015058422968;view=1up;seq=26
[36] Allen, “Dear and Serviceable,” 263-264.
[37] Papers of Benjamin Franklin, 19:20.
[38] Thomas Joseph Pettigrew, F. L. S., Memoirs of the Life and Writings of the Late John Coakley Lettsom With a Selection From His Correspondence, Vol. I, (London: Nichols, Son, and Bentley, 1817), 144 of Correspondence.
[39] Papers of Benjamin Franklin, 19:321b.
[40] Ibid., 19:314.
[41] Ibid., 19:353a.
[43] Simon David John Chaplin, John Hunter and the ‘museum oeconomy’, 1750-1800, Department of History, King’s College London. Thesis submitted for the degree of Doctor of Philosophy of the University of London., 202.
“Following Falconar’s death [1778] the lease [27 Craven Street] was advertised, and the buildings were described as:
A genteel and commodious house, in good Repair, with Coach-house and Stabling for two Horses…consisting of two rooms and light closets on each floor, with outbuildings in the Yard, a Museum, a Compleat Theatre, and other conveniences. (Daily Advertiser, 27 August 1778)”
[44] Simon Chaplin, “Dissection and Display in Eighteenth-Century London,” in Anatomical Dissection in Enlightenment England and Beyond: Autopsy, Pathology and Display, ed. Dr. Piers Mitchell, (Burlington: Ashgate Publishing Company, 2012), 108.
“Given that a nearby building at 35 [ No. 26 in Franklin’s time] was occupied by the man-midwife John Leake, who advertised lectures – including lessons in the art of making preparations – at his ‘theatre’ between 1764 and 1788, it is possible that some facilities were shared. In both cases, however, the buildings [Leake’s residence at No. 26 and Hewson’s residence next door at 27] served a dual function as domestic accommodation and as sites for lecturing and dissection.”
[45] George Gulliver, F.R.S., The Works of William Hewson, F. R. S., (London: Printed for the Sydenham Society, MDCCCXLVI), xviii.
[46] Westminster Rate Books, Craven Street – 1773, courtesy of the City of Westminster Archives.
[47] S.W. Hillson et al., “Benjamin Franklin, William Hewson, and the Craven Street Bones,” Archaeology International, Vol. 2, (Nov. 22, 1998): 14-16.
http://dx.doi.org/10.5334/ai.0206
[48] Westminster Rate Books, Craven Street – 1774, 1775, courtesy of the City of Westminster Archives.
[49] Survey of London, Historical Notes/No. 36, Craven Street (not sourced).
[50] Gordon S. Wood, The Americanization of Benjamin Franklin, (New York: The Penguin Press, 2004), 261.
[51] Pettigrew, Memoirs, 146 of Correspondence.
[52] http://founders.archives.gov/documents/Franklin/01-22-02-0178, note 7. “Falconar married Hewson’s sister five months after the Doctor’s death; most of the Craven Street circle attended the wedding, and BF gave away the bride: Polly to Barbara Hewson, Oct. 4, 1774, APS” (American Philosophical Society); “England Marriages, 1538–1973 ,” database, FamilySearch (https://familysearch.org/ark:/61903/1:1:V52W-TGS : accessed September 15, 2015), Magnus Falconar and Dorothy Hewson, September 12, 1774; citing Saint Martin In The Fields, Westminster, London, England, reference ; FHL microfilm 561156, 561157, 561158, 942 B4HA V. 25, 942 B4HA V. 66.
[53] I chose to rely on the Westminster Rate Books for the numbering system on Craven Street. The books were consistent throughout the eighteenth century in the ordering of residents on the street and were used as the basis for the 1792 re-numbering. For the most part, commercial directories aligned with them as well. If by chance a directory didn’t initially align, it would inevitably produce future editions that did.
Benjamin Franklin, Benjamin Franklin House, London
More from David Turnquist
If one looked into Benjamin Franklin’s time on Craven Street, they might...
I think it’s very ironic that on the street maps included in your excellent article, Craven Street is so close to Scotland Yard. Because following the back and forth juxtapositions of numbers 7, 27 and 36 Craven Street (throw in 75 Northumberland Court and 1 Craven Street, too) was a case that could confound Sherlock Holmes.
Excellent job of deciphering street renumbering material spanning sixty years, including that of a wrong house number (# 7) being erroneously identified and then perpetuated in subsequent street map printings. It’s gratifying at least to know that the present day #36 Craven Street is the correct house for Ben Franklin tourists to visit. Except for #1 Craven Street for the last three years Franklin was in London, but we won’t get into that.
Again, excellent article, David!","['An alphabetical list of names and places of abode of the merchants and principal traders of the cities of London and Westminster, the Borough of Southwark, and their environs, with the number affixed to each house.']",6567,multifieldqa_en,en,,c39f97567cff21a8591f02117632ea71e7b566f65d2d62ab,"An alphabetical list of names and places of abode of the merchants and principal traders of the cities of London and Westminster, the Borough of Southwark, and their environs, with the number affixed to each house.",214
"Can individual molecules of indeno[1,2-a]fluorene switch between open-shell and closed-shell states?","Paper Info

Title: Bistability between π-diradical open-shell and closed-shell states in indeno[1,2-a]fluorene
Publish Date: Unkown
Author List: Shantanu Mishra (from IBM Research Europe -Zurich), Manuel Vilas-Varela (from Department of Organic Chemistry, Center for Research in Biological Chemistry and Molecular Materials (CiQUS), University of Santiago de Compostela), Leonard-Alexander Lieske (from IBM Research Europe -Zurich), Ricardo Ortiz (from Donostia International Physics Center (DIPC)), Igor Rončević (from Department of Chemistry, University of Oxford), Florian Albrecht (from IBM Research Europe -Zurich), Diego Peña (from Department of Organic Chemistry, Center for Research in Biological Chemistry and Molecular Materials (CiQUS), University of Santiago de Compostela), Leo Gross (from IBM Research Europe -Zurich)

Figure

Fig. 1 | Non-benzenoid non-alternant polycyclic conjugated hydrocarbons.a, Classical nonbenzenoid non-alternant polycyclic conjugated hydrocarbons: pentalene, azulene and heptalene.b, Generation of indacenes and indenoindenes through benzinterposition and benzannelation of pentalene, respectively.Gray filled rings represent Clar sextets.c, Closed-shell Kekulé (left) and openshell non-Kekulé (right) resonance structures of QDMs.Note that meta-QDM is a non-Kekulé molecule.All indenofluorene isomers, being derived through benzannelation of indacenes, contain a central QDM moiety.d, Closed-shell Kekulé (top) and open-shell non-Kekulé (bottom) resonance structures of indenofluorenes.Compared to their closed-shell structures, 1 and 5 gain two Clar sextets in the openshell structure, while 2-4 gain only one Clar sextet in the open-shell structure.Colored bonds in d highlight the ortho-and para-QDM moieties in the two closed-shell Kekulé structures of 5. e, Scheme of on-surface generation of 5 by voltage pulse-induced dehydrogenation of 6 (C20H14).Structures 7 and 8 represent the two monoradical species (C20H13).
Fig. 2 | Characterization of open-shell indeno[1,2-a]fluorene on bilayer NaCl/Au(111).a, DFTcalculated wave functions of the frontier orbitals of 5OS in the triplet configuration for the spin up (occupied) level (isovalue: 0.002 e -Å -3 ).Blue and red colors represent opposite phases of the wave function.b, Corresponding DFT-calculated spin density of 5OS (isovalue: 0.01 e -Å -3).Blue and orange colors represent spin up and spin down densities, respectively.c, Probability density of the SOMOs of 5OS (isovalue: 0.001 e -Å -3 ).d, DFT-calculated bond lengths of 5OS.e, Constant-height I(V) spectra acquired on a species of 5 assigned as 5OS, along with the corresponding dI/dV(V) spectra.Open feedback parameters: V = -2 V, I = 0.17 pA (negative bias side) and V = 2 V, I = 0.17 pA (positive bias side).Acquisition position of the spectra is shown in Supplementary Fig.7.f, Scheme of many-body transitions associated to the measured ionic resonances of 5OS.Also shown are STM images of assigned 5OS at biases where the corresponding transitions become accessible.Scanning parameters: I = 0.3 pA (V = -1.2V and -1.5 V) and 0.2 pA (V = 1.3 V and 1.6 V). g, Laplace-filtered AFM image of assigned 5OS.STM set point: V = 0.2 V, I = 0.5 pA on bilayer NaCl, Δz = -0.3Å.The tip-height offset Δz for each panel is provided with respect to the STM setpoint, and positive (negative) values of Δz denote tip approach (retraction) from the STM setpoint.f and g show the same molecule at the same adsorption site, which is next to a trilayer NaCl island.The bright and dark features in the trilayer NaCl island in g correspond to Cl -and Na + ions, respectively.Scale bars: 10 Å (f) and 5 Å (g).
Fig. 3 | Characterization of closed-shell indeno[1,2-a]fluorene on bilayer NaCl/Au(111).a, DFTcalculated wave functions of the frontier orbitals of closed-shell 5 0 (isovalue: 0.002 e -Å -3 ).The wave functions shown here are calculated for the 5para geometry.b, DFT-calculated bond lengths of 5ortho (top) and 5para (bottom).c, Constant-height I(V) spectra acquired on a species of 5 assigned as 5para, along with the corresponding dI/dV(V) spectra.Open feedback parameters: V = -2 V, I = 0.15 pA (negative bias side) and V = 2.2 V, I = 0.15 pA (positive bias side).Acquisition position of the spectra is shown in Supplementary Fig. 7. d, Scheme of many-body transitions associated to the measured ionic resonances of 5para.Also shown are STM images of assigned 5para at biases where the corresponding transitions become accessible.Scanning parameters: I = 0.15 pA (V = -1.5 V) and 0.2 pA (V = 1.7 V). e, Laplace-filtered AFM image of assigned 5para.STM set point: V = 0.2 V, I = 0.5 pA on bilayer NaCl, Δz = -0.7 Å. f, Selected bonds labeled for highlighting bond order differences between 5para and 5ortho.For the bond pairs a/b, c/d and e/f, the bonds labeled in bold exhibit a higher bond order than their neighboring labeled bonds in 5para.g, Laplace-filtered AFM images of 5 on bilayer NaCl/Cu(111) showing switching between 5OS and 5para as the molecule changes its adsorption position.The faint protrusion adjacent to 5 is a defect that stabilizes the adsorption of 5. STM set point: V = 0.2 V, I = 0.5 pA on bilayer NaCl, Δz = -0.3Å. STM and STS data in c and d are acquired on the same species, while the AFM data in e is acquired on a different species.Scale bars: 10 Å (d) and 5 Å (e,g).
NMR (300 MHz, CDCl3) δ: 7.51 (m, 2H), 7.40 -7.28 (m, 5H), 7.27 -7.20 (m, 2H), 7.13 (d, J = 7.7 Hz, 1H), 2.07 (s, 3H), 1.77 (s, 3H) ppm. 13C NMR-DEPT (75 MHz, CDCl3, 1:1 mixture of atropisomers) δ: 141.2 (C), 141.1 (C), 140.0 (C), 139.4 (2C), 137.5 (C), 137.4 (C), 136.0 (3C), 134.8 (C), 134.5 (C), 134.1 (C), 134.0 (C), 133.7 (C), 133.6 (C), 131.6 (CH), 131.2 (CH), 131.1 (CH), 130.7 (CH), 129.8 (CH), 129.7 (CH), 129.5 (CH), 129.4 (CH), 129.0 (CH), 128.9 (CH), 128.7 (2CH), 128.6 (2CH), 127.2 (CH), 127.1 (CH), 127.0 (CH), 126.9 (CH), 126.7 (CH), 126.6 (CH), 20.6 (CH3), 20.5 (CH3), 17.7 (CH3), 17.5 (CH3) ppm.MS (APCI) m/z (%): 327 (M+1, 100).HRMS: C20H16Cl2; calculated: 327.0702, found: 327.0709.
NMR (500 MHz, CDCl3) δ: 7.93 (d, J = 7.6 Hz, 1H), 7.85 (d, J = 7.5 Hz, 1H), 7.78 (d, J = 7.7 Hz, 1H), 7.65 (d, J = 7.4 Hz, 1H), 7.61 (d, J = 7.5 Hz, 1H), 7.59 (d, J = 7.7 Hz, 1H), 7.47 (ddd, J = 8.4, 7.2, 1.1 Hz, 1H), 7.42 (dd, J = 8.1, 7.0 Hz, 1H), 7.35 (m, 2H), 4.22 (s, 3H), 4.02 (s, 3H).ppm. 13C NMR-DEPT (125 MHz, CDCl3) δ: 144.1 (C), 143.3 (C), 142.3 (C), 141.9 (C), 141.8 (C), 141.2 (C), 138.2 (C), 136.5 (C), 127.0 (CH), 126.9 (CH), 126.7 (CH), 126.6 (CH), 125.3 (CH), 125.2 (CH), 123.6 (CH), 122.2 (CH), 119.9 (CH), 118.4 (CH), 37.4 (CH2), 36.3 (CH2).ppm.MS (APCI) m/z (%): 254 (M+, 88).HRMS: C20H14; calculated: 254.1090, found: 254.1090.

abstract

Indenofluorenes are non-benzenoid conjugated hydrocarbons that have received great interest owing to their unusual electronic structure and potential applications in nonlinear optics and photovoltaics. Here, we report the generation of unsubstituted indeno[1,2-a]fluorene, the final and yet unreported parent indenofluorene regioisomer, on various surfaces by cleavage of two C-H bonds in 7,12-dihydro indeno[1,2-a]fluorene through voltage pulses applied by the tip of a combined scanning tunneling microscope and atomic force microscope.
On bilayer NaCl on Au(111), indeno[1,2a]fluorene is in the neutral charge state, while it exhibits charge bistability between neutral and anionic states on the lower work function surfaces of bilayer NaCl on Ag(111) and Cu(111). In the neutral state, indeno[1,2-a]fluorene exhibits either of two ground states: an open-shell π-diradical state, predicted to be a triplet by density functional and multireference many-body perturbation theory calculations, or a closedshell state with a para-quinodimethane moiety in the as-indacene core.
Switching between open-and closed-shell states of a single molecule is observed by changing its adsorption site on NaCl. The inclusion of non-benzenoid carbocyclic rings is a viable route to tune the physicochemical properties of polycyclic conjugated hydrocarbons (PCHs) . Non-benzenoid polycycles may lead to local changes in strain, conjugation, aromaticity, and, relevant to the context of the present work, induce an open-shell ground state of the corresponding PCHs .
Many nonbenzenoid PCHs are also non-alternant, where the presence of odd-membered polycycles breaks the bipartite symmetry of the molecular network . Figure shows classical examples of non-benzenoid non-alternant PCHs, namely, pentalene, azulene and heptalene. Whereas azulene is a stable PCH exhibiting Hückel aromaticity ([4n+2] π-electrons, n = 2), pentalene and heptalene are unstable Hückel antiaromatic compounds with [4n] π-electrons, n = 2 (pentalene) and n = 3 (heptalene).
Benzinterposition of pentalene generates indacenes, consisting of two isomers s-indacene and as-indacene (Fig. ). Apart from being antiaromatic, indacenes also contain proaromatic quinodimethane (QDM) moieties (Fig. ) , which endows them with potential open-shell character. While the parent s-indacene and asindacene have never been isolated, stable derivatives of s-indacene bearing bulky substituents have been synthesized .
A feasible strategy to isolate congeners of otherwise unstable non-benzenoid non-alternant PCHs is through fusion of benzenoid rings at the ends of the π-system, that is, benzannelation. For example, while the parent pentalene is unstable, the benzannelated congener indeno[2,1-a]indene is stable under ambient conditions (Fig. ) .
However, the position of benzannelation is crucial for stability: although indeno[2,1a]indene is stable, its regioisomer indeno[1,2-a]indene (Fig. ) oxidizes under ambient conditions . Similarly, benzannelation of indacenes gives rise to the family of PCHs known as indenofluorenes (Fig. ), which constitute the topic of the present work.
Depending on the benzannelation position and the indacene core, five regioisomers can be constructed, namely, indeno [ Practical interest in indenofluorenes stems from their low frontier orbital gap and excellent electrochemical characteristics that render them as useful components in organic electronic devices .
The potential open-shell character of indenofluorenes has led to several theoretical studies on their use as non-linear optical materials and as candidates for singlet fission in organic photovoltaics . Recent theoretical work has also shown that indenofluorene-based ladder polymers may exhibit fractionalized excitations.
Fundamentally, indenofluorenes represent model systems to study the interplay between aromaticity and magnetism at the molecular scale . Motivated by many of these prospects, the last decade has witnessed intensive synthetic efforts toward the realization of indenofluorenes. Derivatives of 1-4 have been realized in solution , while 1-3 have also been synthesized on surfaces and characterized using scanning tunneling microscopy (STM) and atomic force microscopy (AFM), which provide information on molecular orbital densities , molecular structure and oxidation state .
With regards to the open-shell character of indenofluorenes, 2-4 are theoretically and experimentally interpreted to be closed-shell, while calculations indicate that 1 and 5 should exhibit open-shell ground states . Bulk characterization of mesitylsubstituted 1, including X-ray crystallography, temperature-dependent NMR, and electron spin resonance spectroscopy, provided indications of its open-shell ground state .
Electronic characterization of 1 on Au(111) surface using scanning tunneling spectroscopy (STS) revealed a low electronic gap of 0.4 eV (ref. ). However, no experimental proof of an openshell ground state of 1 on Au(111), such as detection of singly occupied molecular orbitals (SOMOs) or spin excitations and correlations due to unpaired electrons , was shown.
In this work, we report the generation and characterization of unsubstituted 5. Our research is motivated by theoretical calculations that indicate 5 to exhibit the largest diradical character among all indenofluorene isomers . The same calculations also predict that 5 should possess a triplet ground state.
Therefore, 5 would qualify as a Kekulé triplet, of which only a handful of examples exist . However, definitive synthesis of 5 has never been reported so far. Previously, Dressler et al. reported transient isolation of mesityl-substituted 5, where it decomposed both in the solution and in solid state , and only the structural proof of the corresponding dianion was obtained.
On-surface generation of a derivative of 5, starting from truxene as a precursor, was recently reported . STM data on this compound, containing the indeno[1,2-a]fluorene moiety as part of a larger PCH, was interpreted to indicate its open-shell ground state. However, the results did not imply the ground state of unsubstituted 5. Here, we show that on insulating surfaces 5 can exhibit either of two ground states: an open-shell or a closed-shell.
We infer the existence of these two ground states based on high-resolution AFM imaging with bond-order discrimination and STM imaging of molecular orbital densities . AFM imaging reveals molecules with two different geometries. Characteristic bond-order differences in the two geometries concur with the geometry of either an open-or a closed-shell state.
Concurrently, STM images at ionic resonances show molecular orbital densities corresponding to SOMOs for the open-shell geometry, but orbital densities of the highest occupied molecular orbital (HOMO) and lowest unoccupied molecular orbital (LUMO) for the closed-shell geometry. Our experimental results are in good agreement with density functional theory (DFT) and multireference perturbation theory calculations.
Finally, we observe switching between open-and closed-shell states of a single molecule by changing its adsorption site on the surface. Synthetic strategy toward indeno[1,2-a]fluorene. The generation of 5 relies on the solution-phase synthesis of the precursor 7,12-dihydro indeno[1,2-a]fluorene (6). Details on synthesis and characterization of 6 are reported in Supplementary Figs.
. Single molecules of 6 are deposited on coinage metal (Au(111), Ag(111) and Cu(111)) or insulator surfaces. In our work, insulating surfaces correspond to two monolayer-thick (denoted as bilayer) NaCl on coinage metal surfaces. Voltage pulses ranging between 4-6 V are applied by the tip of a combined STM/AFM system, which result in cleavage of one C-H bond at each of the pentagonal apices of 6, thereby leading to the generation of 5 (Fig. ).
In the main text, we focus on the generation and characterization of 5 on insulating surfaces. Generation and characterization of 5 on coinage metal surfaces is shown in Supplementary Fig. . ). Blue and orange colors represent spin up and spin down densities, respectively. c, Probability density of the SOMOs of 5OS (isovalue: 0.001 e -Å -3 ).
d, DFT-calculated bond lengths of 5OS. e, Constant-height I(V) spectra acquired on a species of 5 assigned as 5OS, along with the corresponding dI/dV(V) spectra. Open feedback parameters: V = -2 V, I = 0.17 pA (negative bias side) and V = 2 V, I = 0.17 pA (positive bias side). Acquisition position of the spectra is shown in Supplementary Fig. . f, Scheme of many-body transitions associated to the measured ionic resonances of 5OS.
Also shown are STM images of assigned 5OS at biases where the corresponding transitions become accessible. Scanning parameters: I = 0.3 pA (V = -1.2 V and -1.5 V) and 0.2 pA (V = 1.3 V and 1.6 V). g, Laplace-filtered AFM image of assigned 5OS. STM set point: V = 0.2 V, I = 0.5 pA on bilayer NaCl, Δz = -0.3
Å. The tip-height offset Δz for each panel is provided with respect to the STM setpoint, and positive (negative) values of Δz denote tip approach (retraction) from the STM setpoint. f and g show the same molecule at the same adsorption site, which is next to a trilayer NaCl island. The bright and dark features in the trilayer NaCl island in g correspond to Cl -and Na + ions, respectively.
Scale bars: 10 Å (f) and 5 Å (g). To experimentally explore the electronic structure of 5, we used bilayer NaCl films on coinage metal surfaces to electronically decouple the molecule from the metal surfaces. Before presenting the experimental findings, we summarize the results of our theoretical calculations performed on 5 in the neutral charge state (denoted as 5 0 ).
We start by performing DFT calculations on 5 0 in the gas phase. Geometry optimization performed at the spin-unrestricted UB3LYP/6-31G level of theory leads to one local minimum, 5OS, the geometry of which corresponds to the open-shell resonance structure of 5 (Fig. , the label OS denotes open-shell).
The triplet electronic configuration of 5OS is the lowest-energy state, with the openshell singlet configuration 90 meV higher in energy. Geometry optimization performed at the restricted closed-shell RB3LYP/6-31G level reveals two local minima, 5para and 5ortho, the geometries of which (Fig. ) exhibit bond length alternations in line with the presence of a para-or an ortho-QDM moiety, respectively, in the as-indacene core of the closed-shell resonance structures of 5 (Fig. ) .
Relative to 5OS in the triplet configuration, 5para and 5ortho are 0.40 and 0.43 eV higher in energy, respectively. Additional DFT results are shown in Supplementary Fig. . To gain more accurate insights into the theoretical electronic structure of 5, we performed multireference perturbation theory calculations (Supplementary Fig. ) based on quasi-degenerate second-order n-electron valence state perturbation theory (QD-NEVPT2).
In so far as the order of the ground and excited states are concerned, the results of QD-NEVPT2 calculations qualitatively match with DFT calculations. For 5OS, the triplet configuration remains the lowest-energy state, with the open-shell singlet configuration 60 meV higher in energy. The energy differences between the open-and closed-shell states are substantially reduced in QD-NEVPT2 calculations, with 5para and 5ortho only 0.11 and 0.21 eV higher in energy, respectively, compared to 5OS in the triplet configuration.
We also performed nucleus-independent chemical shift calculations to probe local aromaticity of 5 in the openand closed-shell states. While 5OS in the triplet configuration exhibits local aromaticity at the terminal benzenoid rings, 5OS in the open-shell singlet configuration, 5para and 5ortho all display antiaromaticity (Supplementary Fig. ).
The choice of the insulating surface determines the charge state of 5: while 5 adopts neutral charge state on the high work function bilayer NaCl/Au(111) surface (irrespective of its openor closed-shell state, Supplementary Fig. ), 5 exhibits charge bistability between 5 0 and the anionic state 5 -1 on the lower work function bilayer NaCl/Ag(111) and Cu(111) surfaces (Supplementary Figs. ).
In the main text, we focus on the characterization of 5 on bilayer NaCl/Au(111). Characterization of charge bistable 5 is reported in Supplementary Figs. . We first describe experiments on 5 on bilayer NaCl/Au(111), where 5 exhibits a geometry corresponding to the calculated 5OS geometry, and an open-shell electronic configuration.
We compare the experimental data on this species to calculations on 5OS with a triplet configuration, as theory predicts a triplet ground state for 5OS. For 5OS, the calculated frontier orbitals correspond to the SOMOs ψ1 and ψ2 (Fig. ), whose spin up levels are occupied and the spin down levels are empty.
Figure shows the DFT-calculated bond lengths of 5OS, where the two salient features, namely, the small difference in the bond lengths within each ring and the notably longer bond lengths in the pentagonal rings, agree with the open-shell resonance structure of 5 (Fig. ). Figure shows an AFM image of 5 adsorbed on bilayer NaCl/Au(111) that we assign as 5OS, where the bond-order differences qualitatively correspond to the calculated 5OS geometry (discussed and compared to the closed-shell state below).
Differential conductance spectra (dI/dV(V), where I and V denote the tunneling current and bias voltage, respectively) acquired on assigned 5OS exhibit two peaks centered at -1.5 V and 1.6 V (Fig. ), which we assign to the positive and negative ion resonances (PIR and NIR), respectively. Figure shows the corresponding STM images acquired at the onset (V = -1.2
V/1.3 V) and the peak (V = -1.5 V/1.6 V) of the ionic resonances. To draw a correspondence between the STM images and the molecular orbital densities, we consider tunneling events as many-body electronic transitions between different charge states of 5OS (Fig. ). Within this framework, the PIR corresponds to transitions between 5 0 and the cationic state 5 .
At the onset of the PIR at -1.2 V, an electron can only be detached from the SOMO ψ1 and the corresponding STM image at -1.2 V shows the orbital density of ψ1. Increasing the bias to the peak of the PIR at -1.5 V, it becomes possible to also empty the SOMO ψ2, such that the corresponding STM image shows the superposition of ψ1 and ψ2, that is, |ψ1| 2 + |ψ2| 2 (ref.
). Similarly, the NIR corresponds to transitions between 5 0 and 5 -1 . At the NIR onset of 1.3 V, only electron attachment to ψ2 is energetically possible. At 1.6 V, electron attachment to ψ1 also becomes possible, and the corresponding STM image shows the superposition of ψ1 and ψ2. The observation of the orbital densities of SOMOs, and not the hybridized HOMO and LUMO, proves the open-shell ground state of assigned 5OS.
Measurements of the monoradical species with a doublet ground state are shown in Supplementary Fig. . Unexpectedly, another species of 5 was also experimentally observed that exhibited a closedshell ground state. In contrast to 5OS, where the frontier orbitals correspond to the SOMOs ψ1 and ψ2, DFT calculations predict orbitals of different shapes and symmetries for 5para and 5ortho, denoted as α and β and shown in Fig. .
For 5ortho, α and β correspond to HOMO and LUMO, respectively. The orbitals are inverted in energy and occupation for 5para, where β is the HOMO and α is the LUMO. Fig. shows an AFM image of 5 that we assign as 5para. We experimentally infer its closed-shell state first by using qualitative bond order discrimination by AFM.
In high-resolution AFM imaging, chemical bonds with higher bond order are imaged brighter (that is, with higher frequency shift Δf) due to stronger repulsive forces, and they appear shorter . In Fig. , we label seven bonds whose bond orders show significant qualitative differences in the calculated 5ortho, 5para (Fig. ) and 5OS (Fig. ) geometries.
In 5para, the bonds b and d exhibit a higher bond order than a and c, respectively. This pattern is reversed for 5ortho, while the bond orders of the bonds a-d are all similar and small for 5OS. Furthermore, in 5para bond f exhibits a higher bond order than e, while in 5ortho and 5OS bonds e and f exhibit similar bond order (because they belong to Clar sextets).
Finally, the bond labeled g shows a higher bond order in 5para than in 5ortho and 5OS. The AFM image of assigned 5para shown in Fig. indicates higher bond orders of the bonds b, d and f compared to a, c and e, respectively. In addition, the bond g appears almost point-like and with enhanced Δf contrast compared to its neighboring bonds, indicative of a high bond order (see Supplementary Fig. for height-dependent measurements).
These observations concur with the calculated 5para geometry (Fig. ). Importantly, all these distinguishing bond-order differences are distinctly different in the AFM image of 5OS shown in Fig. , which is consistent with the calculated 5OS geometry (Fig. ). In the AFM images of 5OS (Fig. and Supplementary Fig. ), the bonds a-d at the pentagon apices appear with similar contrast and apparent bond length.
The bonds e and f at one of the terminal benzenoid rings also exhibit similar contrast and apparent bond length, while the central bond g appears longer compared to assigned 5para. Further compelling evidence for the closed-shell state of assigned 5para is obtained by STM and STS. dI/dV(V) spectra acquired on an assigned 5para species exhibit two peaks centered at -1.4 V (PIR) and 1.6 V (NIR) (Fig. ).
STM images acquired at these biases (Fig. ) show the orbital densities of β (-1.4 V) and α (1.6 V). First, the observation of α and β as the frontier orbitals of this species, and not the SOMOs, strongly indicates its closed-shell state. Second, consistent with AFM measurements that indicate good correspondence to the calculated 5para geometry, we observe β as the HOMO and α as the LUMO.
For 5ortho, α should be observed as the HOMO and β as the LUMO. We did not observe molecules with the signatures of 5ortho in our experiments. We observed molecules in open-(5OS, Fig. ) and closed-shell (5para, Fig. ) states in similar occurrence after their generation from 6 on the surface. We could also switch individual molecules between open-and closed-shell states as shown in Fig. and Supplementary Fig. .
To this end, a change in the adsorption site of a molecule was induced by STM imaging at ionic resonances, which often resulted in movement of the molecule. The example presented in Fig. shows a molecule that was switched from 5para to 5OS and back to 5para. The switching is not directed, that is, we cannot choose which of the two species will be formed when changing the adsorption site, and we observed 5OS and 5para in approximately equal yields upon changing the adsorption site.
The molecule in Fig. is adsorbed on top of a defect that stabilizes its adsorption geometry on bilayer NaCl. At defect-free adsorption sites on bilayer NaCl, that is, without a third layer NaCl island or atomic defects in the vicinity of the molecule, 5 could be stably imaged neither by AFM nor by STM at ionic resonances (Supplementary Fig. ).
Without changing the adsorption site, the state of 5 (open-or closedshell) never changed, including the experiments on bilayer NaCl/Ag(111) and Cu(111), on which the charge state of 5 could be switched (Supplementary Figs. ). Also on these lower work function surfaces, both open-and closed-shell species were observed for 5 0 and both showed charge bistability between 5 0 (5OS or 5para) and 5 -1 (Supplementary Figs. ).
The geometrical structure of 5 -1 probed by AFM, and its electronic structure probed by STM imaging at the NIR (corresponding to transitions between 5 -1 and the dianionic state 5 -2 ), are identical within the measurement accuracy for the charged species of both 5OS and 5para. When cycling the charge state of 5 between 5 0 and 5 -1 several times, we always observed the same state (5OS or 5para) when returning to 5 0 , provided the molecule did not move during the charging/discharging process.
Based on our experimental observations we conclude that indeno[1,2-a]fluorene (5), the last unknown indenofluorene isomer, can be stabilized in and switched between an open-shell (5OS) and a closed-shell (5para) state on NaCl. For the former, both DFT and QD-NEVPT2 calculations predict a triplet electronic configuration.
Therefore, 5 can be considered to exhibit the spin-crossover effect, involving magnetic switching between high-spin (5OS) and low-spin (5para) states, coupled with a reversible structural transformation. So far, the spin-crossover effect has mainly only been observed in transition-metal-based coordination compounds with a near-octahedral geometry .
The observation that the switching between open-and closedshell states is related to changes in the adsorption site but is not achieved by charge-state cycling alone, indicates that the NaCl surface and local defects facilitate different electronic configurations of 5 depending on the adsorption site.
Gas-phase QD-NEVPT2 calculations predict that 5OS is the ground state, and the closed-shell 5para and 5ortho states are 0.11 and 0.21 eV higher in energy. The experiments, showing bidirectional switching between 5OS and 5para, indicate that a change in the adsorption site can induce sufficient change in the geometry of 5 (leading to a corresponding change in the ground state electronic configuration) and thus induce switching.
Switching between open-and closed-shell states in 5 does not require the breaking or formation of covalent bonds , but a change of adsorption site on NaCl where the molecule is physisorbed. Our results should have implications for single-molecule devices, capitalizing on the altered electronic and chemical properties of a system in π-diradical open-shell and closed-shell states such as frontier orbital and singlet-triplet gaps, and chemical reactivity.
For possible future applications as a single-molecule switch, it might be possible to also switch between open-and closed-shell states by changing the local electric field, such as by using chargeable adsorbates . Scanning probe microscopy measurements and sample preparation. STM and AFM measurements were performed in a home-built system operating at base pressures below 1×10 -10 mbar and a base temperature of 5 K. Bias voltages are provided with respect to the sample.
All STM, AFM and spectroscopy measurements were performed with carbon monoxide (CO) functionalized tips. AFM measurements were performed in non-contact mode with a qPlus sensor . The sensor was operated in frequency modulation mode with a constant oscillation amplitude of 0.5 Å. STM measurements were performed in constantcurrent mode, AFM measurements were performed in constant-height mode with V = 0 V, and I(V) and Δf(V) spectra were acquired in constant-height mode.
Positive (negative) values of the tip-height offset Δz represent tip approach (retraction) from the STM setpoint. All dI/dV(V) spectra are obtained by numerical differentiation of the corresponding I(V) spectra. STM and AFM images, and spectroscopy curves, were post-processed using Gaussian low-pass filters.
Au(111), Ag(111) and Cu(111) surfaces were cleaned by iterative cycles of sputtering with Ne + ions and annealing up to 800 K. NaCl was thermally evaporated on Au(111), Ag(111) and Cu(111) surfaces held at 323 K, 303 K and 283 K, respectively. This protocol results in the growth of predominantly bilayer (100)-terminated islands, with a minority of trilayer islands.
Sub-monolayer coverage of 6 on surfaces was obtained by flashing an oxidized silicon wafer containing the precursor molecules in front of the cold sample in the microscope. CO molecules for tip functionalization were dosed from the gas phase on the cold sample. Density functional theory calculations. DFT was employed using the PSI4 program package .
All molecules with different charge (neutral and anionic) and electronic (open-and closed-shell) states were independently investigated in the gas phase. The B3LYP exchangecorrelation functional with 6-31G basis set was employed for structural relaxation and singlepoint energy calculations. The convergence criteria were set to 10 −4 eV Å −1 for the total forces and 10 −6 eV for the total energies.
Multireference calculations. Multireference calculations were performed on the DFToptimized geometries using the QD-NEVPT2 level of theory , with three singlet roots and one triplet root included in the state-averaged calculation. A (10,10) active space (that is, 10 electrons in 10 orbitals) was used along with the def2-TZVP basis set .
Increasing either the active space size or expanding the basis set resulted in changes of about 50 meV for relative energies of the singlet and triplet states. These calculations were performed using the ORCA program package . Nucleus-independent chemical shift (NICS) calculations. Isotropic nucleus-independent chemical shift values were evaluated at the centre of each ring using the B3LYP exchangecorrelation functional with def2-TZVP basis set using the Gaussian 16 software package .
Starting materials (reagent grade) were purchased from TCI and Sigma-Aldrich and used without further purification. Reactions were carried out in flame-dried glassware and under an inert atmosphere of purified Ar using Schlenk techniques. Thin-layer chromatography (TLC) was performed on Silica Gel 60 F-254 plates (Merck).
Column chromatography was performed on silica gel (40-60 µm). Nuclear magnetic resonance (NMR) spectra were recorded on a Bruker Varian Mercury 300 or Bruker Varian Inova 500 spectrometers. Mass spectrometry (MS) data were recorded in a Bruker Micro-TOF spectrometer. The synthesis of compound 6 was developed following the two-step synthetic route shown in Supplementary Fig. , which is based on the preparation of methylene-bridge polyarenes by means of Pd-catalyzed activation of benzylic C-H bonds .
Supplementary Figure | Synthetic route to obtain compound 6. The complex Pd2(dba)3 (20 mg, 0.02 mmol) was added over a deoxygenated mixture of 1,3-dibromo-2,4-dimethylbenzene (9, 100 mg, 0.38 mmol), boronic acid 10 (178 mg, 1.14 mmol), K2CO3 (314 mg, 2.28 mmol) and XPhos (35 mg, 0.08 mmol) in toluene (1:1, 10 mL), and the resulting mixture was heated at 90 °C for 2 h.
After cooling to room temperature, the solvents were evaporated under reduced pressure. The reaction crude was purified by column chromatography (SiO2; hexane:CH2Cl2 9:1) affording 11 (94 mg, 76%) as a colorless oil. The complex Pd(OAc)2 (7 mg, 0.03 mmol) was added over a deoxygenated mixture of terphenyl 11 (90 mg, 0.27 mmol), K2CO3 (114 mg, 0.83 mmol) and ligand L (26 mg, 0.06 mmol) in NMP (2 mL).
The resulting mixture was heated at 160 °C for 4 h. After cooling to room temperature, H2O (30 mL) was added, and the mixture was extracted with EtOAc (3x15 mL). The combined organic extracts were dried over anhydrous Na2SO4, filtered, and evaporated under reduced pressure. The reaction crude was purified by column chromatography (SiO2; hexane:CH2Cl2 9:1) affording compound 6 (8 mg, 11%) as a white solid. in AFM imaging due to their reduced adsorption height compared to the rest of the carbon atoms.
We attribute this observation to the significantly different lattice parameter of Cu(111) (2.57 Å) compared to Au(111) and Ag(111) (2.95 Å and 2.94 Å, respectively) , such that the apical carbon atoms of the pentagonal rings of 5 adsorb on the on-top atomic sites on Au(111) and Ag(111), but not on Cu(111).
Our speculation is based on a previous study of polymers of 1 on Au(111) by Di Giovannantonio et al. , where both tilted and planar individual units of 1 were observed depending on whether the apical carbon atoms of the pentagonal rings in 1 adsorbed on the on-top or hollow sites of the surface, respectively.
Given the strong molecule-metal interaction, we found no electronic state signatures of 5 on all three metal surfaces. STM set point for AFM images: V = 0. e, Frontier orbital spectrum of 5 -1 . In the anionic state, ψ2 becomes doubly occupied and ψ1 is the SOMO. Filled and empty circles denote occupied and empty orbitals, respectively.
For each panel, zero of the energy axis has been aligned to the respective highest-energy occupied orbital.","['Yes, individual molecules of indeno[1,2-a]fluorene can switch between open-shell and closed-shell states by changing their adsorption site on the surface.']",5523,multifieldqa_en,en,,566881d2138d7e29cd6dd2b661b6f7ffe4c515c92fdaf837,"Yes, individual molecules of indeno[1,2-a]fluorene can switch between open-shell and closed-shell states by changing their adsorption site on the surface.",154
What are the titles of one of Kam W. Leong's publications in Journal of Controlled Release?,"Publications of Kam W. Leong
Publications of Kam W. Leong :chronological alphabetical combined bibtex listing:
K.W. Leong, Synthetic mast-cell granules as adjuvants to promote and polarize immunity in lymph nodes (2013) [PDF]
K.W. Leong, Tuning Physical Properties of Nanocomplexes through Microfluidics-Assisted Confinement (2013) [PDF]
K.W. Leong, Nucleic acid scavengers inhibit thrombosis without increasing bleeding (2013) [PDF]
K.W. Leong, Nanotopography as modulator of human mesenchymal stem cell function (2013) [PDF]
K.W. Leong, Efficacy of engineered FVIII-producing skeletal muscle enhanced by growth factor-releasing co-axial electrospun fibers (2013) [PDF]
Zhao, F. and Veldhuis, J. J. and Duan, Y. J. and Yang, Y. and Christoforou, N. and Ma, T. and Leong, K. W., Low Oxygen Tension and Synthetic Nanogratings Improve the Uniformity and Stemness of Human Mesenchymal Stem Cell Layer, Molecular Therapy, vol. 18 no. 5 (2010), pp. 1010-1018 [abs]
Kadiyala, I. and Loo, Y. H. and Roy, K. and Rice, J. and Leong, K. W., Transport of chitosan-DNA nanoparticles in human intestinal M-cell model versus normal intestinal enterocytes, European Journal of Pharmaceutical Sciences, vol. 39 no. 1-3 (2010), pp. 103-109 [abs]
Wang, Y. and Quek, C. H. and Leong, K.W. and Fang, J., Synthesis and Cytotoxity of Luminescent InP Quantum Dots, MRS Symposium Proceeding, vol. 1241E (2010)
Jiang, X. and Zheng, Y. and Chen, H. H. and Leong, K. W. and Wang, T. H. and Mao, H. Q., Dual-Sensitive Micellar Nanoparticles Regulate DNA Unpacking and Enhance Gene-Delivery Efficiency, Adv Mater (2010)
Ho, Y. P. and Leong, K. W., Quantum dot-based theranostics, Nanoscale, vol. 2 no. 1 (2010), pp. 60-68 [PDF] [abs]
Phua, K. and Leong, K. W., Microscale oral delivery devices incorporating nanoparticles, Nanomedicine, vol. 5 no. 2 (2010), pp. 161-163
Grigsby, C. L. and Leong, K. W., Balancing protection and release of DNA: tools to address a bottleneck of non-viral gene delivery, Journal of the Royal Society Interface, vol. 7 (2010), pp. S67-S82 [abs]
Chalut, K. J. and Kulangara, K. and Giacomelli, M. G. and Wax, A. and Leong, K. W., Deformation of stem cell nuclei by nanotopographical cues, Soft Matter, vol. 6 no. 8 (2010), pp. 1675-1681 [abs]
Chen, S. and Jones, J. A. and Xu, Y. and Low, H. Y. and Anderson, J. M. and Leong, K. W., Characterization of topographical effects on macrophage behavior in a foreign body response model, Biomaterials, vol. 31 no. 13 (2010), pp. 3479-91 [PDF] [abs]
Yim, E. K. F. and Darling, E. M. and Kulangara, K. and Guilak, F. and Leong, K. W., Nanotopography-induced changes in focal adhesions, cytoskeletal organization, and mechanical properties of human mesenchymal stem cells, Biomaterials, vol. 31 no. 6 (2010), pp. 1299-1306 [PDF] [abs]
Yow, S. Z. and Quek, C. H. and Yim, E. K. F. and Lim, C. T. and Leong, K. W., Collagen-based fibrous scaffold for spatial organization of encapsulated and seeded human mesenchymal stem cells, Biomaterials, vol. 30 no. 6 (2009), pp. 1133-1142 [abs]
Kunder, C. A. and John, A. L. S. and Li, G. J. and Leong, K. W. and Berwin, B. and Staats, H. F. and Abraham, S. N., Mast cell-derived particles deliver peripheral signals to remote lymph nodes, Journal of Experimental Medicine, vol. 206 no. 11 (2009), pp. 2455-2467 [abs]
Ho, Y.P. and Chen, H.H. and Leong, K.W. and Wang, T.H., Combining QD-FRET and microfluidics to monitor DNA nanocomplex self-assembly in real-time, J Vis Exp (2009), pp. 1432
Kulangara, K. and Leong, K. W., Substrate topography shapes cell function, Soft Matter, vol. 5 no. 21 (2009), pp. 4072-4076 [abs]
Chakraborty, S. and Liao, I. C. and Adler, A. and Leong, K. W., Electrohydrodynamics: A facile technique to fabricate drug delivery systems, Advanced Drug Delivery Reviews, vol. 61 no. 12 (2009), pp. 1043-1054 [abs]
Oney, S. and Lam, R. T. S. and Bompiani, K. M. and Blake, C. M. and Quick, G. and Heidel, J. D. and Liu, J. Y. C. and Mack, B. C. and Davis, M. E. and Leong, K. W. and Sullenger, B. A., Development of universal antidotes to control aptamer activity, Nature Medicine, vol. 15 no. 10 (2009), pp. 1224-1228 [PDF] [abs]
Chen, H. H. and Ho, Y. P. and Jiang, X. and Mao, H. Q. and Wang, T. H. and Leong, K. W., Simultaneous non-invasive analysis of DNA condensation and stability by two-step QD-FRET, Nano Today, vol. 4 no. 2 (2009), pp. 125-134 [PDF] [abs]
Ho, Y. P. and Chen, H. H. and Leong, K. W. and Wang, T. H., The convergence of quantum-dot-mediated fluorescence resonance energy transfer and microfluidics for monitoring DNA polyplex self-assembly in real time, Nanotechnology, vol. 20 no. 9 (2009), pp. - [abs]
Liao, I. C. and Chen, S. L. and Liu, J. B. and Leong, K. W., Sustained viral gene delivery through core-shell fibers, Journal of Controlled Release, vol. 139 no. 1 (2009), pp. 48-55 [abs]
Lou, Y. L. and Peng, Y. S. and Chen, B. H. and Wang, L. F. and Leong, K. W., Poly(ethylene imine)-g-chitosan using EX-810 as a spacer for nonviral gene delivery vectors, Journal of Biomedical Materials Research Part A, vol. 88A no. 4 (2009), pp. 1058-1068 [abs]
Chew, S. Y. and Mi, R. and Hoke, A. and Leong, K. W., The effect of the alignment of electrospun fibrous scaffolds on Schwann cell maturation, Biomaterials, vol. 29 no. 6 (2008), pp. 653-61 [abs]
Chen, H. H. and Ho, Y. P. and Jiang, X. and Mao, H. Q. and Wang, T. H. and Leong, K. W., Quantitative comparison of intracellular unpacking kinetics of polyplexes by a model constructed from quantum Dot-FRET, Molecular Therapy, vol. 16 no. 2 (2008), pp. 324-332 [abs]
Chan, B. P. and Leong, K. W., Scaffolding in tissue engineering: general approaches and tissue-specific considerations, European Spine Journal, vol. 17 (2008), pp. S467-S479 [abs]
Tsurushima, H. and Yuan, X. and Dillehay, L. E. and Leong, K. W., Radiation-inducible caspase-8 gene therapy for malignant brain tumors, International Journal of Radiation Oncology Biology Physics, vol. 71 no. 2 (2008), pp. 517-525 [abs]
Bowman, K. and Sarkar, R. and Raut, S. and Leong, K. W., Gene transfer to hemophilia A mice via oral delivery of FVIII-chitosan nanoparticles, Journal of Controlled Release, vol. 132 no. 3 (2008), pp. 252-259 [abs]
Choi, J. S. and Leong, K. W. and Yoo, H. S., In vivo wound healing of diabetic ulcers using electrospun nanofibers immobilized with human epidermal growth factor (EGF), Biomaterials, vol. 29 no. 5 (2008), pp. 587-96 [abs]
Liao, I. C. and Liu, J. B. and Bursac, N. and Leong, K. W., Effect of Electromechanical Stimulation on the Maturation of Myotubes on Aligned Electrospun Fibers, Cellular and Molecular Bioengineering, vol. 1 no. 2-3 (2008), pp. 133-145 [abs]
Prow, T. W. and Bhutto, I. and Kim, S. Y. and Grebe, R. and Merges, C. and McLeod, D. S. and Uno, K. and Mennon, M. and Rodriguez, L. and Leong, K. and Lutty, G. A., Ocular nanoparticle toxicity and transfection of the retina and retinal pigment epithelium, Nanomedicine-Nanotechnology Biology and Medicine, vol. 4 no. 4 (2008), pp. 340-349 [abs]
Tan, S. C. W. and Pan, W. X. and Ma, G. and Cai, N. and Leong, K. W. and Liao, K., Viscoelastic behaviour of human mesenchymal stem cells, Bmc Cell Biology, vol. 9 (2008), pp. - [abs]
Chalut, K. J. and Chen, S. and Finan, J. D. and Giacomelli, M. G. and Guilak, F. and Leong, K. W. and Wax, A., Label-free, high-throughput measurements of dynamic changes in cell nuclei using angle-resolved low coherence interferometry, Biophysical Journal, vol. 94 no. 12 (2008), pp. 4948-4956 [abs]
Haider, M. and Cappello, J. and Ghandehari, H. and Leong, K. W., In vitro chondrogenesis of mesenchymal stem cells in recombinant silk-elastinlike hydrogels, Pharmaceutical Research, vol. 25 no. 3 (2008), pp. 692-699 [abs]
N. Bursac and Y. H. Loo and K. Leong and L. Tung, Novel anisotropic engineered cardiac tissues: Studies of electrical propagation, Biochemical And Biophysical Research Communications, vol. 361 no. 4 (October, 2007), pp. 847 -- 853, ISSN 0006-291X [abs]
Chen, Beiyi and Dang, Jiyoung and Tan, Tuan Lin and Fang, Ning and Chen, Wei Ning and Leong, Kam W. and Chan, Vincent, Dynamics of smooth muscle cell deadhesion from thermosensitive hydroxybutyl chitosan, Biomaterials, vol. 28 no. 8 (2007), pp. 1503 - 1514 [027] [abs]
Chen, B. and Dang, J. and Tan, T. L. and Fang, N. and Chen, W. N. and Leong, K. W. and Chan, V., Dynamics of smooth muscle cell deadhesion from thermosensitive hydroxybutyl chitosan, Biomaterials, vol. 28 no. 8 (2007), pp. 1503-14 [abs]
Park, D. J. and Choi, J. H. and Leong, K. W. and Kwon, J. W. and Eun, H. S., Tissue-engineered bone formation with gene transfer and mesenchymal stem cells in a minimally invasive technique, Laryngoscope, vol. 117 no. 7 (2007), pp. 1267-71 [abs]
Tsurushima, H. and Yuan, X. and Dillehay, L. E. and Leong, K. W., Radioresponsive tumor necrosis factor-related apoptosisinducing ligand (TRAIL) gene therapy for malignant brain tumors, Cancer Gene Therapy, vol. 14 no. 8 (2007), pp. 706-716 [abs]
Chai, C. and Leong, K. W., Biomaterials approach to expand and direct differentiation of stem cells, Molecular Therapy, vol. 15 no. 3 (2007), pp. 467-480 [abs]
Zhang, Y. and Chai, C. and Jiang, X. S. and Teoh, S. H. and Leong, K. W., Fibronectin immobilized by covalent conjugation or physical adsorption shows different bioactivity on aminated-PET, Materials Science & Engineering C-Biomimetic and Supramolecular Systems, vol. 27 no. 2 (2007), pp. 213-219 [abs]
Song, R. J. and Liu, S. Q. and Leong, K. W., Effects of MIP-1 alpha, MIP-3 alpha, and MIP-3 beta on the induction of HIV Gag-specific immune response with DNA vaccines, Molecular Therapy, vol. 15 no. 5 (2007), pp. 1007-1015 [abs]
Yim, E. K. F. and Liao, I. C. and Leong, K. W., Tissue compatibility of interfacial polyelectrolyte complexation fibrous scaffold: Evaluation of blood compatibility and biocompatibility, Tissue Engineering, vol. 13 no. 2 (2007), pp. 423-433 [abs]
Sharma, B. and Williams, C. G. and Kim, T. K. and Sun, D. N. and Malik, A. and Khan, M. and Leong, K. and Elisseeff, J. H., Designing zonal organization into tissue-engineered cartilage, Tissue Engineering, vol. 13 no. 2 (2007), pp. 405-414 [abs]
Chua, K. N. and Tang, Y. N. and Quek, C. H. and Ramakrishna, S. and Leong, K. W. and Mao, H. Q., A dual-functional fibrous scaffold enhances P450 activity of cultured primary rat hepatocytes, Acta Biomaterialia, vol. 3 no. 5 (2007), pp. 643-650 [abs]
Chua, K. N. and Chai, C. and Lee, P. C. and Ramakrishna, S. and Leong, K. W. and Mao, H. Q., Functional nanofiber scaffolds with different spacers modulate adhesion and expansion of cryopreserved umbilical cord blood hematopoietic stem/progenitor cells, Experimental Hematology, vol. 35 no. 5 (2007), pp. 771-781 [abs]
Yim, E. K. F. and Pang, S. W. and Leong, K. W., Synthetic nanostructures inducing differentiation of human mesenchymal stem cells into neuronal lineage, Experimental Cell Research, vol. 313 no. 9 (2007), pp. 1820-1829 [abs]
Chew, S. Y. and Mi, R. F. and Hoke, A. and Leong, K. W., Aligned protein-polymer composite fibers enhance nerve regeneration: A potential tissue-engineering platform, Advanced Functional Materials, vol. 17 no. 8 (2007), pp. 1288-1296 [abs]
Tsurushima, H. and Yuan, X. and Dillehay, L. E. and Leong, K. W., Radio-responsive gene therapy for malignant glioma cells without the radiosensitive promoter: Caspase-3 gene therapy combined with radiation, Cancer Letters, vol. 246 no. 1-2 (2007), pp. 318-323 [abs]
Dang, J.M. and Leong, K. W., Myogenic induction of aligned mesenchymal stem cell sheets by culture on thermally responsive electrospun nanofibers, Advanced Materials, vol. 19 no. 19 (2007), pp. 2775-2779
Dai, H. and Jiang, X. and Tan, G. C. and Chen, Y. and Torbenson, M. and Leong, K. W. and Mao, H. Q., Chitosan-DNA nanoparticles delivered by intrabiliary infusion enhance liver-targeted gene delivery, International Journal of Nanomedicine, vol. 1 no. 4 (2006), pp. 507-522 [abs]
Le Visage, C. and Kim, S. W. and Tateno, K. and Sieber, A. N. and Kostuik, J. P. and Leong, K. W., Interaction of human mesenchymal stem cells with disc cells - Changes in extracellular matrix biosynthesis, Spine, vol. 31 no. 18 (2006), pp. 2036-2042
Ong, S. Y. and Dai, H. and Leong, K. W., Inducing hepatic differentiation of human mesenchymal stem cells in pellet culture, Biomaterials, vol. 27 no. 22 (2006), pp. 4087-4097
Bright, C. and Park, Y. S. and Sieber, A. N. and Kostuik, J. P. and Leong, K. W., In vivo evaluation of plasmid DNA encoding OP-1 protein for spine fusion, Spine, vol. 31 no. 19 (2006), pp. 2163-2172
Yim, E. K. and Wan, A. C. and Le Visage, C. and Liao, I. C. and Leong, K. W., Proliferation and differentiation of human mesenchymal stem cell encapsulated in polyelectrolyte complexation fibrous scaffold, Biomaterials, vol. 27 no. 36 (2006), pp. 6111-22 [abs]
Luong-Van, E. and Grondahl, L. and Chua, K. N. and Leong, K. W. and Nurcombe, V. and Cool, S. M., Controlled release of heparin from poly(epsilon-caprolactone) electrospun fibers, Biomaterials, vol. 27 no. 9 (2006), pp. 2042-2050
Dang, J. M. and Leong, K. W., Natural polymers for gene delivery and tissue engineering, Advanced Drug Delivery Reviews, vol. 58 no. 4 (2006), pp. 487-499
Li, J. and Li, X. and Ni, X. P. and Wang, X. and Li, H. Z. and Leong, K. W., Self-assembled supramolecular hydrogels formed by biodegradable PEO-PHB-PEO triblock copolymers and alpha-cyclodextrin for controlled drug delivery, Biomaterials, vol. 27 no. 22 (2006), pp. 4132-4140
Yim, E. K. F. and Wen, J. and Leong, K. W., Enhanced extracellular matrix production and differentiation of human embryonic germ cell derivatives in biodegradable poly(epsilon-caprolactone-co-ethyl ethylene phosphate) scaffold, Acta Biomaterialia, vol. 2 no. 4 (2006), pp. 365-376
Chew, S. Y. and Hufnagel, T. C. and Lim, C. T. and Leong, K. W., Mechanical properties of single electrospun drug-encapsulated nanofibres, Nanotechnology, vol. 17 no. 15 (2006), pp. 3880-3891
Zhang, Y. and Chai, C. and Jiang, X. S. and Teoh, S. H. and Leong, K. W., Co-culture of umbilical cord blood CD34(+) cells with human mesenchymal stem cells, Tissue Engineering, vol. 12 no. 8",['Sustained viral gene delivery through core-shell fibers and Gene transfer to hemophilia A mice via oral delivery of FVIII-chitosan nanoparticles.'],2345,multifieldqa_en,en,,edbbdb9727c3a51310d24895d08c5a90673cb9d514770878,Sustained viral gene delivery through core-shell fibers and Gene transfer to hemophilia A mice via oral delivery of FVIII-chitosan nanoparticles.,145
What is the purpose of an ICD?,"Do you know the difference between V.T. and T.V?
Like any exclusive club, heart disease has its own jargon, understandable only by other members of the club, particularly by cardiac care providers. For example, I remember lying in my CCU bed (that’s the Coronary Intensive Care Unit), trying to memorize the letters LAD (that’s the Left Anterior Descending, the large coronary artery whose 99% blockage had caused my MI (myocardial infarction – in my case, the so-called ‘widowmaker’ heart attack).
To help others needing simultaneous translation of this new lingo in your research or in your own medical records, here’s a helpful list of some of the most common acronyms/terms you’ll likely find around the cardiac ward.
NOTE from CAROLYN: This entire patient-friendly, jargon-free glossary (all 8,000 words!) is also part of my book “A Woman’s Guide to Living with Heart Disease“ (Johns Hopkins University Press, November 2017).
AA – Anti-arrhythmic: Drugs used to treat patients who have irregular heart rhythms.
Ablation – See Cardiac Ablation.
ACE Inhibitor – Angiotension Converting Enzyme inhibitor: A drug that lowers blood pressure by interfering with the breakdown of a protein-like substance involved in regulating blood pressure.
ACS – Acute Coronary Syndrome: An emergency condition brought on by sudden reduced blood flow to the heart. The first sign of acute coronary syndrome can be sudden stopping of your heart (cardiac arrest).
AED – Automatic External Defibrillator: A portable defibrillator for use during a cardiac emergency; it can be used on patients experiencing sudden cardiac arrest by applying a brief electroshock to the heart through electrodes placed on the chest.
AF or Afib – Atrial Fibrillation: An irregular and often rapid heart rate that can cause poor blood flow to the body. Afib symptoms include heart palpitations, shortness of breath, weakness or fainting. Episodes of atrial fibrillation can come and go, or you may have chronic atrial fibrillation.
AFL – Atrial Flutter: A type of arrhythmia where the upper chambers of the heart (the atria) beat very fast, causing the walls of the lower chambers (the ventricles) to beat inefficiently as well.
A-HCM – Apical Hypertrophic Cardiomyopathy: Also called Yamaguchi Syndrome or Yamaguchi Hypertrophy, a non-obstructive form of cardiomyopathy (a disease of the heart muscle that leads to generalized deterioration of the muscle and its pumping ability) in which a portion of the heart muscle is hypertrophied (thickened) without any obvious cause although there may be a genetic link. It was first described in individuals of Japanese descent.
AI – Aortic Insufficiency: A heart valve disease in which the aortic valve does not close tightly, leading to the backward flow of blood from the aorta (the largest blood vessel) into the left ventricle (a chamber of the heart).
AIVR – Accelerated Idioventricular Rhythm: Ventricular rhythm whose rate is greater than 49 beats/min but less than 100 beats/min, usually benign. (Ventricles are the two main chambers of the heart, left and right).
Angina (stable) – A condition marked by distressing symptoms typically between neck and navel that come on with exertion and go away with rest, caused by an inadequate blood supply to the heart muscle typically because of narrowed coronary arteries feeding the heart muscle. Also known as Angina Pectoris. Unstable angina (UA) occurs when fatty deposits (plaques) in a blood vessel rupture or a blood clot forms, blocking or reducing flow through a narrowed artery, suddenly and severely decreasing blood flow to the heart muscle. Unstable angina is not relieved by rest; it’s dangerous and requires emergency medical attention.
Antiplatelet drugs – Medications that block the formation of blood clots by preventing the clumping of platelets (examples: Plavix, Effient, Brillinta, Ticlid, etc). Heart patients, especially those with implanted stents after PCI, are often prescribed dual antiplatelet therapy (DAPT) which includes one of these prescribed meds along with daily low-dose aspirin.
Aorta – The main artery of the body, carrying blood from the left side of the heart to the arteries of all limbs and organs except the lungs.
Aortic Stenosis: A disease of the heart valves in which the opening of the aortic valve is narrowed. Also called AS.
Aortic valve – One of four valves in the heart, this valve allows blood from the left ventricle to be pumped up (ejected) into the aorta, but prevents blood from returning to the heart once it’s in the aorta.
AP – Apical Pulse: A central pulse located at the apex (pointy bottom) of the heart.
Apex – the lowest (pointy) tip of the heart that points downward at the base, forming what almost looks like a rounded point.
Apical Hypertrophic Cardiomyopathy (A-HCM): Also called Yamaguchi Syndrome or Yamaguchi Hypertrophy, a non-obstructive form of cardiomyopathy (a disease of the heart muscle that leads to generalized deterioration of the muscle and its pumping ability) in which a portion of the heart muscle is hypertrophied (thickened) without any obvious cause. There may be a genetic link. It was first described in people of Japanese descent.
Arrhythmia – A condition in which the heart beats with an irregular or abnormal rhythm.
AS – Aortic Stenosis: A disease of the heart valves in which the opening of the aortic valve is narrowed.
ASD – Atrial Septal Defect: See Septal Defect.
Atrial Flutter – A heart rhythm problem (arrhythmia) originating from the right atrium, most often involving a large circuit that travels around the area of the tricuspid valve (between the right atrium and the right ventricle (this is called typical atrial flutter). Less commonly, atrial flutter can also result from circuits in other areas of the right or left atrium that cause the heart to beat fast (called atypical atrial flutter).
Atrial Septum, the membrane that separates the left and the right upper chambers of the heart (the atria).
Atrium – A chamber of the heart that receives blood from the veins and forces it into a ventricle or ventricles. Plural: atria.
AV – Atrioventricular: A group of cells in the heart located between the upper two chambers (the atria) and the lower two chambers (the ventricles) that regulate the electrical current that passes through it to the ventricles. Also Atrioventricular Block: An interruption or disturbance of the electrical signal between the heart’s upper two chambers (the atria) and lower two chambers (the ventricles). Also Aortic valve: The valve that regulates blood flow from the heart into the aorta.
AVNRT – Atrioventricular Nodal Re-entry Tachycardia: a heart rhythm problem that happens when there’s an electrical short circuit in the centre of the heart, one of the most common types of SVT, most often seen in people in their twenties and thirties, and more common in women than in men.
BAV – Bicuspid Aortic Valve: The most common malformation of the heart valves in which the aortic valve has only two cusps instead of three.
BB – Beta Blocker: A blood pressure-lowering drug that limits the activity of epinephrine, a hormone that increases blood pressure.
BBB – Bundle Branch Block: – A condition in which parts of the heart’s conduction system are defective and unable to normally conduct the electrical signal, causing an irregular heart rhythm (arrhythmia).
BMI – Body mass index: A number that doctors use to determine if you’re overweight. BMI is calculated using a formula of weight in kilograms divided by height in meters squared (BMI =W [kg]/H [m2]). Better yet, just click here to figure out your own BMI.
BNP blood test – BNP (B-type Natriuretic Peptide) is a substance secreted from the ventricles or lower chambers of the heart in response to changes in pressure that happen when heart failure develops and/or worsens. The level of BNP in the blood increases when heart failure symptoms worsen, and decreases when the heart failure condition is stable.
BP – Blood Pressure: The force or pressure exerted by the heart in pumping blood; the pressure of blood in the arteries. See also hypertension.
BrS – Brugada Syndrome: Brugada syndrome is a genetic heart disease that is characterized by distinctively abnormal electrocardiogram (EKG/ECG) findings and an increased risk of sudden cardiac arrest.
CAA – Coronary artery anomaly: A congenital defect in one or more of the coronary arteries of the heart.
CABG – Coronary Artery Bypass Graft: A surgical procedure that reroutes blood flow around a diseased or blocked blood vessel that supplies blood to the heart by grafting either a piece of vein harvested from the leg or the artery from under the breastbone.
CA – Coronary Artery: The arteries arising from the aorta that arch down over the top of the heart and divide into branches. They provide blood to the heart muscle.
CAD – Coronary Artery Disease: A narrowing of the arteries that supply blood to the heart. The condition results from a plaque rupture/blood clot or spasm and greatly increases the risk of a heart attack.
Cardiac Ablation – A procedure performed by an Electrophysiologist (EP) – a cardiologist with specialized training in treating heart rhythm problems – that typically uses catheters — long, flexible tubes inserted through a vein in the groin and threaded to the heart — to correct structural problems in the heart that cause an arrhythmia. Cardiac ablation works by scarring or destroying the tissue in your heart that triggers an abnormal heart rhythm.
Cardiac Arrest – Also known as Sudden Cardiac Arrest: The stopping of the heartbeat, usually because of interference with the electrical signal that regulates each heartbeat (often associated with coronary heart disease). Can lead to Sudden Cardiac Death.
Cardiac Catheterization – An invasive procedure in which a catheter is inserted through a blood vessel in the wrist/arm or groin with x-ray guidance. This procedure can help provide information about blood supply through the coronary arteries, blood pressure, blood flow throughout the chambers of the heart, collection of blood samples, and x-rays of the heart’s ventricles or arteries. It’s typically performed in the cath lab during angiography.
Cardiac Resynchronization Therapy (CRT) also called bi-ventricular pacemaker: an electronic pacing device that’s surgically implanted in the chest to treat the delay in heart ventricle contractions that occur in some people with heart failure.
Cardiac Tamponade – Pressure on the heart that occurs when blood or fluid builds up in the space between the heart muscle (myocardium) and the outer covering sac of the heart (pericardium). Also called Tamponade.
Cardiomyopathy – a chronic disease of the heart muscle (myocardium), in which the muscle is abnormally enlarged, thickened, and/or stiffened.
Cardioversion – A medical procedure in which an abnormally fast heart rate (tachycardia) or cardiac arrhythmia like atrial fibrillation is converted to a normal rhythm using electricity or drugs. Synchronized electrical cardioversion uses a therapeutic dose of electric current to the heart at a specific moment in the cardiac cycle. Chemical cardioversion uses medications to convert to normal rhythm.
Cath lab – the room in the hospital/medical clinic where cardiac catheterization procedures take place (for example, when a stent is implanted into a blocked coronary artery).
CCB – Calcium Channel Blocker: A drug that lowers blood pressure by regulating calcium-related electrical activity in the heart.
CDS – Cardiac Depression Scale: A scale that can help assess the effects of depression occurring as a result of a heart disease diagnosis.
CHF – Heart Failure (also called Congestive Heart Failure): A condition in which the heart cannot pump all the blood returning to it, leading to a backup of blood in the vessels and an accumulation of fluid in the body’s tissues, including the lungs.
CM – Cardiomyopathy: A disease of the heart muscle that leads to generalized deterioration of the muscle and its pumping ability.
CO – Cardiac Output: The amount of blood the heart pumps through the circulatory system in one minute.
Collateral arteries – These extra coronary blood vessels are sometimes able to bypass a blockage in an artery in order to supply enough oxygenated blood to enable the heart muscle to survive when in danger of being damaged because of blockage(s).
Collateral arteries – Blood vessels that provide an alternative arterial supply of blood to an area of the heart that’s in danger of being deprived of oxygenated blood because of one or more blocked arteries.
Congenital heart defect – one of about 35 different types of heart conditions that happen when the heart or the blood vessels near the heart don’t develop normally before a baby is born (in about 1% of live births). Because of medical advances that treat babies born with heart defects, there are now for the first time more adults with congenital heart disease than children.
Congestive heart failure (CHF) – a chronic progressive condition that affects the pumping power of your heart muscle. Often referred to simply as heart failure, CHF specifically refers to the stage in which fluid builds up around the heart and causes it to pump inefficiently.
COPD – Chronic Obstructive Pulmonary Disease: A lung disease defined by persistently poor airflow as a result of breakdown of lung tissue (known as emphysema) and dysfunction of the small airways.Often associated with smoking, it typically worsens over time.
Coronary Microvascular Disease – A heart condition that causes impaired blood flow to the heart muscle through the small vessels of the heart. Also called Microvascular Disease or Small Vessel Disease.
Coronary Reactivity Test – An angiography procedure specifically designed to examine the blood vessels in the heart and how they respond to different medications. Physicians use these images to distinguish different types of blood vessel reactivity dysfunction (such as Coronary Microvascular Disease).
Costochondritis– the cause of severe chest pain, but NOT heart-related; it’s an inflammation of the cartilage that connects a rib to the breastbone.
Coumadin – A drug taken to prevent the blood from clotting and to treat blood clots. Coumadin is believed to reduce the risk of blood clots causing strokes or heart attacks. See also Warfarin.
Cox Maze procedure – A complex “cut-and-sew” surgical procedure done to treat atrial fibrillation through a complicated set of incisions made in a maze-like pattern on the left and right atria (the upper chambers of the heart) to permanently interrupt the abnormal electrical signals that are causing the irregular heartbeats of Afib. See also: Mini-Maze.
CP – Chest Pain (may also be felt as squeezing, pressure, fullness, pressure, heaviness, burning or tightness in the chest).
CPR – Cardiopulmonary Resuscitation: An emergency procedure in which the heart and lungs are made to work by manually compressing the chest overlying the heart and forcing air into the lungs, used to maintain circulation when the heart stops pumping during Cardiac Arrest. Current guidelines suggest hands-only CPR. See also AED.
CQ10 – Co-enzyme Q10: A dietary supplement sometimes recommended for heart patients taking statin drugs.
CRP – C-reactive protein: A byproduct of inflammation, produce by the liver, found in the blood in some cases of acute inflammation.
CRT – Cardiac Resynchronization Therapy also called bi-ventricular pacemaker: an electronic pacing device that’s surgically implanted in the chest to treat the delay in heart ventricle contractions that occur in some people with heart failure.
CT – Computed tomography (CT or CAT scan): An x-ray technique that uses a computer to create cross-sectional images of the body.
CTA – Computerized Tomographic Angiogram: An imaging test to look at the arteries that supply the heart muscle with blood. Unlike a traditional coronary angiogram, CT angiograms don’t use a catheter threaded through your blood vessels to your heart but instead rely on a powerful X-ray machine to produce images of your heart and heart vessels.
CV – Coronary Vein: One of the veins of the heart that drain blood from the heart’s muscular tissue and empty into the right atrium.
CV – Cardiovascular: Pertaining to the heart and blood vessels that make up the circulatory system.
DBP – Diastolic blood pressure: The lowest blood pressure measured in the arteries. It occurs when the heart muscle is relaxed between beats.
DCM – Dilated Cardiomyopathy: A disease of the heart muscle, primarily affecting the heart’s main pumping chamber (left ventricle). The left ventricle becomes enlarged (dilated) and can’t pump blood to your body with as much force as a healthy heart can.
DDI – Drug-drug interaction: A situation in which a medication affects the activity of another medication when both are administered together.
DIL – Diltiazem: A calcium channel blocker drug that acts as a vasodilator; used in the treatment of angina pectoris, hypertension, and supraventricular tachycardia.
Diuretic – A class of drugs used to lower blood pressure. Also known as “water pills”.
Dobutamine stress echocardiography: This is a form of a stress echocardiogram diagnostic test. But instead of exercising on a treadmill or exercise bike to stress the heart, the stress is obtained by giving a drug that stimulates the heart and makes it “think” it’s exercising. The test is used to evaluate your heart and valve function if you are unable to exercise. It is also used to determine how well your heart tolerates activity, and your likelihood of having coronary artery disease (blocked arteries), and it can evaluate the effectiveness of your cardiac treatment plan. See also TTE and Stress Echocardiogram.
Dressler’s syndrome – Happens to a small number of people three to four weeks after a heart attack. The heart muscle that died during the attack sets the immune system in motion, calling on lymphocytes, one of the white blood cells, to infiltrate the coverings of the heart (pericardium) and the lungs (pleura). It also starts generating antibodies, which attack those two coverings. Chest pain (CP) is the predominant symptom; treated with anti-inflammatory drugs.
Dual Antiplatelet Therapy – Medications that block the formation of blood clots by preventing the clumping of platelets (examples Plavix, Effient, Brillinta, Ticlid, etc.) are often prescribed along with aspirin as part of what’s known as dual antiplatelet therapy, especially to patients who have undergone PCI and stent implantation.
DVT – Deep Vein Thrombosis: A blood clot in a deep vein in the calf.
ECG / EKG – Electrocardiogram: A test in which several electronic sensors are placed on the body to monitor electrical activity associated with the heartbeat.
Ectopic beats – small changes in an otherwise normal heartbeat that lead to extra or skipped heartbeats, often occurring without a clear cause, most often harmless.
EF – Ejection Fraction: A measurement of blood that is pumped out of a filled ventricle. The normal rate is 50-60%.
EKG/ECG – Electrocardiogram: A test in which several electronic sensors are placed on the body to monitor electrical activity associated with the heartbeat.
Endothelium: A single-cell layer of flat endothelial cells lining the closed internal spaces of the body such as the inside of blood vessels. Endothelial dysfunction affects the ability of these cells to help dilate blood vessels, control inflammation or prevent blood clots. The endothelium is associated with most forms of cardiovascular disease, such as hypertension, coronary artery disease, chronic heart failure, peripheral vascular disease, diabetes, chronic kidney failure, and severe viral infections.
Enhanced External Counterpulsation – EECP is an FDA-approved non-invasive, non-drug treatment for angina. It works by promoting the development of collateral coronary arteries. The therapy is widely used in prominent heart clinics such as the Cleveland Clinic, Mayo Clinic and Johns Hopkins – especially for patients who are not good candidates for invasive procedures such as bypass surgery, angioplasty or stenting.
EP – Electrophysiologist: A cardiologist who has additional training in diagnosing/treating heart rhythm disorders.
EPS – Electrophysiology Study: A test that uses cardiac catheterization to study patients who have arrhythmias (abnormal hear rhythm). An electrical current stimulates the heart in an effort to provoke an arrhythmia, which is immediately treated with medications. EPS is used primarily to identify the origin of the arrhythmia and to test the effectiveness of medications used to treat abnormal heart rhythms.
EVH – Endoscopic Vessel Harvesting: To create the bypass graft during CABG open heart surgery, a surgeon will remove or “harvest” healthy blood vessels from another part of the body, often from the patient’s leg or arm. This vessel becomes a graft, with one end attaching to a blood source above and the other end below the blocked area. See CABG.
Exercise stress test – An exercise test (walking/running on a treadmill or pedalling a stationary bike) to make your heart work harder and beat faster. An EKG is recorded while you exercise to monitor any abnormal changes in your heart under stress, with or without the aid of drugs to enhance this assessment. See also: MIBI, Echocardiogram, Nuclear Stress Test.
Familial hypercholesterolemia (FH) – A genetic predisposition to dangerously high cholesterol levels. FH is an inherited disorder that can lead to aggressive and premature cardiovascular disease, including problems like heart attacks, strokes, or narrowing of the heart valves.
Femoral Artery: a major artery in your groin/upper thigh area, through which a thin catheter is inserted, eventually making its way into the heart during angioplasty to implant a stent; currently the most widely used angioplasty approach in the United States, but many other countries now prefer the Radial Artery access in the wrist.
FFR – Fractional Flow Reserve: A test used during coronary catheterization (angiogram) to measure pressure differences across a coronary artery stenosis (narrowing or blockage) defined as as the pressure behind a blockage relative to the pressure before the blockage.
HC – High Cholesterol: When fatty deposits build up in your coronary arteries.
HCTZ – Hydrochlorothiazide: A drug used to lower blood pressure; it acts by inhibiting the kidneys’ ability to retain water. Used to be called “water pills”.
Heart Failure – a chronic progressive condition that affects the pumping power of your heart muscle. Sometimes called Congestive Heart Failure (CHF).
Holter Monitor – A portable monitoring device that patients wear for recording heartbeats over a period of 24 hours or more.
HTN – Hypertension: High blood pressure, the force of blood pushing against the walls of arteries as it flows through them.
Hypokinesia – Decreased heart wall motion during each heartbeat, associated with cardiomyopathy, heart failure, or heart attack. Hypokinesia can involve small areas of the heart (segmental) or entire sections of heart muscle (global). Also called hypokinesis.
ICD – Implantable Cardioverter Defibrillator: A surgically implanted electronic device to treat life-threatening heartbeat irregularities.
IHD – Ischemic Heart Disease: heart problems caused by narrowing of the coronary arteries, causing a decreased blood supply to the heart muscle. Also called coronary artery disease and coronary heart disease.
INR – International Normalized Ratio: A laboratory test measure of blood coagulation, often used as a standard for monitoring the effects of the anti-coagulant drug, warfarin (coumadin).
IST – Inappropriate sinus tachycardia: A heart condition seen most often in young women, in which a person’s resting heart rate is abnormally high (greater than 100 bpm), their heart rate increases rapidly with minimal exertion, and this rapid heart rate is accompanied by symptoms of palpitations, fatigue, and/or exercise intolerance.
Interventional cardiologist – A cardiologist who is trained to perform invasive heart procedures like angiography, angioplasty, percutaneous coronary intervention (PCI), implanting stents, etc.
IVS – Interventricular Septum: The stout wall that separates the lower chambers (the ventricles) of the heart from one another.
IVUS – Intravascular Ultrasound: A form of echocardiography performed during cardiac catheterization in which a transducer (a device that can act as a transmitter (sender) and receiver of ultrasound information) is threaded into the heart blood vessels via a catheter; it’s used to provide detailed information about the blockage inside the blood vessels.
LAD – Left Anterior Descending coronary artery: One of the heart’s coronary artery branches from the left main coronary artery which supplies blood to the left ventricle.
LAFB – Left Anterior Fascicular Block: A cardiac condition,distinguished from Left Bundle Branch Block because only the anterior half of the left bundle branch is defective and more common than left posterior fascicular block.
LAHB – Left Anterior Hemiblock: The Left Bundle Branch divides into two major branches – the anterior and the posterior fascicles. Occasionally, a block can occur in one of these fascicles.
Left Circumflex Artery – The artery carries oxygenated blood from the heart to the body; it’s a branch of the Left Main Coronary Artery after the latter runs its course in between the aorta and the Main Pulmonary Artery.
Left Main Coronary Artery – The artery that branches from the aorta to supply oxygenated blood to the heart via the Left Anterior Descending Artery (LAD) and the Left Circumflex Artery.
Lipids – fat-like substances found in your blood and body tissues; a lipid panel is a blood test that measures the level of specific lipids in blood to help assess your risk of cardiovascular disease, measuring four types of lipids: total cholesterol, HDL cholesterol, LDL cholesterol, and triglycerides.
Lipoprotein-a or Lp(a) – molecules made of proteins and fat, carrying cholesterol and similar substances through the blood. A high level of Lp(a) is considered a risk factor for heart disease; detectable via a blood test.
Long QT syndrome (LQTS): A heart rhythm disorder that can potentially cause fast, chaotic heartbeats that may trigger a sudden fainting spell or seizure. In some cases, the heart may beat erratically for so long that it can cause sudden death.
LV – Left Ventricle – One of four chambers (two atria and two ventricles) in the human heart, it receives oxygenated blood from the left atrium via the mitral valve, and pumps it into the aorta via the aortic valve.
LVAD – Left ventricular assist device: A mechanical device that can be placed outside the body or implanted inside the body. An LVAD does not replace the heart – it “assists” or “helps” it pump oxygen-rich blood from the left ventricle to the rest of the body, usually as a bridge to heart transplant.
LVH – Left Ventricular Hypertrophy: A thickening of the myocardium (muscle) of the Left Ventricle (LV) of the heart..
Lumen – The hollow area within a tube, such as a blood vessel.
Main Pulmonary Artery – Carries oxygen-depleted blood from the heart to the lungs.
MIBI – Nuclear Stress Test/Cardiac Perfusion Scan/Sestamibi: tests that are used to assess the blood flow to the heart muscle (myocardium) when it is stressed by exercise or medication, and to find out what areas of the myocardium have decreased blood flow due to coronary artery disease. This is done by injecting a tiny amount of radionuclide like thallium or technetium (chemicals which release a type of radioactivity called gamma rays) into a vein in the arm or hand.
Microvascular disease – a heart condition that causes impaired blood flow to the heart muscle through the small blood vessels of the heart. Symptoms mimic those of a heart attack. Also called Coronary Microvascular Disease or Small Vessel Disease. I live with this diagnosis and have written more about it here, here and here.
Mini-Maze – a surgical procedure to treat atrial fibrillation, less invasive than what’s called the Cox Maze III procedure (a “cut-and-sew” procedure), and performed on a beating heart without opening the chest.
Mitral Valve: One of four valves in the heart, the structure that controls blood flow between the heart’s left atrium (upper chamber) and left ventricle (lower chamber). The mitral valve has two flaps (cusps). See also MV and/or Valves.
Mitral valve prolapse: a condition in which the two valve flaps of the mitral valve don’t close smoothly or evenly, but instead bulge (prolapse) upward into the left atrium; also known as click-murmur syndrome, Barlow’s syndrome or floppy valve syndrome.
MR – Mitral regurgitation: (also mitral insufficiency or mitral incompetence) a heart condition in which the mitral valve does not close properly when the heart pumps out blood. It’s the abnormal leaking of blood from the left ventricle, through the mitral valve and into the left atrium when the left ventricle contracts.
MRI – Magnetic Resonance Imaging: A technique that produces images of the heart and other body structures by measuring the response of certain elements (such as hydrogen) in the body to a magnetic field. An MRI can produce detailed pictures of the heart and its various structures without the need to inject a dye.
MS – Mitral Stenosis: A narrowing of the mitral valve, which controls blood flow from the heart’s upper left chamber (the left atrium) to its lower left chamber (the left ventricle). May result from an inherited (congenital) problem or from rheumatic fever.
MUGA – Multiple-Gated Acquisition Scanning: A non-invasive nuclear test that uses a radioactive isotope called technetium to evaluate the functioning of the heart’s ventricles.
Murmur – Noises superimposed on normal heart sounds. They are caused by congenital defects or damaged heart valves that do not close properly and allow blood to leak back into the originating chamber.
MV – Mitral Valve: The structure that controls blood flow between the heart’s left atrium (upper chamber) and left ventricle (lower chamber).
Myocardial Infarction (MI, heart attack) – The damage or death of an area of the heart muscle (myocardium) resulting from a blocked blood supply to the area. The affected tissue dies, injuring the heart.
Myocardium – The muscular tissue of the heart.
New Wall-Motion Abnormalities – Results seen on an echocardiogram test report (see NWMA, below).
Nitroglycerin – A medicine that helps relax and dilate arteries; often used to treat cardiac chest pain (angina). Also called NTG or GTN.
NSR – Normal Sinus Rhythm: The characteristic rhythm of the healthy human heart. NSR is considered to be present if the heart rate is in the normal range, the P waves are normal on the EKG/ECG, and the rate does not vary significantly.
NSTEMI – Non-ST-segment-elevation myocardial infarction: The milder form of the two main types of heart attack. An NSTEMI heart attack does not produce an ST-segment elevation seen on an electrocardiogram test (EKG). See also STEMI.
Nuclear Stress Test – A diagnostic test that usually involves two exercise stress tests, one while you’re exercising on a treadmill/stationary bike or with medication that stresses your heart, and another set while you’re at rest. A nuclear stress test is used to gather information about how well your heart works during physical activity and at rest. See also: Exercise stress test, Nuclear perfusion test, MIBI.
Open heart surgery – Any surgery in which the chest is opened and surgery is done on the heart muscle, valves, coronary arteries, or other parts of the heart (such as the aorta). See also CABG.
Pacemaker – A surgically implanted electronic device that helps regulate the heartbeat.
PAD – Peripheral Artery Disease: A common circulatory problem in which narrowed arteries reduce blood flow to the limbs, usually to the legs. Symptoms include leg pain when walking (called intermittent claudication).
PAF – Paroxysmal Atrial Fibrillation: Atrial fibrillation that lasts from a few seconds to days, then stops on its own. See also Atrial Fibrillation.
Palpitations – A noticeably rapid, strong, or irregular heartbeat due to agitation, exertion or illness.
Paroxysmal Atrial Fibrillation – An unusual heart arrhythmia of unknown origin, at one time believed to be associated with an unusual sensitivity to alcohol consumption.
PDA – patent ductus arteriosus: A persistent opening between two major blood vessels leading from the heart. The opening is called ductus arteriosus and is a normal part of a baby’s circulatory system before birth that usually closes shortly after birth. But when it remains open, it’s called a patent ductus arteriosus. If it’s small, it may never need treatment, but a large PDA left untreated can allow poorly oxygenated blood to flow in the wrong direction, weakening the heart muscle and causing heart failure or other complications.
Pericardium: two thin layers of a sac-like tissue that surround the heart, hold it in place and help it work.
PET – Positron Emission Tomography: A non-invasive scanning technique that uses small amounts of radioactive positrons (positively charged particles) to visualize body function and metabolism. In cardiology, PET scans are used to evaluate heart muscle function in patients with coronary artery disease or cardiomyopathy.
PFO – Patent Forman Ovale: An opening between the left and right atria (the upper chambers) of the heart. Everyone has a PFO before birth, but in 1 out of every 3 or 4 people, the opening does not close naturally as it should after birth.
Plaque – A deposit of fatty (and other) substances in the inner lining of the artery wall; it is characteristic of atherosclerosis.
POTS – Postural Orthostatic Tachycardia Syndrome: A disorder that causes an increased heart rate when a person stands upright.
PPCM – Post-partum cardiomyopathy: A form of cardiomyopathy that causes heart failure toward the end of pregnancy or in the months after delivery, in the absence of any other cause of heart failure.
Preeclampsia – a late-pregnancy complication identified by spikes in blood pressure, protein in the urine, possible vision problems. Women who experience pregnancy complications like preeclampsia are at significantly higher risk for heart disease.
Prinzmetal’s Variant Angina – Chest pain caused by a spasm in a coronary artery that supplies blood to the heart muscle.
PSVT – Paroxysmal Supraventricular Tachycardia: – An occasional rapid heart rate (150-250 beats per minute) that is caused by events triggered in areas above the heart’s lower chambers (the ventricles). “Paroxysmal” means from time to time. See also supraventricular tachycardia (SVT).
Pulmonary Valve: One of the four valves in the heart, located between the pulmonary artery and the right ventricle of the heart, moves blood toward the lungs and keeps it from sloshing back into the heart.
PV – Pulmonary Vein: A vein carrying oxygenated blood from the lungs to the left atrium of the heart.
PVC – Premature Ventricular Contraction: An early or extra heartbeat that happens when the heart’s lower chambers (the ventricles) contract too soon, out of sequence with the normal heartbeat. In the absence of any underlying heart disease, PVCs do not generally indicate a problem with electrical stability, and are usually benign.
RA – Right Atrium: The right upper chamber of the heart. The right atrium receives de-oxygenated blood from the body through the vena cava and pumps it into the right ventricle which then sends it to the lungs to be oxygenated.
Radial Artery: the artery in the wrist where a thin catheter is inserted through the body’s network of arteries in the arm and eventually into the heart during a procedure to implant a stent. Doctors may also call this transradial access, the transradial approach, or transradial angioplasty. Because it’s associated with fewer complications, this is increasingly considered the default access approach in most countries, except in the U.S. where the traditional Femoral Artery (groin) approach is still the most popular access.
RBBB – Right Bundle Branch Block: A delay or obstruction along the pathway that electrical impulses travel to make your heart beat. The delay or blockage occurs on the pathway that sends electrical impulses to the right side of your heart. See also Left Bundle Branch Block.
RCA – Right Coronary Artery: An artery that supplies blood to the right side of the heart.
Restenosis – The re-closing or re-narrowing of an artery after an interventional procedure such as angioplasty or stent placement. Sometimes called “stent failure”.
RHD – Rheumatic Heart Disease: Permanent damage to the valves of the heart caused especially by repeated attacks of rheumatic fever.
RM – Right Main coronary artery: A blood vessel that supplies oxygenated blood to the walls of the heart’s ventricles and the right atrium.
RV – Right Ventricle: The lower right chamber of the heart that receives de-oxygenated blood from the right atrium and pumps it under low pressure into the lungs via the pulmonary artery.
SA – Sinus node: The “natural” pacemaker of the heart. The node is a group of specialized cells in the top of the right atrium which produces the electrical impulses that travel down to eventually reach the ventricular muscle, causing the heart to contract.
SB – Sinus Bradycardia: Abnormally slow heartbeat.
SBP – Systolic Blood Pressure: The highest blood pressure measured in the arteries. It occurs when the heart contracts with each heartbeat. Example: the first number in 120/80.
SCAD – Spontaneous Coronary Artery Dissection: A rare emergency condition that occurs when a tear forms in one of the blood vessels in the heart, causing a heart attack, abnormalities in heart rhythm and/or sudden death. SCAD tends to strike young healthy women with few if any cardiac risk factors.
SD – Septal defect: A hole in the wall of the heart separating the atria (two upper chambers of the heart) or in the wall of the heart separating the ventricles (two lower chambers).
Sestamibi stress test – See MIBI.
Short QT intervals (SQT): An abnormal heart rhythm where the heart muscle takes a shorter time to recharge between beats. It can cause a variety of complications from fainting and dizziness to sudden cardiac arrest.
Sick Sinus Syndrome (also known as sinus node dysfunction) is caused by an electrical problem in the heart; a group of related heart conditions that can affect how the heart beats, most commonly in older adults, although it can be diagnosed in people of any age. “Sick sinus” refers to the sinoatrial node (see below). In people with sick sinus syndrome, the SA node does not function normally.
Sinoatrial node (SA): also commonly called the sinus node; it’s a small bundle of neurons situated in the upper part of the wall of the right atrium (the right upper chamber of the heart). The heart’s electrical impulses are generated there. It’s the normal natural pacemaker of the heart and is responsible for the initiation of each heartbeat.
Spontaneous Coronary Artery Dissection (SCAD) – A rare emergency condition that occurs when a tear forms in one of the blood vessels in the heart, causing a heart attack, abnormalities in heart rhythm and/or sudden death. SCAD tends to strike young healthy women with few if any cardiac risk factors.
SSS – Sick Sinus Syndrome: The failure of the sinus node to regulate the heart’s rhythm.
ST – Sinus Tachycardia: A heart rhythm with elevated rate of impulses originating from the sinoatrial node, defined as greater than 100 beats per minute (bpm) in an average adult. The normal heart rate in the average adult ranges from 60–100 bpm. Also called sinus tach or sinus tachy.
Statins – Any of a class of drugs that lower the levels of low-density lipoproteins (LDL) – the ‘bad’ cholesterol in the blood – by inhibiting the activity of an enzyme involved in the production of cholesterol in the liver. Examples of brand name statins: Lipitor, Crestor, Zocor, Mevacor, Levachol, Lescol, etc. Also available as a cheaper generic form of the drug.
STEMI – ST-elevation heart attack (myocardial infarction). The more severe form of the two main types of heart attack. A STEMI produces a characteristic elevation in the ST segment on an electrocardiogram (EKG). The elevated ST segment is how this type of heart attack got its name. See also NSTEMI.
Stent – An implantable device made of expandable, metal mesh (looks a bit like a tiny chicken wire tube) that is placed (by using a balloon catheter) at the site of a narrowing coronary artery during an angioplasty procedure. The stent is then expanded when the balloon fills, the balloon is removed, and the stent is left in place to help keep the artery open. TRIVIA ALERT: the coronary stent was named after Charles Stent (1807-1885), an English dentist who invented a compound to produce dentures and other things like skin grafts and hollow tubes (essentially what a metal coronary stent is). His real claim to fame occurred when he suggested using his material to coat underwater trans-Atlantic cable, which had broken several times as a result of corrosion by seawater. You’re welcome.
Stint – a common spelling mistake when what you really mean is the word “stent” (see above).
Stress Echocardiography – A standard echocardiogram test that’s performed while the person exercises on a treadmill or stationary bicycle. This test can be used to visualize the motion of the heart’s walls and pumping action when the heart is stressed, possibly revealing a lack of blood flow that isn’t always apparent on other heart tests. The echocardiogram is performed just before and just after the exercise part of the procedure. See also TTE.
Sudden Cardiac Arrest – The stopping of the heartbeat, usually because of interference with the electrical signal (often associated with coronary heart disease). Can lead to Sudden Cardiac Death.
Takotsubo Cardiomyopathy – A heart condition that can mimic a heart attack. Sometimes called Broken Heart Syndrome, it is not a heart attack, but it feels just like one, with common symptoms like severe chest pain and shortness of breath. It sometimes follows a severe emotional stress. Over 90% of reported cases are in women ages 58 to 75. Also referred to as Broken Heart Syndrome, stress cardiomyopathy, stress-induced cardiomyopathy or apical ballooning syndrome.
TAVR – Transcatheter aortic valve replacement: A minimally invasive procedure to repair a damaged or diseased aortic valve. A catheter is inserted into an artery in the groin and threaded to the heart. A balloon at the end of the catheter, with a replacement valve folded around it, delivers the new valve to take the place of the old. Also called TAVI (Transcatheter aortic valve implantation).
Tetralogy of Fallot – A rare condition caused by a combination of four heart defects that are present at birth, affecting the structure of the heart and causing oxygen-poor blood to flow out of the heart and into the rest of the body. Infants and children with Tetralogy of Fallot usually have blue-tinged skin because their blood doesn’t carry enough oxygen. Often diagnosed in infancy, but sometimes not until later in life depending on severity.
Tg – Triglycerides: The most common fatty substance found in the blood; normally stored as an energy source in fat tissue. High triglyceride levels may thicken the blood and make a person more susceptible to clot formation. High triglyceride levels tend to accompany high cholesterol levels and other risk factors for heart disease, such as obesity.
TIA – Transient Ischemic Attack: A stroke-like event that lasts only for a short time and is caused by a temporarily blocked blood vessel.
TEE – Transesophageal echocardiogram: This test involves an ultrasound transducer inserted down the throat into the esophagus in order to take clear images of the heart structures without the interference of the lungs and chest.
Treadmill Stress Test – See Exercise Stress Test.
troponin – a type of cardiac enzyme found in heart muscle, and released into the blood when there is damage to the heart (for example, during a heart attack). A positive blood test that shows elevated troponin is the preferred test for a suspected heart attack because it is more specific for heart injury than other blood tests, especially the newer high sensitivity troponin tests (hs-cTnT).
TTE – Transthoracic Echocardiogram: This is the standard echocardiogram, a painless test similar to X-ray, but without the radiation, using a hand-held device called a transducer placed on the chest to transmit high frequency sound waves (ultrasound). These sound waves bounce off the heart structures, producing images and sounds that can be used by the doctor to detect heart damage and disease.
TV – Tricuspid Valve: One of four one-way valves in the heart, a structure that controls blood flow from the heart’s upper right chamber (the right atrium) into the lower right chamber (the right ventricle).
UA or USA – Unstable Angina: Chest pain that occurs when diseased blood vessels restrict blood flow to the heart; symptoms are not relieved by rest; considered a dangerous and emergency crisis requiring immediate medical help.
Valves: Your heart has four one-way valves that keep blood flowing in the right direction. Blood enters the heart first through the tricuspid valve, and next goes through the pulmonary valve (sometimes called the pulmonic valve) on its way to the lungs. Then the blood returning from the lungs passes through the mitral (bicuspid) valve and leaves the heart through the aortic valve.
Vasodilator: A drug that causes dilation (widening) of blood vessels.
Vasospasm: A blood vessel spasm that causes sudden constriction, reducing its diameter and blood flow to the heart muscle. See also Prinzmetal’s Variant Angina.
VB – Ventricular Bigeminy: A heart rhythm condition in which the heart experiences two beats of the pulse in rapid succession.
Vena Cava – a large vein that carryies de-oxygenated blood into the heart. There are two in humans, the inferior vena cava (carrying blood from the lower body) and the superior vena cava (carrying blood from the head, arms, and upper body).
Ventricle – each of the two main chambers of the heart, left and right.
VF – Ventricular Fibrillation: A condition in which the ventricles (two lower chambers of the heart) contract in a rapid, unsynchronized fashion. When fibrillation occurs, the ventricles cannot pump blood throughout the body. Most sudden cardiac deaths are caused by VF or ventricular tachycardia (VT).
VLDL – Very Low Density Lipoprotein: Molecules made up of mostly triglycerides, cholesterol and proteins. VLDL, also known as the “very bad” cholesterol, carries cholesterol from the liver to organs and tissues in the body. It may lead to low density lipoproteins (LDL), associated with higher heart disease risks. VLDL levels are tricky to measure routinely, and are usually estimated as a percentage of your triglyceride levels. By reducing triglycerides, you are usually also reducing your VLDL levels.
Warfarin – A drug taken to prevent the blood from clotting and to treat blood clots. Warfarin is believed to reduce the risk of blood clots causing strokes or heart attacks. Also known as Coumadin.
Widowmaker heart attack – The type of heart attack I survived, since you asked. A nickname doctors use to describe a severely blocked left main coronary artery or proximal left anterior descending coronary artery of the heart. This term is used because if the artery gets abruptly and completely blocked, it can cause a massive heart attack that will likely lead to sudden cardiac death. Please note the gender imbalance here: despite the number of women like me who do experience this type of cardiac event, doctors are not calling this the widowermaker, after all.
WPW – Wolff-Parkinson-White Syndrome: A condition in which an extra electrical pathway connects the atria (two upper chambers) and the ventricles (two lower chambers). It may cause a rapid heartbeat.
NOTE FROM CAROLYN: I was very happy when we were able to include this entire glossary in my book, “A Woman’s Guide to Living with Heart Disease“ (Johns Hopkins University Press, 2017).
Are we missing any important heart acronyms/terms from this list? Let me know!
Please can someone explain something for me. I am a 53 yr old woman and generally fit and healthy. Had 2 ECG’s due to a one off dizzy spell during a stressful time dealing with my fathers terminal diagnosis. The 2nd ECG request did give me concern as i did not know why i had to have one. On 24/01/19 at my doctors appointment she explained that on 3 the leads it showed inverted T waves. And she explained that it may suggest angina. I was so shocked. Wasn’t expecting that. She gave me a GNT (nitroglycerin) spray in case I do get pain and take 75Mg of aspirin. I’m now waiting for a Cardiology referral.
I am so stressed and consumed by what might be wrong. My maternal grandmother had angina and valve issues. Her 3 brothers all had double bypasses. Could I have inherited this? I am not overweight at 63kg and 5.ft 9. I walk 20-25 miles a week at work and general walking here and there. I started HRT (patches evorol 25 -50) in July as menopause pain was making me feel like I was 90 and was getting me down.
I am worried so much now and analysing every ache/ twinge I get. I feel like a hypochondriac at the moment. I’m worried what will happen at the cardiologist and what the test will entail and tell me. I am waiting on cholesterol test which I had on 25/01/19. Can I have inverted T waves and be fine. Please help I am so scared and crying far too much.
Hello Colleen – the first thing is: please take a big deep breath before you read another word here! I’m not a physician so of course cannot comment on your specific case, but I can tell you generally that the definition of “angina” (as this glossary lists above) is “distressing symptoms”, typically chest pain that gets worse with exertion, and goes away with rest. That’s classic stable angina… typically caused by something that’s reducing blood flow to the heart muscle (causing the chest pain of angina).
A family history that might make a difference for you personally is only in what’s called your ‘first degree’ relatives: for example, if your mother or sister were diagnosed with heart disease before age 65, or if your Dad or brother were diagnosed before age 55, then doctors would consider that you have a family history as a risk factor for heart disease. There’s little if any scientific evidence that a grandparent or uncle’s heart disease history has any effect on your own risk.
It is a very good thing that you’re having further tests and a referral to a cardiologist, if only to ease your mind. There are many reasons for inverted T-waves, ranging from cardiac issues to completely benign conditions. One way of looking at this is choosing to believe that seeing a cardiologist will ease your mind one way or the other – so this is something to look forward to, not dread. If the cardiologist spots something suspicious, a treatment plan will be created. If not, you can wave goodbye and go back to happily living your life.
Try thinking of this cardiology appointment just as you would if your car were making some frightening noises and you were bringing it to your mechanic for a check up. You could work yourself into a complete state worrying ahead of time if the car trouble is going to be serious, or you could look at this appointment as the solution – at last! – to figuring out what’s wrong so the mechanic can recommend the next step.
Thank you for this list of so many definitions provided in plain English. what a valuable resource this is. THANK YOU, I have been looking for translations FOR PATIENTS not med school graduates– like this for three years.
My family doctor had me wear a 24 hr EKG. After reading the results, she has scheduled a scope to look inside my heart by a specialist. Completely forgoing a stress test. Said I have major changes in the EKG, what type of changes could they be looking at? Had LAD STENT INSERTED 7 YRS AGO – WHAT COULD THEY BE LOOKING FOR?
This is a great wealth of information, Carolyn! I looked and did not see my diagnosis, which is aortic stenosis. I looked under aortic as well as stenosis. Did I just miss it somehow?
I learned some new information, I am a bit familiar now, but not when I had my MI, it was like learning a new language. But, my favorite part was seeing SCAD on this list! Thank you.
Thanks and welcome! I was thinking of editing that SCAD definition actually: I suspect that that it isn’t so much that SCAD is “rare”, but it’s more that it’s “rarely correctly diagnosed”.
I totally agree that SCAD is not as rare as I believed for many years. Once awareness is spread to all medical staff, I believe many lives will be saved. Hoping for a brighter future for all SCAD patients.
I hope so too, Cathy. Perhaps when more SCAD studies (like Mayo Clinic’s) are published and read by more and more MDs, it will no longer be “rarely correctly diagnosed”.
It’s great to see IST on here. I was diagnosed with it 9 years ago and the lack of awareness is frustrating.
What a great resource for heart patients and their families!
Thanks so much, Ashley. I recently updated my original 2011 list after the world-famous Cleveland Clinic tweeted their glossary recently and I noticed that their list had a few glaring omissions (like SCAD and Brugada Syndrome) so this made me wonder what my list might be missing, too. Let me know if there’s anything else you think should be included, okay?
How is your health these days? How are you feeling?
New for me too. I have just been diagnosed with A-HCM: Apical Hypertrophic Cardiomyopathy.
I’ll add that one to my list, Kathleen – thanks!
Just saw this, Carolyn, and you’ve compiled a great resource. One note on A-HCM: Present thinking is that it’s due to a genetic modification. Runs in families though sometimes occurs spontaneously. I have not as yet done genetic testing, though it’s been offered.
Thanks Kathleen – like many cardiac diagnoses, it sounds like a moving target… Good luck to you!
This list is great. I’ve just been diagnosed and am utterly overwhelmed. Even in the WomenHeart online support community, I often have no clue most days what others are talking about with all these initials about their heart tests and specific disease. This is VERY helpful, thank you SO MUCH. Love your website which has been a godsend since my diagnosis.",['Implantable Cardioverter Defibrillator (ICD) is a surgically implanted electronic device to treat life-threatening heartbeat irregularities.'],8925,multifieldqa_en,en,,2f73cf12085e3fb879c775fc4851b72092a060cad5a927c6,Implantable Cardioverter Defibrillator (ICD) is a surgically implanted electronic device to treat life-threatening heartbeat irregularities.,140
How does the specific-heat ratio affect the average motion of the bubble?,"Paper Info

Title: Specific-heat ratio effects on the interaction between shock wave and heavy-cylindrical bubble: based on discrete Boltzmann method
Publish Date: May 29, 2023
Author List: Yanbiao Gan (from School of Liberal Arts and Sciences, Hebei Key Laboratory of Trans-Media Aerial Underwater Vehicle, North China Institute of Aerospace Engineering), Yudong Zhang (from School of Mechanics and Safety Engineering, Zhengzhou University)

Figure

Figure 1: Research orientation and tasks of DBM.
Figure 2: Sketch of D2V16 model.The numbers in the figure represent the index i in Eq. (3).
Figure 3: The computational configuration of the shock-bubble interaction.
In the figure, results from odd rows are experimental, and the even rows indicate DBM simulation results.The typical wave patterns and bubble's main characteristic structures are marked out in the figures.Numbers in the pictures represent the time in µs.Schlieren images of DBM results are calculated from the density gradient formula, i.e., |∇ρ|/|∇ρ| max , with |∇ρ| = (∂ ρ/∂ x) 2 + (∂ ρ/∂ y)2 .At t = 0µs, the incident shock wave impacts the upstream interface, and subsequently generates a transmitted shock (TS) propagating downstream in the bubble and a reflected shock wave moving upward in ambient gas.The incident shock wave travels downstream contin-
The definitions and the corresponding physical meanings of the common TNE quantities in DBM, where the operator ∑ ix,iy indicates integrating over all the fluid units and multiply the unit area dxdy.From a certain perspective, the TNE strength is increasing; While from a different perspective, the TNE strength, on the other hand, may be decreasing.It is one of the concrete manifestations of the complexity of non-equilibrium flow behavior.
Figure 4: Snapshots of schlieren images of the interaction between a shock wave and a heavy-cylindrical bubble.The odd rows represent experimental results from Ref. [31] with permission, and the even rows are DBM simulation results.The typical wave patterns and the bubble's main characteristic structure are marked out in the figures.Numbers in the picture represent the time in µs.
Figure 5: The temporal variations of length and width of the bubble.The symbols represent DBM results and the lines are experimental.The definition of the length and the width of the bubble can be seen in the illustration.Experimental results are obtained from Fig. 12, in Ref. [31] with permission.
Figure 6: Density contours and particle tracer images at three different moments (i.e., t = 0.07,t = 0.11, and t = 0.16) with various specific-heat ratios.The odd rows represent density contours, and the even rows are tracer particle images.
Figure 9: Vorticity contours at t = 0.134, with various specific-heat ratios.The arrows in the vorticity image point out the apparent difference between case γ = 1.4 and case γ = 1.09.
Figure 11: Density contours (first row) and mixing degree M (second row) at several typical moments.
Figure 13: (a) Temporal evolution of D * 3,1 and D * 4,2 .(b) Temporal evolution of D * 2 and D * 3 .Lines with different colors represent the cases with various specificheat ratios.
The development of schemes for checking TNE state, extracting TNE information and describing corresponding TNE effects in DBM.

abstract

Specific-heat ratio effects on the interaction between a planar shock wave and a two-dimensional heavy-cylindrical bubble are studied by the discrete Boltzmann method. Snapshots of schlieren images and evolutions of characteristic scales, being consistent with experiments, are obtained. The specific-heat ratio effects on some relevant dynamic behaviors such as the bubble shape, deformation process, average motion, vortex motion, mixing degree of the fluid system are carefully studied, as well as the related Thermodynamic Non-Equilibriums (TNE) behaviors including the TNE strength, entropy production rate of the system.
Specifically, it is found that the influence of specific-heat ratio on the entropy production contributed by non-organized energy flux (NOEF) is more significant than that caused by non-organized momentum flux (NOMF). Effects of specific-heat ratio on entropy production caused by NOMF and NOEF are contrary.
The effects of specific-heat ratio on various TNE quantities show interesting differences. These differences consistently show the complexity of TNE flows which is still far from clear understanding.

Introduction

The applications of shock-accelerated inhomogeneous flows (SAIFs) are of significant value in biomedicine, energy utilization, and astrophysics fields, including but not limited to scenarios such as the impact of shock waves on kidney stones, the interaction between shock waves with foams, the impacting of detonation wave with burning flames in supersonic combustion systems, the formation of supernova remnants, etc .
Shock-bubble interaction (SBI) is one of the most fundamental problems in the research of SAIFs. Its applications and academic research are interdisciplinary. Generally, there are two kinds of problems encountered in SBI research: (i) The geometry of shock waves, the shape of material interfaces, and the structure of container are complex in the actual scene.
They will result in various wave patterns and significantly affect the flow morphology and bubble's evolution. (ii) There usually exist multi-physics coupling problem in the engineering application of SBI. Such as the supersonic combustion machines. When the shock waves passing through the reactants, it may lead to phase transition and chemical reactions, making the flow morphology more complex and inducing small structure (or fast-changing pattern) .
In an underwater explosion experiment, the interaction between shock waves and bubbles may refer to the cavitation and annihilation effects. The other scene is the inertial confinement fusion (ICF), in which the laser ablation, electron heat conduction, self-generated electromagnetic field, radiation, and many other factors may complicate the investigation of hydrodynamic instabilities .
Commonly, research on SBI mainly includes three methods: theoretical derivation, experiment, and numerical simulation. As a fundamental research method, theoretical research can provide a clear understanding of physical processes. In 1960, Rudinger et al. developed a theory that permits computing the response of bubbles to accelerations .
In order to describe the formation and evolution processes of vortex structure quantitatively, many scholars have developed circulation models . However, theoretical works provide limited information. Meanwhile, in the late stage of SBI evolution, the bubble deformation and flow morphology dominated by the developed Richtmyer-Meshkov instability (RMI) and Kelvin-Helmholtz instability (KHI) are difficult to be predicted accu-rately.
As the research method closest to engineering application, the experimental results are often regarded as standard results to verify the rationality and accuracy of theoretical and numerical works. To study the SBI process accurately, the scholars have made a series of improvements to experimental equipment or technique, including the generation techniques of different types of shock waves, interface formation methods, schlieren facilities, and image recognition techniques .
Among these, two of important and valuable works are performed by Ding et al.. Based on the soap film technique, they formed kinds of initial interfaces with different curvatures through the wire-restriction method and captured the wave patterns and interface evolution with high-speed schlieren photography .
Other works, such as evolutions of a spherical gas interface under reshock conditions , developments of a membrane-less SF 6 gas cylinder under reshock conditions , and interactions of a cylindrical converging shock wave with an initially perturbed gaseous interface , are also performed by many other scholars.
However, we know that the experimental studies mainly depend on the experimental platform. When studying some complex and demanding condition problems, it takes much work to build the experimental platform. In this situation, numerical simulation research becomes an option. Generally, there are three kinds of physical modeling methods (or models) for SBI numerical research, i.e., the macroscopic, mesoscopic, and microscopic modeling methods.
Most of the existing numerical researches on SBI are related to the macroscopic modeling methods (such as the Euler and Navier-Stokes (NS) models) based on the continuous hypothesis (or equilibrium and nearequilibrium hypothesis) . For example, presented the computational results on the evolution of the shock-accelerated heavy bubbles through the multi-fluid Eulerian equation .
There also exist a few SBI works based on the mesoscopic modeling method, such as the Direct Simulation Monte Carlo method . The microscopic modeling methods such as the Molecular dynamics (MD) simulation, is capable of capturing much more flow behaviors but restricted to smaller spatiotemporal scales because of its huge computing costs.
In the numerical research on SBI, three points need to be concerned. (i) Investigation of kinetic modeling that describes the non-continuity/non-equilibrium flows. Most of the current researches are based on macroscopic models. However, there exist abundant small structure (and fast-changing patterns) behaviors and effects such as the shock wave, boundary layer, material defects, etc.
For cases with small structures, the mean free path of molecules cannot be ignored compared to the characteristic length, i.e., the non-continuity (discreteness) of the system is pronounced, which challenge the rationality and physical function of the macroscopic models based on the continuity hypothesis.
For cases with fast-changing patterns, the system dose not have enough time to relax to the thermodynamic equilibrium state, i.e., the system may significantly deviate from the thermodynamic equilibrium state. Therefore, the rational-ity and physical function of the macroscopic models based on the hypothesis of thermodynamic equilibrium (or near thermodynamic equilibrium) will be challenged.
(ii) Improvement of method that describes the evolution characteristics of bubbles and flows morphology. Most of the studies describe bubble characteristics and flows morphology from a macroscopic view. The mesoscopic characteristics such as the kinetic effects which help understand the kinetic process, are rarely to be studied.
(iii) Further studies of effects of specific-heat ratio on SBI process. The specific-heat ratio is an essential index for studying the compressibility of the gas. Research from Igra et al. has shown that the differences in the specific-heat ratio of bubbles would cause various wave patterns and pressure distribution inside the bubbles during the interaction process .
Besides, many works on hydrodynamic instability have also demonstrated the importance of investigating the specific-heat ratio effect . Among these, Chen et al. investigated the specific-heat ratio effects on temperature gradient and the TNE characteristics of compressible Rayleigh-Taylor (RT) system .
For the above three points, in this work we apply the recently proposed discrete Boltzmann method (DBM) . The Lattice Boltzmann Method (LBM) research has two complementary branches . One aims to work as a kind of new scheme for numerical solving various partial differential equation(s). The other aims to work as a kind of new method for constructing kinetic model to bridge the macro and micro descriptions.
The two branches have different goals and consequently have different rules. The current DBM is developed from the second branch of LBM and focusing more on the Thermodynamic Non-Equilibrium (TNE) behaviors that the macro modeling generally ignore. It breaks through the continuity and near-equilibrium assumptions of traditional fluid modeling, discards the lattice gas image of standard LBM, and adds various methods based on phase space for checking, exhibiting, describing and analyzing the non-equilibrium state and resulting effects.
More information extraction technologies and analysis methods for complex field are introduced with time. The numerical simulation includes three parts, as shown in Fig. . (1) Physical modelling, (2) Algorithm design, (3) Numerical experiments and analysis of complex physical fields. The research of equation algorithm corresponds to the part (2) of the above three parts.
The DBM aims at parts (1) and (3) of the three mentioned above. It belongs to a physical model construction method rather than a numerical solution for the equations. The tasks of DBM are to: (i) Ensure the rationality of the physical model (theoretical model) and balance the simplicity for the problem to be studied; (ii) Try to extract more valuable physical information from massive data and complex physical fields.
Based on the coarse-grained modeling method of nonequilibrium statistical physics, the DBM aims to solve the following dilemma: (i) The traditional hydrodynamic modelings are based on the continuous hypothesis (or near-equilibrium hypothesis). They only concern the evolution of three conserved kinetic moments of the distribution function, i.e. the density, momentum and energy, so their physical functions are insufficient.
(ii) The situation that the MD can be used is restricted to too small spatial-temporal scales. The physical requirement for the modeling is that except for the Hydrodynamic Non-Equilibriums (HNE), the most related TNE are also needed to be captured. Theoretically, the Boltzmann equation is suitable for all-regime flows, including the continuum regime, slip regime, transition regime, and free molecule flow regime.
Based on the Chapman-Enskog (CE) multiscale analysis , through retaining various orders of Kn number (or considering different orders of TNE effects), the Boltzmann equation can be reduced to the various orders of hydrodynamic equations. They can be used to describe the hydrodynamic behaviors, i.e., the conservations of mass, momentum and energy , in corresponding flow regimes.
Because what the traditional hydrodynamic equations describe are only the conservation laws of mass, momentum and energy. Consequently, it should be pointed out that, the information lost in the traditional hydrodynamic equations increases sharply with increasing the Kn number. With increasing the Kn number, to ensure the describing capability not to decrease significantly, the more appropriate hydrodynamic equations should be the Extended Hydrodynamic Equations (EHEs) which include not only the evolution equations of conserved kinetic moments but also the most relevant nonconserved kinetic moments of distribution function.
For convenience of description we refer the modeling method that derives EHEs from the fundamental kinetic equation to Kinetic Macroscopic Modeling (KMM) method. It is clear that, the complex process of CE expansion is necessary and the simulation is still based on the macroscopic equations in KMM. As a comparison, the DBM is a kind of Kinetic Direct Modeling (KDM) method.
In DBM modeling, the CE analysis is only used to quickly determine which kinetic moments should keep values unchanged, the final EHEs are not needed, and the simulation is not based on the complicated EHEs. As the TNE degree of the flow to be described rises gradually, the complexity of the derivation process and difficulty of numerical simulation in the KMM method increase sharply.
However, in the DBM method, to describe flows in a one-order more deeper depth of TNE, only two more related kinetic moments need to be added. Since without needing to derive and solve the EHEs, as the TNE degree deepens, the complexity of the DBM approach increases much slower than that of KMM method.
The core step in DBM modeling is to provide a feasible scheme for detecting, describing, presenting, and analyzing TNE effects and behaviors beyond traditional macroscopic modeling. Based on the non-equilibrium statistical physics, we can use the non-conservative moments of ( f − f eq ) to describe how and how much the system deviates from the thermodynamic equilibrium state and to check corresponding effects due to deviating from the thermodynamic equilibrium.
The non-conservative moments of ( f − f eq ) open a phase space, and this space and its subspaces provide an intuitive geometric correspondence for describing complex TNE system properties. The development of schemes for checking TNE state, extracting TNE information and describing corresponding TNE effects in DBM are seen in Table .
Actually, this set of TNE describing methods has been applied in many kinds of complex fluid systems such as hydrodynamic instability system , combustion and detonation systems , multiphase flow system , plasma system , etc. Besides the scheme for detecting, describing, presenting, and analyzing TNE effects and behaviors, the DBM incorporates other methods for analyzing the complex physical field.
One of them is the tracer particle method. The introduction of the tracer particle method makes the gradually blurred interface appear clearly . The rest of the paper is structured as follows. Section 2 shows Year Scheme for investigating TNE effects and behaviors Before 2012 Two classes of LBMs did not show a significant difference in physical function.
2012 Use the non-conservative moments of ( f − f eq ) to check and describe TNE . This is the starting point of current DBM approach. 2015 Open TNE phase space based on non-conservative moments of ( f − f eq ) and define a TNE strength using the distance from a state point to the origin. This is the starting point of the phase space description method .
2018 Extend the distance concepts in phase space to describe the TNE difference/similarity of TNE states and kinetic processes . 2021 Further extend the phase space description methodology to any set of system characteristics . the modeling method. Then, the numerical simulations and results are presented in Section 3, which includes two subsections.
Section 4 concludes the paper. Other complementary information is given in the Appendix.

Model construction

Based on the Bhatnagar-Gross-Krook (BGK) singlerelaxation model, a two-fluid DBM with a flexible specific-heat ratio is presented in this part. From the origin Boltzmann equation to a DBM, four fundamental steps are needed: (i) Simplification and modification of the Boltzmann equation according to the research requirement.
(ii) Discretization of the particle velocity space under the condition that the reserved kinetic moments keep their values unchanged. (iii) Checking the TNE state and extracting TNE information. (iv) The selection/design of the boundary conditions.

Simplification and modification of the Boltzmann equation

As we know, the collision term in the original Boltzmann contains high dimensional distribution functions. Therefore, the direct solution to it needs too much computing consumption. The most common method to simplify the collision operator is to introduce a local equilibrium distribution function ( f eq ) and write the complex collision operator in a linearized form, i.e., the original BGK collision operator − 1 τ ( f − f eq ), where τ is the relaxation time .
The original BGK operator describes the situation where the system is always in the quasi-equilibrium state. Namely, it characterizes only the situation where the Kn number of the system is small enough and f ≈ f eq . The currently used BGK operator for non-equilibrium flows in the field is a modified version incorporating the meanfield theory description .
Based on the above considerations, the simplified Boltzmann equation describing the SBI process is where the two-dimensional equilibrium distribution function is ) where ρ, T , v, u, I, R, and η are the mass density, temperature, particle velocity vector, flow velocity vector, the number of the extra degrees of freedom including molecular rotation and vibration inside the molecules, gas constant, and a free parameter that describes the energy of the extra degrees of freedom, respectively.
The specific-heat ratio is flexible by adjusting parameter I, i.e., γ = (D + I + 2)/(D + I), where D = 2 represents the two-dimensional space.

Discretization of the particle velocity space and determination of f σ ,eq i

The continuous Boltzmann equation should be discretized for simulating. Specifically, the continuous velocity space can be replaced by a limited number of particle velocities. So that the values of continuous kinetic moments can be obtained from the summation form of kinetic moments. In this process, it requires the reserved kinetic moments, which are used to characterize the system behaviors, keep their values unchanged after discretizing the velocity space.
Namely, f the reserved kinetic moments. According to the CE analysis, f can be expressed by f eq . Therefore, in the process of discretization, the reserved kinetic moments of f eq should keep their values unchanged, i.e., where i represents the kind of discrete velocities and α (α = x or y) is the direction in cartesian coordinate.
To simulate the interaction between two different fluids, a two-fluid DBM should be constructed. Based on the singlerelaxation model, the discrete two-fluid Boltzmann equation can be written as : where σ represents the types of material particle and f σ ,eq i = f σ ,eq i (ρ σ , u, T). In two-fluid DBM, the macroscopic quantities of the mixture and each component are
where ρ σ and u σ are the mass density and flow velocity of the component σ , respectively. ρ and u represent the mass density and flow velocity of the mixture, respectively. There exist two kinds of temperature (internal energy) definitions in two-fluid DBM because the definition of temperature (internal energy) depends on the flow velocity to be chosen as a reference.
The first definition is by choosing the velocity of the mixture to be a reference, i.e., So that the expressions of temperature of component σ and mixture are where We can also choose the flow velocity of component as a reference, i.e., , where u σ is the flow velocity of component σ . The corresponding definitions of temperature for component σ and the mixture are
where ∆E * I is It is clear to see that these two definitions of temperature for mixture are the same, but for temperature of component σ are different. We choose the first definition in this manuscript. To solve the Eq. ( ), it is necessary to determine the values of f σ ,eq i . Its values depend on the reserved kinetic moments which characterize the main system behaviors.
In DBM modeling, the CE multiscale analysis is used to determine quickly the reserved kinetic moments. Specifically, when constructing a DBM which only the first order term of Kn number is retained (i.e., only the first order TNE effects are retained), seven kinetic moments should be reserved, i.e., the M 0 , M 1 , M 2,0 , M 2 , M 3,1 , M 3 , M 4,2 .
Two more kinetic moments ( M 4 and M 5,3 ) are needed when the second order TNE is considered . However, it should be noted that the function of CE analysis in DBM modeling is only to determine the kinetic moments that need to be preserved. Whether or not to derive the hydrodynamic equations does not affect the DBM simulation.
The kinetic moments used in our physical modeling are shown in the Appendix B. Their expressions can be obtained by integrating v and η with continuous-form f eq . For better understanding, the Appendix C gives the two-fluid hydrodynamic equations recovered from the Boltzmann equation. The kinetic moments in Appendix B can be written in matrix form, i.e., C • f σ ,eq = fσ,eq , (
where C is the matrix of discrete velocity and feq represents the kinetic moments. A proper discrete velocity model is needed to confirm the values of f σ ,eq i . The f σ ,eq can be obtained by solving the inverse matrix, i.e., f σ ,eq = C −1 • fσ,eq , where C −1 is the inverse matrix of C. It is very convenient to obtain the inverse matrix of C through some mathematical softwares such as Mathematica, etc.
The D2V16 model is chosen in this paper, its sketches can be seen in Fig. . The specific values of D2V16 are given in the following equations: where ""cyc"" indicates cyclic permutation and c is an adjustable parameter of the discrete velocity model. The sketch of η in D2V16 is η i = η 0 for i = 1 − 4, and η i = 0 for i = 5 − 16.

Checking the TNE state and extracting TNE information

Many physical quantities can characterize the degree of TNE in a fluid system, such as relaxation time, Kn number, viscosity, heat conduction, the gradients of macroscopic quantity, etc. They are all helpful to characterize the TNE strength and describe the TNE behaviors of a fluid system from their perspectives.
But it is not enough only relying on these quantities. Besides the above physical quantities describing the TNE behaviors, in DBM modeling, we can also use the non-conservative moments of ( f − f eq ) to characterize the TNE state and extract TNE information from the fluid system. Fundamentally, four TNE quantities can be defined in a firstorder DBM, i.e., ∆ σ * 2 , ∆ σ * 3,1 , ∆ σ * 3 , and ∆ σ * 4,2 .
Their definitions can be seen in Table , where v * i = v i − u represents the central velocity and u is the macro flow velocity of the mixture. Physically, ∆ σ * 2 = ∆ σ * 2,αβ e α e β and ∆ σ * 3,1 = ∆ σ * 3,1 e α represent the viscous stress tensor (or non-organized momentum flux, NOMF) and heat flux tensor (or non-organized energy flux, NOEF), respectively.
The e α (e β ) is the unit vector in the α (β ) direction. The later two higher-order TNE quantities contain more condensed information. Specifically, and it indicates the flux information of ∆ σ * 2 . To describe the TNE strength of the whole fluid system, some TNE quantities contained more condensed information are also defined, i.e.,
Other TNE quantities can be defined based on specific requirements. All the independent components of TNE characteristic quantities open a highdimensional phase space, and this space and its subspaces provide an intuitive image for characterizing the TNE state and understanding TNE behaviors . It should be emphasized that: (i) The TNE strength/intensity/degree is the most basic parameter of non-equilibrium flow description; And any definition of non-equilibrium strength/intensity/degree depends on the research perspective.
(ii) The physical meaning of D * m,n is the TNE strength of this perspective. (iii) From a certain perspective, the TNE strength is increasing; While from a different perspective, the TNE strength, on the other hand, may be decreasing. It is normal, one of the concrete manifestations of the complexity of non-equilibrium flow behavior.
Strictly speaking, those TNE intensity and effect descriptions that do not account for the research perspective are not correct. Do not explain the research perspective, the corresponding is not dependent on the research perspective.

Numerical simulations and results

In this section, we first validate the DBM code by comparing the DBM results with experimental results. Then, the effects of specific-heat ratio on the dynamic process and TNE behaviors on SBI are investigated.

Comparison with experimental results

In the following part, we use a first-order two-fluid DBM to simulate the interaction between a planar shock wave with a 2-D heavy-cylindrical bubbles, and compare the DBM results with the experimental results from Ref. . The computational configuration can be seen in Fig. . In a flow field which is filled with Air, there is a static bubble composed of 26% Air and 74% SF 6 .
A shock with Ma = 1.2 would pass through the bubble from left to right. The initial conditions of ambient gas are ρ 0 = 1.29kg/m 3 , T 0 = 293K, p 0 = 101.3kPa. Ignoring the pressure difference between interior gas and ambient gas, the initial parameters of the bubble are ρ bubble = 4.859kg/m 3 , p bubble = 101.3kPa,
and T 0 = 293K. For simulating, these actual physical quantities should be transferred to dimensionless parameters. This process can refer to the Appendix A. The dimensionless conditions of macroscopic quantities of the fluid field in initial time are (ρ, T, u x , u y ) bubble = (4.0347, 1.0, 0.0, 0.0), (ρ, T, u x , u y ) 1 = (1.3416,
1.128, 0.3616, 0.0), (ρ, T, u x , u y ) 0 = (1.0, 1.0, 0.0, 0.0), where the subscript ""0"" (""1"") represents downstream (upstream) region. In two-fluid DBM code, the distribution function f Air is used to describe the ambient gas, i.e., Air. The f bubble characters the bubble which is a mixture that composed of Air and SF 6 .
The grid number is N x × N y = 800 × 400, where the N x and N y are grid number in x and y direction, respectively. This grid size has passed the mesh convergence test. The below results also show that it is sufficient to meet the requirements of the following research problem. Other parameters used for the simulation are: c = 1.0, η Air = η bubble = 10.0,
I Air = 3, I bubble = 15, ∆x = ∆y = 1.2 × 10 −4 and ∆t = 1 × 10 −6 . The viscosity effect is feeble compared to the shock compression effect, so it does not significantly affect the deformation of the bubble. Therefore, in this part, the relaxation time τ is set sufficiently small. The inflow (outflow) boundary condition is used in the left (right) boundary, and the periodic boundary is adopted in the y direction.
The first-order forward difference scheme is used to calculate the temporal derivative, and the second-order nonoscillatory nonfree dissipative scheme is adopted to solve the spatial derivative in Eq. ( ) . Two quantitative comparisons between experimental results and DBM simulations are shown in the following part, including snapshots of schlieren images and evolutions of characteristic scales for the bubble.
The first is shown in Fig. non-organized momentum flux (NOMF) uously to form a diffracted shock (DS). As TS propagates, it will split into three branches due to the considerable pressure perturbations caused by the gradual decay of the DS strength . Afterward, as shown in the subfigure at about t = 128µs, two high pressure regions (ROH) generate because of the interaction of these branches.
Subsequently, at about t = 148µs, the two ROHs meet, causing the shock focusing. On the one hand, at about t = 168µs, the shock focusing causes the generation of downstream-propagating second transmitted shock (STS) and upward-moving rarefaction wave. On the other hand, it will produce high pressure region inside the bubble, which later leads to a jet structure, as shown at about t = 288µs.
At about t = 428µs, due to the deposited vorticity, there will produce a pair of counter-rotating vortexes at the pole region of the bubble. The further development of the vortex pair and the effect of viscosity decrease the amplitude of the jet. Finally, the jet structure disappears. The second quantitative comparison is the interface structure described by the length and width of the bubble, as shown in Fig. .
The experimental data are extracted from Fig. , in Ref. . Quantitative agreements between DBM simulation and experimental results are seen. For the profile of bubble width, there are mainly two stages. At an early time (t < 150µs), it decreases to a minimum value because of the shock compression effect.
After the shock wave passes through the bubble (t > 150µs), the developed vortex pair caused by the deposited vorticity gradually dominates the growth of bubble width. Different from width evolution, the temporal variation of length experiences three stages. In the early stages (t < 150µs), it decreases quickly due to the shock compression effect.
Then, the jet structure emerges, which results in a growth in length (150µs < t < 250µs). Because the upstream interface moves faster than the downstream interface, the bubble length would decrease at 250µs < t < 500µs. In the third stage (t > 500µs), the vortex pair forms and then leads to a continuous development of bubble length.
Both the length and width experience oscillations in the later stages due to complex wave patterns. The quantitative agreements between DBM simulation and experimental results indicate the following two facts: (i) the order of TNE considered in the current DBM is sufficient, (ii) the choosing of discrete velocities and spatial-temporal steps and simulation parameters like the relaxation times is suitable for characterizing the deformation of bubble, wave patterns, main characteristics of flow morphology.

Effects of specific-heat ratio on SBI

The major of current works on SBI research have not focused on specific-heat ratio effects. In this part, the simulation parameters are fine-adjusted based on the parameters in Section 3.1 to highlight the influence of specific-heat ratio. Through adjusting the extra degree of freedom I, five cases with various specific-heat ratios of the bubble are simulated, i.e., γ = 1.4,
1.28, 1.18, 1.12, and 1.09. Two kinds of analysis methods, including tracer particle method and two-fluid model, are used to characterize qualitatively the macroscopic behaviors such as the shape, deformation process, mixing degree, etc. The related TNE behaviors are also studied.

Effects of specific-heat ratio on jet shape, deformation process, and average motion

We first observe the specific-heat ratio effect on the bubble shape from the view of density contour and images of particle tracer visually. As shown in Fig. , pictures with three typical moments are plotted, i.e., t = 0.07,t = 0.11, and t = 0.16. The odd rows represent density contours and the even rows are tracer particle images.
It can be seen that the specific-heat ratio significantly affects the length and shape of the jet structure. The smaller the specific-heat ratio is, the stouter the jet structure can be seen. The reason is that the specific-heat ratio significantly changes the propagation speed of shock waves and wave patterns inside the bubble.
The specific-heat ratio also influences the vortex structure in early stage but contributes little effects to it in later stage. In the later stage, for cases with different specific-heat ratios, the differences in vortex pairs are almost invisible. Then, the effects of specific-heat ratio on deformation process are analyzed.
Shown in Fig. are the evolutions of characteristic scales which used to describe the bubble size, i.e., width and length. It can be seen that the smaller the specific-heat ratio of bubble, the smaller the bubble width and length. For the fluid with smaller specific-heat ratio, it is easier to be compressed.
Therefore, the characteristic scales of bubbles with smaller specific-heat ratio tend to be compressed smaller. It can also be seen that the case with the largest specific-heat ratio reaches the minimum characteristic scales firstly. The reason is that the shock wave propagates faster in case with larger specific-heat ratio.
Through the method of tracer, information on the average motion of the bubble is easy to obtain. Shown in Fig. are the average position and average velocity of the bubble, with different specific-heat ratios. It is found that, in the shock compression stage (t < 0.03), the effect of specific-heat ratio contributes little to the average motion.
However, when the shock wave passes through the bubble (t > 0.03), a larger specific-heat ratio speeds up the average motion of the bubbles. The reason is that the bubbles with smaller specific-heat ratio need more energy to compress their size, so their translational energy is smaller.

Effects of specific-heat ratio on vortex motion

Vorticity is one of the most important physical quantities in describing the vortex motion. In the 2-D case, the vorticity can be calculated by the following equation: The positive (negative) value of ω represents the positive (negative) direction along the z axis. Vorticity contours at t = 0.134, with various specific-heat ratios, are shown in Fig. .
The discernable difference between cases with various specific-heat ratios can be observed. The arrows in the vorticity images point out the obvious differences around the interface between case γ = 1.4 and case γ = 1.09. That is to say, there exists influences of specific-heat ratio on the rotational motion of the bubble.
The strength of vorticity is described by circulation Γ, where Γ = ∑ ω∆x∆y. Γ + = ∑ ω| ω>0 ∆x∆y is the positive circulation and Γ − = ∑ ω| ω<0 ∆x∆y represents the negative circulation. Figure shows the temporal evolution of circulations on SBI process. It can be seen that the values of Γ are equal to zero all the time because the values of Γ + and Γ − are the same.
But they point in the opposite direction. In the shock compression stage (t < 0.03), the specific-heat ratio effect contributes little to the circulation of the bubble. When the shock wave sweeps through the bubble (t > 0.03), the specific-heat ratio affects the value of circulation obviously. The cases with a smaller specific-heat ratio experiences a larger range of amplitude of change, which is caused by its good compressibility.

Effects of specific-heat ratio on mixing degree

The mixing process is a fundamental research content on SBI. In two-fluid DBM, the mixing degree at each fluid unit can be defined as M = 4 • M A M B , where M σ represents the mass fraction of component σ . The higher the value of M, the higher the mixing amplitude. Images of density (first cow) and mixing degree M (second row) at several typical moments are shown in Fig. .
As can be seen, the mass mixing occurs in the region where two media contact. In addition, the mixing degree M g described the whole fluid field can be defined where the symbol ""−"" indicates integrating the M σ over the whole fluid field and then dividing the grid size N x • N y . Shown in Fig. is the temporal evolution of the global mixing degree M g .
As can be seen, temporal profiles of the global mixing degree show two stages: t < 0.03 and t > 0.03. When t < 0.03, there is almost no difference between cases with various specific-heat ratios. But for t > 0.03, the stronger the specific-heat ratio effect, the larger the mixing degree. Actually, there are mainly two indicators that measure the global mixing degree: the amplitude of mixing and the area of the mixing zone between two fluids.
At the stage t > 0.03, the shock compression dominates the mix by enhancing the mixing amplitude and increasing the area of the mixing zone simultaneously. In this stage, the specific-heat ratio effect contributes little to the mix. However, when the shock passes through the bubble, the deformation of interface and the evolution of vortex core both significantly raise the area of the mixing zone.
As can be seen in Fig. , the smaller specific-heat ratio of bubble, the stronger global mixing degree of fluid field. Intuitively, for the fluid with smaller specific-heat ratio, it is easier to deform and compress, which is beneficial for the fluid mixing. It can also be explained by the diffusion formula, i.e., .
The specific-heat ratio affects both the temperature T and the gradient of density simultaneously. Therefore, these two aspects comprehensively influence the material diffusion between the two fluids. Due to the complex reflected shock wave, the global mixing degree shows a tendency for oscillating growth.

Effects of specific-heat ratio on TNE behaviors

The investigation of TNE behaviors is of great importance for understanding the kinetics process on SBI. These TNE quantities describe the fluid system deviating from the thermodynamic state from their own perspectives. The effects of specific-heat ratio on global TNE strength, i.e., D * 2 , D * 3 , D * 3,1 , and D * 4,2 , are shown in Fig. .
It can be seen that the effects of specific-heat ratios on various TNE quantities are different. Theoretically, the influence of specific-heat ratio on the non-equilibrium effect is reflected in two aspects: transport coefficient and macroscopic quantity gradient. For example, on the one hand, the specific-heat ratio reduces heat conductivity, while on the other hand, it enhances the temperature gradient.
Therefore, the effect of specific heat ratio on NOEF is the comprehensive result of the competition between the two aspects. As shown in Fig. , the smaller the specific-heat ratio, the stronger strength of D * 3,1 . It indicates that the specific-heat ratio increase the strength of D * 3,1 by raising the heat conductivity .
For the strength of D * 3 , as shown in Fig. (b), it is seen that it decreases as the specific-heat ratio becomes small. The reason is that a smaller specific-heat ratio decreases the temperature gradient. Effects of specific-heat ratio on D * 4,2 show two-stage. In the shock compression stage (t < 0.03), the smaller specificheat ratio, the larger the strength of D * 4,2 .
But the situation is reversed at the stage t > 0.03. For strength of D * 2 , the specificheat effects are more significant in later stage.

Effects of specific-heat ratio on entropy production rate

and entropy production The concepts of entropy are commonly used in complex flows . In DBM, there are two kinds of en- tropy production rates, i.e., ṠNOEF and ṠNOMF . They are key factors in compression science field. The former is induced by temperature gradient and the NOEF (∆ * 3,1 ). The latter is affected by velocity gradient and the NOMF (∆ * 2 ).
The entropy production rates are defined by the following formulas : Integrating the ṠNOEF and ṠNOMF over time t, the entropy generations over this period of time are obtained, i.e., S NOEF = t 0 ṠNOEF dt and S NOMF = t 0 ṠNOMF dt. Plotted in Fig. ) and 14(b) are the temporal evolution of ṠNOMF and ṠNOEF , respectively.
The evolution of entropy generation rate is related to two aspects: (i) the propagation of the shock wave, and (ii) the deformation of the bubble. The former generates a macroscopic quantity gradient, and the latter makes the contact interface wider, longer, and deformed. Depending on the location of the shock wavefront, there exist two critical moments in this SBI process: (i) at around t = 0.03, the shock wave just sweeps through the bubble, and (ii) at t = 0.06, the shock wave exits the flow field.
Therefore, the temporal evolution of the entropy production rate shows three stages, i.e., t < 0.03, 0.03 < t < 0.06, and t > 0.06. At the stage t < 0.03, the shock compression stage, the shock effects compress the bubble. It generates the large macroscopic quantity gradients, resulting in a quick increase of ṠNOMF .
At around t = 0.03, the shock wave passed through the bubble. So the values of ṠNOMF decreases. The values of ṠNOMF would continue to decrease due to the gradually wider contact interface caused by the diffusion effect. At around t = 0.06, the shock wave comes out of the flow field so that the values of ṠNOMF drops rapidly.
In the third stage, i.e., t > 0.06, because of the diffusive effect, the general trend of ṠNOMF is downward. However, it shows an oscillatory trend due to the influence of various reflected shock waves. The specific heat ratio indirectly changes the value of ṠNOMF by changing the velocity gradient. The smaller the specific-heat ratio, the larger ṠNOMF .
Different understanding can be seen in Fig. , where the temporal evolution of ṠNOEF is plotted. In the first stage (t < 0.03), cases with different specific-heat ratios show various trends. At the stage where the bubble deformation is not very large, i.e., 0.03 < t < 0.06, values of ṠNOEF fluctuate near the average value.
In the third stage (t > 0.06), evolutions of ṠNOEF in cases with larger specific-heat ratios show an apparent growing tendency. Differently, the values of ṠNOEF in cases with smaller specific-heat ratios remain almost unchanged. The influence of specific heat ratio on the ṠNOEF , similar with the effect on NOEF, is also affected by the heat conductivity and the temperature gradient.
It can be seen that, except for the case of γ = 1.09, the larger the specific-heat ratio, the higher entropy production rate ṠNOEF . The temporal evolutions of ṠNOEF of case γ = 1.09 and case γ = 1.12 are very similar. Consequently, the specific-heat ratio increases the ṠNOEF by raising the temperature gradient.
Further understanding can be seen in Fig. , where the entropy productions over this period are plotted. For convenience, the sum and difference between S NOMF and S NOEF are also plotted in the figure. The variation range of S NOEF is larger than that of S NOMF . It indicates that the influence of specific-heat ratio on S NOEF is more significant than that on S NOMF .
Effects of specific-heat ratio on entropy production caused by NOMF and NOEF are contrary. Specifically, it can be seen that the entropy production contributed by NOMF increases with re- duced specific-heat ratio. But the entropy production caused by NOEF first reduces with decreasing specific-heat ratio and then approaches to a saturation value.
The S NOEF in case γ = 1.09 is almost the same with it in case γ = 1.12. When the specificheat ratio γ is smaller than a threshold value γ c (γ c ≈ 1.315), the entropy production induced by NOEF is more significant than that caused by NOMF. However, in the case of γ > γ c , the situation reverses. The temporal evolution of the total entropy production (S NOMF +S NOEF ) is similar to the S NOEF profile.
The difference between S NOMF and S NOEF increases with decreasing specific-heat ratio.

Conclusions

Specific-heat ratio effects on the interaction between a planar shock wave and a 2-D heavy-cylindrical bubble are studied by a two-fluid DBM which has a flexible specific-heat ratio and includes several schemes for analyzing the complex physical fields. Besides the HNE that NS easily describes, the DBM pays more attention to the related TNE that NS is not convenient to describe.
First, both the snapshots of schlieren images and evolutions of characteristic scales from DBM simulation are compared with those from experiment. The quantitative agreements between them indicate the following two facts: (i) the order of TNE considered in the current DBM is sufficient, (ii) the choosing of discrete velocities, spatial-temporal steps, and simulation parameters like the relaxation times are suitable for the following physical researches.
Then, five cases with various specific-heat ratios are simulated. Several analysis methods for complex physical fields, including the description scheme of TNE behaviors, tracer particle method, and two-fluid model, are used to characterize the effects of specific-heat ratio on the bubble shape, deformation process, average motion, vortex motion, mixing degree of the fluid system, TNE strength, and entropy production.
Specifically, for bubble shape, bubbles with different specific-heat ratios display various jet structures. The smaller the specific-heat ratio is, the stouter the jet structure can be seen. For the case with smaller specific-heat ratio, the fluid is easier to be compressed. So, the characteristic scales of bubbles with smaller specific-heat ratio tend to be compressed smaller.
For the bubble, the smaller the specific-heat ratio, the slower average motion. In the shock compression stage, the specific-heat ratio contributes little effects to the vortex motion. Differently, after the shock passes through the bubble, it significantly influences the vorticity around the interface and the corresponding amplitude of circulation due to the development of KHI.
The larger the difference in specific-heat ratio between the bubble and ambient gas, the higher the degree of material mixing. Effects of specific-heat ratio on various TNE quantities are different. These differences consistently show the complexity of TNE flows which is still far from a clear understanding.
In addition, it is found that the temporal evolution of the entropy production rates ṠNOMF and ṠNOEF both show three stages because of the influence of the shock wave location. The smaller the specific-heat ratio, the larger the velocity gradient, which indirectly enhances the strength of ṠNOMF . The specific-heat ratio increases the ṠNOEF by raising the temperature gradient.
The influence of specific-heat ratio on S NOEF is more significant than that on S NOMF . Effects of specific-heat ratio on entropy production caused by NOMF and NOEF are contrary. Specifically, the entropy production contributed by NOMF increases with reduced specific-heat ratio. But the entropy production caused by NOEF first reduces with decreasing specific-heat ratio and then approaches to a saturation value.
When the specific-heat ratio γ is smaller than a threshold value γ c (γ c ≈ 1.315), the entropy production induced by NOEF is more significant than that caused by NOMF. However, in the case of γ > γ c , the situation reverses. The fundamental research in this paper helps to understand the interaction mechanism between shock waves and bubbles in ICF, supersonic combustors, underwater explosions, etc.
The effects of viscosity and heat conduction on the interaction between shock waves and bubbles will be studied in the following work. where the subscript ""m, n"" means that the m-order tensor is contracted to n-order tensor. According to the CE multiscale analysis, the Boltzmann-BGK equation can be reduced to the hydrodynamic equations.
In the following part, the derivation process from Boltzmann-BGK equation to a two-fluid hydrodynamic equation are shown. More details can see the reference presented by Zhang et al. . The discrete Boltzmann equation for component σ is (C.1) In Eq. C.1 there are two equilibrium distribution functions, i.e., f σ ,seq = f σ ,seq (ρ σ , u σ , T σ ) and f σ ,eq = f σ ,eq (ρ σ , u, T ).
For convenience, S σ i is defined as We perform the CE expansion around the f σ ,seq . That is, the distribution function f σ i can be expanded as where ε is a coefficient referring to Knudsen number. The partial derivatives of time and space can also be expanded to Substituting the above four equations into Eq.
(C.1), we can obtain (C.6) When retaining to ε terms, the following equation is obtained When retaining to ε 2 terms, we can obtain where M 2,αβ ( f σ ,(1) ) = ∑ i v iα v iβ f σ ,(1) i and M 3,1,α ( f σ ,(1) ) = . Substituting Eq. (C.14) into the above three equations, and replacing the time derivatives with the space derivatives, we obtain It should be noted that the ability to recover the corresponding level of macroscopic fluid mechanics equations is only part of the physical function of DBM.
The corresponding to the physical functions of DBM is the EHEs, which, in addition to the conserved moments evolution equations corresponding to the three conservation laws of mass, momentum and energy, also includes some of the most closely related nonconserved moments evolution equations. We refer the EHEs derivation based on kinetic equation to as KMM.
The necessity of the expanded part, the evolution equations of the relevant non-conserved moments, increases rapidly as increasing the degree of non-continuity/non-equilibrium. As the degree of non-continuity/non-equilibrium increases, the complexity will rapidly make KMM simulation studies, deriving and solving EHE, impossible.",['The specific-heat ratio affects the average motion of the bubble. The bubbles with smaller specific-heat ratios have slower average motion.'],8366,multifieldqa_en,en,,110688fd7522324ef26cbf3ec4f4b7ab29aa0b687aa1ade5,The specific-heat ratio affects the average motion of the bubble. The bubbles with smaller specific-heat ratios have slower average motion.,139
How are the relationships between catch per set and fishing behavior variables different for different measures of catch per unit effort (CPUE)?,"Overfishing is a major threat to the survival of shark species, primarily driven by international trade in high-value fins, as well as meat, liver oil, skin and cartilage. The Convention on the International Trade in Endangered Species of Wild Fauna and Flora (CITES) aims to ensure that commercial trade does not threaten wild species, and several shark species have recently been listed on CITES as part of international efforts to ensure that trade does not threaten their survival. However, as international trade regulations alone will be insufficient to reduce overexploitation of sharks, they must be accompanied by practical fisheries management measures to reduce fishing mortality. To examine which management measures might be practical in the context of a targeted shark fishery, we collected data from 52 vessels across 595 fishing trips from January 2014 to December 2015 at Tanjung Luar fishing port in East Lombok, Indonesia. We recorded 11,920 landed individuals across 42 species, a high proportion of which were threatened and regulated species. Catch per unit effort depended primarily on the number of hooks and type of fishing gear used, and to a lesser degree on month, boat engine power, number of sets and fishing ground. The most significant factors influencing the likelihood of catching threatened and regulated species were month, fishing ground, engine power and hook number. We observed significant negative relationships between standardised catch per unit effort and several indicators of fishing effort, suggesting diminishing returns above relatively low levels of fishing effort. Our results suggest that management measures focusing on fishing effort controls, gear restrictions and modifications and spatiotemporal closures could have significant benefits for the conservation of shark species, and may help to improve the overall sustainability of the Tanjung Luar shark fishery. These management measures may also be applicable to shark fisheries in other parts of Indonesia and beyond, as sharks increasingly become the focus of global conservation efforts.
Copyright: © 2018 Yulianto et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.
Funding: Data collection of this study was funded by the Darwin Initiative (grant number 2805). The funder had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.
Overfishing is the greatest global threat to marine fish stocks [1–5]. Several shark species (Selachimorpha) are particularly vulnerable to overexploitation due to their conservative life history strategies, large body sizes and the high economic value of their preserved body parts [6–8]. With increasing fishing pressure in recent decades, primarily driven by international demand for a range of consumer goods (including fins, liver oil, skin, cartilage and meat), it is estimated that annual fishing mortality now exceeds the intrinsic rebound potential of most commercially exploited species [5, 9, 10]. This fishing pressure is taking its toll, with an estimated one in four Chondrichthyan species now threatened with extinction, making sharks amongst the most threatened species groups in the world .
It is also increasingly acknowledged that sharks play a critical role in maintaining functional and productive ocean ecosystems , as well as providing an important source of food and income for many coastal communities . Recognising both the plight and importance of shark populations, there is growing professional and public interest to improve shark conservation, and the management of shark fisheries and trade . This is reflected in several recent policy decisions to afford new international regulations for 12 species of sharks across seven genera under the Convention on the International Trade of Endangered Species of Wild Fauna and Flora (CITES). This is a promising step for shark conservation; however, in order to create tangible outcomes for species conservation CITES must be implemented through domestic measures that are adapted to national and local contexts.
Indonesia is the world’s largest shark fishing nation [9, 14], and a global priority for shark conservation . Until recently Indonesia’s shark fishery has largely functioned as de facto open-access [12, 16]. However, in the past five years the Indonesian government has demonstrated a clear commitment to shark conservation and resource management, with domestic measures put in place to implement international obligations under CITES . Exploitation of all CITES-listed species is now regulated, either through full species protection or export controls (these species are hereafter referred to as ‘regulated’ species). However, CITES only affords protection to a small number of Indonesia’s 112 known shark species , of which 83 are threatened with extinction according to the IUCN Red List of Threatened Species (i.e. Vulnerable (VU), Endangered (EN) or Critically Endangered (CR) , these species are hereafter referred to as ‘threatened’ species), many of which continue to be landed throughout the country . Further, these policy measures predominantly regulate trade at the point of export, but do not necessarily influence fisher behaviour or local demand at the point of catch, such that the ‘trickle-down’ impacts on species mortality are unknown. In addition, effectively implementing species-specific shark mortality controls remains challenging due to the non-selectivity of fishing gears, and practical and cultural barriers to changing fisher preferences for certain gear-types and fishing methods. As such, existing regulations alone (e.g. Indonesian Law on Fisheries 31/2004 and its derivative regulations) will likely be insufficient to curb mortality of threatened and regulated species, as fishers must be both willing and able to change their fishing behaviour . Moreover, most of Indonesia’s shark fisheries are small-scale, and in relatively poor coastal communities where there are often no legal, sustainable marine-based alternatives to shark fishing that offer similar financial returns [22, 23]. It is therefore imperative to consider the ethical and socioeconomic impacts of shark trade controls. Most shark species listed under CITES are listed on Appendix II, which is designed for sustainable use. International trade is permitted for CITES Appendix II species provided it is non-detrimental to wild populations of the species, as proven through a scientific non-detriment finding (NDF) report and implemented through a system of export permits. However, in Indonesia there is currently a lack of species-specific trade data for conducting NDFs and setting sustainable export quotas, such that the Indonesian government has to introduce trade bans for these species in order to meet CITES obligations. With new CITES-listings for thresher sharks (Alopias spp.) and silky shark (Carcharhinus falciformis) recently coming into force, this is likely to have huge implications for Indonesia’s economically important shark industry, and the coastal communities depending on it. In order to balance conservation and socioeconomic objectives, robust management systems must be put in place that ensure and allow sustainable fishing and trade. This necessitates the identification of practical management measures that can reduce mortality of threatened and regulated species at the point of catch, and provide realistic options for fishers to effectively and measurably improve the sustainability of their fishing practices.
This study analyses two years of qualitative and quantitative data from one of Indonesia’s targeted shark fisheries in Tanjung Luar, West Nusa Tenggara Province. We outline the key characteristics of the fishery, including fishing behaviour and overall catch volumes and composition. We analyse the impacts of different fishing techniques, and present factors influencing overall catch per unit effort (CPUE) of individual shark fishing trips, as well as factors influencing the likelihood of catching threatened and regulated species. Finally, we discuss the implications of our findings, and provide practical recommendations for fisheries management measures, which can support CITES implementation for sharks and reduce the catch of threatened and regulated species, in Indonesia and beyond.
This work was conducted under a Memorandum of Understanding (MoU) and Technical Cooperation Agreement (TCA) between the Wildlife Conservation Society (WCS) and the Ministry of Environment and Forestry (MoEF), Ministry Marine Affairs and Fisheries (MMAF) and the Marine and Fisheries Agency (MFA) of West Nusa Tenggara Province. These documents were approved and signed by Sonny Partono (Director General of Conservation of Natural Resources and Ecosystem MoEF), Sjarief Widjaja (Secretary General MMAF), and Djoko Suprianto (Acting Head of MFA of West Nusa Tenggara Province). Due to this MoU and TCA no specific research permit was required. We collected data by measuring sharks that were already caught, dead, and landed by fishers in Tanjung Luar, with no incentives, compensation or specific requests for killing sharks for this study. WCS participates in the Conservation Initiative on Human Rights and the rules and guidelines of our Internal Review Board ensures that any research protects the rights of human subjects. We did not apply for an IRB permit for this study because our study design focused on collecting fish and fisheries data as opposed to personal socio-economic data. The FDGs and interviews were conducted to obtain early scoping information about fishing practices, and to establish protocols for more detailed fisheries data collection (as used in this study), and socio-economic data collection (as used in a later study (Lestari et al ), which underwent further ethical review due to the specific focus on human subjects).
Tanjung Luar, located in East Lombok, West Nusa Tenggara Province, Indonesia (Fig 1), is a landing site for one of Indonesia’s most well-known targeted shark fisheries. Tanjung Luar serves at least 1,000 vessels, and the majority of these are less than 10 gross tonnes (GT) in size . A group of specialised fishers operating from Tanjung Luar village and a neighbouring island, Gili Maringkik, specifically target sharks. Shark catch is landed in a dedicated auction facility at the Tanjung Luar port. The shark industry is well established in Tanjung Luar, with product processing facilities and trade connections to local, national and international markets. Research by Lestari et al.  indicates that the shark industry is significantly more profitable than non-shark fisheries in Tanjung Luar, particularly for boat owners. Strong patron-client relationships exist between boat owners and fishers, with shark fishers exhibiting high dependency on shark fishing, limited occupational diversity and low adaptive capacity for shifting into other fisheries .
Fig 1. Sharks landing monitoring site and fishing grounds of shark fishers that land at Tanjung Luar.
In January 2014 we conducted preliminary scoping research to better understand the operational and socioeconomic characteristics of Tanjung Luar’s shark fishery. During a three-week scoping visit a team of four trained Indonesian enumerators conducted semi-structured interviews and focus group discussions (FDGs) with fishers, boat owners and traders, alongside naturalistic observation in the field. Respondents were selected through purposive sampling, since the research was exploratory in nature and a priori sampling decisions were not possible . We conducted a total of 34 semi-structured interviews (S1 File) and four FDGs, which were attended by a total of 30 individuals. All interviews and discussions took place in Indonesian, with the help of a local enumerator who was fluent in the Tanjung Luar local dialect. Interviews took approximately 30 minutes, with no remuneration for participating. All respondents gave their full prior and informed consent before contributing to the research. During the interviews and FDGs we gathered information on number of boats, fishing gears used, fishing grounds, fishery operational characteristics, and shark supply chain, including estimated volumes and value of shark catch relative to other fisheries. We improved the accuracy of information on shark fishery characteristics and fishing behaviour through informal daily interactions and discussions with 131 shark fishers during our daily landings data collection and community engagement activities. More detailed socioeconomic data were collected in a full household survey in 2016, as outlined in Lestari et al. .
Shark landings data were collected by three experienced enumerators, who were trained in species identification and data collection methods during a two-day workshop and three weeks of field mentoring to ensure the accuracy of the data collected. Landings were recorded every morning at the Tanjung Luar shark auction facility where shark fishers usually landed dead sharks, from 5am to 10am from January 2014 to December 2015. The enumerators recorded data on catch composition and fishing behaviour (Table 1) from 52 different vessels across a total of 595 fishing trips. The enumerators also measured the weight of selected sharks to calculate biomass and length-weight relationship.
Table 1. Types of data collected on fishing behaviour and catch composition during daily landings data collection at Tanjung Luar.
From fishing behaviour and catch data we calculated the overall species composition of catch. We calculated catch per unit effort (CPUE) by number of individuals using both catch per set (hereafter CPUE per set) and catch per 100 hooks per set (hereafter standardised CPUE) [25,26]. This was deemed necessary since different vessels and gear-types systematically deploy different numbers of hooks, and standardised CPUE allows for a more meaningful comparison.
To understand factors influencing overall CPUE we log transformed CPUE per trip to fit a normal distribution, and fitted linear models (LMs) of CPUE per trip to fishing behaviour variables (Table 1). We considered all variables and used minimum AIC values with stepwise analysis of variance to identify the best fit and most significant influencing variables.
To inform the development of practical fisheries management measures (e.g. gear restrictions), we also specifically analysed differences in CPUE for surface and bottom longline gears employed in the fishery, using two-way ANOVAs.
Factors affecting catch of threatened and regulated species.
To identify variables influencing the catch of threatened and regulated species we conducted a two-step process. In the first step, we identified factors influencing the likelihood of catching any threatened/regulated species during a given fishing trip, by creating binary response variables for whether a threatened species had been caught during a trip (yes = 1, no = 0), and separately for whether a regulated species had been caught during a trip (yes = 1, no = 0). We then fitted generalised linear models (GLMs) with binomial errors to the binary response variables, separately for catch of threatened species and catch of regulated species. In the second step we identified variables that significantly influenced the CPUE of threatened species and the CPUE of regulated species, given that any were caught. We removed all records in which no threatened or regulated species were caught, log transformed standardised CPUE of threatened and regulated species, and fitted linear models (LMs) of standardised CPUE of threatened species and standardised CPUE to regulated species to fishing behaviour variables. Again, we considered all meaningful models and used minimum AIC values with stepwise analysis of variance to identify the best fit  and most significant influencing variables. This approach was necessary since catch of threatened and regulated species is zero-inflated, and creating binary response variables with a binomial error structure allowed for a simpler and more powerful statistical analysis. Note that we conducted two separate analyses, one for threatened species only and one for regulated species only, but used the same methods and process, as outlined above, for each analysis. We did not group threatened and protected species together, since although some species are both threatened and protected, this is not the case for all shark species landed in Tanjung Luar.
A total of 52 shark fishing vessels operate from Tanjung Luar, all of which are classified as small-scale according to the Indonesian Ministry of Marine Affairs and Fisheries (MMAF) vessel categorisation system, with <7GT capacity. These vessels are operated by approximately 150 highly-specialised shark fishers, from Tanjung Luar village and Gili Maringkik, who make up roughly 5% of the local fisher population. The shark industry is more profitable than non-shark fisheries, and shark fishers report high household dependency on shark resources, low occupational diversity, and limited capacity and aspirations to move into other fisheries or industries.
Surface and bottom longlines are used as the primary fishing gears to target sharks, with pelagic fish (e.g. Euthynnus spp., Rastrellinger spp.) used as bait. Surface and bottom longlines systematically vary in length, depth deployed, number of sets, number of hooks used, and soak times (Table 2). Gear types are typically associated with certain vessel types, and fishers–captain and crew—tend to exhibit preferences for specific gear types. Shark fishers also use gillnets and troll lines as secondary gears, to catch bait and opportunistically target other species, such as grouper, snapper, skipjack and mackerel tuna.
Table 2. Characteristics of surface and bottom longlines.
The shark fishing vessels can be divided into two broad categories according to fishing behaviour: larger vessels (≥14 m) with higher horsepower (HP) engines spend more time at sea than smaller vessels (≤12m) (p<0.001), and reach fishing grounds outside of West Nusa Tenggara. These vessels primarily fish in southern Sumbawa and Sumba Islands, however, they also reach as far as eastern Flores, Timor Island, and the Java Sea (Fig 1). Larger, higher HP vessels also tend to employ surface longlines (p<0.001), and since they spend more time at sea, have a higher number of sets per trip than smaller vessels (p<0.001). Smaller vessels (≤12 m) with smaller engines tend to remain in waters around West Nusa Tenggara only, carrying out shorter fishing trips using bottom longlines (Table 3).
Table 3. Characterisation of the different fishing vessels used to target sharks in Tanjung Luar.
During the study period we recorded shark catch from a total of 595 fishing trips. We recorded 11,678 individual sharks, with an average total catch of 963 individuals per month (SD ± 434) and 19.7 individuals per trip (SD ± 15.6). Standardised CPUE (per 100 hooks per set) ranged from 0.05 to 22.13 individuals, with an average of 0.96 and a mode of 0.20. Catch consisted of 42 different species from 18 families (Table 4). 22% of all landings were classified as threatened species (i.e. VU, EN, CR) according to the IUCN Red List of Threatened Species, and 73% were near threatened. Almost half (46.3%) of landings were regulated (i.e. CITES-listed) species. The most commonly caught species were silky shark (Carcharhinus falciformis), black tip shark (Carcharhinus limbatus) and scalloped hammerhead (Sphyrna lewini).
Table 4. Sharks species landed in Tanjung Luar from January 2014 –December 2015 (VU = Vulnerable, EN = Endangered, NT = Near Threatened, LC = Least Concern, NE = Not Evaluated (VU and EN classified as ‘threatened’ in this study); II = CITES Appendix II, N = Not CITES-listed (II species classified as ‘regulated’ in this study)).
Measures of CPUE for the Tanjung Luar shark fishery vary spatially and temporally, and with several aspects of fishing effort including gear type, hook number, engine power and number of sets. An initial comparison of average catch per trip and catch per set of the two major gear types, surface longline and bottom longline, indicates that CPUE of surface longlines was significantly higher than that of bottom longlines (ANOVA, p<0.001). CPUE (individuals per set) was also positively associated with number of hooks, engine power, and number of sets (Fig 2). However, these relationships are for unstandardised CPUE i.e. without controlling for number of hooks.
Plots of CPUE: Number of individuals per set (A) and number of individuals per 100 hooks per set (standardised CPUE) (B) by gear type (1), number of hooks (2), number of sets (3) and engine horsepower (4).
When controlling for hook number using standardised CPUE (individuals per 100 hooks per set) the relationships were reversed, with standardised CPUE of bottom longlines significantly higher than that of surface longlines (ANOVA, p<0.001; Fig 2). A similar pattern was observed when comparing relationships between CPUE (individuals per set) and standardised CPUE for other measures of fishing effort, including numbers of hooks, engine power and number of sets (Fig 2). There was a positive relationship between unstandardised CPUE (individuals per set) and number of hooks, number of sets and engine power, but a negative relationship between CPUE and these fishing behaviour variables when CPUE was standardised by hook number (individuals per 100 hooks per set).
The best fit LM of standardised CPUE indicated that the most significant factors influencing standardised CPUE were fishing gear and number of hooks (p<0.001). Month, engine power, number of sets and fishing ground were also identified as significant variables (Table 5), although there was considerable covariance between these factors. Standardised CPUE was significantly lower in January, and decreased with higher numbers of hooks, despite a higher total catch per trip and set (Fig 2).
Table 5. Analysis of variance for linear model of standardised CPUE (individuals per 100 hooks per set) data from Tanjung Luar; significant values (p<0.05) are given in bold.
Best fit GLMs indicated that the most significant factors influencing the likelihood of catching threatened species were month (January and November were significantly lower: p<0.001 and p<0.05, respectively) and fishing ground (Other (i.e. fishing grounds outside of WNTP and ENTP) was significantly higher: p<0.01). Significant factors associated with standardised CPUE of threatened species were number of hooks (p<0.001), fishing ground (other: p<0.001, ENTP p<0.05), engine power (p<0.001) and trip length (p<0.001) (Table 6 and Fig 3).
Plots of most significant factors affecting standardised CPUE (number of individuals per 100 hooks per set) of threatened species: a) hook number, b) fishing ground, c) engine power and d) trip length.
Analysis of variance for the best fit models of factors affecting: a) the likelihood of catching and the standardised CPUE of threatened species b) the likelihood of catching and the standardised CPUE of regulated species.
The most significant factors influencing the likelihood of catching regulated species were month (January was significantly lower: p<0.001), number of hooks (p<0.001) and engine power (<0.01). Significant factors associated with standardised CPUE of regulated species were number of hooks (p<0.001), fishing gear (<0.001), number of sets (p<0.001), engine power (p<0.01) and month (November and January: p<0.05) (Table 5 and Fig 4).
Plots of most significant factors affecting standardised CPUE (number of individuals per 100 hooks per set) of regulated species: a) hook number, b) gear type, c) number of sets.
Although Tanjung Luar’s targeted shark fishery is small in scale, considerable numbers of shark are landed, including a large proportion of threatened and regulated species. A key finding is that measures of CPUE, for all sharks and for threatened and regulated species, vary spatially and temporally, and with several aspects of fishing effort including gear type, hook number, engine power and number of sets. Moreover, the relationships between CPUE and fishing behaviour variables are different for different measures of CPUE (CPUE per trip, CPUE per set, CPUE per 100 hooks per set). This highlights the importance of using appropriate standardisation for meaningful comparisons of CPUE across different gears and vessel types, and has important implications for fisheries management.
Unstandardised CPUE (individuals per set) was significantly lower in January. This is during the west monsoon season, which is characterised by high rainfall and adverse conditions at sea for fishing. Unstandardised CPUE was also significantly lower in West Nusa Tenggara Province (WNTP) than East Nusa Tenggara Province (ENTP) and other provinces, suggesting a lower abundance of sharks in this area. Engine power had a significant positive influence on unstandardised CPUE, and was also associated with longer trips and more sets, which was likely due to the ability of vessels with larger engines to travel longer distances, over longer time periods, and with higher numbers of sets, to favoured fishing grounds. Unstandardised CPUE was also significantly higher for surface longlines than bottom longlines. However, when standardising CPUE for the number of hooks (i.e. individuals per 100 hooks per set) this relationship was reversed. Bottom longlines exhibit a higher standardised CPUE, with negative relationships between catch per 100 hooks per set and number of hooks and frequency of sets. Vessels with moderate engine horsepower (50-59hp) also had the highest standardised CPUE. Since surface longlines systematically employ significantly more hooks than bottom longlines (400–600 vs 25–200 hooks), and tend to be associated with larger boats, longer trips and more sets, these findings suggest that although increasing fishing effort increased total catch for these gears and trips, there were diminishing returns of this increased effort above low to moderate levels.
A large proportion of Tanjung Luar’s shark catch consisted of threatened (22%) and regulated species (46%). Month is a significant factor in explaining standardised CPUE of both threatened and regulated species, which could indicate seasonal variation in the abundance of these species in the Tanjung Luar fishing grounds, or seasonal impacts on CPUE due to poor weather conditions. Fishing ground was a significant factor in explaining the catch of threatened species but not the catch in regulated species. This may be due to differences in range, distribution and relative abundance of species within these groups. Threatened species make up a relatively small proportion of Tanjung Luar’s catch in comparison to regulated species, which make up almost half of the catch (46%). As such, regulated species may generally be more abundant and spatially diffuse than threatened species, and therefore caught more uniformly across fishing grounds. For example, regulated species catch is dominated by silky sharks (Carcharhinus falciformis), which are circum-tropical and coastal-pelagic, and exhibit limited site-fidelity or aggregation behaviour, while threatened species catch is dominated by scalloped hammerheads (Sphyrna lewini), which are known to aggregate in schools. These schools of scalloped hammerheads may be more restricted to specific aggregation sites outside of WNTP and ENTP waters, while silky sharks are found in uniform abundance throughout fishing grounds.
As with CPUE of all catch, there was a positive relationship between unstandardised CPUE (catch per set) of threatened and regulated species and number of hooks, but a significant negative relationship between standardised CPUE (catch per 100 hooks per set). This was likely due to diminishing returns of adding additional hooks, and indicates that the effort for threatened and regulated species was exceeding maximum sustainable yield effort, such that increases in effort (e.g. hook number) were leading to decreases in catch [28–30].
Due to the profitability of the shark industry in Tanjung Luar, and limited adaptive capacity and willingness of shark fishers to move into other industries, it is necessary to identify practical and ethical management interventions that can improve the sustainability of the fishery whilst also mitigating the negative socio-economic consequences for coastal communities. Our findings indicate that spatiotemporal closures and restrictions on fishing effort could improve the overall catch per unit effort and sustainability of the Tanjung Luar shark fishery, and lead to positive conservation outcomes for priority species.
Since the location of shark fishing grounds plays a significant role in determining the likelihood of catching threatened species and their associated CPUE, improved marine spatial planning, with the identification of marine protected areas (MPAs) that protect critical shark habitat and shark populations, could reduce catch of species of conservation concern [31–33] and increase abundance of sharks [34, 35]. Provincial governments in West Papua and West Nusa Tenggara have already established ‘shark sanctuary’ MPAs, which protect critical shark habitat and ban shark fishing within their boundaries [16, 36], and monitoring data indicates positive impacts of shark-specific closures on shark abundance [37, 38]. Strengthening Indonesia’s existing MPA network for shark conservation, such as making all MPAs no-take zones for sharks and expanding spatial protection to critical shark habitat, including aggregation sites or pupping and nursery grounds for species of conservation concern, could have considerable conservation benefits. It should be noted, however, that MPAs may only be effective for certain species, such as those with small ranges or site-fidelity . More research is required to identify critical shark habitat and life history stages. For Tanjung Luar these efforts could focus on better understanding scalloped hammerhead (Sphyrna lewini) aggregation sites. Well-targeted spatial closures for this species could significantly reduce catch of threatened species in this fishery.
The relationships between gear type, several aspects of fishing effort (i.e. hook number, engine power, number of sets, trip length), standardised CPUE of all shark species and standardised CPUE of threatened and regulated species suggest that there is an optimal effort that could increase overall CPUE of the fishery and significantly reduce fishing mortality of species of conservation concern. For example, our data suggest that CPUE peaks with low to intermediate trip lengths and gear sets, intermediate engine power and hook numbers of less than 75 per set longline. Although standardised CPUE of threatened and regulated species is also higher when fewer hooks are deployed, the catch per set and overall mortality is significantly lower. Regulations that control the number of hooks in combination with incentives for shark fishers to tightly manage the number of hooks they deploy could significantly reduce mortality of threatened and endangered species, maximise the overall CPUE of the fishery, and reduce operational costs for fishers, making shark fishing in Tanjung Luar more sustainable and more cost effective [39–41].
Acknowledging that almost half of Tanjung Luar’s shark catch consists of CITES-listed species, developing measures that ensure both the sustainability of the fishery, and full traceability and control of onward trade, will be crucial for implementing CITES . The Indonesian government has demonstrated a strong commitment to regulating shark trade and implementing CITES [17–18], as demonstrated through several policy decisions to confer full and partial protection to CITES-listed shark and ray species (Marine Affairs and Fisheries Ministerial Decree No 4./KEPMEN-KP/2014, Regulation No. 48/PERMEN-KP/2016). This includes zero quotas/export bans for hammerhead and oceanic whitetip sharks. However, these export bans should be considered intermediate policy measures as monitoring systems and data availability are improved, and sustainable quotas are established. This will be challenging, as shark products are often traded in large volumes of fresh and/or preserved body parts, with high morphological similarity between products from regulated species and non-regulated species. To guarantee that trade is not detrimental to the survival of species, sustainable fisheries management will need to be complemented with species-specific trade quotas. This will require catch documentation systems which trace shark products from point of catch to point of export and rapid, low-cost species identification methods.
As baseline data on shark population health are limited, and there is no standardised, fisheries-independent system for monitoring long-term changes in shark populations, indirect bio-indicators (e.g. endo- and ectoparasites, [43–45]) could help to elucidate the impact of management measures on fisheries and populations of wild species. In the future, shark conservation and fisheries management could benefit from long-term monitoring of agreed indices of population abundance and health status.
These lessons may also apply to shark fisheries in other parts of the world. As sharks increasingly become the focus of global conservation efforts it should be acknowledged that species protection alone will not be enough to reduce mortality of priority species. More needs to be done to identify practical fisheries management measures that can reduce pressure on the most vulnerable species and populations, but also support sustainable use of species that are less susceptible to overfishing. Shark fishing forms an integral part of the livelihood strategies of many coastal communities [22, 23], and prohibiting catches will not necessarily lead to positive conservation outcomes [21, 46]. Management interventions must take into account local context and the motivations and well-being of fisher communities in order to be ethical, feasible and impactful.
S1 Dataset. Data of landed sharks at Tanjung Luar auction that had been used for this study.
S1 File. Questionnaires have been used to interview shark fishers, collector, traders, and processors.
We wish to acknowledge the support provided by fishers in Tanjung Luar for their great cooperation during fieldwork. We also thank I Made Dharma Aryawan, Muhsin, Abdul Kohar, and Abdurrafik for their assistance during field research, Benaya M Simeon, Peni Lestari, and Siska Agustina for helping with data processing, Ken Kassem for carefully reading the manuscript and providing useful inputs, and the anonymous reviewers for their constructive comments.
3. Hutchings JA, Reynolds JD. Marine fish population collapses: consequences for recovery and extinction risk. AIBS Bulletin. 2004 Apr;54(4):297–309.
4. Costello C, Ovando D, Clavelle T, Strauss CK, Hilborn R, Melnychuk MC, et al. Global fishery prospects under contrasting management regimes. Proceedings of the national academy of sciences. 2016 May 3;113(18):5125–9.
5. Davidson LN, Krawchuk MA, Dulvy NK. Why have global shark and ray landings declined: improved management or overfishing?. Fish and Fisheries. 2016 Jun 1;17(2):438–58.
6. Stevens JD, Bonfil R, Dulvy NK, Walker PA. The effects of fishing on sharks, rays, and chimaeras (chondrichthyans), and the implications for marine ecosystems. ICES Journal of Marine Science. 2000 Jun 1;57(3):476–94.
9. Dent F, Clarke S. State of the global market for shark products. FAO Fisheries and Aquaculture Technical Paper (FAO) eng no. 590. 2015.
12. Christensen J, Tull M, editors. Historical perspectives of fisheries exploitation in the Indo-Pacific. Springer Science & Business Media; 2014 Apr 1.
13. Simpfendorfer CA, Heupel MR, White WT, Dulvy NK. The importance of research and public opinion to conservation management of sharks and rays: a synthesis. Marine and Freshwater Research. 2011 Jul 21;62(6):518–27.
14. Lack M, Sant G. The future of sharks: a review of action and inaction. TRAFFIC International and the Pew Environment Group. 2011 Jan:44.
15. Bräutigam A, Callow M, Campbell IR, Camhi MD, Cornish AS, Dulvy NK, et al. Global priorities for conserving sharks and rays: A 2015–2025 strategy. The Global Sharks and Rays Initiative; 2015. 27p.
16. Satria A, Matsuda Y. Decentralization of fisheries management in Indonesia. Marine Policy. 2004 Sep 30;28(5):437–50.
17. Dharmadi , Fahmi , Satria F. Fisheries management and conservation of sharks in Indonesia. African journal of marine science. 2015 Apr 3;37(2):249–58.
20. Sembiring A, Pertiwi NP, Mahardini A, Wulandari R, Kurniasih EM, Kuncoro AW, Cahyani ND, Anggoro AW, Ulfa M, Madduppa H, Carpenter KE. DNA barcoding reveals targeted fisheries for endangered sharks in Indonesia. Fisheries Research. 2015 Apr 30;164:130–4.
21. Clarke S. Re-examining the shark trade as a tool for conservation. SPC Fisheries Newsletter. 2014:49–56.
22. Jaiteh VF, Loneragan NR, Warren C. The end of shark finning? Impacts of declining catches and fin demand on coastal community livelihoods. Marine Policy. 2017 Mar 24.
24. Cohen D, Crabtree B. Qualitative research guidelines project. Robert Wood Johnson Foundation, Princeton. 2006 Available from: http://www.qualres.org/index.html Cited in August 2016.
25. Skud BE. Manipulation of fixed gear and the effect on catch-per-unit effort. FAO Fisheries Report (FAO). 1984.
26. Damalas D, Megalofonou P, Apostolopoulou M. Environmental, spatial, temporal and operational effects on swordfish (Xiphias gladius) catch rates of eastern Mediterranean Sea longline fisheries. Fisheries Research. 2007 Apr 30;84(2):233–46.
27. Burnham KP, Anderson DR. Model selection and multimodel inference: a practical information-theoretic approach. Springer Science & Business Media; 2003 Dec 4.
28. Schaefer MB. Some aspects of the dynamics of populations important to the management of the commercial marine fisheries. Inter-American Tropical Tuna Commission Bulletin. 1954;1(2):23–56.
29. Fox WW Jr. An exponential surplus-yield model for optimizing exploited fish populations. Transactions of the American Fisheries Society. 1970 Jan 1;99(1):80–8.
30. Purwanto P, Nugroho D, Suwarso S. Potential production of the five predominant small pelagic fish species groups in the Java Sea. Indonesian Fisheries Research Journal. 2014 Dec 1;20(2):59–67.
31. Barker MJ, Schluessel V. Managing global shark fisheries: suggestions for prioritizing management strategies. Aquatic Conservation: Marine and Freshwater Ecosystems. 2005 Jul 1;15(4):325–47.
34. Ward-Paige CA, Worm B. Global evaluation of shark sanctuaries. Global Environmental Change. 2017 Nov 30;47:174–89.
35. Speed CW, Cappo M, Meekan MG. Evidence for rapid recovery of shark populations within a coral reef marine protected area. Biological Conservation. 2018 Apr 30;220:308–19.
36. West Nusa Tenggara Provincial Government. 2017. [Management and zoning plan of Lunyuk Marine Protected Area]. Mataram: West Nusa Tenggara Provincial Government; 2017. Indonesian.
37. Jaiteh VF, Lindfield SJ, Mangubhai S, Warren C, Fitzpatrick B, Loneragan NR. Higher abundance of marine predators and changes in fishers' behavior following spatial protection within the world's biggest shark fishery. Frontiers in Marine Science. 2016 Apr 7;3:43.
39. Kumoru L. The shark longline fishery in Papua New Guinea. InReport prepared for Billfish and bycatch research group, at the 176th meeting of the standing committee on Tuna and Billfish, Mooloolaba, Australia, 9th-16th July 2003 2003 Jul.
40. Cartamil D, Santana-Morales O, Escobedo-Olvera M, Kacev D, Castillo-Geniz L, Graham JB, Rubin RD, Sosa-Nishizaki O. The artisanal elasmobranch fishery of the Pacific coast of Baja California, Mexico. Fisheries Research. 2011 Mar 31;108(2):393–403.
42. Vincent AC, Sadovy de Mitcheson YJ, Fowler SL, Lieberman S. The role of CITES in the conservation of marine fishes subject to international trade. Fish and Fisheries. 2014 Dec 1;15(4):563–92.
43. Palm HW. Fish parasites as biological indicators in a changing world: can we monitor environmental impact and climate change?. InProgress in Parasitology 2011 (pp. 223–250). Springer Berlin Heidelberg.
44. Palm HW, Yulianto I, Piatkowski U. Trypanorhynch Assemblages Indicate Ecological and Phylogenetical Attributes of Their Elasmobranch Final Hosts. Fishes. 2017 Jun 17;2(2):8.
46. Booth H. Using the case of illegal manta ray trade in Indonesia to evaluate the impact of wildlife trade policy (Master Thesis, Imperial College London).
47. White WT, Last PR, Stevens JD, Yearsley GK. Economically important sharks & rays of Indonesia. Austr-alian Centre for International Agricultural Research (ACIAR); 2006.",['The relationships between catch per set and fishing behavior variables differ when comparing unstandardized CPUE and standardized CPUE.'],6133,multifieldqa_en,en,,cd0ecda68ad8031330b971fbfdd3794916e815109f004d3b,The relationships between catch per set and fishing behavior variables differ when comparing unstandardized CPUE and standardized CPUE.,135
What are some potential applications of ferromagnetic semiconductors?,"\section{introduction}
In the past few years, the synthesis of ferromagnetic semiconductors has become a major challenge for spintronics. Actually, growing a magnetic and semiconducting material could lead to promising advances like spin injection into non magnetic semiconductors, or electrical manipulation of carrier induced magnetism in magnetic semiconductors \cite{ohno00,Bouk02}. Up to now, major efforts have focused on diluted magnetic semiconductors (DMS) in which the host semiconducting matrix is randomly substituted by transition metal (TM) ions such as Mn, Cr, Ni, Fe or Co \cite{Diet02}. However Curie temperatures ($T_{C}$) in DMS remain rather low and TM concentrations must be drastically raised in order to increase $T_{C}$ up to room temperature. That usually leads to phase separation and the formation of secondary phases. It was recently shown that phase separation induced by spinodal decomposition could lead to a significant increase of $T_{C}$ \cite{Diet06,Fuku06}. For semiconductors showing $T_{C}$ higher than room temperature one can foresee the fabrication of nanodevices such as memory nanodots, or nanochannels for spin injection. Therefore, the precise control of inhomogeneities appears as a new challenge which may open a way to industrial applications of ferromagnetism in semiconductors.

The increasing interest in group-IV magnetic semiconductors can also be explained by their potential compatibility with the existing silicon technology. In 2002, carrier mediated ferromagnetism was reported in  MBE grown Ge$_{0.94}$Mn$_{0.06}$ films by Park \textit{et al.} \cite{Park02}. The maximum critical temperature was 116 K. Recently many publications indicate a significant increase of $T_{C}$ in Ge$_{1-x}$Mn$_{x}$ material depending on growth conditions \cite{Pint05,Li05,tsui03}. Cho \textit{et al.} reported a Curie temperature as high as 285 K \cite{Cho02}. 
Taking into account the strong tendency of Mn ions to form intermetallic compounds in germanium, a detailed investigation of the nanoscale structure is required. Up to now, only a few studies have focused on the nanoscale composition in Ge$_{1-x}$Mn$_{x}$ films. Local chemical inhomogeneities have been recently reported by Kang \textit{et al.} \cite{Kang05} who evidenced a micrometer scale segregation of manganese in large Mn rich stripes. Ge$_3$Mn$_5$ as well as Ge$_8$Mn$_{11}$ clusters embedded in a germanium matrix have been reported by many authors. However, Curie temperatures never exceed 300 K \cite{Bihl06,Morr06,Pass06,Ahle06}. Ge$_3$Mn$_5$ clusters exhibit a Curie temperature of 296 K \cite{Mass90}. This phase frequently observed in Ge$_{1-x}$Mn$_{x}$ films is the most stable (Ge,Mn) alloy. The other stable compound Ge$_8$Mn$_{11}$ has also been observed in nanocrystallites surrounded with pure germanium \cite{Park01}. Ge$_8$Mn$_{11}$ and Ge$_3$Mn$_5$ phases are ferromagnetic but their metallic character considerably complicates their potential use as spin injectors.
Recently, some new Mn-rich nanostructures have been evidenced in Ge$_{1-x}$Mn$_{x}$ layers. Sugahara \textit{et al.} \cite{Sugh05} reported the formation of high Mn content (between 10 \% and 20 \% of Mn) amorphous Ge$_{1-x}$Mn$_x$ precipitates  in a Mn-free germanium matrix. Mn-rich coherent cubic clusters were observed by Ahlers \textit{et al.} \cite{Ahle06} which exhibit a Curie temperatures below 200 K. Finally, high-$T_{C}$ ($>$ 400 K) Mn-rich nanocolumns have been evidenced \cite{Jame06} which could lead to silicon compatible room temperature operational devices.\\
In the present paper, we investigate the structural and magnetic properties of Ge$_{1-x}$Mn$_x$ thin films for low growth temperatures ($<$ 200$^{\circ}$C) and low Mn concentrations (between 1 \% and 11 \%). By combining TEM, x-Ray diffraction and SQUID magnetometry, we could identify different magnetic phases. We show that depending on growth conditions, we obtain either Mn-rich nanocolumns or Ge$_{3}$Mn$_{5}$ clusters embedded in a germanium matrix. We discuss the structural and magnetic properties of these nanostructures as a function of manganese concentration and growth temperature. We also discuss the magnetic anisotropy of nanocolumns and  
Ge$_3$Mn$_5$ clusters. 

\section{Sample growth}

Growth was performed using solid sources molecular beam epitaxy (MBE) by co-depositing Ge and Mn evaporated from standard Knudsen effusion cells.  Deposition rate was low ($\approx$ 0.2 \AA.s$^{-1}$). Germanium substrates were epi-ready Ge(001) wafers with a residual n-type doping and resistivity of 10$^{15}$ cm$^{-3}$ and 5 $\Omega.cm$ respectively. After thermal desorption of the surface oxide, a 40 nm thick Ge buffer layer was grown at 250$^{\circ}$C, resulting in a 2 $\times$ 1 surface reconstruction as observed by reflection high energy electron diffraction (RHEED) (see Fig. 1a). Next, 80 nm thick Ge$_{1-x}$Mn$_{x}$ films were subsequently grown  at low substrate temperature (from 80$^{\circ}$C to 200$^{\circ}$C). Mn content has been determined by x-ray fluorescence measurements performed on thick samples ($\approx$ 1 $\mu m$ thick) and complementary Rutherford Back Scattering (RBS) on thin Ge$_{1-x}$Mn$_{x}$ films grown on silicon. Mn concentrations range from 1 \% to 11\% Mn.

For Ge$_{1-x}$Mn$_{x}$ films grown at substrate temperatures below 180$^{\circ}$C, after the first monolayer (ML) deposition, the  2 $\times$ 1 surface reconstruction almost totally disappears. After depositing few MLs, a slightly diffuse 1 $\times$ 1 streaky RHEED pattern and a very weak 2 $\times$ 1 reconstruction (Fig. 1b) indicate a predominantly two-dimensional growth. For growth temperatures above 180$^{\circ}$C additional spots appear in the RHEED pattern during the Ge$_{1-x}$Mn$_{x}$ growth (Fig. 1c). These spots may correspond to the formation of very small secondary phase crystallites. The nature of these crystallites will be discussed below.

Transmission electron microscopy (TEM) observations were performed using a JEOL 4000EX microscope with an acceleration voltage of 400 kV. Energy filtered transmission electron microscopy (EFTEM) was done using a JEOL 3010 microscope equipped with a Gatan Image Filter . Sample preparation was carried out by standard mechanical polishing and argon ion milling for cross-section investigations and plane views were prepared by wet etching with H$_3$PO$_4$-H$_2$O$_2$ solution \cite{Kaga82}.

\begin{figure}[htb]
    \center
    \includegraphics[width=.29\linewidth]{./fig1a.eps}
    \includegraphics[width=.29\linewidth]{./fig1b.eps}
    \includegraphics[width=.29\linewidth]{./fig1c.eps}
    \caption{RHEED patterns recorded during the growth of Ge$_{1-x}$Mn$_{x}$ films: (a) 2 $\times$ 1 surface reconstruction of the germanium buffer layer. (b) 1 $\times$ 1 streaky RHEED pattern obtained at low growth temperatures ($T_g<$180$^{\circ}$C). (c) RHEED pattern of a sample grown at $T_g=$180$^{\circ}$C. The additional spots reveal the presence of Ge$_3$Mn$_5$ clusters at the surface of the film.}
\label{fig1}
\end{figure}

\section{Structural properties \label{structural}}

\begin{figure}[htb]
    \center
	\includegraphics[width=.49\linewidth]{./fig2a.eps}
	\includegraphics[width=.49\linewidth]{./fig2b.eps}
	 \includegraphics[width=.49\linewidth]{./fig2c.eps}
	 \includegraphics[width=.49\linewidth]{./fig2d.eps}
    \caption{Transmission electron micrographs of a Ge$_{1-x}$Mn$_{x}$ film grown at 130$^{\circ}$C and containing 6 \% of manganese. (a) cross-section along the [110] axis : we clearly see the presence of nanocolumns elongated along the growth axis. (b) High resolution image of the interface between the Ge$_{1-x}$Mn$_{x}$ film and the Ge buffer layer. The Ge$_{1-x}$Mn$_{x}$ film exhibits the same diamond structure as pure germanium. No defect can be seen which could be caused by the presence of nanocolumns. (c) Plane view micrograph performed on the same sample confirms the columnar structure and gives the density and size distribution of nanocolumns. (d) Mn chemical map obtained by energy filtered transmission electron microcopy (EFTEM). The background was carefully substracted from pre-edge images. Bright areas correspond to Mn-rich regions.}
\label{fig2}
\end{figure}

In samples grown at 130$^{\circ}$C and containing 6 \% Mn, we can observe vertical elongated nanostructures \textit{i.e.} nanocolumns as shown in Fig. 2a. Nanocolumns extend through the whole Ge$_{1-x}$Mn$_{x}$ film thickness. From the high resolution TEM image shown in Fig. 2b, we deduce their average diameter around 3 nm. Moreover in Fig. 2b, the interface between the Ge buffer layer and the Ge$_{1-x}$Mn$_{x}$ film is flat and no defect propagates from the interface into the film. The Ge$_{1-x}$Mn$_{x}$ film is a perfect single crystal in epitaxial relationship with the substrate. In Fig. 2c is shown a plane view micrograph of the same sample confirming the presence of nanocolumns in the film. From this image, we can deduce the size and density of nanocolumns. The nanocolumns density is 13000 $\rm{\mu m}^{-2}$ with a mean diameter of 3 nm which is coherent with cross-section measurements. In order to estimate the chemical composition of these nanocolumns, we further performed chemical mapping using EFTEM. In Fig. 2d we show a cross sectional Mn chemical map of the Ge$_{1-x}$Mn$_{x}$ film. This map shows that the formation of nanocolumns is a consequence of Mn segregation. Nanocolumns are Mn rich and the surrounding matrix is Mn poor. However, it is impossible to deduce the Mn concentration in Ge$_{1-x}$Mn$_{x}$ nanocolumns from this cross section. Indeed, in cross section observations, the columns diameter is much smaller than the probed film thickness and the signal comes from the superposititon of the Ge matrix and Mn-rich nanocolumns. In order to quantify Mn concentration inside the nanocolumns and inside the Ge matrix, EELS measurements (not shown here) have been performed in a plane view geometry \cite{Jame06}. These observations revealed that the matrix Mn content is below 1 \% (detection limit of our instrument). Measuring the surface occupied by the matrix and the nanocolumns in plane view TEM images, and considering the average Mn concentration in the sample (6 \%), we can estimate the Mn concentration in the nanocolumns. The Mn concentration measured by EELS being between 0\% and 1\%, we can conclude that the Mn content in the nanocolumns is between 30 \% and 38 \%.\\
For samples grown between 80$^\circ$C and 150$^\circ$C cross section and plane view TEM observations reveal the presence of Mn rich nanocolumns surrounded with a Mn poor Ge matrix. In order to investigate the influence of Mn concentration on the structural properties of Ge$_{1-x}$Mn$_{x}$ films, ten samples have been grown at 100$^\circ$C and at 150$^\circ$C with Mn concentrations of 1.3 \%, 2.3 \%, 4 \%, 7 \% and 11.3 \%. Their structural properties have been investigated by plane view TEM observations. 

\begin{figure}[htb]
    \center
    \includegraphics[width=.98\linewidth]{./fig3a.eps}
	\includegraphics[width=.45\linewidth]{./fig3b.eps}
		\includegraphics[width=.45\linewidth]{./fig3c.eps}
    \caption{Nanocolumns size and density as a function of growth conditions. Samples considered have been grown at 100$^{\circ}$C and 150$^{\circ}$C respectively. (a) Mn concentration dependence of the size distribution. (b) columns density as a function of Mn concentration. (c) Volume fraction of the nanocolumns as a function of Mn concentration.}
 \label{fig3}
\end{figure}

For samples grown at 100$^\circ$C with Mn concentrations below 5 \% the nanocolumns mean diameter is 1.8$\pm$0.2 nm. The evolution of columns density as a fonction of Mn concentration is reported in figure 3b. By increasing the Mn concentration from 1.3 \% to 4 \% we observe a significant increase of the columns density from 13000 to 30000 $\rm{\mu m}^{-2}$. For Mn concentrations higher than 5 \% the density seems to reach a plateau corresponding to 35000 $\rm{\mu m}^{-2}$ and their diameter slightly increases from 1.8 nm at 4 \% to 2.8 nm at 11.3 \%. By plotting the volume fraction occupied by the columns in the film as a function of Mn concentration, we observe a linear dependence for Mn contents below 5 \%. The non-linear behavior above 5 \% may indicate that the mechanism of Mn incorporation is different in this concentration range, leading to an increase of Mn concentration in the columns or in the matrix. For samples grown at 100$^\circ$C, nanocolumns are always fully coherent with the surrounding matrix (Fig. 4a). 

Increasing the Mn content in the samples grown at 150$^\circ$C from 1.3 \% to 11.3 \% leads to a decrease of the columns density (fig 3b). Moreover, their average diameter increases significantly and size distributions become very broad (see Fig. 3a). For the highest Mn concentration (11.3 \%) we observe the coexistence of very small columns with a diameter of 2.5 nm and very large columns with a diameter of 9 nm. In samples grown at 150$^\circ$C containing 11.3 \% of Mn, the crystalline structure of nanocolumns is also highly modified. In plane view TEM micrographs, one can see columns exhibiting several different crystalline structures. We still observe some columns which are fully coherent with the Ge matrix like in the samples grown at lower temperature. Nevertheless, observations performed on these samples grown at 150$^\circ$C and with 11.3\% Mn reveal some uniaxially \cite{Jame06} or fully relaxed columns exhibiting a misfit of 4 \% between the matrix and the columns and leading to misfit dislocations at the interface between the column and the matrix (see fig. 4b). Thus we can conclude that coherent columns are probably in strong compression and the surrounding matrix in tension. On the same samples (T$_g$=150$^{\circ}$C, 11.3\% Mn), we also observe a large number of highly disordered nanocolumns leading to an amorphous like TEM contrast(fig. 4c).

\begin{figure}[htb]
    \center
   \includegraphics[width=.31\linewidth]{./fig4a.eps}
	\includegraphics[width=.31\linewidth]{./fig4b.eps}
	\includegraphics[width=.31\linewidth]{./fig4c.eps}
    \caption{Plane view high resolution transmission electron micrographs of different types of nanocolumns : (a) typical structure of a column grown at 100$^{\circ}$C. The crystal structure is exactly the same as germanium . (b) Partially relaxed nanocolumn. One can see dislocations at the interface between the columns and the matrix leading to stress relaxation. (c) Amorphous nanocolumn. These columns are typical in samples grown at 150$^{\circ}$C with high Mn contents.}
 \label{fig4}
\end{figure}

In conclusion, we have evidenced a complex mechanism of Mn incorporation in Mn doped Ge films grown at low temperature. In particular Mn incorporation is highly inhomogeneous. For very low growth temperatures (below 120$^\circ$C) the diffusion of Mn atoms leads to the formation of Mn rich, vertical nanocolumns. Their density mostly depends on Mn concentration and their mean diameter is about 2 nm. These results can be compared with the theoretical predictions of Fukushima \textit{et al.} \cite{Fuku06}: they proposed a model of spinodal decomposition in (Ga,Mn)N and (Zn,Cr)Te based on layer by layer growth conditions and a strong pair attraction between Mn atoms which leads to the formation of nanocolumns. This model may also properly describe the formation of Mn rich nanocolumns in our samples. Layer by layer growth conditions can be deduced from RHEED pattern evolution during growth. For all the samples grown at low temperature, RHEED observations clearly indicate two-dimensional growth. Moreover, Ge/Ge$_{1-x}$Mn$_{x}$/Ge heterostructures have been grown and observed by TEM (see Fig. 5). Ge$_{1-x}$Mn$_{x}$/Ge (as well as Ge/Ge$_{1-x}$Mn$_{x}$) interfaces are very flat and sharp thus confirming a two-dimensional, layer by layer growth mode. Therefore we can assume that the formation of Mn rich nanocolumns is a consequence of 2D-spinodal decomposition.

\begin{figure}[htb]
    \center
	\includegraphics[width=.7\linewidth]{./fig5.eps}
    \caption{Cross section high resolution micrograph of a Ge/Ge$_{1-x}$Mn$_{x}$/Ge/Ge$_{1-x}$Mn$_{x}$/Ge heterostructure. This sample has been grown at 130 $^{\circ}$C with 6\% Mn. Ge$_{1-x}$Mn$_{x}$ layers are 15 nm thick and Ge spacers 5 nm thick. We clearly see the sharpness of both Ge$_{1-x}$Mn$_{x}$/Ge and Ge/Ge$_{1-x}$Mn$_{x}$ interfaces. Mn segregation leading to the columns formation already takes place in very thin Ge$_{1-x}$Mn$_{x}$ films.}
\label{fig5}
\end{figure}

For growth temperatures higher than 160$^\circ$C, cross section TEM and EFTEM observations (not shown here) reveal the coexistence of two Mn-rich phases: nanocolumns and Ge$_{3}$Mn$_{5}$ nanoclusters embedded in the germanium matrix. A typical high resolution TEM image is shown in figure 6. 
Ge$_{3}$Mn$_{5}$ clusters are not visible in RHEED patterns for temperatures below 180$^\circ$C. To investigate the nature of these clusters, we performed x-ray diffraction in $\theta-2\theta$ mode. Diffraction scans were acquired on a high resolution diffractometer using the copper K$_\alpha$ radiation and on the GMT station of the BM32 beamline at the European Synchrotron Radiation Facility (ESRF). Three samples grown at different temperatures and/or annealed at high temperature were investigated. The two first samples are Ge$_{1-x}$Mn$_{x}$ films grown at 130$^\circ$C and 170$^\circ$C respectively. The third one has been grown at 130$^\circ$C and post-growth annealed at 650$^\circ$C. By analysing x-ray diffraction spectra, we can evidence two different crystalline structures. For the sample grown at 130$^\circ$C, the $\theta-2\theta$ scan only reveals the (004) Bragg peak of the germanium crystal, confirming the good epitaxial relationship between the layer and the substrate, and the absence of secondary phases in the film in spite of a high dynamics of the order of 10$^7$. For both samples grown at 170$^\circ$C and annealed at 650$^\circ$C, $\theta-2\theta$ spectra are identical. In addition to the (004) peak of germanium, we observe three additional weak peaks. The first one corresponds to the (002) germanium forbidden peak which probably comes from a small distortion of the germanium crystal, and the two other peaks are respectively attributed to the (002) and (004) Bragg peaks of a secondary phase. The $c$ lattice parameter of Ge$_3$Mn$_5$ hexagonal crystal is 5.053 \AA \ \cite{Fort90} which is in very good agreement with the values obtained from diffraction data for both (002) and (004) lines assuming that the $c$ axis of Ge$_3$Mn$_5$ is along the [001] direction of the Ge substrate.

\begin{figure}[htb]
    \center
	\includegraphics[width=.7\linewidth]{./fig6.eps}
	\caption{Cross section high resolution transmission electron micrograph of a sample grown at 170$^{\circ}$C. We observe the coexistence of two different Mn-rich phases: Ge$_{1-x}$Mn$_{x}$ nanocolumns and Ge$_3$Mn$_5$ clusters.}
\label{fig6}
\end{figure}

In summary, in a wide range of growth temperatures and Mn concentrations, we have evidenced a two-dimensional spinodal decomposition leading to the formation of Mn-rich nanocolumns in Ge$_{1-x}$Mn$_{x}$ films. This decomposition is probably the consequence of: $(i)$ a strong pair attraction between Mn atoms, $(ii)$ a strong surface diffusion of Mn atoms in germanium even at low growth temperatures and $(iii)$ layer by layer growth conditions. We have also investigated the influence of growth parameters on the spinodal decomposition: at low growth temperatures (100$^{\circ}$C), increasing the Mn content leads to higher columns densities while at higher growth temperatures (150$^{\circ}$C), the columns density remains nearly constant whereas their size increases drastically. By plotting the nanocolumns density as a function of Mn content, we have shown that the mechanism of Mn incorporation in Ge changes above 5 \% of Mn. Finally, using TEM observations and x-ray diffraction, we have shown that Ge$_3$Mn$_5$ nanoclusters start to form at growth temperatures higher than 160$^\circ$C.

\section{Magnetic properties \label{magnetic}}

We have thoroughly investigated the magnetic properties of thin Ge$_{1-x}$Mn$_{x}$ films for different growth temperatures and Mn concentrations. In this section, we focus on Mn concentrations between 2 \% and 11 \%. We could clearly identify four different magnetic phases in Ge$_{1-x}$Mn$_{x}$ films : diluted Mn atoms in the germanium matrix, low $T_{C}$ nanocolumns ($T_{C}$ $\leq$ 170 K), high $T_{C}$ nanocolumns ($T_{C}$ $\geq$ 400 K) and Ge$_{3}$Mn$_{5}$ clusters ($T_{C}$ $\thickapprox$ 300 K). The relative weight of each phase clearly depends on the growth temperature and to a lesser extend on Mn concentration. For low growth temperature ($<$ 120$^{\circ}$C), we show that nanocolumns are actually made of four uncorrelated superparamagnetic nanostructures. Increasing T$_{g}$ above 120$^{\circ}$C, we first obtain continuous columns exhibiting low $T_{C}$ ($<$ 170 K) and high $T_{C}$ ($>$ 400 K) for $T_{g}\approx$130$^{\circ}$C. The larger columns become ferromagnetic \textit{i.e.} $T_{B}>T_{C}$. Meanwhile Ge$_{3}$Mn$_{5}$ clusters start to form. Finally for higher $T_{g}$, the magnetic contribution from Ge$_{3}$Mn$_{5}$ clusters keeps increasing while the nanocolumns signal progressively disappears.

\begin{figure}[htb]
\center
   \includegraphics[width=.6\linewidth]{./fig7a.eps}
   \includegraphics[width=.3\linewidth]{./fig7b.eps}
\caption{(a) Temperature dependence of the saturation magnetization (in $\mu_{B}$/Mn) of Ge$_{0.93}$Mn$_{0.07}$ samples for different growth temperatures. The magnetic field is applied in the film plane. The inset shows the temperature dependence of a sample grown at 130$^{\circ}$C and annealed at 650$^{\circ}$C for 15 minutes. After annealing, the magnetic signal mostly arises from Ge$_{3}$Mn$_{5}$ clusters. (b) ZFC-FC measurements performed on Ge$_{0.93}$Mn$_{0.07}$ samples for different growth temperatures. The in-plane applied field is 0.015 T. The ZFC peak at low temperature ($\leq$150 K) can be attributed to the superparamagnetic nanocolumns. This peak widens and shifts towards high blocking temperatures when increasing growth temperature. The second peak above 150 K in the ZFC curve which increases with increasing growth temperature is attributed to superparamagnetic Ge$_{3}$Mn$_{5}$ clusters. The increasing ZFC-FC irreversibility at $\approx$ 300 K is due to the increasing contribution from large ferromagnetic Ge$_{3}$Mn$_{5}$ clusters. The nanocolumns signal completely vanishes after annealing at 650$^{\circ}$C for 15 minutes.}
\label{fig7}
\end{figure}

In Fig. 7a, the saturation magnetization at 2 Tesla in $\mu_{B}$/Mn of Ge$_{1-x}$Mn$_{x}$ films with 7 \% of Mn is plotted as a function of temperature for different growth temperatures ranging from $T_{g}$=90$^{\circ}$C up to 160$^{\circ}$C. The inset shows the temperature dependence of the magnetization at 2 Tesla after annealing at 650$^{\circ}$C during 15 minutes. Figure 7b displays the corresponding Zero Field Cooled - Field Cooled (ZFC-FC) curves recorded at 0.015 Tesla. In the ZFC-FC procedure, the sample is first cooled down to 5 K in zero magnetic field and the susceptibility is subsequently recorded at 0.015 Tesla while increasing the temperature up to 400 K (ZFC curve). Then, the susceptibility is recorded under the same magnetic field while decreasing the temperature down to 5 K (FC curve). Three different regimes can be clearly distinguished. \\
For $T_{g}\leq$120$^{\circ}$C, the temperature dependence of the saturation magnetization remains nearly the same while increasing growth temperature. The overall magnetic signal vanishing above 200 K is attributed to the nanocolumns whereas the increasing signal below 50 K originates from diluted Mn atoms in the surrounding matrix. The Mn concentration dependence of the saturation magnetization is displayed in figure 8. For the lowest Mn concentration (4 \%), the contribution from diluted Mn atoms is very high and drops sharply for higher Mn concentrations (7 \%, 9 \% and 11.3 \%). Therefore the fraction of Mn atoms in the diluted matrix decreases with Mn concentration probably because Mn atoms are more and more incorporated in the nanocolumns. In parallel, the Curie temperature of nanocolumns increases with the Mn concentration reaching 170 K for 11.3 \% of Mn. This behavior may be related to different Mn compositions and to the increasing diameter of nanocolumns (from 1.8 nm to 2.8 nm) as discussed in section \ref{structural}.

\begin{figure}[htb]
\center
   \includegraphics[width=.7\linewidth]{./fig8.eps}
    \caption{Temperature dependence of the saturation magnetization (in $\mu_{B}$/Mn) of Ge$_{1-x}$Mn$_{x}$ films grown at 100$^{\circ}$C plotted for different Mn concentrations: 4.1 \%; 7 \%; 8.9 \% and 11.3 \%.}
\label{fig8}
\end{figure}

ZFC-FC measurements show that the nanocolumns are superparamagnetic. The magnetic signal from the diluted Mn atoms in the matrix is too weak to be detected in susceptibility measurements at low temperature. In samples containing 4 \% of Mn, ZFC and FC curves superimpose down to low temperatures. As we do not observe hysteresis loops at low temperature, we believe that at this Mn concentration nanocolumns are  superparamagnetic in the whole temperature range and the blocking temperature cannot be measured. For higher Mn contents, the ZFC curve exhibits a very narrow peak with a maximum at the blocking temperature of 15 K whatever the Mn concentration and growth temperature (see Fig. 7b). Therefore the anisotropy barrier distribution is narrow and assuming that nanocolumns have the same magnetic anisotropy, this is a consequence of the very narrow size distribution of the nanocolumns as observed by TEM. To probe the anisotropy barrier distribution, we have performed ZFC-FC measurements but instead of warming the sample up to 400 K, we stopped at a lower temperature $T_{0}$. 

\begin{figure}[htb]
\center
   \includegraphics[width=.6\linewidth]{./fig9.eps}
\caption{Schematic drawing of the anisotropy barrier distribution n($E_{B}$) of superparamagnetic nanostructures. If magnetic anisotropy does not depend on the particle size, this distribution exactly reflects their magnetic size distribution. In this drawing the blocking temperature ($T_{B}$) corresponds to the distribution maximum. At a given temperature $T_{0}$ such that 25$k_{B}T_{0}$ falls into the anisotropy barrier distribution, the largest nanostructures with an anisotropy energy larger than 25$k_{B}T_{0}$ are blocked whereas the others are superparamagnetic.}
\label{fig9}
\end{figure}

If this temperature falls into the anisotropy barrier distribution as depicted in Fig. 9, the FC curve deviates from the ZFC curve. Indeed the smallest nanostructures have become superparamagnetic at $T_{0}$ and when decreasing again the temperature, their magnetization freezes along a direction close to the magnetic field and the FC susceptibility is higher than the ZFC susceptibility. Therefore any irreversibility in this procedure points at the presence of superparamagnetic nanostructures. The results are given in Fig. 10a. ZFC and FC curves clearly superimpose up to $T_{0}$=250 K thus the nanocolumns are superparamagnetic up to their Curie temperature and no Ge$_{3}$Mn$_{5}$ clusters could be detected. Moreover for low $T_{0}$ values, a peak appears at low temperature in FC curves which evidences strong antiferromagnetic interactions between the nanocolumns \cite{Chan00}.

\begin{figure}[htb]
\center
    \includegraphics[width=.35\linewidth]{./fig10a.eps}
    \includegraphics[width=.63\linewidth]{./fig10b.eps}
\caption{(a) ZFC-FC measurements performed on a Ge$_{0.887}$Mn$_{0.113}$ sample grown at 115$^{\circ}$C. The in-plane applied field is 0.015 T. Magnetization was recorded up to different T$_{0}$ temperatures: 30 K, 50 K, 100 K, 150 K, 200 K and 250 K. Curves are shifted up for more clarity. (b) ZFC-FC curves for in-plane and out-of-plane applied fields (0.015 T).}
\label{fig10}
\end{figure}

In order to derive the magnetic size and anisotropy of the Mn-rich nanocolumns embedded in the Ge matrix, we have fitted the inverse normalized in-plane (resp. out-of-plane) susceptibility: $\chi_{\parallel}^{-1}$ (resp. $\chi_{\perp}^{-1}$). The corresponding experimental ZFC-FC curves are reported in Fig. 10b. Since susceptibility measurements are performed at low field (0.015 T), the matrix magnetic signal remains negligible. In order to normalize susceptibility data, we need to divide the magnetic moment by the saturated magnetic moment recorded at 5 T. However the matrix magnetic signal becomes very strong at 5 T and low temperature so that we need to subtract it from the saturated magnetic moment using a simple Curie function. From Fig. 10b, we can conclude that nanocolumns are isotropic. Therefore to fit experimental data we use the following expression well suited for isotropic systems or cubic anisotropy: $\chi_{\parallel}^{-1}= \chi_{\perp}^{-1}\approx 3k_{B}T/M(T)+\mu_{0}H_{eff}(T)$. $k_{B}$ is the Boltzmann constant, $M=M_{s}v$ is the magnetic moment of a single-domain nanostructure (macrospin approximation) where $M_{s}$ is its magnetization and $v$ its volume. The in-plane magnetic field is applied along $[110]$ or $[-110]$ crystal axes. Since the nanostructures Curie temperature does not exceed 170 K, the temperature dependence of the saturation magnetization is also accounted for by writting $M(T)$. Antiferromagnetic interactions between nanostructures are also considered by adding an effective field estimated in the mean field approximation \cite{Fruc02}: $\mu_{0}H_{eff}(T)$.
The only fitting parameters are the maximum magnetic moment (\textit{i.e.} at low temperature) per nanostructure: $M$ (in Bohr magnetons $\mu_{B}$) and the maximum interaction field (\textit{i.e.} at low temperature): $\mu_{0}H_{eff}$.

\begin{figure}[htb]
\center
   \includegraphics[width=.7\linewidth]{./fig11.eps}
\caption{Temperature dependence of the inverse in-plane (open circles) and out-of-plane (open squares) normalized susceptibilities of a Ge$_{0.887}$Mn$_{0.113}$ sample grown at 115$^{\circ}$C. Fits were performed assuming isotropic nanostructures or cubic anisotropy. Dashed line is for in-plane susceptibility and solid line for out-of-plane susceptibility.}
\label{fig11}
\end{figure}

In Fig. 11, the best fits lead to $M\approx$1250 $\mu_{B}$ and $\mu_{0}H_{eff}\approx$102 mT for in-plane susceptibility and $M\approx$1600 $\mu_{B}$ and $\mu_{0}H_{eff}\approx$98 mT for out-of-plane susceptibility. It gives an average magnetic moment of 1425 $\mu_{B}$ per column and an effective interaction field of 100 mT. Using this magnetic moment and its temperature dependence, magnetization curves could be fitted using a Langevin function and $M(H/T)$ curves superimpose for $T<$100 K. However, from the saturated magnetic moment of the columns and their density (35000 $\rm{\mu m}^{-2}$), we find almost 6000 $\mu_{B}$ per column. Therefore, for low growth temperatures, we need to assume that nanocolumns are actually made of almost four independent elongated magnetic nanostructures. The effective field for antiferromagnetic interactions between nanostructures estimated from the susceptibility fits is at least one order of magnitude larger than what is expected from pure magnetostatic coupling. This difference may be due to either an additional antiferromagnetic coupling through the matrix which origin remains unexplained or to the mean field approximation which is no more valid in this strong coupling regime. As for magnetic anisotropy, the nanostructures behave as isotropic magnetic systems or exhibit a cubic magnetic anisotropy. First we can confirm that nanostructures are not amorphous otherwise shape anisotropy would dominate leading to out-of-plane anisotropy. We can also rule out a random distribution of magnetic easy axes since the nanostructures are clearly crystallized in the diamond structure and would exhibit at least a cubic anisotropy (except if the random distribution of Mn atoms within the nanostructures can yield random easy axes). Since the nanostructures are in strong in-plane compression (their lattice parameter is larger than the matrix one), the cubic symmetry of the diamond structure is broken and magnetic cubic anisotropy is thus unlikely. We rather believe that out-of-plane shape anisotropy is nearly compensated by in-plane magnetoelastic anisotropy due to compression leading to a \textit{pseudo} cubic anisotropy. From the blocking temperature (15 K) and the magnetic volume of the nanostructures , we can derive their magnetic anisotropy constant using $Kv=25k_{B}T_{B}$: K$\approx$10 kJ.m$^{-3}$ which is of the same order of magnitude as shape anisotropy.

\begin{figure}[htb]
\center
    \includegraphics[width=.35\linewidth]{./fig12a.eps}
    \includegraphics[width=.63\linewidth]{./fig12b.eps} 
\caption{(a) ZFC-FC measurements performed on a Ge$_{0.93}$Mn$_{0.07}$ sample grown at 122$^{\circ}$C. The in-plane applied field is 0.015 T. Magnetization was recorded up to different T$_{0}$ temperatures: 50 K, 100 K, 150 K, 200 K and 250 K. Curves are shifted up for more clarity. (b) ZFC-FC curves for in-plane and out-of-plane applied fields  (0.015 T).}
\label{fig12}
\end{figure}

For growth temperatures $T_{g}\geq$120$^{\circ}$C and Mn concentrations $\geq$ 7 \%, samples exhibit a magnetic signal above 200 K corresponding to Ge$_{3}$Mn$_{5}$ clusters (see Fig. 7a). As we can see, SQUID measurements are much more sensitive to the presence of Ge$_{3}$Mn$_{5}$ clusters, even at low concentration, than TEM and x-ray diffraction used in section \ref{structural}. We also observe a sharp transition in the ZFC curve (see Fig. 7b, Fig. 12a and 12b): the peak becomes very large and is shifted towards high blocking temperatures (the signal is maximum at $T=$23 K). This can be easily understood as a magnetic percolation of the four independent nanostructures obtained at low growth temperatures into a single magnetic nanocolumn. Therefore the magnetic volume increases sharply as well as blocking temperatures. At the same time, the size distribution widens as observed in TEM. In Fig. 12a, we have performed ZFC-FC measurements at different $T_{0}$ temperatures. The ZFC-FC irreversibility is observed up to the Curie temperature of $\approx$120 K meaning that a fraction of nanocolumns is ferromagnetic (\textit{i.e.} $T_{B}\geq T_{C}$).
In Fig. 12b, in-plane and out-of-plane ZFC curves nearly superimpose for $T\leq$150 K due to the isotropic magnetic behavior of the nanocolumns: in-plane magnetoelastic anisotropy is still compensating out-of-plane shape anisotropy. Moreover the magnetic signal above 150 K corresponding to Ge$_{3}$Mn$_{5}$ clusters that start to form in this growth temperature range is strongly anisotropic. This perpendicular anisotropy confirms the epitaxial relation: (0002) Ge$_{3}$Mn$_{5}$ $\parallel$ (002) Ge discussed in Ref.\cite{Bihl06}. The magnetic easy axis of the clusters lies along the hexagonal $c$-axis which is perpendicular to the film plane.

\begin{figure}[ht]
\center
   \includegraphics[width=.35\linewidth]{./fig13a.eps}
   \includegraphics[width=.63\linewidth]{./fig13b.eps} 
\caption{(a) ZFC-FC measurements performed on a Ge$_{0.887}$Mn$_{0.113}$ sample grown at 145$^{\circ}$C. The in-plane applied field is 0.015 T. Magnetization was recorded up to different T$_{0}$ temperatures: 50 K, 100 K, 150 K, 200 K, 250 K and 300 K. Curves are shifted up for more clarity. (b) ZFC-FC curves for in-plane and out-of-plane applied fields (0.015 T).}
\label{fig13}
\end{figure}

For growth temperatures $T_{g}\geq$145$^{\circ}$C the cluster magnetic signal dominates (Fig. 13b). Superparamagnetic nanostructures are investigated performing ZFC-FC measurements at different $T_{0}$ temperatures (Fig. 13a). The first ZFC peak at low temperature \textit{i.e.} $\leq$ 150 K is attributed to  low-$T_{C}$ nanocolumns ($T_{C}\approx$130 K). This peak is wider than for lower growth temperatures and its maximum is further shifted up to 30 K. These results are in agreement with TEM observations: increasing $T_{g}$ leads to larger nanocolumns (\textit{i.e.} higher blocking temperatures) and wider size distributions. ZFC-FC irreversibility is observed up to the Curie temperature due to the presence of ferromagnetic columns. The second peak above 180 K in the ZFC curve is attributed to Ge$_{3}$Mn$_{5}$ clusters and the corresponding ZFC-FC irreversibility persisting up to 300 K means that some clusters are ferromagnetic. We clearly evidence the out-of-plane anisotropy of Ge$_{3}$Mn$_{5}$ clusters and the isotropic magnetic behavior of nanocolumns (Fig. 13b). In this growth temperature range, we have also investigated the Mn concentration dependence of magnetic properties. 

\begin{figure}[ht]
\center
    \includegraphics[width=.49\linewidth]{./fig14a.eps}
    \includegraphics[width=.49\linewidth]{./fig14b.eps} 
\caption{Temperature dependence of the saturation magnetization (in $\mu_{B}$/Mn) of Ge$_{1-x}$Mn$_{x}$ films grown at 150$^{\circ}$C plotted for different Mn concentrations: 2.3 \%; 4 \%; 7 \%; 9 \%; 11.3 \%. (b) ZFC-FC measurements performed on Ge$_{1-x}$Mn$_{x}$ films grown at 150$^{\circ}$C. The in-plane applied field is 0.025 T for 2.3 \% and 4 \% and 0.015 T for 8 \% and 11.3 \%. }
\label{fig14}
\end{figure}

In Fig. 14a, for low Mn concentrations (2.3 \% and 4 \%) the contribution from diluted Mn atoms in the germanium matrix to the saturation magnetization is very high and nearly vanishes for higher Mn concentrations (7 \%, 9 \% and 13 \%) as observed for low growth temperatures. Above 7 \%, the magnetic signal mainly comes from nanocolumns and Ge$_{3}$Mn$_{5}$ clusters. We can derive more information from ZFC-FC measurements (Fig. 14b). Indeed, for 2.3 \% of Mn, ZFC and FC curves nearly superimpose down to low temperature meaning that nanocolumns are superparamagnetic in the whole temperature range. Moreover the weak irreversibility arising at 300 K  means that some Ge$_{3}$Mn$_{5}$ clusters have already formed in the samples even at very low Mn concentrations. For 4 \% of Mn, we can observe a peak with a maximum at the blocking temperature (12 K) in the ZFC curve. We can also derive the Curie temperature of nanocolumns: $\approx$45 K. The irresversibility arising at 300 K still comes from Ge$_{3}$Mn$_{5}$ clusters. Increasing the Mn concentration above 7 \% leads to: higher blocking temperatures (20 K and 30 K) due to larger nanocolumns and wider ZFC peaks due to wider size distributions in agreement with TEM observations (see Fig. 3a). Curie temperatures also increase (110 K and 130 K) as well as the contribution from Ge$_{3}$Mn$_{5}$ clusters.\\
Finally when increasing $T_{g}$ above 160$^{\circ}$C, the nanocolumns magnetic signal vanishes and only Ge$_{3}$Mn$_{5}$ clusters and diluted Mn atoms coexist. The overall magnetic signal becomes comparable to the one measured on annealed samples in which only Ge$_{3}$Mn$_{5}$ clusters are observed by TEM (see Fig. 7a).\\
The magnetic properties of high-$T_{C}$ nanocolumns obtained for $T_{g}$ close to 130$^{\circ}$C are discussed in detail in Ref.\cite{Jame06}.\\
In conclusion, at low growth temperatures ($T_{g}\leq$120$^{\circ}$C), nanocolumns are made of almost 4 independent elongated magnetic nanostructures. For $T_{g}\geq$120$^{\circ}$C, these independent nanostructures percolate into a single nanocolumn sharply leading to higher blocking temperatures. Increasing $T_{g}$ leads to larger columns with a wider size distribution as evidenced by ZFC-FC measurements and given by TEM observations. In parallel, some Ge$_{3}$Mn$_{5}$ clusters start to form and their contribution increases when increasing $T_{g}$. Results on magnetic anisotropy seems counter-intuitive. Indeed Ge$_{3}$Mn$_{5}$ clusters exhibit strong out-of-plane anisotropy whereas nanocolumns which are highly elongated magnetic structures are almost isotropic. This effect is probably due to compensating in-plane magnetoelastic coupling (due to the columns compression) and out-of-plane shape anisotropy. 

\section{Conclusion}

In this paper, we have investigated the structural and magnetic properties of thin Ge$_{1-x}$Mn$_{x}$ films grown by low temperature molecular beam epitaxy. A wide range of growth temperatures and Mn concentrations have been explored. All the samples contain Mn-rich nanocolumns as a consequence of 2D-spinodal decomposition. However their size, crystalline structure and magnetic properties depend on growth temperature and Mn concentration. For low growth temperatures, nanocolumns are very small (their diameter ranges between 1.8 nm for 1.3 \% of Mn and 2.8 nm for 11.3 \% of Mn), their Curie temperature is rather low ($<$ 170 K) and they behave as almost four uncorrelated superparamagnetic nanostructures. Increasing Mn concentration leads to higher columns densities while diameters remain nearly unchanged. For higher growth temperatures, the nanocolumns mean diameter increases and their size distribution widens. Moreover the 4 independent magnetic nanostructures percolate into a single magnetic nanocolumn. Some columns are ferromagnetic even if Curie temperatures remain quite low. In this regime, increasing Mn concentration leads to larger columns while their density remains nearly the same. In parallel, Ge$_{3}$Mn$_{5}$ nanoclusters start to form in the film with their $c$-axis perpendicular to the film plane. In both temperature regimes, the Mn incorporation mechanism in the nanocolumns and/or in the matrix changes above 5 \% of Mn and nanocolumns exhibit an isotropic magnetic behaviour due to the competing effects of out-of-plane shape anisotropy and in-plane magnetoelastic coupling. Finally for a narrow range of growth temperatures around 130$^{\circ}$C, nanocolumns exhibit Curie temperatures higher than 400 K. Our goal is now to investigate the crystalline structure inside the nanocolumns, in particular the position of Mn atoms in the distorted diamond structure, which is essential to understand magnetic and future transport properties in Ge$_{1-x}$Mn$_{x}$ films.

\section{Aknowledgements}
The authors would like to thank Dr. F. Rieutord for grazing incidence x-ray diffraction measurements performed on the GMT station of BM32 beamline at the European Synchrotron Radiation Facility.

","['Spin injection into non magnetic semiconductors, or electrical manipulation of carrier induced magnetism in magnetic semiconductors.']",5917,multifieldqa_en,en,,f9b9a553107eed5b994a9aaf07e8ccac487feaa8ff108ec6,"Spin injection into non magnetic semiconductors, or electrical manipulation of carrier induced magnetism in magnetic semiconductors.",132
What are the symptoms of vitamin K deficiency?,"Vitamin K - Wikipedia
(Redirected from Vitamin k)
This article needs more medical references for verification or relies too heavily on primary sources. Please review the contents of the article and add the appropriate references if you can. Unsourced or poorly sourced material may be challenged and removed. (November 2015)
This article is about the family of vitamers. For vitamin K1 the form usually used as a supplement, see Phytomenadione.
Vitamin K structures. MK-4 and MK-7 are both subtypes of K2.
Vitamin K deficiency, Warfarin overdose
Vitamin K is a group of structurally similar, fat-soluble vitamins the human body requires for complete synthesis of certain proteins that are prerequisites for blood coagulation and which the body also needs for controlling binding of calcium in bones and other tissues. The vitamin K-related modification of the proteins allows them to bind calcium ions, which they cannot do otherwise. Without vitamin K, blood coagulation is seriously impaired, and uncontrolled bleeding occurs. Low levels of vitamin K also weaken bones and promote calcification of arteries and other soft tissues[citation needed].
Chemically, the vitamin K family comprises 2-methyl-1,4-naphthoquinone (3-) derivatives. Vitamin K includes two natural vitamers: vitamin K1 and vitamin K2.[1] Vitamin K2, in turn, consists of a number of related chemical subtypes, with differing lengths of carbon side chains made of isoprenoid groups of atoms.
Vitamin K1, also known as phylloquinone, is made by plants, and is found in highest amounts in green leafy vegetables because it is directly involved in photosynthesis. It may be thought of as the plant form of vitamin K. It is active as a vitamin in animals and performs the classic functions of vitamin K, including its activity in the production of blood-clotting proteins. Animals may also convert it to vitamin K2.
Bacteria in the gut flora can also convert K1 into vitamin K2. In addition, bacteria typically lengthen the isoprenoid side chain of vitamin K2 to produce a range of vitamin K2 forms, most notably the MK-7 to MK-11 homologues of vitamin K2. All forms of K2 other than MK-4 can only be produced by bacteria, which use these forms in anaerobic respiration. The MK-7 and other bacterially derived forms of vitamin K2 exhibit vitamin K activity in animals, but MK-7's extra utility over MK-4, if any, is unclear and is a matter of investigation.
Three synthetic types of vitamin K are known: vitamins K3, K4, and K5. Although the natural K1 and all K2 homologues and synthetic K4 and K5 have proven nontoxic, the synthetic form K3 (menadione) has shown toxicity.[2]
1.2 Cardiovascular health
1.4 Coumarin poisoning
4.1 Conversion of vitamin K1 to vitamin K2
4.2 Vitamin K2
6 Absorption and dietary need
7 Dietary reference intake
10 Biochemistry
10.1 Function in animals
10.2 Gamma-carboxyglutamate proteins
10.3 Methods of assessment
10.4 Function in bacteria
11 Injection in newborns
11.3 Controversy
A review of 2014 concluded that there is positive evidence that monotherapy using MK-4, one of the forms of Vitamin K2, reduces fracture incidence in post-menopausal women with osteoporosis, and suggested further research on the combined use of MK-4 with bisphosphonates. In contrast, an earlier review article of 2013 concluded that there is no good evidence that vitamin K supplementation helps prevent osteoporosis or fractures in postmenopausal women.[3]
A Cochrane systematic review of 2006 suggested that supplementation with Vitamin K1 and with MK4 reduces bone loss; in particular, a strong effect of MK-4 on incident fractures among Japanese patients was emphasized.[4]
A review article of 2016 suggested to consider, as one of several measures for bone health, increasing the intake of foods rich in vitamins K1 and K2.[5]
Cardiovascular health[edit]
Adequate intake of vitamin K is associated with the inhibition of arterial calcification and stiffening,[6] but there have been few interventional studies and no good evidence that vitamin K supplementation is of any benefit in the primary prevention of cardiovascular disease.[7]
One 10-year population study, the Rotterdam Study, did show a clear and significant inverse relationship between the highest intake levels of menaquinone (mainly MK-4 from eggs and meat, and MK-8 and MK-9 from cheese) and cardiovascular disease and all-cause mortality in older men and women.[8]
Vitamin K has been promoted in supplement form with claims it can slow tumor growth; there is however no good medical evidence that supports such claims.[9]
Coumarin poisoning[edit]
Vitamin K is part of the suggested treatment regime for poisoning by rodenticide (coumarin poisoning).[10]
Although allergic reaction from supplementation is possible, no known toxicity is associated with high doses of the phylloquinone (vitamin K1) or menaquinone (vitamin K2) forms of vitamin K, so no tolerable upper intake level (UL) has been set.[11]
Blood clotting (coagulation) studies in humans using 45 mg per day of vitamin K2 (as MK-4)[12] and even up to 135 mg per day (45 mg three times daily) of K2 (as MK-4),[13] showed no increase in blood clot risk. Even doses in rats as high as 250 mg/kg, body weight did not alter the tendency for blood-clot formation to occur.[14]
Unlike the safe natural forms of vitamin K1 and vitamin K2 and their various isomers, a synthetic form of vitamin K, vitamin K3 (menadione), is demonstrably toxic at high levels. The U.S. FDA has banned this form from over-the-counter sale in the United States because large doses have been shown to cause allergic reactions, hemolytic anemia, and cytotoxicity in liver cells.[2]
Phylloquinone (K1)[15][16] or menaquinone (K2) are capable of reversing the anticoagulant activity of the anticoagulant warfarin (tradename Coumadin). Warfarin works by blocking recycling of vitamin K, so that the body and tissues have lower levels of active vitamin K, and thus a deficiency of vitamin K.
Supplemental vitamin K (for which oral dosing is often more active than injectable dosing in human adults) reverses the vitamin K deficiency caused by warfarin, and therefore reduces the intended anticoagulant action of warfarin and related drugs.[17] Sometimes small amounts of vitamin K are given orally to patients taking warfarin so that the action of the drug is more predictable.[17] The proper anticoagulant action of the drug is a function of vitamin K intake and drug dose, and due to differing absorption must be individualized for each patient.[citation needed] The action of warfarin and vitamin K both require two to five days after dosing to have maximum effect, and neither warfarin or vitamin K shows much effect in the first 24 hours after they are given.[18]
The newer anticoagulants dabigatran and rivaroxaban have different mechanisms of action that do not interact with vitamin K, and may be taken with supplemental vitamin K.[19][20]
Vitamin K2 (menaquinone). In menaquinone, the side chain is composed of a varying number of isoprenoid residues. The most common number of these residues is four, since animal enzymes normally produce menaquinone-4 from plant phylloquinone.
A sample of phytomenadione for injection, also called phylloquinone
The three synthetic forms of vitamin K are vitamins K3 (menadione), K4, and K5, which are used in many areas, including the pet food industry (vitamin K3) and to inhibit fungal growth (vitamin K5).[21]
Conversion of vitamin K1 to vitamin K2[edit]
Vitamin K1 (phylloquinone) – both forms of the vitamin contain a functional naphthoquinone ring and an aliphatic side chain. Phylloquinone has a phytyl side chain.
The MK-4 form of vitamin K2 is produced by conversion of vitamin K1 in the testes, pancreas, and arterial walls.[22] While major questions still surround the biochemical pathway for this transformation, the conversion is not dependent on gut bacteria, as it occurs in germ-free rats[23][24] and in parenterally-administered K1 in rats.[25][26] In fact, tissues that accumulate high amounts of MK-4 have a remarkable capacity to convert up to 90% of the available K1 into MK-4.[27][28] There is evidence that the conversion proceeds by removal of the phytyl tail of K1 to produce menadione as an intermediate, which is then condensed with an activated geranylgeranyl moiety (see also prenylation) to produce vitamin K2 in the MK-4 (menatetrione) form.[29]
Vitamin K2[edit]
Main article: Vitamin K2
Vitamin K2 (menaquinone) includes several subtypes. The two subtypes most studied are menaquinone-4 (menatetrenone, MK-4) and menaquinone-7 (MK-7).
Vitamin K1, the precursor of most vitamin K in nature, is a stereoisomer of phylloquinone, an important chemical in green plants, where it functions as an electron acceptor in photosystem I during photosynthesis. For this reason, vitamin K1 is found in large quantities in the photosynthetic tissues of plants (green leaves, and dark green leafy vegetables such as romaine lettuce, kale and spinach), but it occurs in far smaller quantities in other plant tissues (roots, fruits, etc.). Iceberg lettuce contains relatively little. The function of phylloquinone in plants appears to have no resemblance to its later metabolic and biochemical function (as ""vitamin K"") in animals, where it performs a completely different biochemical reaction.
Vitamin K (in animals) is involved in the carboxylation of certain glutamate residues in proteins to form gamma-carboxyglutamate (Gla) residues. The modified residues are often (but not always) situated within specific protein domains called Gla domains. Gla residues are usually involved in binding calcium, and are essential for the biological activity of all known Gla proteins.[30]
At this time[update], 17 human proteins with Gla domains have been discovered, and they play key roles in the regulation of three physiological processes:
Blood coagulation: prothrombin (factor II), factors VII, IX, and X, and proteins C, S, and Z[31]
Bone metabolism: osteocalcin, also called bone Gla protein (BGP), matrix Gla protein (MGP),[32] periostin,[33] and the recently discovered Gla-rich protein (GRP).[34][35]
Vascular biology: growth arrest-specific protein 6 (Gas6)[36]
Unknown function: proline-rich γ-carboxyglutamyl proteins (PRGPs) 1 and 2, and transmembrane γ-carboxy glutamyl proteins (TMGs) 3 and 4.[37]
Like other lipid-soluble vitamins (A, D and E), vitamin K is stored in the fatty tissue of the human body.
Absorption and dietary need[edit]
Previous theory held that dietary deficiency is extremely rare unless the small intestine was heavily damaged, resulting in malabsorption of the molecule. Another at-risk group for deficiency were those subject to decreased production of K2 by normal intestinal microbiota, as seen in broad spectrum antibiotic use.[38] Taking broad-spectrum antibiotics can reduce vitamin K production in the gut by nearly 74% in people compared with those not taking these antibiotics.[39] Diets low in vitamin K also decrease the body's vitamin K concentration.[40] Those with chronic kidney disease are at risk for vitamin K deficiency, as well as vitamin D deficiency, and particularly those with the apoE4 genotype.[41] Additionally, in the elderly there is a reduction in vitamin K2 production.[42]
The National Academy of Medicine (NAM) updated an estimate of what constitutes an adequate intake (AI) for vitamin K in 2001. The NAM does not distinguish between K1 and K2 – both are counted as vitamin K. At that time there was not sufficient evidence to set the more rigorous estimated average requirement (EAR) or recommended dietary allowance (RDA) given for most of the essential vitamins and minerals. The current daily AIs for vitamin K for adult women and men are 90 μg and 120 μg respectively. The AI for pregnancy and lactation is 90 μg. For infants up to 12 months the AI is 2–2.5 μg, and for children aged 1 to 18 years the AI increases with age from 30 to 75 μg. As for safety, the FNB also sets tolerable upper intake levels (known as ULs) for vitamins and minerals when evidence is sufficient. In the case of vitamin K no UL is set, as evidence for adverse effects is not sufficient. Collectively EARs, RDAs, AIs and ULs are referred to as dietary reference intakes.[43] The European Food Safety Authority reviewed the same safety question and did not set an UL.[44]
For U.S. food and dietary supplement labeling purposes, the amount in a serving is expressed as a percentage of daily value (%DV). For vitamin K labeling purposes the daily value was 80 μg, but as of May 2016 it has been revised upwards to 120 μg. A table of the pre-change adult daily values is provided at reference daily intake. Food and supplement companies have until 28 July 2018 to comply with the change.
See also: Vitamin K2 § Dietary sources
K1 (μg)[45]
Kale, cooked
Collards, cooked
Collards, raw
Swiss chard, cooked
Swiss chard, raw
Turnip greens, raw
Romaine lettuce, raw
Table from ""Important information to know when you are taking: Warfarin (Coumadin) and Vitamin K"", Clinical Center, National Institutes of Health Drug Nutrient Interaction Task Force.[46]
Vitamin K1 is found chiefly in leafy green vegetables such as dandelion greens (which contain 778.4 μg per 100 g, or 741% of the recommended daily amount), spinach, swiss chard, lettuce and Brassica vegetables (such as cabbage, kale, cauliflower, broccoli, and brussels sprouts) and often the absorption is greater when accompanied by fats such as butter or oils; some fruits, such as avocados, kiwifruit and grapes, are also high in vitamin K. By way of reference, two tablespoons of parsley contains 153% of the recommended daily amount of vitamin K.[47] Some vegetable oils, notably soybean oil, contain vitamin K, but at levels that would require relatively large calorie consumption to meet the USDA-recommended levels.[48] colonic bacteria synthesize a significant portion of humans' vitamin K needs; newborns often receive a vitamin K shot at birth to tide them over until their colons become colonized at five to seven days of age from the consumption of breast milk.
The tight binding of vitamin K1 to thylakoid membranes in chloroplasts makes it less bioavailable. For example, cooked spinach has a 5% bioavailability of phylloquinone, however, fat added to it increases bioavailability to 13% due to the increased solubility of vitamin K in fat.[49]
Main article: Vitamin K deficiency
Average diets are usually not lacking in vitamin K, and primary deficiency is rare in healthy adults. Newborn infants are at an increased risk of deficiency. Other populations with an increased prevalence of vitamin K deficiency include those who suffer from liver damage or disease (e.g. alcoholics), cystic fibrosis, or inflammatory bowel diseases, or have recently had abdominal surgeries. Secondary vitamin K deficiency can occur in people with bulimia, those on stringent diets, and those taking anticoagulants. Other drugs associated with vitamin K deficiency include salicylates, barbiturates, and cefamandole, although the mechanisms are still unknown. Vitamin K1 deficiency can result in coagulopathy, a bleeding disorder.[50]Symptoms of K1 deficiency include anemia, bruising, nosebleeds and bleeding of the gums in both sexes, and heavy menstrual bleeding in women.
Osteoporosis[51][52] and coronary heart disease[53][54] are strongly associated with lower levels of K2 (menaquinone). Vitamin K2 (as menaquinones MK-4 through MK-10) intake level is inversely related to severe aortic calcification and all-cause mortality.[8]
Function in animals[edit]
Mechanism of action of vitamin K1.
The function of vitamin K2 in the animal cell is to add a carboxylic acid functional group to a glutamate (Glu) amino acid residue in a protein, to form a gamma-carboxyglutamate (Gla) residue. This is a somewhat uncommon posttranslational modification of the protein, which is then known as a ""Gla protein"". The presence of two −COOH (carboxylic acid) groups on the same carbon in the gamma-carboxyglutamate residue allows it to chelate calcium ions. The binding of calcium ions in this way very often triggers the function or binding of Gla-protein enzymes, such as the so-called vitamin K-dependent clotting factors discussed below.
Within the cell, vitamin K undergoes electron reduction to a reduced form called vitamin K hydroquinone, catalyzed by the enzyme vitamin K epoxide reductase (VKOR).[55] Another enzyme then oxidizes vitamin K hydroquinone to allow carboxylation of Glu to Gla; this enzyme is called gamma-glutamyl carboxylase[56][57] or the vitamin K-dependent carboxylase. The carboxylation reaction only proceeds if the carboxylase enzyme is able to oxidize vitamin K hydroquinone to vitamin K epoxide at the same time. The carboxylation and epoxidation reactions are said to be coupled. Vitamin K epoxide is then reconverted to vitamin K by VKOR. The reduction and subsequent reoxidation of vitamin K coupled with carboxylation of Glu is called the vitamin K cycle.[58] Humans are rarely deficient in vitamin K1 because, in part, vitamin K1 is continuously recycled in cells.[59]
Warfarin and other 4-hydroxycoumarins block the action of VKOR.[60] This results in decreased concentrations of vitamin K and vitamin K hydroquinone in tissues, such that the carboxylation reaction catalyzed by the glutamyl carboxylase is inefficient. This results in the production of clotting factors with inadequate Gla. Without Gla on the amino termini of these factors, they no longer bind stably to the blood vessel endothelium and cannot activate clotting to allow formation of a clot during tissue injury. As it is impossible to predict what dose of warfarin will give the desired degree of clotting suppression, warfarin treatment must be carefully monitored to avoid overdose.
Gamma-carboxyglutamate proteins[edit]
Main article: Gla domain
The following human Gla-containing proteins (""Gla proteins"") have been characterized to the level of primary structure: blood coagulation factors II (prothrombin), VII, IX, and X, anticoagulant proteins C and S, and the factor X-targeting protein Z. The bone Gla protein osteocalcin, the calcification-inhibiting matrix Gla protein (MGP), the cell growth regulating growth arrest specific gene 6 protein (Gas6), and the four transmembrane Gla proteins (TMGPs), the function of which is at present unknown. Gas6 can function as a growth factor to activate the Axl receptor tyrosine kinase and stimulate cell proliferation or prevent apoptosis in some cells. In all cases in which their function was known, the presence of the Gla residues in these proteins turned out to be essential for functional activity.
Gla proteins are known to occur in a wide variety of vertebrates: mammals, birds, reptiles, and fish. The venom of a number of Australian snakes acts by activating the human blood-clotting system. In some cases, activation is accomplished by snake Gla-containing enzymes that bind to the endothelium of human blood vessels and catalyze the conversion of procoagulant clotting factors into activated ones, leading to unwanted and potentially deadly clotting.
Another interesting class of invertebrate Gla-containing proteins is synthesized by the fish-hunting snail Conus geographus.[61] These snails produce a venom containing hundreds of neuroactive peptides, or conotoxins, which is sufficiently toxic to kill an adult human. Several of the conotoxins contain two to five Gla residues.[62]
Methods of assessment[edit]
Vitamin K status can be assessed by:
The prothrombin time (PT) test measures the time required for blood to clot. A blood sample is mixed with citric acid and put in a fibrometer; delayed clot formation indicates a deficiency. This test is insensitive to mild deficiency, as the values do not change until the concentration of prothrombin in the blood has declined by at least 50%.[63]
Undercarboxylated prothrombin (PIVKA-II); in a study of 53 newborns, found ""PT (prothrombin time) is a less sensitive marker than PIVKA II"",[64] and as indicated above, PT is unable to detect subclinical deficiencies that can be detected with PIVKA-II testing.
Plasma phylloquinone was found to be positively correlated with phylloquinone intake in elderly British women, but not men,[65] but an article by Schurgers et al. reported no correlation between FFQ[further explanation needed] and plasma phylloquinone.[66]
Urinary γ-carboxyglutamic acid responds to changes in dietary vitamin K intake. Several days are required before any change can be observed. In a study by Booth et al., increases of phylloquinone intakes from 100 μg to between 377 and 417 μg for five days did not induce a significant change. Response may be age-specific.[67]
Undercarboxylated osteocalcin (UcOc) levels have been inversely correlated with stores of vitamin K[68] and bone strength in developing rat tibiae. Another study following 78 post-menopausal Korean women found a supplement regimen of vitamins K and D, and calcium, but not a regimen of vitamin D and calcium, was inversely correlated with reduced UcOc levels.[69]
Function in bacteria[edit]
Many bacteria, such as Escherichia coli found in the large intestine, can synthesize vitamin K2 (menaquinone-7 or MK-7, up to MK-11),[70] but not vitamin K1 (phylloquinone). In these bacteria, menaquinone transfers two electrons between two different small molecules, during oxygen-independent metabolic energy production processes (anaerobic respiration).[71] For example, a small molecule with an excess of electrons (also called an electron donor) such as lactate, formate, or NADH, with the help of an enzyme, passes two electrons to menaquinone. The menaquinone, with the help of another enzyme, then transfers these two electrons to a suitable oxidant, such fumarate or nitrate (also called an electron acceptor). Adding two electrons to fumarate or nitrate converts the molecule to succinate or nitrite plus water, respectively.
Some of these reactions generate a cellular energy source, ATP, in a manner similar to eukaryotic cell aerobic respiration, except the final electron acceptor is not molecular oxygen, but fumarate or nitrate. In aerobic respiration, the final oxidant is molecular oxygen (O2), which accepts four electrons from an electron donor such as NADH to be converted to water. E. coli, as facultative anaerobes, can carry out both aerobic respiration and menaquinone-mediated anaerobic respiration.
Injection in newborns[edit]
The blood clotting factors of newborn babies are roughly 30–60% that of adult values; this may be due to the reduced synthesis of precursor proteins and the sterility of their guts. Human milk contains 1–4 μg/L of vitamin K1, while formula-derived milk can contain up to 100 μg/L in supplemented formulas. Vitamin K2 concentrations in human milk appear to be much lower than those of vitamin K1. Occurrence of vitamin K deficiency bleeding in the first week of the infant's life is estimated at 0.25–1.7%, with a prevalence of 2–10 cases per 100,000 births.[72] Premature babies have even lower levels of the vitamin, so they are at a higher risk from this deficiency.
Bleeding in infants due to vitamin K deficiency can be severe, leading to hospitalization, blood transfusions, brain damage, and death. Supplementation can prevent most cases of vitamin K deficiency bleeding in the newborn. Intramuscular administration is more effective in preventing late vitamin K deficiency bleeding than oral administration.[73][74]
As a result of the occurrences of vitamin K deficiency bleeding, the Committee on Nutrition of the American Academy of Pediatrics has recommended 0.5–1 mg of vitamin K1 be administered to all newborns shortly after birth.[74]
In the UK vitamin K supplementation is recommended for all newborns within the first 24 hours.[75] This is usually given as a single intramuscular injection of 1 mg shortly after birth but as a second-line option can be given by three oral doses over the first month.[76]
Controversy arose in the early 1990s regarding this practice, when two studies suggested a relationship between parenteral administration of vitamin K and childhood cancer,[77] however, poor methods and small sample sizes led to the discrediting of these studies, and a review of the evidence published in 2000 by Ross and Davies found no link between the two.[78] Doctors reported emerging concerns in 2013,[79] after treating children for serious bleeding problems. They cited lack-of newborn vitamin K administration, as the reason that the problems occurred, and recommended that breastfed babies could have an increased risk unless they receive a preventative dose.
In the early 1930s, Danish scientist Henrik Dam investigated the role of cholesterol by feeding chickens a cholesterol-depleted diet.[80] He initially replicated experiments reported by scientists at the Ontario Agricultural College (OAC).[81] McFarlane, Graham and Richardson, working on the chick feed program at OAC, had used chloroform to remove all fat from chick chow. They noticed that chicks fed only fat-depleted chow developed hemorrhages and started bleeding from tag sites.[82] Dam found that these defects could not be restored by adding purified cholesterol to the diet. It appeared that – together with the cholesterol – a second compound had been extracted from the food, and this compound was called the coagulation vitamin. The new vitamin received the letter K because the initial discoveries were reported in a German journal, in which it was designated as Koagulationsvitamin. Edward Adelbert Doisy of Saint Louis University did much of the research that led to the discovery of the structure and chemical nature of vitamin K.[83] Dam and Doisy shared the 1943 Nobel Prize for medicine for their work on vitamin K (K1 and K2) published in 1939. Several laboratories synthesized the compound(s) in 1939.[84]
For several decades, the vitamin K-deficient chick model was the only method of quantifying vitamin K in various foods: the chicks were made vitamin K-deficient and subsequently fed with known amounts of vitamin K-containing food. The extent to which blood coagulation was restored by the diet was taken as a measure for its vitamin K content. Three groups of physicians independently found this: Biochemical Institute, University of Copenhagen (Dam and Johannes Glavind), University of Iowa Department of Pathology (Emory Warner, Kenneth Brinkhous, and Harry Pratt Smith), and the Mayo Clinic (Hugh Butt, Albert Snell, and Arnold Osterberg).[85]
The first published report of successful treatment with vitamin K of life-threatening hemorrhage in a jaundiced patient with prothrombin deficiency was made in 1938 by Smith, Warner, and Brinkhous.[86]
The precise function of vitamin K was not discovered until 1974, when three laboratories (Stenflo et al.,[87] Nelsestuen et al.,[88] and Magnusson et al.[89]) isolated the vitamin K-dependent coagulation factor prothrombin (factor II) from cows that received a high dose of a vitamin K antagonist, warfarin. It was shown that, while warfarin-treated cows had a form of prothrombin that contained 10 glutamate (Glu) amino acid residues near the amino terminus of this protein, the normal (untreated) cows contained 10 unusual residues that were chemically identified as γ-carboxyglutamate (Gla). The extra carboxyl group in Gla made clear that vitamin K plays a role in a carboxylation reaction during which Glu is converted into Gla.
The biochemistry of how vitamin K is used to convert Glu to Gla has been elucidated over the past thirty years in academic laboratories throughout the world.
^ ""Vitamin K Overview"". University of Maryland Medical Center. ^ a b Higdon, Jane (Feb 2008). ""Vitamin K"". Linus Pauling Institute, Oregon State University. Retrieved 12 Apr 2008. ^ Hamidi, M. S.; Gajic-Veljanoski, O.; Cheung, A. M. (2013). ""Vitamin K and bone health"". Journal of Clinical Densitometry (Review). 16 (4): 409–413. doi:10.1016/j.jocd.2013.08.017. PMID 24090644. ^ Cockayne, S.; Adamson, J.; Lanham-New, S.; Shearer, M. J.; Gilbody, S; Torgerson, D. J. (Jun 2006). ""Vitamin K and the prevention of fractures: systematic review and meta-analysis of randomized controlled trials"". Archives of Internal Medicine (Review). 166 (12): 1256–1261. doi:10.1001/archinte.166.12.1256. PMID 16801507. ^ O'Keefe, J. H.; Bergman, N.; Carrera Bastos, P.; Fontes Villalba, M.; Di Nicolantonio, J. J.; Cordain, L. (2016). ""Nutritional strategies for skeletal and cardiovascular health: hard bones, soft arteries, rather than vice versa"". Open Heart (Review). 3 (1): e000325. doi:10.1136/openhrt-2015-000325. PMC 4809188. PMID 27042317. ^ Maresz, K. (Feb 2015). ""Proper Calcium Use: Vitamin K2 as a Promoter of Bone and Cardiovascular Health"". Integrative Medicine (Review). 14 (1): 34–39. PMC 4566462. PMID 26770129. ^ Hartley, L.; Clar, C.; Ghannam, O.; Flowers, N.; Stranges, S.; Rees, K. (Sep 2015). ""Vitamin K for the primary prevention of cardiovascular disease"". The Cochrane Database of Systematic Reviews (Systematic review). 9 (9): CD011148. doi:10.1002/14651858.CD011148.pub2. PMID 26389791. ^ a b Geleijnse, J. M.; Vermeer, C.; Grobbee, D. E.; Schurgers, L. J.; Knapen, M. H.; van der Meer, I. M.; Hofman, A.; Witteman, J. C. (Nov 2004). ""Dietary intake of menaquinone is associated with a reduced risk of coronary heart disease: the Rotterdam Study"". Journal of Nutrition. 134 (11): 3100–3105. PMID 15514282. ^ Ades, T. B., ed. (2009). ""Vitamin K"". American Cancer Society Complete Guide to Complementary and Alternative Cancer Therapies (2nd ed.). American Cancer Society. pp. 558–563. ISBN 978-0-944235-71-3. ^ Lung, D. (Dec 2015). Tarabar, A., ed. ""Rodenticide Toxicity Treatment & Management"". Medscape. WebMD. ^ Rasmussen, S. E.; Andersen, N. L.; Dragsted, L. O.; Larsen, J. C. (Mar 2006). ""A safe strategy for addition of vitamins and minerals to foods"". European Journal of Nutrition. 45 (3): 123–135. doi:10.1007/s00394-005-0580-9. PMID 16200467. ^ Ushiroyama, T.; Ikeda, A.; Ueki, M (Mar 2002). ""Effect of continuous combined therapy with vitamin K2 and vitamin D3 on bone mineral density and coagulofibrinolysis function in postmenopausal women"". Maturitas. 41 (3): 211–221. doi:10.1016/S0378-5122(01)00275-4. PMID 11886767. ^ Asakura, H.; Myou, S.; Ontachi, Y.; Mizutani, T.; Kato, M.; Saito, M.; Morishita, E.; Yamazaki, M.; Nakao, S. (Dec 2001). ""Vitamin K administration to elderly patients with osteoporosis induces no hemostatic activation, even in those with suspected vitamin K deficiency"". Osteoporosis International. 12 (12): 996–1000. doi:10.1007/s001980170007. PMID 11846334. ^ Ronden, J. E.; Groenen-van Dooren, M. M.; Hornstra, G.; Vermeer, C. (Jul 1997). ""Modulation of arterial thrombosis tendency in rats by vitamin K and its side chains"". Atherosclerosis. 132 (1): 61–67. doi:10.1016/S0021-9150(97)00087-7. PMID 9247360. ^ Ansell, J.; Hirsh, J.; Poller, L.; Bussey, H.; Jacobson, A.; Hylek, E (Sep 2004). ""The pharmacology and management of the vitamin K antagonists: the Seventh ACCP Conference on Antithrombotic and Thrombolytic Therapy"". Chest. 126 (3 Suppl.): 204S–233S. doi:10.1378/chest.126.3_suppl.204S. PMID 15383473. ^ Crowther, M. A.; Douketis, J. D.; Schnurr, T.; Steidl, L.; Mera, V.; Ultori, C.; Venco, A.; Ageno, W. (Aug 2002). ""Oral vitamin K lowers the international normalized ratio more rapidly than subcutaneous vitamin K in the treatment of warfarin-associated coagulopathy. A randomized, controlled trial"". Annals of Internal Medicine. 137 (4): 251–254. doi:10.7326/0003-4819-137-4-200208200-00009. PMID 12186515. ^ a b ""Important Information to Know When You Are Taking: Warfarin (Coumadin) and Vitamin K"" (PDF). National Institute of Health Clinical Center Drug-Nutrient Interaction Task Force. Retrieved 17 Apr 2015. ^ ""Guidelines For Warfarin Reversal With Vitamin K"" (PDF). American Society of Health-System Pharmacists. Retrieved 17 Apr 2015. ^ ""Pradaxa Drug Interactions"". Pradaxapro.com. 19 Mar 2012. Retrieved 21 Apr 2013. ^ Bauersachs, R.; Berkowitz, S. D.; Brenner, B.; Buller, H. R.; Decousus, H.; Gallus, A. S.; Lensing, A. W.; Misselwitz, F.; Prins, M. H.; Raskob, G. E.; Segers, A.; Verhamme, P.; Wells, P.; Agnelli, G.; Bounameaux, H.; Cohen, A.; Davidson, B. L.; Piovella, F.; Schellong, S. (Dec 2010). ""Oral rivaroxaban for symptomatic venous thromboembolism"". New England Journal of Medicine. 363 (26): 2499–2510. doi:10.1056/NEJMoa1007903. PMID 21128814. ^ McGee, W. (1 Feb 2007). ""Vitamin K"". MedlinePlus. Retrieved 2 Apr 2009. ^ Shearer, M. J.; Newman, P. (Oct 2008). ""Metabolism and cell biology of vitamin K"". Thrombosis and Haemostasis. 100 (4): 530–547. doi:10.1160/TH08-03-0147. PMID 18841274. ^ Davidson, R. T.; Foley, A. L.; Engelke, J. A.; Suttie, J. W. (Feb 1998). ""Conversion of dietary phylloquinone to tissue menaquinone-4 in rats is not dependent on gut bacteria"". Journal of Nutrition. 128 (2): 220–223. PMID 9446847. ^ Ronden, J. E.; Drittij-Reijnders, M. J.; Vermeer, C.; Thijssen, H. H. (Jan 1998). ""Intestinal flora is not an intermediate in the phylloquinone–menaquinone-4 conversion in the rat"". Biochimica et Biophysica Acta. 1379 (1): 69–75. doi:10.1016/S0304-4165(97)00089-5. PMID 9468334. ^ Thijssen, H. .H.; Drittij-Reijnders, M. J. (Sep 1994). ""Vitamin K distribution in rat tissues: dietary phylloquinone is a source of tissue menaquinone-4"". The British Journal of Nutrition. 72 (3): 415–425. doi:10.1079/BJN19940043. PMID 7947656. ^ Will, B. H.; Usui, Y.; Suttie, J. W. (Dec 1992). ""Comparative metabolism and requirement of vitamin K in chicks and rats"". Journal of Nutrition. 122 (12): 2354–2360. PMID 1453219. ^ Davidson, R. T.; Foley, A. L.; Engelke, J. A.; Suttie, J. W. (Feb 1998). ""Conversion of dietary phylloquinone to tissue menaquinone-4 in rats is not dependent on gut bacteria"". Journal of Nutrition. 128 (2): 220–223. PMID 9446847. ^ Ronden, J. E.; Drittij-Reijnders, M. J.; Vermeer, C.; Thijssen, H. H. (Jan 1998). ""Intestinal flora is not an intermediate in the phylloquinone-menaquinone-4 conversion in the rat"". Biochimica et Biophysica Acta. 1379 (1): 69–75. doi:10.1016/S0304-4165(97)00089-5. PMID 9468334. ^ Al Rajabi, Ala (2011). The Enzymatic Conversion of Phylloquinone to Menaquinone-4 (PhD thesis). Tufts University, Friedman School of Nutrition Science and Policy. ^ Furie, B.; Bouchard, B. A.; Furie, B. C. (Mar 1999). ""Vitamin K-dependent biosynthesis of gamma-carboxyglutamic acid"". Blood. 93 (6): 1798–1808. PMID 10068650. ^ Mann, K. G. (Aug 1999). ""Biochemistry and physiology of blood coagulation"". Thrombosis and Haemostasis. 82 (2): 165–174. PMID 10605701. ^ Price, P. A. (1988). ""Role of vitamin-K-dependent proteins in bone metabolism"". Annual Review of Nutrition. 8: 565–583. doi:10.1146/annurev.nu.08.070188.003025. PMID 3060178. ^ Coutu, D. L.; Wu, J. H.; Monette, A.; Rivard, G. E.; Blostein, M. D.; Galipeau, J (Jun 2008). ""Periostin, a member of a novel family of vitamin K-dependent proteins, is expressed by mesenchymal stromal cells"". Journal of Biological Chemistry. 283 (26): 17991–18001. doi:10.1074/jbc.M708029200. PMID 18450759. ^ Viegas, C. S.; Simes, D. C.; Laizé, V.; Williamson, M. K.; Price, P. A.; Cancela, M. L. (Dec 2008). ""Gla-rich protein (GRP), a new vitamin K-dependent protein identified from sturgeon cartilage and highly conserved in vertebrates"". Journal of Biological Chemistry. 283 (52): 36655–36664. doi:10.1074/jbc.M802761200. PMC 2605998. PMID 18836183. ^ Viegas, C. S.; Cavaco, S.; Neves, P. L.; Ferreira, A.; João, A.; Williamson, M. K.; Price, P. A.; Cancela, M. L.; Simes, D. C. (Dec 2009). ""Gla-rich protein is a novel vitamin K-dependent protein present in serum that accumulates at sites of pathological calcifications"". American Journal of Pathology. 175 (6): 2288–2298. doi:10.2353/ajpath.2009.090474. PMC 2789615. PMID 19893032. ^ Hafizi, S.; Dahlbäck, B. (Dec 2006). ""Gas6 and protein S. Vitamin K-dependent ligands for the Axl receptor tyrosine kinase subfamily"". The FEBS Journal. 273 (23): 5231–5244. doi:10.1111/j.1742-4658.2006.05529.x. PMID 17064312. ^ Kulman, J. D.; Harris, J. E.; Xie, L.; Davie, E. W. (May 2007). ""Proline-rich Gla protein 2 is a cell-surface vitamin K-dependent protein that binds to the transcriptional coactivator Yes-associated protein"". Proceedings of the National Academy of Sciences of the United States of America. 104 (21): 8767–8772. doi:10.1073/pnas.0703195104. PMC 1885577. PMID 17502622. ^ ""Vitamin K"". MedlinePlus. US National Library of Medicine, National Institutes of Health. Sep 2016. Retrieved 26 May 2009. ^ Conly, J; Stein, K. (Dec 1994). ""Reduction of vitamin K2 concentrations in human liver associated with the use of broad spectrum antimicrobials"". Clinical and Investigative Medicine. 17 (6): 531–539. PMID 7895417. ^ Ferland, G.; Sadowski, J. A.; O'Brien, M. E. (Apr 1993). ""Dietary induced subclinical vitamin K deficiency in normal human subjects"". Journal of Clinical Investigation. 91 (4): 1761–1768. doi:10.1172/JCI116386. PMC 288156. PMID 8473516. ^ Holden, R. M.; Morton, A. R.; Garland, J. S.; Pavlov, A.; Day, A. G.; Booth, S. L. (Apr 2010). ""Vitamins K and D status in stages 3-5 chronic kidney disease"". Clinical Journal of the American Society of Nephrology. 5 (4): 590–597. doi:10.2215/CJN.06420909. PMC 2849681. PMID 20167683. ^ Hodges, S. J.; Pilkington, M. J.; Shearer, M. J.; Bitensky, L.; Chayen, J (Jan 1990). ""Age-related changes in the circulating levels of congeners of vitamin K2, menaquinone-7 and menaquinone-8"". Clinical Science. 78 (1): 63–66. PMID 2153497. ^ ""Vitamin K"". Dietary Reference Intakes for Vitamin A, Vitamin K, Arsenic, Boron, Chromium, Copper, Iodine, Iron, Manganese, Molybdenum, Nickel, Silicon, Vanadium, and Zinc (PDF). National Academy Press. 2001. p. 162–196. ^ Tolerable Upper Intake Levels For Vitamins And Minerals (PDF), European Food Safety Authority, 2006 ^ a b Rhéaume-Bleue, p. 42
^ ""Important information to know when you are taking: Warfarin (Coumadin) and Vitamin K"" (PDF). National Institutes of Health Clinical Center. ^ ""Nutrition Facts and Information for Parsley, raw"". Nutritiondata.com. Retrieved 21 Apr 2013. ^ ""Nutrition facts, calories in food, labels, nutritional information and analysis"". Nutritiondata.com. 13 Feb 2008. Retrieved 21 Apr 2013. ^ ""Vitamin K"". Vivo.colostate.edu. 2 Jul 1999. Retrieved 21 Apr 2013. ^ ""Vitamin K"". Micronutrient Data Centre. ^ Ikeda, Y.; Iki, M.; Morita, A.; Kajita, E.; Kagamimori, S.; Kagawa, Y.; Yoneshima, H. (May 2006). ""Intake of fermented soybeans, natto, is associated with reduced bone loss in postmenopausal women: Japanese Population-Based Osteoporosis (JPOS) Study"". Journal of Nutrition. 136 (5): 1323–1328. PMID 16614424. ^ Katsuyama, H.; Ideguchi, S.; Fukunaga, M.; Saijoh, K.; Sunami, S. (Jun 2002). ""Usual dietary intake of fermented soybeans (Natto) is associated with bone mineral density in premenopausal women"". Journal of Nutritional Science and Vitaminology. 48 (3): 207–215. doi:10.3177/jnsv.48.207. PMID 12350079. ^ Sano, M.; Fujita, H.; Morita, I.; Uematsu, H.; Murota, S. (Dec 1999). ""Vitamin K2 (menatetrenone) induces iNOS in bovine vascular smooth muscle cells: no relationship between nitric oxide production and gamma-carboxylation"". Journal of Nutritional Science and Vitaminology. 45 (6): 711–723. doi:10.3177/jnsv.45.711. PMID 10737225. ^ Gast, G. C ; de Roos, N. M.; Sluijs, I.; Bots, M. L.; Beulens, J. W.; Geleijnse, J. M.; Witteman, J. C.; Grobbee, D. E.; Peeters, P. H.; van der Schouw, Y. T. (Sep 2009). ""A high menaquinone intake reduces the incidence of coronary heart disease"". Nutrition, Metabolism, and Cardiovascular Diseases. 19 (7): 504–510. doi:10.1016/j.numecd.2008.10.004. PMID 19179058. ^ Oldenburg, J.; Bevans, C. G.; Müller, C. R.; Watzka, M. (2006). ""Vitamin K epoxide reductase complex subunit 1 (VKORC1): the key protein of the vitamin K cycle"". Antioxidants & Redox Signaling. 8 (3–4): 347–353. doi:10.1089/ars.2006.8.347. PMID 16677080. ^ Suttie, J. W. (1985). ""Vitamin K-dependent carboxylase"". Annual Review of Biochemistry. 54: 459–477. doi:10.1146/annurev.bi.54.070185.002331. PMID 3896125. ^ Presnell, S. R.; Stafford, D. W. (Jun 2002). ""The vitamin K-dependent carboxylase"". Thrombosis and Haemostasis. 87 (6): 937–946. PMID 12083499. ^ Stafford, D. W. (Aug 2005). ""The vitamin K cycle"". Journal of Thrombosis and Haemostasis. 3 (8): 1873–1878. doi:10.1111/j.1538-7836.2005.01419.x. PMID 16102054. ^ Rhéaume-Bleue, p. 79.
^ Whitlon, D. S.; Sadowski, J. A.; Suttie, J. W. (Apr 1978). ""Mechanism of coumarin action: significance of vitamin K epoxide reductase inhibition"". Biochemistry. 17 (8): 1371–1377. doi:10.1021/bi00601a003. PMID 646989. ^ Terlau, H.; Olivera, B. M. (Jan 2004). ""Conus venoms: a rich source of novel ion channel-targeted peptides"". Physiological Reviews. 84 (1): 41–68. doi:10.1152/physrev.00020.2003. PMID 14715910. ^ Buczek, O.; Bulaj, G.; Olivera, BM (Dec 2005). ""Conotoxins and the posttranslational modification of secreted gene products"". Cellular and Molecular Life Sciences. 62 (24): 3067–3079. doi:10.1007/s00018-005-5283-0. PMID 16314929. ^ ""Prothrombin Time"". WebMD. ^ Dituri, F.; Buonocore, G.; Pietravalle, A.; Naddeo, F.; Cortesi, M; Pasqualetti, P; Tataranno M. L.; R., Agostino (Sep 2012). ""PIVKA-II plasma levels as markers of subclinical vitamin K deficiency in term infants"". Journal of Maternal, Fetal & Neonatal Medicine. 25 (9): 1660–1663. doi:10.3109/14767058.2012.657273. PMID 22280352. ^ Thane, C. W.; Bates, C. J.; Shearer, M. J.; Unadkat, N; Harrington, D. J.; Paul, A. A.; Prentice, A.; Bolton-Smith, C. (Jun 2002). ""Plasma phylloquinone (vitamin K1) concentration and its relationship to intake in a national sample of British elderly people"". British Journal of Nutrition. 87 (6): 615–622. doi:10.1079/BJNBJN2002582. PMID 12067432. ^ McKeown, N. M.; Jacques, P. F.; Gundberg, C. M.; Peterson, J. W.; Tucker, K. L.; Kiel, D. P.; Wilson, P. W.; Booth, SL (Jun 2002). ""Dietary and nondietary determinants of vitamin K biochemical measures in men and women"" (PDF). Journal of Nutrition. 132 (6): 1329–1334. PMID 12042454. ^ Yamano, M.; Yamanaka, Y.; Yasunaga, K.; Uchida, K. (Sep 1989). ""Effect of vitamin K deficiency on urinary gamma-carboxyglutamic acid excretion in rats"". Nihon Ketsueki Gakkai Zasshi. 52 (6): 1078–1086. PMID 2588957. ^ Matsumoto, T.; Miyakawa, T.; Yamamoto, D. (Mar 2012). ""Effects of vitamin K on the morphometric and material properties of bone in the tibiae of growing rats"". Metabolism. 61 (3): 407–414. doi:10.1016/j.metabol.2011.07.018. PMID 21944271. ^ Je, S.-H.; Joo, N.-S.; Choi, B.-H.; Kim, K.-M.; Kim, B.-T.; Park, S.-B.; Cho, D.-Y.; Kim, K.-N.; Lee, D.-J. (Aug 2011). ""Vitamin K supplement along with vitamin D and calcium reduced serum concentration of undercarboxylated osteocalcin while increasing bone mineral density in Korean postmenopausal women over sixty-years-old"". Journal of Korean Medical Science. 26 (8): 1093–1098. doi:10.3346/jkms.2011.26.8.1093. PMC 3154347. PMID 21860562. ^ Bentley, R.; Meganathan, R. (Sep 1982). ""Biosynthesis of vitamin K (menaquinone) in bacteria"" (PDF). Microbiological Reviews. 46 (3): 241–280. PMC 281544. PMID 6127606. ^ Haddock, B. A.; Jones, C. W. (Mar 1977). ""Bacterial respiration"" (PDF). Bacteriological Reviews. 41 (1): 47–99. PMC 413996. PMID 140652. ^ Shearer, M. J. (Jan 1995). ""Vitamin K"". Lancet. 345 (8944): 229–234. doi:10.1016/S0140-6736(95)90227-9. PMID 7823718. ^ Greer, J. P.; Foerster, J.; Lukens, J. N.; Rodgers, G. M.; Paraskevas, F.; Glader, B. (eds.). Wintrobe's Clinical Hematology (11th ed.). Philadelphia, Pennsylvania: Lippincott, Williams and Wilkens. ^ a b American Academy of Pediatrics Committee on Fetus Newborn. (Jul 2003). ""Controversies concerning vitamin K and the newborn. American Academy of Pediatrics Committee on Fetus and Newborn"" (PDF). Pediatrics. 112 (1.1): 191–192. doi:10.1542/peds.112.1.191. PMID 12837888. ^ Logan, S.; Gilbert, R. (1998). ""Vitamin K For Newborn Babies"" (PDF). Department of Health. Retrieved 12 Oct 2014. ^ ""Postnatal care: Routine postnatal care of women and their babies [CG37]"". www.nice.org.uk. NICE. Jul 2006. Retrieved 12 Oct 2014. ^ Parker, L.; Cole, M.; Craft, A. W.; Hey, E. N. (1998). ""Neonatal vitamin K administration and childhood cancer in the north of England: retrospective case-control study"". BMJ (Clinical Research Edition). 316 (7126): 189–193. doi:10.1136/bmj.316.7126.189. PMC 2665412. PMID 9468683. ^ McMillan, D. D. (1997). ""Routine administration of vitamin K to newborns"". Paediatric Child Health. 2 (6): 429–431. ^ ""Newborns get rare disorder after parents refused shots"". Having four cases since February just at Vanderbilt was a little bit concerning to me ^ Dam, C. P. H. (1935). ""The Antihaemorrhagic Vitamin of the Chick: Occurrence And Chemical Nature"". Nature. 135 (3417): 652–653. doi:10.1038/135652b0. ^ Dam, C. P. H. (1941). ""The discovery of vitamin K, its biological functions and therapeutical application"" (PDF). Nobel Prize Laureate Lecture. ^ McAlister, V. C. (2006). ""Control of coagulation: a gift of Canadian agriculture"" (PDF). Clinical and Investigative Medicine. 29 (6): 373–377. ^ MacCorquodale, D. W.; Binkley, S. B.; Thayer, S. A.; Doisy, E. A. (1939). ""On the constitution of Vitamin K1"". Journal of the American Chemical Society. 61 (7): 1928–1929. doi:10.1021/ja01876a510. ^ Fieser, L. F. (1939). ""Synthesis of Vitamin K1"". Journal of the American Chemical Society. 61 (12): 3467–3475. doi:10.1021/ja01267a072. ^ Dam, C. P. H. (12 Dec 1946). ""The discovery of vitamin K, its biological functions and therapeutical application"" (PDF). Nobel Prize lecture. ^ Warner, E. D.; Brinkhous, K. M.; Smith, H. P. (1938). ""Bleeding Tendency of Obstructive Jaundice"". Proceedings of the Society of Experimental Biology and Medicine. 37 (4): 628–630. doi:10.3181/00379727-37-9668P. ^ Stenflo, J; Fernlund, P.; Egan, W.; Roepstorff, P. (Jul 1974). ""Vitamin K dependent modifications of glutamic acid residues in prothrombin"". Proceedings of the National Academy of Sciences of the United States of America. 71 (7): 2730–2733. doi:10.1073/pnas.71.7.2730. PMC 388542. PMID 4528109. ^ Nelsestuen, G. L.; Zytkovicz, T. H.; Howard, J. B. (Oct 1974). ""The mode of action of vitamin K. Identification of gamma-carboxyglutamic acid as a component of prothrombin"" (PDF). Journal of Biological Chemistry. 249 (19): 6347–6350. PMID 4214105. ^ Magnusson, S.; Sottrup-Jensen, L.; Petersen, T. E.; Morris, H. R.; Dell, A. (Aug 1974). ""Primary structure of the vitamin K-dependent part of prothrombin"". FEBS Letters. 44 (2): 189–193. doi:10.1016/0014-5793(74)80723-4. PMID 4472513. Bibliography[edit]
Rhéaume-Bleue, Kate (2012). Vitamin K2 and the Calcium Paradox. John Wiley & Sons, Canada. ISBN 1-118-06572-7. External links[edit]
""Vitamin K: Another Reason to Eat Your Greens"". v
TPP / ThDP (B1)
FMN, FAD (B2)
NAD+, NADH, NADP+, NADPH (B3)
Coenzyme A (B5)
PLP / P5P (B6)
THFA / H4FA, DHFA / H2FA, MTHF (B9)
AdoCbl, MeCbl (B12)
Phylloquinone (K1), Menaquinone (K2)
non-vitamins
Coenzyme B
Heme / Haem (A, B, C, O)
Molybdopterin/Molybdenum cofactor
THMPT / H4MPT
Fe2+, Fe3+
vitamins: see vitamins
Antihemorrhagics (B02)
(coagulation)
Phytomenadione (K1)
Menadione (K3)
intrinsic: IX/Nonacog alfa
VIII/Moroctocog alfa/Turoctocog alfa
extrinsic: VII/Eptacog alfa
common: X
II/Thrombin
I/Fibrinogen
XIII/Catridecacog
combinations: Prothrombin complex concentrate (II, VII, IX, X, protein C and S)
Carbazochrome
thrombopoietin receptor agonist (Romiplostim
Eltrombopag)
Tetragalacturonic acid hydroxymethylester
Epinephrine/Adrenalone
amino acids (Aminocaproic acid
Aminomethylbenzoic acid)
serpins (Aprotinin
Alfa1 antitrypsin
Camostat).","['Symptoms of vitamin K deficiency include anemia, bruising, nosebleeds, bleeding of the gums, and heavy menstrual bleeding in women.']",7146,multifieldqa_en,en,,ad753e5807afddae5d0a133de86b5224cb074edfcc9477bf,"Symptoms of vitamin K deficiency include anemia, bruising, nosebleeds, bleeding of the gums, and heavy menstrual bleeding in women.",131
What did the court in In re Ferguson conclude about the transformation prong of the Bilski test?,"Xpp-pdf support utility
Xpp-pdf support utility
PATENT, TRADEMARK
& COPYRIGHT !
Reproduced with permission from BNA’s Patent,Trademark 11/20/09, 11/20/2009. Copyright ஽ 2009 by The Bu-reau of National Affairs, Inc. (800-372-1033) http://www.bna.com As the patent community anticipates a decision by the U.S. Supreme Court on subject matter patentability, recent rulings by the Federal Circuit and the Board of Patent Appeals and Interferences suggest strategies for preparing method patent applications that will sur- vive the Federal Circuit’s ‘‘machine-or-transformation’’ test.
The Changing Landscape of Method Claims in the Wake of In re Bilski:What We Can Learn from Recent Decisions of Federal Courts and the Board ofPatent Appeals rulings on software-based and other business methodpatent applications.
On review before the high court is the en banc ruling ‘‘Pure’’ business methods are out. Algorithms by the U.S. Court of Appeals for the Federal Circuit1 are out. Machines and data transformations that, in order to be eligible for patent protection, an in- ventive method must either be tied to a machine or re- While the patent community waits for the Supreme cite a transformation of an article.2 This ‘‘machine-or- Court’s decision in Bilski v. Kappos, No. 08-964 (U.S.
transformation’’ test replaced the Freeman-Walter- argued Nov. 9, 2009) (79 PTCJ 33, 11/13/09), patent ap- Abele3 test and the ‘‘useful, concrete and tangible plicants seeking to write patentable claims are stuckwith trying to conform to the lower courts’ most recent 1 In re Bilski, 545 F.3d 943, 88 USPQ2d 1385 (Fed. Cir.
2008) (en banc) (77 PTCJ 4, 11/7/08).
2 ‘‘The machine-or-transformation test is a two-branched Adriana Suringa Luedke and Bridget M. Hay- inquiry; an applicant may show that a process claim satisfies den are lawyers at Dorsey & Whitney, Min- § 101 either by showing that his claim is tied to a particular neapolis. Luedke can be reached at machine, or by showing that his claim transforms an article.’’ leudke.adriana@dorsey.com. Hayden can be reached at hayden.bridget@dorsey.com. 3 In re Freeman, 573 F.2d 1237, 197 USPQ 464 (C.C.P.A.
1978); In re Walter, 618 F.2d 758, 205 USPQ 397 (C.C.P.A.
COPYRIGHT ஽ 2009 BY THE BUREAU OF NATIONAL AFFAIRS, INC.
result’’ inquiry advocated in State Street,4 each of gregating, and selling real estate property and claims which had been applied by the Federal Circuit and its reciting a method of performing tax-deferred real estate predecessor court in various cases, and both of which property exchanges were not statutory under Section 101. Since no machine was recited, the only issue be- In this article, we examine the 2008 decision of the fore the court was whether the claims met the ‘‘trans- Federal Circuit, federal district court decisions, and de- formation’’ prong of the Bilski test.13 The court held cisions of Patent and Trademark Office’s Board of that the claims ‘‘involve[d] only the transformation or Patent Appeals and Interferences. Based upon the out- manipulation of legal obligations and relationships’’ comes in these cases, we offer guidance as to what is that did not qualify under Bilski.14 patent-eligible under 35 U.S.C. § 101, strategies for pre- Concerning the recitation of the ‘‘creation of deed- senting methods in patent applications and claiming shares’’ in some of the claims, the court found that the these methods, and possible ‘‘fixes’’ for applications deedshares themselves were not physical objects, but drafted pre-Bilski that must now withstand scrutiny un- only represented intangible legal ownership interests in der the new machine-or-transformation test.
property.15 Therefore, the creation of deedshares wasnot sufficient to establish patent eligibility under Bil- A number of recent federal court and board decisions have applied the patent eligibility test set forth in Bilski, implemented step to an otherwise obvious method was not sufficient to avoid invalidity of the claim. In KingPharmeuticals Inc. v. Eon Labs Inc.,17 the district court held invalid claims to a method of increasing the oral Several cases have addressed (and rejected) claims bioavailability of metaxalone because the claims were obvious over the prior art asserted by the accused in- In In re Ferguson,6 the Federal Circuit reviewed the board’s rejection of claims directed to a method of mar- Two dependent claims added a step of informing the keting a product and a ‘‘paradigm’’ for marketing soft- patient of certain results, which the patentee argued ware as nonstatutory subject matter under Section was not obvious. The court rejected this argument, con- 101.7 The appellate court affirmed the board’s rejection, cluding that ‘‘[b]ecause the food effect is an inherent concluding that the method claims were neither tied to property of the prior art and, therefore, unpatentable, a particular machine or apparatus nor did they trans- then informing a patient of that inherent property is form a particular article into a different state or thing.8 The court defined a machine broadly as ‘‘a concrete The court also commented that the added step of in- thing, consisting of parts, or of certain devices or com- forming the patient did not meet the patent eligibility binations of devices,’’ which did not include the ‘‘shared standard set forth in Bilski because the step did not re- marketing force’’ to which the method claims were quire use of a machine or transform the metaxalone into a different state or thing.19 Notably, this conclusion The claims directed to a ‘‘paradigm’’ were non- runs counter to the Supreme Court’s instruction that statutory because the claims did not fall within any of claims are to be examined ‘‘as a whole’’ and not dis- the four statutory categories (machines, manufactures, sected into old and new elements and that are evaluated compositions of matter and processes). Concerning the two closest possible categories, the court concluded Recent board decisions have been consistent with the that the claimed paradigm was not a process, because holdings of the federal courts. For example, in Ex parte no act or series of acts was required, and was not a Roberts,21 the board found ineligible under Section 101 manufacture, because it was not a tangible article re- a ‘‘method of creating a real estate investment instru- sulting from a process of manufacture.10 Concerning ment adapted for performing tax-deferred exchanges’’ the recitation of a ‘‘marketing company’’ in the para- because the claim did not satisfy either the machine or digm claims, the court concluded that the patent appli- cants did ‘‘no more than provide an abstract idea—a Similarly, in Ex parte Haworth,23 a method for ‘‘at- business model for an intangible marketing com- tempting to collect payments from customers having delinquent accounts concurrently with a partner that In Fort Properties Inc. v. American Master Lease owns the delinquent accounts’’ was found to be patent LLC,12 the California district court held that claims re- ineligible because the claim wording was ‘‘broad in that citing a series of transactions involving acquiring, ag- 1980); In re Abele, 684 F.2d 902, 214 USPQ 682 (C.C.P.A.
4 State Street Bank & Trust Co. v. Signature Financial 16 See Ex parte Roberts., 2009-004444 at 4-5 (B.P.A.I. June Group, 149 F.3d 1368, 1370, 47 USPQ2d 1596 (Fed. Cir. 1998) 19, 2009) (holding a ‘‘method of creating a real estate invest- ment instrument adapted for performing tax-deferred ex- changes’’ patent ineligible as not passing the machine-or- 7 The court accepted the board’s definition of ‘‘paradigm’’ 17 593 F. Supp.2d 501 (E.D.N.Y. 2009).
to mean ‘‘a pattern, example or model.’’ Id. at 1362.
20 See Diamond v. Diehr, 450 U.S. 175, 188 (1981).
21 No. 2009-004444 (B.P.A.I. June 19, 2009).
12 2009 WL 249205, *5 (C.D. Cal. Jan. 22, 2009).
23 No. 2009-000350 (B.P.A.I. July 30, 2009).
it refers generally to extending an offer, receiving an machine. Accordingly, the process claims . . . are not acceptance, and paying a commission’’ and did not in- voke, recite or limit the method of implementation us-ing any particular machine or apparatus.24 The court also evaluated similar claims that recited the use of a ‘‘comparator’’ to perform the recited pixel- B. Software Claims Not Expressly Tied to a ‘Particular by-pixel comparison and held that this recitation also did not mandate a machine.29 While the court acknowl-edged that software was offered as one ‘‘option,’’ the Other cases have addressed software methods where court concluded that the claimed function of the com- the claim language was either not expressly tied to com- parator could also be performed in one’s mind or on pa- puter hardware components or the ties to computer per such that a machine was not required. The court components were somewhat ambiguous. In several further noted that, even though the ‘‘comparator’’ was cases, courts have rejected the recitation of generic defined as a ‘‘device,’’ ‘‘the use of the term ‘device’ is computer components as sufficient to satisfy the ‘‘ma- not synonymous with machine.’’30 As a result, none of chine’’ prong of the Bilski test. A number of these deci- the claims at issue met the ‘‘machine’’ prong of the Bil- sions also addressed the ‘‘transformation’’ prong of the Concerning the ‘‘transformation’’ prong, the court re- In Research Corporation Technology Inc. v. Mi- lied in particular upon the Abele decision in expanding crosoft Corp.,25 the district court considered the patent the requirements of this test by requiring that the eligibility of method claims in six patents directed to claimed transformation process be both ‘‘(1) limited to methods of halftoning of gray scale images by using a transformation of specific data, and 2) limited to a vi- pixel-by-pixel comparison of the image against a blue sual depiction representing specific objects or sub- noise mask. Relying on the Federal Circuit’s Bilski stances.’’31 It then concluded that a number of the analysis as well as a decision of its predecessor court, patent claims did not meet the second prong of this ex- In re Abele,26 the judge concluded that a number of the panded test because the claims did not ‘‘require any vi- sual depiction or subsequent display’’ even though the transformation test set forth in Bilski.27 claimed method did transform specific image data.32 Concerning the ‘‘machine’’ prong, the district court The district court also found other claims patent- found that the pixel-by-pixel comparison recited in the eligible under Section 101 because these claims recited claims did not require the use of a machine, but could the use of the comparison data ‘‘to produce a halftoned ‘‘dictate[d] a transformation of specific data, and [were] be done on a sheet of paper using a pen. The com- further limited to a visual depiction which represents parison uses formulas and numbers to generate a bi- specific objects.’’33 Thus, the patent eligibility of the nary value to determine the placement of a dot at a claims turned on whether the claims recited the use of location. Formulas and numbers not tied to a particu- the transformed data to generate a display.
lar machine cannot be patented, under the machine In DealerTrack Inc. v. Huber,34 the district court prong, even with a field-of-use limitation because granted a summary judgment of invalidity under § 101 they represent fundamental principles, and to do so of patent claims directed to ‘‘a computer aided method’’ would preempt the entire field. The patent claims . . .
of managing a credit application reciting the following do not mandate the use of a machine to achieve their algorithmic and algebraic ends. Simply because adigital apparatus such as a computer, calculator, or [A] receiving credit application data from a remote the like could assist with this comparison does not render it patent eligible material. RCT’s argument [B] selectively forwarding the credit application data that a pixel by its nature is electronic and therefore to remote funding source terminal devices; necessitates a machine is a post solution argumentand the Court rejects it. The claim construction specifies that the comparison is of a value to a mask 29 The term ‘‘comparator’’ was construed by the court to be (or set of values) to determine whether the dot is a ‘‘device (or collection of operations, as in software) that com- turned on at a specific location. This process does pares an input number (called the operand) to a number pre- not require a particular machine. The Bilski test is stored in the comparator (called the threshold) and produces clear: the process claims must be tied to a particular as output a binary value (such as ‘‘0,’’ zero) if the input is alge-braically less than the threshold [the result of comparing anoperand against a fixed threshold and setting an operand less 24 Id. at 9-10. See also, e.g., Ex parte Farnes, 2009-002770 than the threshold to one value and an operand greater than (B.P.A.I. June 2, 2009) (rejecting a method claim for develop- or equal to the threshold to another value], and produces the ing a solution to a customer experience issue including steps opposite binary value (such as ‘‘1,’’ one) if the input is algebra- of: ‘‘identifying a target customer,’’ ‘‘defining a current cus- ically greater than or equal to the threshold.’’ Id. at *17 (em- tomer experience,’’ ‘‘summarizing values and benefits’’ to pro- vide to the customer, and ‘‘identifying metrics for measuring success’’); Ex parte Salinkas, 2009-002768 (B.P.A.I. May 18, 31 Id. at *9. Notably, Bilski concluded that the Abele visual 2009) (finding patent ineligible a method of launching a depiction was ‘‘sufficient’’ to establish transformation (545 knowledge network involving ‘‘selecting an executive spon- F.3d at 963), while the Research Corporation court went fur- sor,’’ ‘‘forming a core team of experts,’’ and ‘‘providing pre- ther by making visual depiction ‘‘required’’ to establish trans- 25 2009 WL 2413623 (D. Ariz. July 28, 2009) (78 PTCJ 432, 26 684 F.2d 902, 214 USPQ 682 (C.C.P.A. 1982).
34 2009 WL 2020761 (C.D. Cal. July 7, 2009) (78 PTCJ 341, PATENT, TRADEMARK & COPYRIGHT JOURNAL [C] forwarding funding decision data from at least tation of ‘over the Internet’ suffices to tie a process one of the remote funding source terminal de- claim to a particular machine’’ and concluded that it vices to the remote application entry and display The internet continues to exist despite the addition [D] wherein the selectively forwarding the credit ap- or subtraction of any particular piece of hardware. It may be supposed that the internet itself, rather than [E] sending at least a portion of a credit application any underlying computer or set of computers, is the to more than one of said remote funding sources ‘‘machine’’ to which plaintiff refers. Yet the internet is an abstraction. If every computer user in the world [F] sending at least a portion of a credit application unplugged from the internet, the internet would to more than one of said remote funding sources cease to exist, although every molecule of every ma- sequentially until a finding [sic ] source returns a chine remained in place. One can touch a computer or a network cable, but one cannot touch ‘‘the inter- [G] sending . . . a credit application . . . after a prede- Additionally, the court found that the recitation of the [H] sending the credit application from a first remote internet in this case merely constituted ‘‘insignificant funding source to a second remote funding extra-solution activity’’ and therefore did not qualify as a ‘‘particular machine’’ under Bilski.41 ‘‘[T]ossing in In concluding that the claim did not satisfy the Bilski references to internet commerce’’ was not sufficient to machine-or-transformation test, the court held that the render ‘‘a mental process for collecting data and weigh- claimed central processor, remote application and dis- ing values’’ patent-eligible.42 Additionally, ‘‘limiting’’ play device, and remote funding source terminal device the claim to use over the Internet was not a meaningful could be ‘‘any device’’ and did not constitute a ‘‘’par- limitation, such that the claims ‘‘broadly preempt the ticular machine’ within the meaning of Bilski.’’35 The fundamental mental process of fraud detection using court relied upon several board decisions to support its associations between credit cards.’’43 premise that ‘‘claims reciting the use of general purpose processors or computers do not satisfy the test.’’36 claim,44 notwithstanding the Federal Circuit’s holding In Cybersource Corp. v. Retail Decisions Inc.,37 the in In re Beauregard,45 the district court concluded that district court held claims for ‘‘a method for verifying the ‘‘there is at present no legal doctrine creating a special validity of a credit card transaction over the Internet’’ ‘‘Beauregard claim’’ that would exempt the claim from and ‘‘a computer readable medium containing program the analysis of Bilski.’’ Moreover, ‘‘[s]imply appending instructions for detecting fraud in a credit card transac- ‘A computer readable media including program instruc- tion . . . over the Internet’’ invalid under § 101 based tions’ to an otherwise non-statutory process claim is in- upon the court’s interpretation of Bilski.
sufficient to make it statutory.’’46 Consequently, this Concerning the method claim, the court considered claim also failed the Bilski test.
both the ‘‘transformation’’ and ‘‘machine’’ prongs of the In at least one instance, the U.S. International Trade Bilski test. In concluding that there was no transforma- Commission has interpreted the ‘‘machine’’ prong of tion, the court focused on the intangibility of the ma- Bilski less stringently than did the district courts in the nipulated data. According to the court, transformation cases discussed above. In In the Matter of Certain Video is limited to transformation of a physical article or sub- Game Machines and Related Three-Dimensional Point- stance. Accordingly, the method claim did not qualify ing Devices,47 the accused infringer filed a motion for because the data representing credit cards did not rep- summary judgment alleging that the asserted claims resent tangible articles but instead an intangible series impermissibly sought to patent a mathematical algo- of rights and obligations existing between the account rithm. According to the movant, the recitations of a ‘‘3D pointing device,’’ ‘‘handheld device,’’ or ‘‘free space Concerning whether the claimed method was tied to pointing device’’ were not sufficient to tie the claims to a particular machine, the court assessed whether ‘‘reci- a particular machine, but served ‘‘only to limit the field-of-use of the claimed mathematical algorithm and [did] not otherwise impart patentability on the claimed math- Id. at *3. The court relied upon the holdings in Ex parte Gutta, No. 2008-3000 at 5-6 (B.P.A.I. Jan. 15, 2009) (stating In denying the motion for summary judgment, the ‘‘[t]he recitation in the preamble of ‘[a] computerized method ITC first noted that, ‘‘[w]hile the ultimate determination performed by a data processor’ adds nothing more than a gen- of whether the asserted claims are patentable under eral purpose computer that is associated with the steps of the § 101 is a question of law, the Federal Circuit has ac- process in an unspecified manner.’’); Ex parte Nawathe, No.
2007-3360, 2009 WL 327520, *4 (B.P.A.I. Feb. 9, 2009) (finding‘‘the computerized recitation purports to a general purpose processor [], as opposed to a particular computer specifically programmed for executing the steps of the claimed method.’’); and Ex parte Cornea-Hasegan, No. 2008-4742 at 9-10 (B.P.A.I.
Jan. 13, 2009) (indicating the appellant does not dispute ‘‘the recitation of a processor does not limit the process steps to any 44 Claims having this format are called ‘‘Beauregard’’ specific machine or apparatus.’’). The court also cited Cyber- claims and were found to not be barred by the traditional source Corp. v. Retail Decisions Inc., (discussed below), in sup- printed matter rule in In re Beauregard, 53 F.3d 1583, 1584, 35 port of its interpretation of the required ‘‘particular machine.’’ 37 620 F. Supp. 2d 1068, 92 USPQ2d 1011 (N.D. Cal. 2009) 47 2009 WL 1070801 (U.S.I.T.C. 2009).
knowledged that ‘there may be cases in which the legal given a dataset of feature vectors associated with the question as to patentable subject matter may turn on subsidiary factual issues’ ’’ (citation omitted). In con- for each binary partition under consideration, rank- struing the claims, the tribunal found that there was a ing features using two-category feature ranking; and genuine dispute as to whether the claimed ‘‘devices’’represented a ‘‘particular machine’’ under the Bilski while the predetermined number of features has not test and whether the claimed ‘‘two-dimensional rota- yet been selected: picking a binary partition p; tional transform’’ was merely a mathematical calcula- selecting a feature based on the ranking for binary tion or instead meant ‘‘changing the mathematical rep- resentation of a two-dimensional quantity from oneframe of reference to a differently-oriented frame of ref- adding the selected feature to an output list if not al- erence’’ as asserted by the patentee. Additionally, the ready present in the output list and removing the se- dispute over the meaning of the claimed ‘‘two- lected feature from further consideration for the bi- dimensional rotational transform’’ also raised a dis- puted issue as to whether this element recited a trans- Notably, while the independent claim failed the formation that would qualify under the ‘‘transforma- machine-or-transformation test, its dependent claim tion’’ prong of Bilski. Given these disputed issues, the was eligible because it recited, ‘‘further comprising us- ITC concluded that it was inappropriate to grant sum- ing the selected features in training a classifier for clas- mary judgment as to the patent eligibility of the claims.
sifying data into categories.’’ In view of the specifica- A similar conclusion was reached in Versata Soft- tion, the board indicated that the ‘‘classifier’’ was a par- ware Inc. v. Sun Microsystems Inc.,48 in which the dis- ticular machine ‘‘in that it performs a particular data trict court denied the defendant’s motion for summary classification function that is beyond mere general pur- judgment of invalidity under Section 101 based upon pose computing.’’53 The board also concluded that the the Bilski court’s refusal ‘‘to adopt a broad exclusion claim ‘‘transforms a particular article into a different over software or any other such category of subject state or thing, namely by transforming an untrained matter beyond the exclusion of claims drawn to funda- classifier into a trained classifier.’’54 In Ex parte Casati,55 the board reversed the examin- Less stringent ‘‘machine’’ prong analyses are also er’s Section 101 rejection of a method claim reciting: found at the board level. For example, in Ex parteSchrader,50 the board held patent-eligible under Bilski A method of analyzing data and making predictions, reading process execution data from logs for a busi- A method for obtaining feedback from consumers re- ceiving an advertisement from an ad provided by anad provider through an interactive channel, the collecting the process execution data and storing the process execution data in a memory defining a ware-house; creating a feedback panel including at least one feed-back response concerning said advertisement; and analyzing the process execution data; generatingprediction models in response to the analyzing; and providing said feedback panel to said consumers, using the prediction models to predict an occurrence said feedback panel being activated by a consumer to of an exception in the business process.
provide said feedback response concerning said ad-vertisement to said ad provider through said interac- In this case, giving consideration to the specification, which ‘‘unequivocally describes the data warehouse aspart of the overall system apparatus, and subsequent Here, the board found ‘‘interactive channel’’ to be descriptions describe the memory/warehouse device in part of an ‘‘overall patent eligible system of appara- terms of machine executable functions,’’ the board con- tuses’’ when viewed in the context of the specification, cluded that ‘‘one of ordinary skill in the art would un- which included ‘‘the Internet and World Wide Web, In- derstand that the claimed storing of process execution teractive Television, and self service devices, such as In- data in a memory defining a warehouse constitutes formation Kiosks and Automated Teller Machines.’’51 patent-eligible subject matter under § 101 because the In another recent decision, Ex parte Forman,52 the memory/warehouse element ties the claims to a particu- board found a ‘‘computer-implemented feature selec- tion method’’ including a ‘‘classifier’’ eligible under Other recent board decisions have reached the oppo- Section 101 because it satisfied both the machine and transformation prong. Here, the ‘‘classifier’’ was recitedin a dependent claim, in which its independent claim re-cited: 53 Id. at 13.
54 Id. See also Ex parte Busche, No. 2008-004750 (B.P.A.I.
A computer-implemented feature selection method May 28, 2009) (holding a process claim and a computer pro- for selecting a predetermined number of features for gram product claim, each reciting training a machine, ‘‘are di- a set of binary partitions over a set of categories rected to machines that have such structure as may be adaptedby training.’’) 55 No. 2009-005786 (B.P.A.I. July 31, 2009).
48 2009 WL 1084412, *1 (E.D. Tex. March 31, 2009).
56 Id. at 7. See also Ex parte Dickerson, No. 2009-001172 at 49 Citing Bilski, 545 F.3d at 959 n. 23.
16 (B.P.A.I. July 9, 2009) (holding claims that ‘‘recite a comput- 50 No. 2009-009098 (B.P.A.I. Aug. 31, 2009).
erized method which includes a step of outputting information from a computer . . . are tied to a particular machine or appa- 52 No. 2008-005348 (B.P.A.I. Aug. 17, 2009).
PATENT, TRADEMARK & COPYRIGHT JOURNAL implemented methods ineligible under the Bilski test transformation test applied to this type of claim.63 because the claims failed to tie the method steps to any Then, applying the Bilski test, the board concluded that concrete parts, devices, or combinations of devices. For the claim did not qualify. According to the board, the example, in Ex parte Holtz,57 the board found ineligible under Section 101 a ‘‘method for comparing file tree de-scriptions’’ because the claim ‘‘obtains data (a file struc- does not transform physical subject matter and is not ture), compares data (file structures), generates a tied to a particular machine. . . . Limiting the claims change log, and optimizes the change log without tying to computer readable media does not add any practi- these steps to any concrete parts, devices, or combina- cal limitation to the scope of the claim. Such a field- tions of devices’’ and the ‘‘file structures’’ did not repre- of-use limitation is insufficient to render an other- Similarly, in Ex parte Gutta,58 the board held ineli- gible under § 101 a ‘‘method for identifying one or moremean items for a plurality of items . . . having a sym- II. The Current Scope of Patent Eligibility bolic value of a symbolic attribute,’’ concluding that the These recent cases establish that some types of meth- claim ‘‘computes a variance and selects a mean item ods are clearly patent-eligible under Section 101, others without tying these steps to any concrete parts, devices, clearly are not eligible, and yet others may be depend- or combinations of devices’’ and ‘‘symbolic values are ing on how they are described and claimed.
neither physical objects nor do they represent physicalobjects.’’ First, the eligibility of system and apparatus claims is largely unaffected by the Bilski decision, with the ca- In contrast to the district court’s decision in Cyber- veat that such claims may be more closely scrutinized source Corp., discussed supra, in a recent board deci- for compliance with Diamond v. Diehr and Gottschalk sion, Ex parte Bodin,59 ‘‘a computer program product’’ v. Benson, which prohibit patenting of a claim directed was found to be patent-eligible subject matter as being to ‘‘laws of nature, natural phenomena, [or] abstract embodied in a ‘‘computer readable medium.’’ Here, the board considered whether the phrase ‘‘recorded on the Also, methods that are performed at least in part by a recording medium’’ as it is recited in the body of the machine qualify for patent eligibility under Section 101.
claims was the same as ‘‘recorded on a computer- Thus, for example, some computer-implemented and readable medium.’’ Acknowledging the differences be- software-related inventions remain patentable as long tween a statutory claim to a data structure stored on a as they are properly described and claimed as being computer readable medium compared to a nonstatutory performed by a computer or computer components.
claim to a data structure that referred to ideas reflected The tie to a machine, however, cannot merely be im- in nonstatutory processes, the board stated: ‘‘[w]hen plicit based upon the description and context of the ap- functional descriptive material is recorded on some plication or general language in the preamble of the computer-readable medium, it becomes structurally claim. Instead, the use of a machine to perform one or and functionally interrelated to the medium and will be more of the claimed functions must be expressly de- statutory in most cases since use of technology permits scribed in the body of the claim so as to be a meaning- the function of the descriptive material to be real- ful limitation on the claim. If a method claim can be read in such a way that all functions can be performed Similarly, in Ex parte Azuma,61 a claim to a ‘‘com- by a human, it will likely not pass the machine prong of puter program product . . . comprising: a computer us- able medium’’ was found to be directed to statutory The ‘‘Interim Examination Instructions for Evaluat- subject matter under § 101 because the language ‘‘com- ing Subject Matter Eligibility Under 35 U.S.C. § 101’’ re- puter usable medium’’ referred to tangible storage me- cently issued by the Patent and Trademark Office con- dia, such as a server, floppy drive, main memory and firm that the recitation of a general purpose computer hard disk as disclosed by appellant’s specification, and is sufficient to satisfy Section 101 where the general did not ‘‘implicate the use of a carrier wave.’’ purpose computer is ‘‘programmed to perform the pro- In an older decision, Ex parte Cornea-Hasegan,62 cess steps, . . . in effect, becom[ing] a special purpose however, the Board seemingly came to the opposite conclusion, holding that a claim reciting ‘‘a computer Concerning data transformation, there seems to be readable media including program instructions which agreement of the Federal Circuit and at least one dis- when executed by a processor cause the processor to trict court that a method that is both limited to transfor- perform’’ a series of steps was not patent-eligible under mation of specific data and limited to a visual depiction Bilski. The board first determined that ‘‘analysis of a representing specific objects or substances qualifies un- ‘manufacture’ claim and a ‘process’ claim is the sameunder 63 Id. at 11.
57 No. 2008-004440 at 12-13 (B.P.A.I. Aug. 24, 2009).
65 Diamond v. Diehr, 450 U.S. 175, 185, 205 USPQ 488 58 No. 2008-004366 at 10-11 (B.P.A.I. Aug. 10, 2009).
(1980); Gottschalk v. Benson, 409 U.S. 63, 67, 175 USPQ 673 59 No. 2009-002913 (B.P.A.I. Aug. 5, 2009).
60 Id. at 10 (comparing In re Lowry, 32 F.3d 1579, 1583-84, 66 ‘‘Interim Examination Instructions for Evaluating Sub- 32 USPQ2d 1031 (Fed. Cir. 1994) to In re Warmerdam, 33 F.3d ject Matter Eligibility Under 35 U.S.C. § 101,’’ U.S. Patent and 1354, 1361-62, 31 USPQ2d 1754 (Fed. Cir. 1994)).
Trademark Office, Aug. 24, 2009, at 6 (78 PTCJ 530, 8/28/09).
61 No. 2009-003902 at 10 (B.P.A.I. Sept. 14, 2009).
The authors’ recent experiences with examiners suggest that 62 No. 2008-004742 (B.P.A.I. Jan. 13, 2009).
the examiners are following these instructions.
der Section 101.67 Thus, claims analogous to those in In Concerning claims directed to computer program re Abele68 in which ‘‘data clearly represented physical products, one district court has held that appending ‘‘A and tangible objects, namely the structure of bones, or- computer readable media including program instruc- gans, and other body tissues [so as to recite] the trans- tions’’ to an otherwise non-statutory process claim is in- formation of that raw data into a particular visual depic- sufficient to make it statutory.72 The board has also tion of a physical object on a display’’ are patent- held ineligible claims to ‘‘a computer readable me- dia.’’73 The board has, however, also upheld the eligibil-ity of ‘‘a computer program product’’ as being embod- ied in a computer readable medium.74 Given these in- Bilski has had a significant impact in eliminating consistent decisions, the patent eligibility of claims in patent protection for inventions that are performed en- tirely by humans or can be interpreted as such if read Concerning claims directed to generalized computer broadly. This includes claims that describe processes processing functions, several Board decisions suggest for creating or manipulating legal and financial docu- that, absent a tie to a concrete real-world application, ments and relationships. In this area in particular, many such claims are likely to be deemed an ‘‘algorithm’’ un- pending applications filed prior to Bilski are no longer der Benson and therefore held to be non-statutory. 75 patent-eligible, and many issued patents are no longer Any recitation of a specific field of use for the claimed valid. This retroactive impact of the Bilski decision is process or use of the outcome of such processes are troubling, given the investment in these patents and ap- also more likely to be found ‘‘field-of-use’’ or ‘‘post- plications, which have now been rendered essentially solution activity’’ limitations insufficient to render the worthless despite the suggestion in the Federal Circuit’s claim patent-eligible. Thus, the more tied a claimed pro- earlier State Street decision, now overruled, that such cess is to tangible results or particular applications (not claims qualified for patent protection.
just fields of use), the more likely it is to qualify under Inventions that do not fit within the four statutory categories are also not patent-eligible. The Federal Cir-cuit and the board have rejected claims directed to ‘‘a III. Presenting and Claiming Methods in Patent signal,’’ ‘‘a paradigm,’’ ‘‘a user interface’’ and ‘‘a corr-elator’’ on the basis that these items did not qualify as a ‘‘machine, manufacture, composition of matter or pro- Several strategies for describing and claiming meth- cess’’ under § 101. 70 There is also an increasing focus ods or processes in patent applications may avoid or on the tangibility of the claimed invention in that, to minimize potential Section 101 problems.
qualify as a ‘‘machine’’ or ‘‘manufacture’’ under Section First, the description provided in a patent application should include well-defined steps or functions associ-ated with method or process. For example, when the claims include ‘‘initiating’’ method steps, a description Remaining areas of uncertainty concerning the scope of well-defined physical steps or functions for initiating of Section 101 include (1) what qualifies under Bilski as should be provided, and a concrete item, machine, de- a ‘‘transformation of an article or data,’’ (2) whether vice, or component that is responsible for the initiating claims to computer programs (Beauregard claims) function should be identified. For claiming ‘‘identify- qualify, and (3) whether internal computer processing ing’’ method steps, provide specific parameters for functionality not tied to a specific application or tan- making the identification, such as according to a speci- fied measurement.76 Where data is involved, the source Concerning data transformation, other than Abele- and type of data should be specified.
style claims discussed above, what qualifies as a data or Also, drawings should be provided that depict the article transformation remains unclear. Claims that concrete item, device, component or combination have been held not to meet the transformation prong in- thereof, and each method or process step or function clude claims directed to the creation or manipulation of should be linked expressly to at least one item, device data representing an intangible series of rights and ob- or component in the drawings that performs the step or ligations (e.g., credit card data) and claims directed to function. Broadening language indicating that other the transformation or manipulation of legal obligations components may also be used to perform the function and relationships. Beyond these specific examples, it is may also be included to avoid an unduly narrow inter- difficult to predict what will or will not qualify as a data or article transformation under Bilski.
The claims should affirmatively claim the device, ma- chine or component performing each step or function.
67 In re Bilski, 545 F.3d at 963; Research Corporation Tech- For computer or software-related inventions, the de- nologies, 2009 WL 2413623 at *9.
scription should specify that the software functionality 68 The claimed process involved graphically displaying vari- ances of data from average values wherein the data was X-rayattenuation data produced in a two dimensional field by a com- 72 Cybersource Corp., 620 F. Supp. 2d at 1080.
puted tomography scanner. See In re Bilski, 545 F.3d at 962- 73 Cornea-Hasegan, No. 2008-004742.
74 Ex parte Bodin, No. 2009-002913 (B.P.A.I. Aug. 5, 2009).
69 In re Bilski, 545 F.3d at 963.
75 E.g., Ex parte Greene, No. 2008-004073 (B.P.A.I. Apr. 24, 70 In re Nuijten 500 F.3d 1346, 1357, 84 USPQ2d 1495 (Fed.
2009); Daughtrey, No. 2008-000202; Ex parte Arning, No.
Cir. 2007) (74 PTCJ 631, 9/28/07) (signal); In re Ferguson, 558 2008-003008 (B.P.A.I. Mar. 30, 2009); Cybersource Corp., 620 F.3d 1359, 1366, 90 USPQ2d 1035 (Fed. Cir. 2009) (77 PTCJ F. Supp.2d at 1080 (concerning claim 2).
489, 3/13/09) (paradigm); Ex parte Daughtrey, No. 2008- 76 See Brief of American Bar Association as Amicus Curiae 000202 (B.P.A.I. Apr. 8, 2009) (user interface); Ex parte Laba- Supporting Respondent, Bilski v. Kappos, No. 08-964, ABA die, No. 2008-004310 (B.P.A.I. May 6, 2009) (correlator).
Amicus Br. at 12-13 (U.S. amicus brief filed Oct. 2, 2009) (78 71 E.g., Nuijten, 500 F.3d at 1356-7.
PATENT, TRADEMARK & COPYRIGHT JOURNAL is performed by a computer or computer components.
patent or published application, the option of importing Specificity as to the type of computer component per- subject matter into the specification is limited to ‘‘non- forming each function may be helpful in establishing essential’’ subject matter. In other words, the specifica- eligibility under the Bilski test.
tion can only be amended to disclose a machine for per-forming process steps as long as one skilled in the art IV. Fixing Pre-Bilski Applications to Meet the New would recognize from the original disclosure that the process is implemented by a machine. The key in mak- For patent applications filed prior to the Bilski deci- ing this type of amendment is avoiding (or overcoming) sion, it can be challenging to meet the new require- a rejection under 35 U.S.C. § 112, para. 1, for lack of ments for patent eligibility, particularly when no ma- chine or transformations were expressly described in If incorporation by reference is not an option, a patent applicant may submit evidence, such as a decla- In some cases, there may be sufficient explicit de- ration by the inventor or a duly qualified technical ex- scription of a machine, e.g., a computer, such that the pert, demonstrating that one skilled in the art would un- machine can be added into the body of the claims. For derstand the disclosed method to be one performed by example, patent applications for computer-related in- a machine. Unlike attorney argument, which can be dis- ventions sometimes contain a generic description of regarded, such evidence must be considered by the ex- computers that are used to perform the claimed method, and such a generic description may be suffi- One other option is to reformat the claims. Since Bil- cient to impart patent eligibility to the claims when the ski ostensibly does not apply to system and apparatus general-purpose computer is programmed to become a claims, in some instances it may be possible for an ap- plicant to convert his method claims into system claims For patent applications lacking in an explicit descrip- to avoid application of the Bilski test. This strategy, tion of any machine, however, the application may in- however, is unlikely to succeed where the patent speci- corporate by reference patents or publications that can fication does not describe such a system for implement- be used to bolster the specification and provide support ing the method and therefore does not provide the req- for the requisite claim amendments. When an applica- uisite disclosure of the claimed invention under Section tion incorporates by reference a U.S. patent or pub- lished U.S. patent application, any description from the incorporated references, whether or not the subject The future of the Bilski machine-or-transformation matter is ‘‘essential’’ to support the claims, may be im- test now rests with the Supreme Court. Regardless of ported into the specification. This option may enable the outcome of the appeal, however, it is clear that the importation of the requisite description of a machine, scope of statutory subject matter under Section 101 has which can then also be recited in the claims.77 When been narrowed. The Supreme Court now has a chance the document incorporated by reference is not a U.S.
to clarify what has been excluded; it may even reject ormodify the Bilski machine-or-transformation test. How 77 Manual of Patent Examining Procedure, Eighth Ed., Rev.
this will affect the development and protection of cur- 7/2008, at § 608.01(P); see also 37 C.F.R. § 1.57.
rent and future technologies remains to be seen.
Source: http://www.dorsey.com/files/upload/luedke_bna_patent_journal_nov09.pdf
(resolução 404.2012 retificação 19062012)
RESOLUÇÃO Nº 404 , DE 12 DE JUNHO DE 2012 Dispõe sobre padronização dos procedimentos administrativos na lavratura de Auto de Infração, na expedição de notificação de autuação e de notificação de penalidade de multa e de advertência, por infração de responsabilidade de proprietário e de condutor de veículo e da identificação de condutor infrator, e dá outras providências.
Cheloidi e cicatrici ipertrofiche in dermatologia
a cura del dr. Antonio Del Sorbo - Specialista in Dermatologia e Venereologia antoniodelsorbo@libero.it I Cheloidi di Alibert A volte una ferita anche apparentemente banale, guarisce lasciando una cicatrice voluminosa, rossastra e soprattutto antiestetica. I cheloidi sono cicatrici abnormi che possono far seguito a intervento chirurgico (es: tiroide, mammella, etc) e questo u",['It required the transformation to be limited to specific data and a visual depiction representing specific objects or substances.'],6925,multifieldqa_en,en,,9831469b98405cd1fc1bee7de6f9630ae1cbb5946fd7ca0b,It required the transformation to be limited to specific data and a visual depiction representing specific objects or substances.,129
How does the performance of the PLM with decimation compare to other methods?,"\section{Introduction}
Given a data set and a model with some unknown parameters, the inverse problem aims to find the values of the model parameters that best fit the data.  
In this work, in which we focus on systems of interacting elements,
 the inverse problem concerns  the statistical inference
 of the underling interaction network and of its coupling coefficients from observed data on the dynamics  of the system. 
 Versions of this problem are encountered in physics, biology (e.g., \cite{Balakrishnan11,Ekeberg13,Christoph14}), social sciences and finance (e.g.,\cite{Mastromatteo12,yamanaka_15}), neuroscience (e.g., \cite{Schneidman06,Roudi09a,tyrcha_13}), just to cite a few, and are becoming more and more important due to the increase in the amount of data available from these fields.\\
 \indent 
 A standard approach used in statistical inference is to predict the interaction couplings by maximizing the likelihood function.
 This technique, however, requires the evaluation of the  
 
   partition function that, in the most general case, concerns a number of computations scaling exponentially with the system size.
  
  
    Boltzmann machine learning uses Monte Carlo sampling to compute the gradients of the Log-likelihood looking for stationary points \cite{Murphy12} but this method is computationally manageable only for small systems. A series of faster approximations, such as naive mean-field, independent-pair approximation \cite{Roudi09a, Roudi09b}, inversion of TAP equations \cite{Kappen98,Tanaka98}, small correlations expansion \cite{Sessak09}, adaptive TAP \cite{Opper01}, adaptive  cluster expansion \cite{Cocco12} or Bethe approximations \cite{Ricci-Tersenghi12, Nguyen12}  have, then, been developed. These techniques take as input means and correlations of observed variables and most of them assume a fully connected graph as underlying connectivity network, or expand  around it by perturbative dilution.  In most cases, network reconstruction turns out to  be not accurate for small data sizes and/or when couplings are  strong or, else, if the original interaction network is sparse.\\
\indent
 A further method, substantially improving performances for small data, is the so-called Pseudo-Likelyhood Method (PLM) \cite{Ravikumar10}. In Ref. \cite{Aurell12} Aurell and Ekeberg performed a comparison between PLM and some of the just mentioned mean-field-based algorithms on the pairwise interacting Ising-spin  ($\sigma = \pm 1$) model, showing how PLM performs sensitively better, especially on sparse graphs and in the high-coupling limit, i.e., for low temperature.
 
 In this work, we aim at performing statistical inference  on a model whose interacting variables are continuous $XY$ spins, i.e., $\sigma \equiv \left(\cos \phi,\sin \phi\right)$ with $\phi \in [0, 2\pi )$. The developed tools can, actually, be also straightforward applied  to the $p$-clock model  \cite{Potts52} where the phase $\phi$ takes discretely equispaced $p$ values  in the $2 \pi$ interval, $\phi_a =  a 2 \pi/p$, with $a= 0,1,\dots,p-1$. The $p$-clock model, else called vector Potts model, gives a hierarchy of discretization of the $XY$ model as $p$ increases. For $p=2$, one recovers the Ising model, for $p=4$ the Ashkin-Teller model \cite{Ashkin43}, for $p=6$ the ice-type model \cite{Pauling35,Baxter82} and the eight-vertex model \cite{Sutherland70,Fan70,Baxter71} for $p=8$.  
It turns out to be very useful also for numerical implementations of the continuous $XY$ model. 
Recent analysis on the multi-body $XY$ model has shown that for a limited number of discrete phase values ($p\sim 16, 32$) the thermodynamic critical properties of the $p\to\infty$ $XY$ limit are promptly recovered \cite{Marruzzo15, Marruzzo16}.  
Our main motivation to study statistical inference is that these kind of models have recently turned out to be rather useful in describing the behavior of optical systems, 
including standard mode-locking lasers \cite{Gordon02,Gat04,Angelani07,Marruzzo15} and random lasers \cite{Angelani06a,Leuzzi09a,Antenucci15a,Antenucci15b,Marruzzo16}. 
In particular, the inverse problem on the pairwise XY model analyzed here might be of help in recovering images from light propagated through random media. 


      This paper is organized as follows: in Sec. \ref{sec:model} we introduce the general model  and we discuss its  derivation also  as a model for light transmission through random scattering media. 
    In Sec. \ref{sec:plm} we introduce the PLM with $l_2$ regularization  and with decimation, two variants of the PLM respectively introduced in Ref. \cite{Wainwright06} and \cite{Aurell12} for the inverse Ising problem. 
    Here, we analyze these techniques for continuous $XY$ spins and we test them on thermalized data generated by Exchange Monte Carlo numerical simulations of the original model dynamics. In Sec. \ref{sec:res_reg} we  present the results related to the PLM-$l_2$. In Sec. \ref{sec:res_dec} the results related to the PLM with decimation are reported and its performances are compared to the PLM-$l_2$ and to a variational mean-field method analyzed in Ref. \cite{Tyagi15}. In Sec. \ref{sec:conc}, we outline conclusive remarks and perspectives.

        \section{The leading $XY$ model}
      \label{sec:model}
 The leading model we are considering is defined, for a system of $N$ angular $XY$ variables, by the Hamiltonian 
 \begin{equation}
  \mathcal{H} = - \sum_{ik}^{1,N} J_{ik} \cos{\left(\phi_i-\phi_k\right)} 
  \label{eq:HXY}
 
  \end{equation} 
  
  The $XY$ model is well known in statistical mechanics, displaying important physical
  insights, starting from the Berezinskii-Kosterlitz-Thouless
  transition in two dimensions\cite{Berezinskii70,Berezinskii71,Kosterlitz72} and moving to, e.g., the
  transition of liquid helium to its superfluid state \cite{Brezin82}, the roughening transition of the interface of a crystal in equilibrium with its vapor \cite{Cardy96}. In presence of disorder and frustration \cite{Villain77,Fradkin78} the model has been adopted to describe synchronization problems as the Kuramoto model \cite{Kuramoto75} and in the theoretical modeling of Josephson junction arrays \cite{Teitel83a,Teitel83b} and arrays of coupled lasers \cite{Nixon13}.
  Besides several derivations and implementations of the model in quantum and classical physics, equilibrium or out of equilibrium, ordered or fully frustrated systems, Eq. (\ref{eq:HXY}), in its generic form,
  has found applications also in other fields. A rather fascinating example being the behavior of starlings flocks  \cite{Reynolds87,Deneubourg89,Huth90,Vicsek95, Cavagna13}.
  Our interest on the $XY$ model resides, though, in optics. Phasor and phase models with pairwise and  multi-body interaction terms can, indeed, describe the behavior of electromagnetic modes in both linear and nonlinear optical systems in the analysis of problems such as light propagation and lasing \cite{Gordon02, Antenucci15c, Antenucci15d}. As couplings are strongly frustrated, these models turn out to be especially useful to the study of optical properties in random media \cite{Antenucci15a,Antenucci15b}, as in the noticeable case of random lasers \cite{Wiersma08,Andreasen11,Antenucci15e}  and they might as well be applied to linear scattering problems, e.g., propagation of waves in opaque systems or disordered fibers. 
  
  
  \subsection{A propagating wave model}
  We briefly mention a derivation of the model as a proxy for the propagation of light through random linear media. 
 Scattering of light is held responsible to obstruct our view and make objects opaque. Light rays, once that they enter the material, only exit  after getting scattered multiple times within the material. In such a disordered medium, both the direction and the phase of the propagating waves are random. Transmitted light 
 yields a disordered interference pattern typically  having low intensity, random phase and almost no resolution, called a speckle. Nevertheless, in recent years it has been realized that disorder is rather a blessing in disguise \cite{Vellekoop07,Vellekoop08a,Vellekoop08b}. Several experiments have made it possible to control the behavior of light and other optical processes in a given random disordered medium,  
 by exploiting, e.g.,  the tools developed for wavefront shaping to control the propagation of light and to engineer the confinement of light \cite{Yilmaz13,Riboli14}.
 \\
 \indent
 In a linear dielectric medium, light propagation can be described through a part of the scattering matrix, the transmission matrix $\mathbb{T}$, linking the outgoing to the incoming fields. 
 Consider the case in which there are $N_I$ incoming channels and $N_O$ outgoing ones; we can indicate with $E^{\rm in,out}_k$ the input/output electromagnetic field phasors of channel $k$. In the most general case, i.e., without making any particular assumptions on the field polarizations, each light mode and its polarization polarization state can be represented by means of the $4$-dimensional Stokes vector. Each $ t_{ki}$ element of $\mathbb{T}$, thus, is a $4 \times 4$ M{\""u}ller matrix. If, on the other hand, we know that the source is polarized and the observation is made on the same polarization, one can use a scalar model and adopt Jones calculus \cite{Goodman85,Popoff10a,Akbulut11}:
   \begin{eqnarray}
 E^{\rm out}_k = \sum_{i=1}^{N_I}  t_{ki} E^{\rm in}_i \qquad \forall~ k=1,\ldots,N_O
 \label{eq:transm}
 \end{eqnarray}
  We recall that the elements of the transmission matrix are random complex coefficients\cite{Popoff10a}. For the case of completely unpolarized modes, we can also use a scalar model similar to Eq. \eqref{eq:transm}, but whose variables are  the intensities of the outgoing/incoming fields, rather than the fields themselves.\\ 
In the following, for simplicity, we will consider Eq. (\ref{eq:transm}) as our starting point,
where $E^{\rm out}_k$, $E^{\rm in}_i$ and $t_{ki}$ are all complex scalars. 
If Eq. \eqref{eq:transm} holds for any $k$, we can write:
  \begin{eqnarray}
  \int \prod_{k=1}^{N_O} dE^{\rm out}_k \prod_{k=1}^{N_O}\delta\left(E^{\rm out}_k - \sum_{j=1}^{N_I}  t_{kj} E^{\rm in}_j \right) = 1
  \nonumber
  \\
  \label{eq:deltas}
  \end{eqnarray}

 Observed data are a noisy representation of the true values of the fields. Therefore, in inference problems it is statistically more meaningful to take that noise into account in a probabilistic way, 
 rather than looking  at the precise solutions of the exact equations (whose parameters are unknown). 
 To this aim we can introduce Gaussian distributions whose limit for zero variance are the Dirac deltas in Eq. (\ref{eq:deltas}).
 Moreover, we move to consider the ensemble of all possible solutions of Eq. (\ref{eq:transm}) at given $\mathbb{T}$, looking at  all configurations of input fields. We, thus, define the function:
 
   \begin{eqnarray}
  Z &\equiv &\int_{{\cal S}_{\rm in}} \prod_{j=1}^{N_I}  dE^{\rm in}_j \int_{{\cal S}_{\rm out}}\prod_{k=1}^{N_O} dE^{\rm out}_k 
  \label{def:Z}
\\
    \times
  &&\prod_{k=1}^{N_O}
   \frac{1}{\sqrt{2\pi \Delta^2}}  \exp\left\{-\frac{1}{2 \Delta^2}\left|
  E^{\rm out}_k -\sum_{j=1}^{N_I}  t_{kj} E^{\rm in}_j\right|^2
\right\} 
\nonumber
 \end{eqnarray}
   We stress that the integral of Eq. \eqref{def:Z} is not exactly a Gaussian integral. Indeed, starting from Eq. \eqref{eq:deltas}, two constraints on the electromagnetic field intensities must be taken into account. 

  The space of solutions is  delimited by the total power ${\cal P}$ received by system, i.e., 
  ${\cal S}_{\rm in}: \{E^{\rm in} |\sum_k I^{\rm in}_k = \mathcal{P}\}$, also implying  a constraint on the total amount of energy that is transmitted through the medium, i. e., 
  ${\cal S}_{\rm out}:\{E^{\rm out} |\sum_k I^{\rm out}_k=c\mathcal{P}\}$, where the attenuation factor  $c<1$ accounts for total losses.
  As we will see more in details in the following, being interested in inferring the transmission matrix through the PLM, we can omit to explicitly include these terms in Eq. \eqref{eq:H_J} since they do not depend on $\mathbb{T}$ not adding any information on the gradients with respect to the elements of $\mathbb{T}$.
  
 Taking the same number of incoming and outcoming channels, $N_I=N_O=N/2$, and  ordering the input fields in the first $N/2$ mode indices and the output fields in the last $N/2$ indices, we can drop the ``in'' and ``out'' superscripts and formally write $Z$  as a partition function
    \begin{eqnarray}
        \label{eq:z}
 && Z =\int_{\mathcal S} \prod_{j=1}^{N} dE_j \left(   \frac{1}{\sqrt{2\pi \Delta^2}} \right)^{N/2} 
 \hspace*{-.4cm} \exp\left\{
  -\frac{ {\cal H} [\{E\};\mathbb{T}] }{2\Delta^2}
  \right\}
  \\
&&{\cal H} [\{E\};\mathbb{T}] =
-  \sum_{k=1}^{N/2}\sum_{j=N/2+1}^{N} \left[E^*_j t_{jk} E_k + E_j t^*_{kj} E_k^* 
\right]
 \nonumber
\\
&&\qquad\qquad \qquad + \sum_{j=N/2+1}^{N} |E_j|^2+ \sum_{k,l}^{1,N/2}E_k
U_{kl} E_l^*
 \nonumber
 \\
 \label{eq:H_J}
 &&\hspace*{1.88cm } = - \sum_{nm}^{1,N} E_n J_{nm} E_m^*
 \end{eqnarray}
 where ${\cal H}$ is a real-valued function by construction, we have introduced the effective input-input coupling matrix
\begin{equation}
U_{kl} \equiv \sum_{j=N/2+1}^{N}t^*_{lj} t_{jk} 
 \label{def:U}
 \end{equation}
 and the whole interaction matrix reads (here $\mathbb{T} \equiv \{ t_{jk} \}$)
 \begin{equation}
 \label{def:J}
 \mathbb J\equiv \left(\begin{array}{ccc|ccc}
 \phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}\\
 \phantom{()}&-\mathbb{U} \phantom{()}&\phantom{()}&\phantom{()}&{\mathbb{T}}&\phantom{()}\\
\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}\\
 \hline
\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}\\
 \phantom{()}& \mathbb   T^\dagger&\phantom{()}&\phantom{()}& - \mathbb{I} &\phantom{()}\\
\phantom{a}&\phantom{a}&\phantom{a}&\phantom{a}&\phantom{a}&\phantom{a}\\
 \end{array}\right)
 \end{equation}
 
 Determining the electromagnetic complex amplitude configurations that minimize the {\em cost function} ${\cal H}$, Eq.  (\ref{eq:H_J}),  means to maximize the overall distribution peaked around the solutions of the transmission Eqs. (\ref{eq:transm}). As the variance $\Delta^2\to 0$, eventually, the initial set of Eqs. (\ref{eq:transm}) are recovered. The ${\cal H}$ function, thus, plays the role of an Hamiltonian and  $\Delta^2$ the role of a noise-inducing temperature. The exact numerical problem corresponds to the zero temperature limit of the statistical mechanical problem. Working with real data, though, which are noisy, a finite ``temperature''
  allows for a better representation of the ensemble of solutions to the sets of equations of continuous variables. 
  

  
  
  Now, we can express every phasor in Eq. \eqref{eq:z}  as $E_k = A_k e^{\imath \phi_k}$. As a working hypothesis we will consider the intensities $A_k^2$ as either homogeneous or as \textit{quenched} with respect to phases.
The first condition occurs, for instance, to the input intensities $|E^{\rm in}_k|$ produced by a phase-only spatial light modulator (SLM) with homogeneous illumination \cite{Popoff11}.
With \textit{quenched} here we mean, instead, that the intensity of each mode is the same for every solution of Eq. \eqref{eq:transm} at fixed $\mathbb T$.
We stress that, including intensities in the model does not preclude the inference analysis but it is out of the focus of the present work and will be considered elsewhere. 

If all intensities are uniform in input and in output, this amount to a constant rescaling for each one of the four sectors of matrix $\mathbb J$ in Eq. (\ref{def:J}) that will not change the properties of the matrices.
For instance, if the original transmission matrix is unitary, so it will be the rescaled one and the matrix $\mathbb U$ will be  diagonal.
Otherwise, if intensities are \textit{quenched}, i.e., they can be considered as constants in Eq. (\ref{eq:transm}),
they are inhomogeneous with respect to phases. The generic Hamiltonian element will, therefore, rescale as 
  \begin{eqnarray}
  E^*_n J_{nm} E_m = J_{nm} A_n A_m e^{\imath (\phi_n-\phi_m)} \to J_{nm} e^{\imath (\phi_n-\phi_m)}
  \nonumber
  \end{eqnarray}
  and the properties of the original  $J_{nm}$ components are not conserved  in the rescaled one. In particular, we have no argument, anymore, to possibly set the rescaled $U_{nm}\propto \delta_{nm}$.
  Eventually, we end up with the complex couplings $XY$ model, whose real-valued Hamiltonian is written as
 \begin{eqnarray}
  \mathcal{H}& = &  - \frac{1}{2} \sum_{nm} J_{nm} e^{-\imath (\phi_n - \phi_m)}  + \mbox{c.c.} 
    \label{eq:h_im}
\\    &=&  - \frac{1}{2} \sum_{nm} \left[J^R_{nm} \cos(\phi_n - \phi_m)+
  J^I_{nm}\sin (\phi_n - \phi_m)\right] 
  \nonumber
 \end{eqnarray}
where $J_{nm}^R$ and $J_{nm}^I$ are the real and imaginary parts of $J_{nm}$. Being $\mathbb J$  Hermitian, $J^R_{nm}=J^R_{mn}$ is symmetric and $J_{nm}^I=-J_{mn}^I$ is skew-symmetric.

\begin{comment}
\textcolor{red}{
F: comment about quenched:
I think that to obtain the XY model, it is not necessary that the intensities are strictly quenched (that is also a quite unfeasible situation, I guess).
Indeed eq (2) does not deal with the dynamics of the modes, but just connect the in and out ones.
For this, what it is necessary to have the XY model, it is that the intensities are always the same on the different samples
(so that the matrix $t_{ij}$ is the same for different phase data). If the intensities are fixed, then they can be incorporated in $t_{ij}$ and eq (2) can be written just for phases as described. \\
}
\end{comment}


  \section{Pseudolikelihood Maximization}
  \label{sec:plm}
The inverse problem consists in the reconstruction of the parameters $J_{nm}$ of the Hamiltonian, Eq. (\ref{eq:h_im}).  
Given a set of $M$ data configurations of $N$ spins
 $\bm\sigma = \{ \cos \phi_i^{(\mu)},\sin \phi_i^{(\mu)} \}$, $i = 1,\dots,N$ and $\mu=1,\dots,M$, we want to \emph{infer} the couplings:
 \begin{eqnarray}
\bm \sigma  \rightarrow  \mathbb{J} 
\nonumber
 \end{eqnarray}
 With this purpose in mind,
 in the rest of this section we implement the working equations for the techniques used. 
 In order to test our methods, we generate the input data, i.e., the configurations, by Monte-Carlo simulations of the model.
 The joint probability distribution of the $N$ variables $\bm{\phi}\equiv\{\phi_1,\dots,\phi_N\}$, follows the Gibbs-Boltzmann distribution:
 \begin{equation}\label{eq:p_xy}
 P(\bm{\phi}) = \frac{1}{Z} e^{-\beta \mathcal{H\left(\bm{\phi}\right)}} \quad \mbox{ where } \quad Z = \int \prod_{k=1}^N d\phi_k  e^{-\beta \mathcal{H\left(\bm{\phi}\right)}}  
 \end{equation}
 and where we denote $\beta=\left( 2\Delta^2 \right)^{-1}$ with respect to Eq. (\ref{def:Z}) formalism.
 In order to stick to usual statistical inference notation, in the following we will rescale the couplings by a factor $\beta / 2$: $\beta J_{ij}/2 \rightarrow J_{ij}$. 
 The main idea of the PLM is to work with the conditional probability distribution of one variable $\phi_i$ given all other variables, 
 $\bm{\phi}_{\backslash i}$:
 
  \begin{eqnarray}
	\nonumber
   P(\phi_i | \bm{\phi}_{\backslash i}) &=& \frac{1}{Z_i} \exp \left \{ {H_i^x (\bm{\phi}_{\backslash i})
  	\cos \phi_i + H_i^y (\bm{\phi}_{\backslash i}) \sin \phi_i } \right \}
	\\
 \label{eq:marginal_xy}
	&=&\frac{e^{H_i(\bm{\phi}_{\backslash i}) \cos{\left(\phi_i-\alpha_i(\bm{\phi}_{\backslash i})\right)}}}{2 \pi I_0(H_i)}
  \end{eqnarray}
  where $H_i^x$ and $H_i^y$ are defined as
   \begin{eqnarray}
   H_i^x (\bm{\phi}_{\backslash i})  &=& \sum_{j (\neq i)} J^R_{ij} \cos \phi_j  - \sum_{j  (\neq i) } J_{ij}^{I} \sin \phi_j \phantom{+  h^R_i} \label{eq:26} \\
   H_i^y  (\bm{\phi}_{\backslash i})  &=&  \sum_{j  (\neq i)} J^R_{ij}  \sin \phi_j  + \sum_{j  (\neq i) } J_{ij}^{I}   \cos \phi_j \phantom{ + h_i^{I} }\label{eq:27}
   \end{eqnarray}
and $H_i= \sqrt{(H_i^x)^2 + (H_i^y)^2}$, $\alpha_i = \arctan H_i^y/H_i^x$   and we introduced the modified Bessel function of the first kind:
  \begin{equation}
  \nonumber
   I_k(x) = \frac{1}{2 \pi}\int_{0}^{2 \pi} d \phi e^{x \cos{ \phi}}\cos{k \phi}
  \end{equation}
  
   Given $M$ observation samples $\bm{\phi}^{(\mu)}=\{\phi^\mu_1,\ldots,\phi^\mu_N\}$, $\mu = 1,\dots, M$, the
   pseudo-loglikelihood for the variable $i$ is given by the logarithm of Eq. (\ref{eq:marginal_xy}),
   \begin{eqnarray}
   \label{eq:L_i}
   L_i &=& \frac{1}{M}  \sum_{\mu = 1}^M  \ln P(\phi_i^{(\mu)}|\bm{\phi}^{(\mu)}_{\backslash i})
   \\
   \nonumber
   & =&  \frac{1}{M}  \sum_{\mu = 1}^M  \left[ H_i^{(\mu)} \cos( \phi_i^{(\mu)} - \alpha_i^{(\mu)}) - \ln  2 \pi I_0\left(H_i^{(\mu)}\right)\right] \, .
    \end{eqnarray}
The underlying idea of PLM is that an approximation of the true parameters of the model is obtained for values that maximize the functions $L_i$.
The specific maximization scheme differentiates the different techniques.


   
   
   \subsection{PLM with $l_2$ regularization}
   Especially for the case of sparse graphs, it is useful to add a regularizer, which prevents the maximization routine to move towards high values of 
   $J_{ij}$ and $h_i$ without converging. We will adopt an $l_2$ regularization so that the Pseudolikelihood function (PLF) at site $i$  reads:
   \begin{equation}\label{eq:plf_i}
  {\cal L}_i = L_i
  - \lambda \sum_{i \neq j} \left(J_{ij}^R\right)^2 - \lambda \sum_{i \neq j} \left(J_{ij}^I\right)^2  
   \end{equation}
   with $\lambda>0$.
   Note that the values of $\lambda$ have to be chosen arbitrarily, but not too large, in order not to overcome $L_i$.
   The standard implementation of the PLM consists in maximizing each ${\cal L}_i$, for $i=1\dots N$, separately. The expected values of the couplings are then:
   \begin{equation}
   \{ J_{i j}^*\}_{j\in \partial i} :=  \mbox{arg max}_{ \{ J_{ij} \}}
    \left[{\cal L}_i\right]
   \end{equation}
   In this way, we obtain two estimates for the coupling $J_{ij}$, one from maximization of ${\cal L}_i$, $J_{ij}^{(i)}$, and another one from ${\cal L}_j$, say $J_{ij}^{(j)}$.
    Since the original Hamiltonian of the $XY$ model is Hermitian, we know that the real part of the couplings is symmetric while the imaginary part is skew-symmetric. 
   
     The final estimate for $J_{ij}$ can then be obtained  averaging the two results:
  
  
  
   \begin{equation}\label{eq:symm}
   J_{ij}^{\rm inferred} = \frac{J_{ij}^{(i)} + \bar{J}_{ij}^{(j)}}{2} 
   \end{equation}
   where with $\bar{J}$ we indicate the complex conjugate.
   It is worth noting that the pseudolikelihood $L_i$, Eq. \eqref{eq:L_i}, is characterized by the
   following properties: (i) the normalization term of Eq.\eqref{eq:marginal_xy} can be
   computed analytically at odd with the {\em full} likelihood case that
   in general require a computational time which scales exponentially
   with the size of the systems; (ii) the $\ell_2$-regularized pseudolikelihood
   defined in Eq.\eqref{eq:plf_i} is strictly concave (i.e. it has a single
   maximizer)\cite{Ravikumar10}; (iii) it is consistent, i.e. if $M$ samples are
   generated by a model $P(\phi | J*)$ the maximizer tends to $J*$
   for $M\rightarrow\infty$\cite{besag1975}. Note also that (iii) guarantees that  
   $|J^{(i)}_{ij}-J^{(j)}_{ij}| \rightarrow 0$ for $M\rightarrow \infty$.
   In Secs. \ref{sec:res_reg}, \ref{sec:res_dec} 
   we report the results obtained and we analyze the performances of the PLM having taken the configurations from Monte-Carlo simulations of models whose details are known.
   

   
   \subsection{PLM with decimation}
   Even though the PLM with $l_2$-regularization allows to dwell the inference towards the low temperature region and in the low sampling case with better performances that mean-field methods, in some situations some couplings are overestimated and not at all symmetric. Moreover, in the technique there is the bias of the $l_2$ regularizer.
   Trying to overcome these problems, Decelle and Ricci-Tersenghi introduced a new method \cite{Decelle14}, known as PLM + decimation: the algorithm maximizes the sum of the $L_i$,
   \begin{eqnarray}
    {\cal L}\equiv \frac{1}{N}\sum_{i=1}^N \mbox{L}_i
    \end{eqnarray}  
    and, then, it recursively set to zero couplings which are estimated very small. We expect that as long as we are setting to zero couplings that are unnecessary to fit the data, there should be not much changing on ${\cal L}$. Keeping on with decimation, a point is reached where ${\cal L}$ decreases abruptly indicating  that relevant couplings are being decimated and under-fitting is taking place.
   Let us define  by $x$  the fraction of non-decimated couplings. To have a quantitative measure for the halt criterion of the decimation process, a tilted ${\cal L}$ is defined as,
   \begin{eqnarray}
  \mathcal{L}_t &\equiv& \mathcal{L}  - x \mathcal{L}_{\textup{max}} - (1-x) \mathcal{L}_{\textup{min}} \label{$t$PLF} 
   \end{eqnarray}
   where 
   \begin{itemize}
   \item $\mathcal{L}_{\textup{min}}$ is the pseudolikelyhood of a model with independent variables. In the XY case: $\mathcal{L}_{\textup{min}}=-\ln{2 \pi}$.
   \item
   $\mathcal{L}_{\textup{max}}$ is the pseudolikelyhood in the fully-connected model and it is maximized over all the $N(N-1)/2$ possible couplings. 
   \end{itemize}
   At the first step, when $x=1$, $\mathcal{L}$ takes value $\mathcal{L}_{\rm max}$ and  $\mathcal{L}_t=0$. On the last step, for an empty graph, i.e., $x=0$, $\mathcal{L}$ takes the value $\mathcal{L}_{\rm min}$ and, hence, again $\mathcal{L}_t =0$. 
   In the intermediate steps, during the decimation procedure, as $x$ is decreasing from $1$ to $0$, one observes firstly that $\mathcal{L}_t$ increases linearly and, then, it displays an abrupt decrease indicating that from this point on relevant couplings are being decimated\cite{Decelle14}. In Fig. \ref{Jor1-$t$PLF} we give an instance of this behavior for the 2D short-range XY model with ordered couplings. We notice that the maximum point of $\mathcal{L}_t$ coincides with the minimum point of the reconstruction error, the latter defined as 
   \begin{eqnarray}\label{eq:errj}
   \mbox{err}_J \equiv \sqrt{\frac{\sum_{i<j} (J^{\rm inferred}_{ij} -J^{\rm true}_{ij})^2}{N(N-1)/2}} \label{err}
   \end{eqnarray}
  We stress that the ${\cal L}_t$ maximum is obtained ignoring the underlying graph, while the err$_J$ minimum can be evaluated once the true graph has been reconstructed. 
   
     \begin{figure}[t!]
   	\centering
   	\includegraphics[width=1\linewidth]{Jor1_dec_tPLF_new.eps}
   	\caption{The tilted likelyhood ${\cal L}_t$ curve and the reconstruction error vs the number of decimated couplings  for an ordered, real-valued J on 2D XY model with $N=64$ spins. The peak of ${\cal L}_t$ coincides with the dip of the error.}  
   	\label{Jor1-$t$PLF}
   \end{figure} 
       
   
   In the next sections we will show the results obtained on the  $XY$ model analyzing the performances of the two methods and comparing them also with a mean-field method \cite{Tyagi15}.
   
  
    
   \section{Inferred couplings with PLM-$l_2$}
    \label{sec:res_reg}
    \subsection{$XY$ model with real-valued couplings}
    
    In order to obtain the vector of couplings, $J_{ij}^{\rm inferred}$ the function $-\mathcal{L}_i$ is minimized through the vector of derivatives ${\partial \mathcal{L}_i}/\partial J_{ij}$. The process is repeated for all the couplings obtaining then a fully connected adjacency matrix. The results here presented are obtained with $\lambda = 0.01$.
   For the minimization we have used the MATLAB routine \emph{minFunc\_2012}\cite{min_func}. 
      
      \begin{figure}[t!]
    	\centering
    	\includegraphics[width=1\linewidth]{Jor11_2D_l2_JR_soJR_TPJR}
    	\caption{Top panels: instances of single site coupling reconstruction for the case of $N=64$ XY spins on a 2D lattice with ordered $J$ (left column) and bimodal distributed $J$ (right column). 
    	
    	Bottom panels: sorted couplings.}
    	\label{PL-Jor1}
    \end{figure}

   
To produce the data by means of numerical Monte Carlo simulations a system with $N=64$  spin variables  is considered on a deterministic 2D lattice with periodic boundary conditions. 
Each spin has then connectivity $4$, i.e., we expect to infer an adjacency matrix with $N c = 256$ couplings different from zero. 
The dynamics of the simulated model is based on the Metropolis algorithm and parallel tempering\cite{earl05} is used to speed up the thermalization of the system.
The thermalization is tested looking at the average energy over logarithmic time windows and
the acquisition of independent configurations
starts only after the system is well thermalized.

   For the values of the couplings we considered two cases: an ordered case, indicated in the figure as  $J$ ordered (e.g., left column of Fig. \ref{PL-Jor1}) where the couplings can take values $J_{ij}=0,J$, with $J=1$, 
   and a quenched disordered case, indicated in the figures as  $J$ disordered (e.g., right column of Fig. \ref{PL-Jor1})
   where the couplings can take also  negative values, i.e., 
    $J_{ij}=0,J,-J$, with a certain probability.  The results here presented were obtained with bimodal distributed $J$s: 
   
    $P(J_{ij}=J)=P(J_{ij}=-J)=1/2$.  The performances of the PLM have shown not to depend on $P(J)$. 
   
    We recall that in Sec. \ref{sec:plm} we used the temperature-rescaled notation, i.e., $J_{ij}$ stands for $J_{ij}/T$. 
   
    To analyze the performances of the PLM, in Fig. \ref{PL-Jor1} the inferred couplings, $\mathbb{J}^R_{\rm inf}$, are shown on top of the original couplings,  $\mathbb{J}^R_{\rm true}$.
     The first figure (from top) in the left column shows  the $\mathbb{J}^R_{\rm inf}$ (black) and the $\mathbb{J}^R_{\rm tru}$ (green) for a given spin
     at temperature $T/J=0.7$ and number of samples  $M=1024$. PLM appears to reconstruct the correct couplings, though zero couplings are always given a small inferred non-zero value. 
     In the left column of Fig.  \ref{PL-Jor1},  both the $\mathbb{J}^R_{\rm{inf}}$ and the $\mathbb{J}^R_{\rm{tru}}$ are sorted in decreasing order and plotted on top of each other. 
     We can clearly see that $\mathbb{J}^R_{\rm inf}$ reproduces the expected step function. Even though the jump is smeared, the difference between inferred couplings corresponding to the set of non-zero couplings 
     and to the set of zero couplings can be clearly appreciated.
     Similarly, the plots in the right column of Fig. \ref{PL-Jor1} show the results obtained for the case with  bimodal disordered couplings, for the same working temperature and number of samples. 
     In particular, note that the algorithm infers half positive and half negative couplings, as expected.
     
     
\begin{figure}
\centering
\includegraphics[width=1\linewidth]{Jor11_2D_l2_errJ_varT_varM}
\caption{Reconstruction error $\mbox{err}_J$, cf. Eq. (\ref{eq:errj}), plotted as a function of temperature (left) for three values of the number of samples $M$ and  as a function $M$ (right) for three values of temperature in the ordered system, i.e., $J_{ij}=0,1$. 
The system size is $N=64$.}
\label{PL-err-Jor1}
\end{figure}

In order to analyze the effects of the number of samples and of the temperature regimes, we plot in Fig. \ref{PL-err-Jor1} the reconstruction error, Eq. (\ref{err}), as a function of temperature for three different sample sizes $M=64,128$ and $512$. 
The error is seen to sharply rise al low temperature, incidentally, in the ordered case, for  $T<T_c \sim 0.893$, which is the Kosterlitz-Thouless transition temperature of the 2XY model\cite{Olsson92}. 
 However, we can see that if only $M=64$ samples are considered, $\mbox{err}_J$ remains high independently on the working temperature. 
 In the right plot of Fig. \ref{PL-err-Jor1},  $\mbox{err}_J$ is plotted as a function of $M$ for three different working temperatures $T/J=0.4,0.7$ and $1.3$. As we expect, 
 $\mbox{err}_J$  decreases as $M$ increases. This effect was observed also with mean-field inference techniques on the same model\cite{Tyagi15}.

To better understand the performances of the algorithms, in Fig. \ref{PL-varTP-Jor1} we show several True Positive (TP) curves obtained for various values of $M$ at three different temperatures $T$. As $M$ is large and/or temperature is not too small,  we are able to reconstruct correctly all the couplings present in the system (see bottom plots).
The True Positive curve displays how many times the inference method finds a true link of the original network  as a function of the index of the vector of sorted absolute value of reconstructed couplings $J_{ij}^{\rm inf}$. 
The index $n_{(ij)}$ represents the related spin couples $(ij)$. The TP curve is obtained as follows: 
first  the values $|J^{\rm inf}_{ij}|$ are sorted in descending order and  the spin pairs $(ij)$ are ordered according to the sorting position of $|J^{\rm inf}_{ij}|$. Then,
     	a cycle over the ordered set of pairs $(ij)$, indexed by $n_{(ij)}$, is performed, comparing with the original network coupling $J^{\rm true}_{ij}$ and verifying whether it is zero or not. The true positive curve is computed as
\begin{equation}
\mbox{TP}[n_{(ij)}]= \frac{\mbox{TP}\left[n_{(ij)}-1\right] (n_{ij}-1)+ 1 -\delta_{J^{\rm true}_{ij},0}}{n_{(ij)}}
\end{equation}
As far as $J^{\rm true}_{ij} \neq 0$, TP$=1$. As soon as the true coupling of a given $(ij)$ couple in the sorted list is zero, the TP curve departs from one. 
In our case, where the connectivity per spin of the original system is  $c=4$ and there are $N=64$ spins, we know that we will have  $256$ non-zero couplings.  
     	If the inverse problem is successful, hence, we expect a steep decrease of the TP curve when  $n_{ij}=256$ is overcome.

In Fig. \ref{PL-varTP-Jor1}
it is  shown that,  almost independently of $T/J$, the TP score improves as $M$ increases. Results are plotted for three different temperatures, $T=0.4,1$ and $2.2$, with increasing number of samples $M = 64, 128,512$ and $1024$ (clockwise). 
We can clearly appreciate the improvement in temperature if the size of the data-set is not very large: for small $M$, $T=0.4$ performs better. 
When $M$ is high enough (e.g., $M=1024$), instead, the TP curves do not appear to be strongly influenced by the temperature.

\begin{figure}[t!]
	\centering
	\includegraphics[width=1\linewidth]{Jor11_2D_l2_TPJR_varT_varM}
	\caption{TP  curves  for 2D short-range ordered $XY$ model with $N=64$ spins at three different values of $T/J$ with increasing - clockwise from top - $M$.}
	\label{PL-varTP-Jor1} 
\end{figure} 

\subsection{$XY$ model with complex-valued couplings}
For the complex $XY$ we have to contemporary  infer $2$ apart coupling matrices,  $J^R_{i j}$ and $J^I_{i j}$.  As before, a system of $N=64$ spins is considered on a 2D lattice.
For the couplings we have considered both ordered and bimodal disordered cases.
In Fig. \ref{PL-Jor3}, a single row of the matrix $J$ (top) and the whole sorted couplings (bottom) are displayed for the ordered model (same legend as in Fig. \ref{PL-Jor1}) for the real, $J^R$ (left column), and the imaginary part, $J^I$.  

\begin{figure}[t!]
	\centering
\includegraphics[width=1\linewidth]{Jor3_l2_JRJI_soJRJI_TPJRJI}
	\caption{Results related to the ordered complex XY model with $N=64$ spins on a  2D lattice. Top: instances of  single site reconstruction for the real, JR (left column), and
		the imaginary, JI (right column), part of $J_{ij}$. Bottom: sorted values of JR (left) and JI (right).}
		
	\label{PL-Jor3}
\end{figure}
 
 
  \section{PLM with Decimation}
 \label{sec:res_dec}
 

\begin{figure}[t!]
   	\centering
   	\includegraphics[width=1\linewidth]{Jor1_dec_tPLF_varT_varM}
    	\caption{Tilted Pseudolikelyhood, ${\cal L}_t$, plotted as a function of decimated couplings. Top: Different ${\cal L}_t$ curves obtained for different values of $M$ plotted on top of each other. Here $T=1.3$. The black line indicates the expected number of decimated couplings, $x^*=(N (N-1) - N c)/2=1888$. As we can see, as $M$ increases, the maximum point of ${\cal L}_t$ approaches $x^*$. Bottom: Different ${\cal L}_t$ curves obtained for different values of T with $M=2048$. We can see that, with this value of $M$, no differences can be appreciated on the maximum points of the different  ${\cal L}_t$ curves.}
   	\label{var-$t$PLF}
   \end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{Jor1_dec_tPLF_peak_statistics_varM_prob.eps}
    	\caption{Number of most likely decimated couplings, estimated by the maximum point of $\mathcal{L}_t$, as a function of the number of samples $M$. We can clearly see that the maximum point of $\mathcal{L}_t$ tends toward $x^*$, which is the right expected number of zero couplings in the system.}  
    	\label{PLF_peak_statistics}
    \end{figure}
      
      For the ordered real-valued XY model we show in Fig. \ref{var-$t$PLF}, top panel, the outcome on the tilted pseudolikelyhood, $\mathcal{L}_t$ Eq. \eqref{$t$PLF}, of the progressive decimation: from a fully connected lattice  down to an empty lattice. The figure shows the behaviour of $\mathcal{L}_t$ for three different data sizes $M$. A clear data size dependence of the maximum point of  $\mathcal{L}_t$, signalling the most likely value for decimation, is shown. For small $M$ the most likely number of  couplings is overestimated and for increasing $M$ it tends to the true value, as displayed in Fig. \ref{PLF_peak_statistics}.  In the bottom panel of Fig. \ref{var-$t$PLF} we display instead different 
 $\mathcal{L}_t$ curves obtained for three different values of $T$.
  Even though the values of $\mathcal{L}_t$ decrease with increasing temperature, the value of the most likely number of decimated couplings appears to be quite independent on $T$ with $M=2048$ number of samples.
In Fig. \ref{fig:Lt_complex} we eventually display the tilted pseudolikelyhood for a 2D network with complex valued ordered couplings, where the decimation of the real and imaginary coupling matrices proceeds in parallel, that is, 
when a real coupling is small enough to be decimated its imaginary part is also decimated, and vice versa.
One can see that though the apart errors for the real and imaginary parts are different in absolute values, they display the same dip, to be compared with the maximum point of $\mathcal{L}_t$.
     
       \begin{figure}[t!]
    	\centering
      	\includegraphics[width=1\linewidth]{Jor3_dec_tPLF_new}
      	\caption{Tilted Pseudolikelyhood, ${\cal L}_t$, plotted with the reconstruction errors for the XY model with $N=64$ spins on a 2D lattice. These results refer to the case of  ordered and complex valued couplings. The full (red) line indicates ${\cal L}_t$. The dashed (green) 
      		and the dotted (blue) lines show the reconstruction errors (Eq. \eqref{eq:errj}) obtained for the real and the imaginary couplings respectively. We can see that both ${\rm err_{JR}}$ and ${\rm err_{JI}}$ have a minimum at $x^*$.}
          	\label{fig:Lt_complex}
    \end{figure}

\begin{figure}[t!]
   	\centering
   	\includegraphics[width=1\linewidth]{Jor1_dec_JR_soJR_TPJR}
   	\caption{XY model on a 2D lattice with $N=64$ sites and real valued couplings. The graphs show the inferred (dashed black lines) and true couplings (full green lines) plotted on top of each other. The left and right columns refer to the
   		 cases of ordered and bimodal disordered couplings, respectively. Top figures: single site reconstruction, i.e., one row of the matrix $J$. Bottom figures: couplings are plotted sorted in descending order.}  
   	\label{Jor1_dec}
   \end{figure}
   
\begin{figure}[t!]
    	\centering
    	\includegraphics[width=1\linewidth]{Jor3_dec_JRJI_soJRJI_TPJRJI}
    	\caption{XY model on a 2D lattice with $N=64$ sites and ordered complex-valued couplings.
    		The inferred and true couplings are plotted on top of each other. The left and right columns show the real and imaginary parts, respectively, of the couplings. Top figures refer to a single site reconstruction, i.e., one row of the matrix $J$. Bottom figures report the couplings sorted in descending order.}
    	\label{Jor3_dec}
    \end{figure}
    
       







       \begin{figure}[t!]
     	\centering
     	\includegraphics[width=1\linewidth]{MF_PL_Jor1_2D_TPJR_varT}
     	\caption{True Positive curves obtained with the three techniques: PLM with decimation, (blue) dotted line,  PLM with $l_2$ regularization, (greed) dashed line, and mean-field, (red) full line.  These results refer to real valued ordered couplings with $N=64$ spins on a 2D lattice. The temperature is here $T=0.7$ while the four graphs refer to different sample sizes: $M$ increases clockwise.}
     	\label{MF_PL_TP}
     \end{figure}
    
    \begin{figure}[t!]
    	\centering
    	\includegraphics[width=1\linewidth]{MF_PL_Jor1_2D_errJ_varT_varM}
    	\caption{Variation of reconstruction error, ${\rm err_J}$, with respect to temperature as obtained with the three different techniques, see Fig. \ref{MF_PL_TP}, for four different sample size:  clockwise from top   $M=512,1024, 2048$ and $4096$.} 
    	\label{MF_PL_err}
    \end{figure}
    
     Once the most likely network has been identified through the decimation procedure, we perform the same analysis displayed in Fig. \ref{Jor1_dec}  for ordered and then quenched disordered real-valued couplings
and in Fig. \ref{Jor3_dec} for  complex-valued ordered couplings.  In comparison to the results shown in Sec. \ref{sec:res_reg},
  the PLM with decimation leads to rather cleaner results. In Figs. \ref{MF_PL_err} and \ref{MF_PL_TP} we compare the performances of the PLM with decimation in respect to ones of the PLM with $l_2$-regularization. These two techniques are also analysed in respect to a mean-field technique previously implemented on the same XY systems\cite{Tyagi15}.
  
    For what concerns the network of connecting links, in Fig. \ref{MF_PL_TP} we compare the TP curves obtained with the three techniques. The results refer to the case of ordered and real valued couplings, but similar behaviours were obtained for the other cases analysed. 
  The four graphs are related to different sample sizes, with $M$ increasing clockwise. When $M$ is high enough, all techniques reproduce the true network. 
  However, for lower values of $M$ the performances of the PLM with $l_2$ regularization and with decimation drastically overcome those ones of the previous mean field technique. 
  In particular, for $M=256$ the PLM techniques still reproduce the original network while the mean-field method fails to find more than half of the couplings. 
  When $M=128$, the network is clearly reconstructed only through the PLM with decimation while the PLM with $l_2$ regularization underestimates the couplings. 
  Furthermore, we notice that the PLM method with decimation is able to clearly infer the network of interaction even when $M=N$ signalling that it could be considered also in the under-sampling regime $M<N$.  
 
 
In Fig. \ref{MF_PL_err} we compare the temperature behaviour of the reconstruction error.
In can be observed that for all temperatures and for all sample sizes  the reconstruction error, ${\rm err_J}$, (plotted here in log-scale) obtained with the PLM+decimation is always smaller than 
that one obtained with the other techniques. The temperature behaviour of ${\rm err_J}$ agrees with the one already observed for Ising spins in \cite{Nguyen12b} and for XY spins  in \cite{Tyagi15} with a mean-field approach:  ${\rm err_J}$ displays a minimum around $T\simeq 1$ and then it increases for very lower $T$; however,
 the error obtained with the PLM with decimation is several times smaller  than the error estimated by the other methods.



 
 

     
     \section{Conclusions}
     \label{sec:conc}


Different statistical inference methods have been applied to the inverse problem of the XY model.
After a short review of techniques based on pseudo-likelihood and their formal generalization to the model we have tested their performances against data generated by means of Monte Carlo numerical simulations of known instances
with diluted, sparse, interactions.

The main outcome is that the best performances are obtained by means of the  pseudo-likelihood method combined with decimation. Putting to zero (i.e., decimating) very weak bonds, this technique turns out to be very precise for  problems whose real underlying interaction network is sparse, i.e., the number of couplings per variable does not scale with number of variables.
The PLM + decimation method is compared to the PLM + regularization method, with $\ell_2$ regularization and to a mean-field-based method. The behavior of the quality of the network reconstruction is analyzed by looking at the overall sorted couplings and at the single site couplings, comparing them with the real network, and at the true positive curves in all three approaches. In the PLM +decimation method, moreover, the identification of the number of decimated bonds at which the tilted pseudo-likelihood is maximum allows for a precise estimate of the total number of bonds. Concerning this technique, it is also shown that the network with the most likely number of bonds is also the one of least reconstruction error, where not only the prediction of the presence of a bond is estimated but also its value.

The behavior of the inference quality in temperature and in the size of data samples is also investigated, basically confirming the low $T$ behavior hinted by Nguyen and Berg \cite{Nguyen12b} for the Ising model. In temperature, in particular, the reconstruction error curve displays a minimum at a low temperature, close to the critical point in those cases in which a critical behavior occurs, and a sharp increase as temperature goes to zero. The decimation method, once again, appears to enhance this minimum of the reconstruction error of almost an order of magnitude with respect to other methods.
 
The techniques displayed and the results obtained in this work can be of use in any of the many systems whose theoretical representation is given by Eq. \eqref{eq:HXY} or Eq. \eqref{eq:h_im}, some of which are recalled in Sec. \ref{sec:model}. In particular, a possible application can be the field of light waves propagation through random media and the corresponding problem of the  reconstruction of an object seen through an opaque medium or a disordered optical fiber \cite{Vellekoop07,Vellekoop08a,Vellekoop08b, Popoff10a,Akbulut11,Popoff11,Yilmaz13,Riboli14}.

 	",['It outperforms mean-field methods and the PLM with $l_2$ regularization in terms of reconstruction error and true positive rate.'],6312,multifieldqa_en,en,,f3374a1971e3cfb4f241e04b82d5016d892e9e32a4aa2b53,It outperforms mean-field methods and the PLM with $l_2$ regularization in terms of reconstruction error and true positive rate.,128
What is the significance of the interlayer Berry connection polarizability?,"Paper Info

Title: Crossed Nonlinear Dynamical Hall Effect in Twisted Bilayers
Publish Date: 17 Mar 2023
Author List: 

Figure

FIG. 1.(a) Schematics of experimental setup.(b, c) Valence band structure and intrinsic Hall conductivity with respect to in-plane input for tMoTe2 at twist angles (b) θ = 1.2 • and (c) θ = 2 • in +K valley.Color coding in (b) and (c) denotes the layer composition σ z n (k).
FIG. 2. (a) The interlayer BCP G, and (b) its vorticity [∂ k × G]z on the first valence band from +K valley of 1.2 • tMoTe2.Background color and arrows in (a) denote the magnitude and vector flow, respectively.Grey curves in (b) show energy contours at 1/2 and 3/4 of the band width.The black dashed arrow denotes direction of increasing hole doping level.Black dashed hexagons in (a, b) denote the boundary of moiré Brillouin zone (mBZ).
FIG. 3. (a-c) Three high-symmetry stacking registries for tBG with a commensurate twist angle θ = 21.8 • .Lattice geometries with rotation center on an overlapping atomic site (a, b) and hexagonal center (c).(d) Schematic of the moiré pattern when the twist angle slightly deviates from 21.8 • , here θ = 21 • .Red squares marked by A, B and C are the local regions that resemble commensurate 21.8 • patterns in (a), (b) and (c), respectively.(e, f) Low-energy band structures and intrinsic Hall conductivity of the two geometries [(a) and (b) are equivalent].The shaded areas highlight energy windows ∼ ω around band degeneracies where interband transitions, not considered here, may quantitatively affect the conductivity measured.
FIG. S4.Band structure and layer composition σ z n in +K valley of tBG (left panel) and the intrinsic Hall conductivity (right panel) at three different twist angle θ.The shaded areas highlight energy windows ∼ ω around band degeneracies in which the conductivity results should not be considered.Here σH should be multiplied by a factor of 2 accounting for spin degeneracy.

abstract

We propose an unconventional nonlinear dynamical Hall effect characteristic of twisted bilayers. The joint action of in-plane and out-of-plane ac electric fields generates Hall currents j ∼ Ė⊥ × E in both sum and difference frequencies, and when the two orthogonal fields have common frequency their phase difference controls the on/off, direction and magnitude of the rectified dc Hall current.
This novel intrinsic Hall response has a band geometric origin in the momentum space curl of interlayer Berry connection polarizability, arising from layer hybridization of electrons by the twisted interlayer coupling. The effect allows a unique rectification functionality and a transport probe of chiral symmetry in bilayer systems.
We show sizable effects in twisted homobilayer transition metal dichalcogenides and twisted bilayer graphene over broad range of twist angles. Nonlinear Hall-type response to an in-plane electric field in a two dimensional (2D) system with time reversal symmetry has attracted marked interests . Intensive studies have been devoted to uncovering new types of nonlinear Hall transport induced by quantum geometry and their applications such as terahertz rectification and magnetic information readout .
Restricted by symmetry , the known mechanisms of nonlinear Hall response in quasi-2D nonmagnetic materials are all of extrinsic nature, sensitive to fine details of disorders , which have limited their utilization for practical applications. Moreover, having a single driving field only, the effect has not unleashed the full potential of nonlinearity for enabling controlled gate in logic operation, where separable inputs (i.e., in orthogonal directions) are desirable.
The latter, in the context of Hall effect, calls for control by both out-of-plane and in-plane electric fields. A strategy to introduce quantum geometric response to out-of-plane field in quasi-2D geometry is made possible in van der Waals (vdW) layered structures with twisted stacking . Taking homobilayer as an example, electrons have an active layer degree of freedom that is associated with an out-of-plane electric dipole , whereas interlayer quantum tunneling rotates this pseudospin about in-plane axes that are of topologically nontrivial textures in the twisted landscapes .
Such layer pseudospin structures can underlie novel quantum geometric properties when coupled with out-ofplane field. Recent studies have found layer circular photogalvanic effect and layer-contrasted time-reversaleven Hall effect , arising from band geometric quantities. In this work we unveil a new type of nonlinear Hall effect in time-reversal symmetric twisted bilayers, where an intrinsic Hall current emerges under the combined action of an in-plane electric field E and an out-of-plane ac field E ⊥ (t): j ∼ Ė⊥ × E [see Fig. ].
Having the two driving fields (inputs) and the current response (output) all orthogonal to each other, the effect is dubbed as the crossed nonlinear dynamical Hall effect. This is also the first nonlinear Hall contribution of an intrinsic nature in nonmagnetic materials without external magnetic field, determined solely by the band structures, not relying on extrinsic factors such as disorders and relaxation times.
The effect arises from the interlayer hybridization of electronic states under the chiral crystal symmetry characteristic of twisted bilayers, and has a novel band geometric origin in the momentum space curl of interlayer Berry connection polarizability (BCP). Having two driving fields of the same frequency, a dc Hall current develops, whose on/off, direction and magnitude can all be controlled by the phase difference of the two fields, which does not affect the magnitude of the double-frequency component.
Such a characteristic tunability renders this effect a unique approach to rectification and transport probe of chiral bilayers. As examples, we show sizable effects in small angle twisted transition metal dichalcogenides (tTMDs) and twisted bilayer graphene (tBG), as well as tBG of large angles where Umklapp interlayer tunneling dominates.
Geometric origin of the effect. A bilayer system couples to in-plane and out-of-plane driving electric fields in completely different ways. The in-plane field couples to the 2D crystal momentum, leading to Berry-phase effects in the 2D momentum space . In comparison, the outof-plane field is coupled to the interlayer dipole moment p in the form of −E ⊥ p, where p = ed 0 σz with σz as the Pauli matrix in the layer index subspace and d 0 the interlayer distance.
When the system has a more than twofold rotational axis in the z direction, as in tBG and tTMDs, any in-plane current driven by the out-of-plane field alone is forbidden. It also prohibits the off-diagonal components of the symmetric part of the conductivity tensor σ ab = ∂j a /∂E ||,b with respect to the in-plane input and output.
Since the antisymmetric part of σ ab is not allowed by the Onsager reciprocity in nonmagnetic systems, all the off-diagonal components of σ ab is forbidden, irrespective of the order of out-of-plane field. On the other hand, as we will show, an in-plane Hall conductivity σ xy = −σ yx can still be driven by the product of an in-plane field and the time variation rate of an outof-plane ac field, which is a characteristic effect of chiral bilayers.
To account for the effect, we make use of the semiclassical theory . The velocity of an electron in a bilayer system is given by where k is the 2D crystal momentum. Here and hereafter we suppress the band index for simplicity, unless otherwise noted. The three contributions in this equation come from the band velocity, the anomalous velocities induced by the k -space Berry curvature Ω k and by the hybrid Berry curvature Ω kE ⊥ in the (k, E ⊥ ) space.
For the velocity at the order of interest, the k-space Berry curvature is corrected to the first order of the variation rate of out-of-plane field Ė⊥ as Here A = u k |i∂ k |u k is the unperturbed k-space Berry connection, with |u k being the cell-periodic part of the Bloch wave, whereas is its gauge invariant correction , which can be identified physically as an in-plane positional shift of an electron induced by the time evolution of the out-of-plane field.
For a band with index n, we have whose numerator involves the interband matrix elements of the interlayer dipole and velocity operators, and ε n is the unperturbed band energy. Meanwhile, up to the first order of in-plane field, the hybrid Berry curvature reads Here A E || is the k-space Berry connection induced by E || field , which represents an intralayer positional shift and whose detailed expression is not needed for our purpose.
and is its first order correction induced by the in-plane field. In addition, ε = ε + δε, where δε = eE • G Ė⊥ is the field-induced electron energy . Given that A E || is the E ⊥ -space counterpart of intralayer shift A E || , and that E ⊥ is conjugate to the interlayer dipole moment, we can pictorially interpret A E || as the interlayer shift induced by in-plane field.
It indeed has the desired property of flipping sign under the horizontal mirror-plane reflection, hence is analogous to the so-called interlayer coordinate shift introduced in the study of layer circular photogalvanic effect , which is nothing but the E ⊥ -space counterpart of the shift vector well known in the nonlinear optical phenomenon of shift current.
Therefore, the E ⊥ -space BCP eG/ can be understood as the interlayer BCP. This picture is further augmented by the connotation that the interlayer BCP is featured exclusively by interlayer-hybridized electronic states: According to Eq. ( ), if the state |u n is fully polarized in a specific layer around some momentum k, then G (k) is suppressed.
With the velocity of individual electrons, the charge current density contributed by the electron system can be obtained from where [dk] is shorthand for n d 2 k/(2π) 2 , and the distribution function is taken to be the Fermi function f 0 as we focus on the intrinsic response. The band geometric contributions to ṙ lead to a Hall current
where is intrinsic to the band structure. This band geometric quantity measures the k-space curl of the interlayer BCP over the occupied states, and hence is also a characteristic of layer-hybridized electronic states. Via an integration by parts, it becomes clear that χ int is a Fermi surface property.
Since χ int is a time-reversal even pseudoscalar, it is invariant under rotation, but flips sign under space inversion, mirror reflection and rotoreflection symmetries. As such, χ int is allowed if and only if the system possesses a chiral crystal structure, which is the very case of twisted bilayers .
Moreover, since twisted structures with opposite twist angles are mirror images of each other, whereas the mirror reflection flips the sign of χ int , the direction of Hall current can be reversed by reversing twist direction. Hall rectification and frequency doubling. This effect can be utilized for the rectification and frequency doubling of an in-plane ac input E = E 0 cos ωt, provided that the out-of-plane field has the same frequency, namely E ⊥ = E 0 ⊥ cos (ωt + ϕ).
The phase difference ϕ between the two fields plays an important role in determining the Hall current, which takes the form of j = j 0 sin ϕ + j 2ω sin(2ωt + ϕ). ( Here ω is required to be below the threshold for direct interband transition in order to validate the semiclassical treatment, and σ H has the dimension of conductance and quantifies the Hall response with respect to the in-plane input.
In experiment, the Hall output by the crossed nonlinear dynamic Hall effect can be distinguished readily from the conventional nonlinear Hall effect driven by in-plane field alone, as they are odd and even, respectively, in the inplane field. One notes that while the double-frequency component appears for any ϕ, the rectified output is allowed only if the two crossed driving fields are not in-phase or antiphase.
Its on/off, chirality (right or left), and magnitude are all controlled by the phase difference of the two fields. Such a unique tunability provides not only a prominent experimental hallmark of this effect, but also a controllable route to Hall rectification. In addition, reversing the direction of the out-of-plane field switches that of the Hall current, which also serves as a control knob.
Application to tTMDs. We now study the effect quantitatively in tTMDs, using tMoTe 2 as an example (see details of the continuum model in ). For illustrative purposes, we take ω/2π = 0.1 THz and E 0 ⊥ d 0 = 10 mV in what follows. Figures ) and (c) present the electronic band structures along with the layer composition σ z n (k) at twist angles θ = 1.2 • and θ = 2 • .
In both cases, the energy spectra exhibit isolated narrow bands with strong layer hybridization. At θ = 1.2 • , the conductivity shows two peaks ∼ 0.1e 2 /h at low energies associated with the first two valence bands. The third band does not host any sizable conductivity signal. At higher hole-doping levels, a remarkable conductivity peak ∼ e 2 /h appears near the gap separating the fourth and fifth bands.
At θ = 2 • , the conductivity shows smaller values, but the overall trends are similar: A peak ∼ O(0.01)e 2 /h appears at low energies, while larger responses ∼ O(0.1)e 2 /h can be spotted as the Fermi level decreases. One can understand the behaviors of σ H from the interlayer BCP in Eq. ( ). It favors band near-degeneracy regions in k -space made up of strongly layer hybridized electronic states.
As such, the conductivity is most pro- nounced when the Fermi level is located around such regions, which directly accounts for the peaks of response in Fig. that [∂ k × G] z is negligible at lower energies, and it is dominated by positive values as the doping increases, thus the conductivity rises initially.
When the doping level is higher, regions with [∂ k × G] z < 0 start to contribute, thus the conductivity decreases after reaching a maximum. Application to tBG. The second example is tBG. We focus on commensurate twist angles in the large angle limit in the main text , which possess moiré-lattice assisted strong interlayer tunneling via Umklapp processes .
This case is appealing because the Umklapp interlayer tunneling is a manifestation of discrete translational symmetry of moiré superlattice, which is irrelevant at small twist angles and not captured by the continuum model but plays important roles in physical contexts such as higher order topological insulator and moiré excitons .
The Umklapp tunneling is strongest for the commensurate twist angles of θ = 21.8 • and θ = 38.2 • , whose corresponding periodic moiré superlattices have the smallest lattice constant ( √ 7 of the monolayer counterpart). Such a small moiré scale implies that the exact crystalline symmetry, which depends sensitively on fine details of rotation center, has critical influence on lowenergy response properties.
To capture the Umklapp tunneling, we employ the tight-binding model . Figures ) and (c) show two distinct commensurate structures of tBG at θ = 21.8 • belonging to chiral point groups D 3 and D 6 , respectively. The atomic configurations in Figs. ) are equivalent, which are constructed by twisting AA-stacked bilayer graphene around an overlapping atom site, and that in Fig. ) is obtained by rotating around a hexagonal center.
Band structures of these two configurations are drastically different within a low-energy window of ∼ 10 meV around the κ point . Remarkably, despite large θ, we still get σ H ∼ O(0.001) e 2 /h (D 3 ) and ∼ O(0.1) e 2 /h (D 6 ), which are comparable to those at small angles (cf. Fig. in the Supplemental Material ).
Such sizable responses can be attributed to the strong interlayer coupling enabled by Umklapp processes . Apart from different intensities, the Hall conductivities in the two stacking configurations have distinct energy dependence: In Fig. , σ H shows a single peak centered at zero energy; In Fig. (f), it exhibits two antisymmetric peaks around zero.
The peaks are centered around band degeneracies, and their profiles can be understood from the distribution of [∂ k × G] z . Figure (d) illustrates the atomic structure of tBG with a twist angle slightly deviating from θ = 21.8 • , forming a supermoiré pattern. In short range, the local stacking geometries resemble the commensurate configurations at θ = 21.8 • , while the stacking registries at different locales differ by a translation.
Similar to the moiré landscapes in the small-angle limit, there also exist high-symmetry locales: Regions A and B enclose the D 3 structure, and region C contains the D 6 configuration. Position-dependent Hall response is therefore expected in such a supermoiré. As the intrinsic Hall signal from the D 6 configuration dominates [see Figs.
3(e) vs (f)], the net response mimics that in Fig. . Discussion. We have uncovered the crossed nonlinear dynamical intrinsic Hall effect characteristic of layer hybridized electronic states in twisted bilayers, and elucidated its geometric origin in the k -space curl of interlayer BCP. It offers a new tool for rectification and frequency doubling in chiral vdW bilayers, and is sizable in tTMD and tBG.
Here our focus is on the intrinsic effect, which can be evaluated quantitatively for each material and provides a benchmark for experiments. There may also be extrinsic contributions, similar to the side jump and skew scattering ones in anomalous Hall effect. They typically have distinct scaling behavior with the relaxation time τ from the intrinsic effect, hence can be distinguished from the latter in experiments .
Moreover, they are suppressed in the clean limit ωτ 1 [(ωτ ) 2 1, more precisely] . In high-quality tBG samples, τ ∼ ps at room temperature . Much longer τ can be obtained at lower temperatures. In fact, a recent theory explaining well the resistivity of tBG predicted τ ∼ 10 −8 s at 10 K . As such, high-quality tBG under low temperatures and sub-terahertz input (ω/2π = 0.1 THz) is located in the clean limit, rendering an ideal platform for isolating the intrinsic effect.
This work paves a new route to driving in-plane response by out-of-plane dynamical control of layered vdW structures . The study can be generalized to other observables such as spin current and spin polarization, and the in-plane driving can be statistical forces, like temperature gradient. Such orthogonal controls rely critically on the nonconservation of layer pseudospin degree of freedom endowed by interlayer coupling, and constitute an emerging research field at the crossing of 2D vdW materials, layertronics, twistronics and nonlinear electronics.
This work is supported by the Research Grant Council of Hong Kong (AoE/P-701/20, HKU SRFS2122-7S05), and the Croucher Foundation. W.Y. also acknowledges support by Tencent Foundation. Cong Chen, 1, 2, * Dawei Zhai, 1, 2, * Cong Xiao, 1, 2, † and Wang Yao 1, 2, ‡ 1 Department of Physics, The University of Hong Kong, Hong Kong, China 2 HKU-UCAS Joint Institute of Theoretical and Computational Physics at Hong Kong, China Extra figures for tBG at small twist angles Figure (a) shows the band structure of tBG with θ = 1.47 • obtained from the continuum model .
The central bands are well separated from higher ones, and show Dirac points at κ/κ points protected by valley U (1) symmetry and a composite operation of twofold rotation and time reversal C 2z T . Degeneracies at higher energies can also be identified, for example, around ±75 meV at the γ point. As the two Dirac cones from the two layers intersect around the same area, such degeneracies are usually accompanied by strong layer hybridization [see the color in the left panel of Fig. ].
Additionally, it is well-known that the two layers are strongly coupled when θ is around the magic angle (∼ 1.08 • ), rendering narrow bandwidths for the central bands. As discussed in the main text, coexistence of strong interlayer hybridization and small energy separations is expected to contribute sharp conductivity peaks near band degeneracies, as shown in Fig. .
In this case, the conductivity peak near the Dirac point can reach ∼ 0.1e 2 /h, while the responses around ±0.08 eV are smaller at ∼ 0.01e 2 /h. The above features are maintained when θ is enlarged, as illustrated in Figs. ) and (c) using θ = 2.65 • and θ = 6.01 • . Since interlayer coupling becomes weaker and the bands are more separated at low energies when θ increases, intensity of the conductivity drops significantly.
We stress that G is not defined at degenerate points, and interband transitions may occur when energy separation satisfies |ε n − ε m | ∼ ω, the effects of which are not included in the current formulations. Consequently, the results around band degeneracies within energy ∼ ω [shaded areas in Fig. ] should be excluded.",['The momentum space curl of the interlayer Berry connection polarizability generates the crossed nonlinear dynamical Hall effect.'],3508,multifieldqa_en,en,,7d9bc0ed11dfc39ab91980d15a95fcd9f5902d25f85ec436,The momentum space curl of the interlayer Berry connection polarizability generates the crossed nonlinear dynamical Hall effect.,128
What is Professor Tulis's forthcoming book?,"UT College of Liberal Arts: College of Liberal Arts University of Texas at Austin Departments Graduate Resources Undergraduate Resources Courses Online Courses Dean's Office Alumni & Giving Faculty by Department Search the College of Liberal Arts
next profile Jeffrey Tulis Associate Professor — Ph.D.,
E-mail: tulis@austin.utexas.edu
Office: MEZ 3.152
Political Theory and American Politics
Professor Tulis's interests bridge the fields of political theory and American politics, including more specifically, American political development, constitutional theory, political philosophy and the American presidency. His publications include The Presidency in the Constitutional Order (LSU, 1981; Transaction, 2010), The Rhetorical Presidency (Princeton, 1987), The Constitutional Presidency (Johns Hopkins 2009), The Limits of Constitutional Democracy (Princeton, 2010) and recent journal articles and chapters on constitutional interpretation, the logic of political change, and the meaning of political success. Four collections of essays on The Rhetorical Presidency with responses by Tulis have been published, including a special double issue of Critical Review: An Interdisciplinary Journal of Politics and Society, (2007), where his book is described as ""one of the two or three most important and perceptive works written by a political scientist in the twentieth century.""
He has served as President of the Politics and History Section of the American Political Science Association. He received the President's Associates Teaching Excellence Award at the University of Texas. He has held research fellowships from NEH, ACLS, Olin Foundation, Harvard Law School, and the Mellon Preceptorship at Princeton University, where he taught before moving to Texas. He has held visiting positions at Notre Dame and Harvard. He has served as associate chair of the Department of Government from 1989-2001 and was acting chair during 1992-93. and for part of each year between 1989 and 2001. During the academic year 2008-09, he was a Laurance S. Rockefeller Visiting Fellow at the University Center for Human Values at Princeton. During Spring 2016, he was a Dahrendorf Visiting Fellow at the London School of Economics and Political Science.
His forthcoming books include: Legacies of Losing in American Politics, with Nicole Mellow (University of Chicago Press, Fall 2017), and an expanded edition of The Rhetorical Presidency in the Princeton Classics series (Princeton, Fall 2017). For two decades he served as co-editor of the Johns Hopkins Series in Constitutional Thought, and he currently co-edits (with Sanford Levinson) Constitutional Thinking, a Series at the University Press of Kansas.
GOV 370L • Pres In Constitutional Ord 38840 • Spring 2017 Meets MW 2:30PM-4:00PM CAL 221 show description
GOV 370 Seminar: The Presidency in the Constitutional Order
Spring 2017 Unique # 38840
MW 2:30 to 4pm GDC 2.402
Jeffrey K. Tulis
In this Seminar we will discuss a series of constitutional problems including: the problem of executive energy in the American Constitution; presidential selection and the problem of political legitimacy; separation of powers; delegation of powers, the constitutional status of war and foreign affairs, administration and bureaucracy and the meaning of leadership in the constitutional order.
Seminar will meet twice a week and regular attendance and thorough preparation for discussion is expected. Unexcused absence from more than three classes will result in failure of the participation component of the course. There will also be pop quizzes on the reading that will count as part of your participation grade. In addition to class participation, course requirements include four short analytic essays, and one in-class test. The course grade will be calculated as follows:
Seminar participation: 20%
In-class test: 20%
Three analytic essays 60% (20% each)
Class participation is especially important. Preparation for seminar and for your in-class test will be enhanced by careful note taking on the readings. If students appear to be unprepared, pop quizzes will be given and the grades on them will affect the participation component of your course grade.
Texts: (tentative)
Joseph M. Bessette and Jeffrey K. Tulis, The Constitutional Presidency
Michael Nelson, The Presidency in the Political System (tenth edition)
Richard Ellis and Michael Nelson, Debating the Presidency (third edition)
The Federalist (any edition, or online) GOV 310L • American Government-Honors 38335 • Fall 2016 Meets TTH 3:30PM-5:00PM BEN 1.106 show description
GOV 310 (Honors) (38335) Fall 2016
TTH 3:30-5:00pm, BEN 1.106
This honors seminar offers an introduction to American politics that emphasizes the confluence of ideas, mores, institutions, and interests, in the constitutional system. This course covers more theory, and the readings are more demanding, than other versions of GOV 310. One of the main objectives of the course is to deepen your understanding of the practical aspects of contemporary public affairs by developing your ability to understand the theoretical foundations of American politics. Although we cover the nuts and bolts of politics there is much more theory in this version of GOV 310. If you have registered for this section mainly because 310 is a legislative requirement that you need to fulfill, this is not the right version for you. There is a substantial workload in this class.
Regular attendance, thorough and timely preparation, and active participation are all necessary to do well.
Four essays (approximately 1000 words each). Three of these will be assigned analytic essay topics. The last will be a book review of a title chosen by the student from a long list of provided possibilities. (15% each essay, 60% of total course grade)
Two in-class tests. These will count 15% each, 30% of total course grade.
Class participation. (10% of course grade). Both informed participation and occasional leadership of the seminar will be graded.
No make-up exams or late papers, except for documented medical or other emergencies.
Mark Landy and Sidney M. Milkis, American Government: Enduring Principles, Critical Choices, Third Edition
Mary Nichols and David Nichols, Readings in American Government, Ninth Edition
Thomas Mann and Norman Ornstein, Its Even Worse Than It Looks: How the American Constitutional System Collided With the New Politics of Extremism
Bruce Ackerman,Before the Next Attack: Preserving Civil Liberties in an Age of Terrorism GOV 381L • Constitutional Conflict 38660 • Fall 2016 Meets W 3:30PM-6:30PM BAT 5.102 show description
GOV 381L Fall 2016
Constitutional Conflict
W 3:30-6:30pm, BAT 5.102
Many of the most important debates regarding the nature and character of contemporary American politics are essentially arguments regarding the structure of separation of powers. In this seminar we will consider such questions as whether the American system is prone to deadlock of stalemate in the construction of national policy; whether conflict is a hindrance to institutional responsibility or an essential attribute of responsibility; whether there are “political questions” especially suitable to resolution between President and Congress; how one can distinguish salutary from pathological conflict, and whether it is truly possible to harness the ambition of office holders to the duties of their office.
More specifically, we will review literature and arguments regarding constitutional reform; divided government; separation of powers theory; and case studies of Supreme Court appointments; the budget process; and war powers and foreign affairs. In these contexts we will also discuss current controversies surrounding war authorization, intelligence and secrecy, sequestration, government shut downs and budget resolutions, and debt ceiling politics.
The course is designed to accommodate two different student needs: it will provide a good overview of important literature relevant to the comprehensive examination in American politics and it will provide opportunities for research. This subject area is a treasure trove of “hot” topics, publication possibilities, subjects for MA theses and Ph.D. dissertations. I will tailor the written requirements to the objectives of individual students.
1. All students will prepare a short analytic essay early in the semester, and an annotated bibliography at mid-semester. These assignments will count (30%) of the grade.
2. Students interested primarily in exam preparation will complete an examination near the end of the semester based on study questions assigned in advance. OR
Students interested in research will write a 20-25 page paper. (60%)
3. A basic requirement of the course is that students prepare for each seminar by carefully reading the material assigned for that week. Class discussion is an essential component of the course. (10%)
Tentative Texts:
Jones, Separate But Equal Branches
Silverstein, Imbalance of Powers
Wilson & Schram, Separation of Powers and Good Government
Burgess, Contest for Constitutional Authority
Farrier, Passing the Buck: Congress, the Budget and Deficits
Weissman, A Culture of Deference
Zeisberg, War Powers: The Politics of Constitutional Authority
Fisher, Congressional Abdication on War and Spending
Lowi, The End of Liberalism GOV 379S • Regime Persp Amer Poltc-Honors 38105 • Spring 2016 Meets TH 3:30PM-6:30PM GAR 1.134 (also listed as CTI 335, LAH 350) show description
GOV 379S Regime Perspectives on American Politics
This is a seminar on American politics and culture. Two purposes govern the selection of texts for the course and guide our discussion of them. All of our texts attempt to look at American politics as a whole. Most books and courses on America look at only a part, such as the Presidency, or elections, or popular culture. Here we attempt to think about how the parts of America fit together. Even when these texts speak about a part, for example an institution such as the presidency or the Congress, they present the topic from a vantage point on the whole polity. To see the polity as a whole also means that we will have to revisit and rethink aspects of our political life that we take for granted – that we don’t examine because those parts have become so natural or familiar to us. Seeing the polity whole enables us to render the familiar unfamiliar, to make what we take for granted strange and new.
To see the polity as a whole requires that we get some distance from our subject, much as to see the planet earth as a whole requires one to look at it from outer space. Just as it is difficult to get visual perspective on a place living within it, it is difficult to understand the promise or pathologies of a regime from within. To get critical distance from our politics, we will closely study three sets of texts that look at American politics from a distance. The first part of the course will recover the perspective of the founding debate between Federalists and Anti-federalists. This fundamental debate reveals what is a stake in the basic architecture of the American regime. The second part of the course is a close study of Tocqueville’s Democracy in America. Regarded by many as the best book ever written on democracy and the best book written on America, Tocqueville sees our polity whole because he looks at it from the vantage point of Europe, in general, and France, in particular. In the third part of the seminar we think about American politics from the perspective of thoughtful commentators who feel only nominally included in the polity. Half in and half out, these extraordinary black American writers reveal fissures and fault lines in the American regime. We end the class with a discussion of America’s place in the world today – examining a speech by a writer who articulately raises challenges to our self-understanding that are inarticulately expressed today in rage and ranting from enemies of the United States.
Three take home analytic essays, chosen from a list of topics I provide, each weighted 25% of the course grade. Late essays will not be accepted, except with a doctor’s excuse or a Dean’s excuse for family emergency.
OR as an option: you may write the two short essays (both together weighted 25%) and do a longer 15 page paper on a topic of your choice in consultation with me (weighted 50% of your course grade). Government honors students who are thinking of doing an honors thesis next year may prefer this option to begin to develop research and writing skills for longer work. Students who prefer this option will need to designate their preferred third short essay and have discussed with me a topic for their long paper by March 30. Texts:
Selected Anti-Federalist writings
Tocqueville, Democracy in America
Essays, speeches and articles by Frederick Douglass, W.E.B. Dubois, Booker T. Washington, James Baldwin and Ralph Ellison GOV 382M • Democratic Theory 38120 • Spring 2016 Meets M 3:30PM-6:30PM BAT 1.104 show description
GOV 382M (38120)
Democratic Theory Spring 2016
This is a graduate seminar on contemporary topics in democratic theory. Topics to be covered include: democratic epistemology; deliberative democracy; the meaning of the people; oracular democracy; agonistic democracy; and possibly new theories of republicanism, representation and partisanship.
Texts (tentative)
Helene Landemore, Democratic Reason
Jeffrey Edward Green, The Eyes of the People
Amy Gutmann and Dennis Thompson, Why Deliberative Democracy?
Alan Keenan, Democracy in Question
Jason Frank, Constituent Moments
Jason Frank, Publius and Political Imagination
Nadia Urbanati, Democracy Disfigured
Russell Muirhead, Partisanship in a Polarized Age
Bryan Garsten, manuscript
Active seminar participation; an annotated bibliography or review essay; a research/analytic paper. GOV 310L • American Government-Honors 37615 • Fall 2015 Meets TTH 2:00PM-3:30PM BEN 1.106 show description
TTH 2-3:30/BEN 1.106
Bruce Ackerman,Before the Next Attack: Preserving Civil Liberties in an Age of Terrorism GOV 370L • Presidency In Constitutl Order 37845 • Fall 2015 Meets TTH 5:00PM-6:30PM PAR 310 show description
GOV 370L (37845)
TTH 5-6:30 PAR 310
The Presidency in the Constitutional Order
A study of the place of the presidency in the American political order that stresses tension between power and accountability inherent in the office and the system. Topics include: separation of powers, presidential selection, impeachment, relations with Congress and bureaucracy, emergency powers, presidential character, and leadership.
This is a very demanding writing flag class. If you are enrolling in this class just in order to satisfy the writing flag, you are in the wrong class. Interest in political theory and willingness to work very hard are necessary for success in this class.
Joseph M. Bessette, The Constitutional Presidency
Andrew Rudalevige, The New Imperial Presidency
Bruce Ackerman, The Rise and Decline of the American Republic
Michael Nelson, ed., The Presidency in the Political System
Michael Nelson, ed., The Evolving Presidency
Louis Fisher, Constitutional Conflicts Between Congress and the President
Active and prepared class participation
Regular quizzes on the reading
Four analytic essays (approximately 1200 words).
One term paper, (approximately 5000 words). GOV 379S • Regime Persp On Amer Politics 38100 • Spring 2015 Meets T 3:30PM-6:30PM MEZ 1.104 (also listed as LAH 350) show description
Essays, speeches and articles by Frederick Douglass, W.E.B. Dubois, Booker T. Washington, James Baldwin and Ralph Ellison GOV 382M • Tocqueville 38135 • Spring 2015 Meets M 3:30PM-6:30PM BAT 5.102 show description
This graduate seminar will be devoted to close readings of two principal writings of Tocqueville: Democracy in America and The Ancien Regime and the Revolution. We will also assess some of the best secondary studies of Tocqueville, including work by Sheldon Wolin, Harvey Mansfield, Delba Winthrop, Jon Elster, Francois Furet, and a book by Pierre Manent.
Course requirements will include two very short analytic essays and one seminar paper (20-25 pages). GOV 310L • American Government-Honors 38722 • Fall 2014 Meets TTH 2:00PM-3:30PM GAR 2.112 show description
Joseph M. Bessette and John J. Pitney, American Government and Politics: Deliberation, Democracy and Citizenship
Mary Nichols and David Nichols, Readings in American Government
Bruce Ackerman,Before the Next Attack: Preserving Civil Liberties in an Age of Terrorism GOV 370L • Presidency In Constitutl Order 38977 • Fall 2014 Meets TTH 9:30AM-11:00AM CBA 4.332 show description
A study of the place of the presidency in the American political order that stresses
tension between power and accountability inherent in the office and the system.
Topics include: separation of powers, presidential selection, impeachment,
relations with Congress and bureaucracy, emergency powers, presidential
character, and leadership.
This is a very demanding writing flag class. If you are enrolling in this class just in order
to satisfy the writing flag, you are in the wrong class. Interest in political theory and willingness
to work very hard are necessary for success in this class.
One term paper, (approximately 5000 words). GOV 379S • Regime Persp On Amer Politics 39395 • Spring 2014 Meets T 3:30PM-6:30PM MEZ 1.104 (also listed as CTI 335, LAH 350) show description
Essays, speeches and articles by Frederick Douglass, W.E.B. Dubois, Booker T. Washington, James Baldwin and Ralph Ellison GOV 381L • Constitutional Conflict 39415 • Spring 2014 Meets M 3:30PM-6:30PM BAT 1.104 show description
Lowi, The End of Liberalism GOV 330K • The American President 39140 • Fall 2013 Meets MW 3:00PM-4:30PM MEZ B0.306 show description
This course offers an over view of the place of the presidency in the American political order. Topics covered include: constitutional design of the office; nominations and elections; legislative leadership; leadership of the bureaucracy; staffing and organizing the White House; the presidency and the judiciary; war and emergencies. We will spend extra time this fall on the presidential campaign and election of 2012.
Two in-class examinations (50% of the final grade)
One short (1000 word) take-home essay (30% of the final grade)
Class participation and quizzes (20% of the final grade)
Richard J. Ellis, The Development of the American Presidency (Routledge, 2012)
Richard J. Ellis and Michael Nelson, eds, Debating the American Presidency, (2nd edition, CQ Press, 2009)
Packet of selected primary texts (to be linked or posted on Blackboard). GOV 330K • The American President 39145 • Fall 2013 Meets MW 5:00PM-6:30PM MEZ B0.306 show description
Packet of selected primary texts (to be linked or posted on Blackboard). GOV 381L • American Founding 39040 • Spring 2013 Meets T 6:30PM-9:30PM BAT 1.104 show description
NOTE WELL: Course meets Tuesdays, 6:30 to 9:30pm
Batts Hall 1.104
This is a seminar on American political thought and constitutional design. It is designed for students of American politics and political theory. The principal themes include: 1) the nature of founding and its constitutive significance; 2) the relation of structure and power in American politics; 3) the meaning and significance of the Federalist/Anti-Federalist debate; 4) the philosophic background of the American founding; and 5) the relevance of the founding to debate to prospects for, and pathologies of, American politics today.
We will conduct a close reading of the Madison’s Notes, of The Federalist, and selected Anti-Federalist writings. We will also study a larger and growing body of secondary literature on the constitutional convention, ratification and early American political thought.
James Madison, Notes of the Debates: In the Federal Convention of 1787
The Federalist (Rossiter, ed.)
The Anti-Federalist (Storing, ed.)
David Brian Robertson, The Constitution and America’s Destiny (2005)
Pauline Maier, Ratification (2012)
Gordon Wood, The Idea of America (2011)
Jack Rakove, Original Meanings: Politics & Ideas in the Making of the Constitution
Herbert Storing, What the Anti-Federalists Were For (1981)
Numerous essays and articles (to be posted on line or gathered in packet)
Grading: Active seminar participation, including three short papers and presentations (40%) and one article-length seminar paper (60%) T C 357 • Amer Founding/Probs Const Des 43095 • Spring 2013 Meets M 3:30PM-6:30PM CRD 007B show description
The American Founding and Problems of Constitutional Design
Jeffrey Tulis, Associate Professor, Department of Government
Sanford Levinson, Professor, School of Law
This Plan II seminar will be built around a close reading of the debates that informed the drafting and ratification of the U.S. Constitution. We aim to recover the perspective of these founding thinkers -- their way of thinking -- as much as their concrete ideas, in order to raise fundamental questions about the American political order today. Are some of the most important pathologies of American politics today rooted in design features of our original political architecture? Are the original answers to basic founding questions (such as ""how democratic is our Constitution?) still adequate for contemporary circumstances? What features of the Constitution should we preserve and what features should we amend, if possible? Would it be good for the polity as a whole to reconsider these questions in a new constitutional convention today, or would such an event be a political nightmare? Our reading will include notes from the founding conventions, writings by Federalists and Anti-Federalists, and present-day critiques of the American political order. Our aim will be to generate a dialogue between the thought of the founders and some of the best present day critics and supporters of the Constitution.
James Madison, Notes of the Debates in the Federal Convention
The Federalist, ed. Clinton Rossiter
The Anti-Federalist, ed. Herbert Storing
Pauline Maier, Ratification: The People Debate the Constitution, 1787-1788
Sanford Levinson, Framed: America’s 51 Constitutions and the Crisis of Governance
Bruce Ackerman, The Decline and Fall of the American Republic
Robert Goldwin, ed. How Democratic is the Constitution?
a course packet of selected articles, essays, and additional primary materials.
Class participation, including at least one presentation of a short discussion paper 25%
One take-home analytic essay 25%
One term paper 50%
About the Professors:
Professor Tulis's interests bridge the fields of political theory and American politics, including more specifically, American political development, constitutional theory, political philosophy and the American presidency. He received the President's Associates Teaching Excellence Award at the University of Texas. He has held research fellowships from NEH, ACLS, Olin Foundation, Harvard Law School, and the Mellon Preceptorship at Princeton University, where he taught before moving to Texas. He has held visiting positions at Notre Dame and Harvard. During the academic year 2008-09, he was a Laurance S. Rockefeller Visiting Fellow at the University Center for Human Values at Princeton.
Proefessor Levinson holds the W. St. John Garwood and W. St. John Garwood, Jr. Centennial Chair in Law, he joined the University of Texas Law School in 1980. Previously a member of the Department of Politics at Princeton University, he is also a Professor in the Department of Government at the University of Texas. The author of over 350 articles and book reviews in professional and popular journals--and a regular contributor to the popular blog Balkinization. He received the Lifetime Achievement Award from the Law and Courts Section of the American Political Science Association in 2010. He has been a visiting faculty member of the Boston University, Georgetown, Harvard, New York University, and Yale law schools in the United States and has taught abroad in programs of law in London; Paris; Jerusalem; Auckland, New Zealand; and Melbourne, Australia.
GOV 330K • The American President 38675 • Fall 2012 Meets MW 3:00PM-4:30PM MEZ B0.306 show description
Packet of selected primary texts (to be linked or posted on Blackboard). GOV 330K • The American President 38675 • Fall 2011 Meets MW 3:30PM-5:00PM WAG 420 show description
see syllabus GOV 330K • The American President 38680 • Fall 2011 Meets MW 5:30PM-7:00PM UTC 1.146 show description
see syllabus GOV 379S • Regime Persp On Amer Polit-Hon 39110 • Spring 2011 Meets W 3:30PM-6:30PM BAT 5.102 (also listed as CTI 326, LAH 350) show description
To see the polity as a whole requires that we get some distance from our subject, much as to see the planet earth as a whole requires one to look at it from outer space. Just as it is difficult to get visual perspective on a place living within it, it is difficult to understand the promise or pathologies of a regime from within it. To get critical distance from our politics, we will closely study three sets of texts that look at American politics from a distance. The first part of the course will recover the perspective of the founding debate between Federalists and Anti-federalists. This fundamental debate reveals what is a stake in the basic architecture of the American regime. The second part of the course is a close study of Tocqueville’s Democracy in America. Regarded by many as the best book ever written on democracy and the best book written on America, Tocqueville sees our polity whole because he looks at it from the vantage point of Europe, in general, and France, in particular. In the third part of the seminar we think about American politics from the perspective of thoughtful commentators who feel only nominally included in the polity. Half in and half out, these extraordinary black American writers reveal fissures and fault lines in the American regime. We end the class with a discussion of America’s place in the world today – examining a speech by a writer who articulately raises challenges to our self-understanding that are inarticulately expressed today in rage and ranting from enemies of the United States.
Four take home writing assignments. Analytic essays, each 1000-1500 words. (Grades weighted: 10%, 25%, 25%, and 25%) Late essays will not be accepted, except with a doctor’s excuse or a Dean’s excuse for family emergency. Regular preparation and class participation: 15%.
OR as an option: By prior arrangement with me by the due date of the second analytic essay, students may substitute one longer research paper (15 – 20 pages) for two of the last three analytic papers This paper will be on a topic of the students choosing , if I approve, and the due date will be the same as the last assigned analytic essay. This project would count 50% of the students course grade.
Selected writings by Frederick Douglass, W.E.B. Dubois, Ralph Ellison, James Baldwin
Solzhenitsyn, “A World Split Apart”
Tocqueville, Democracy in America GOV 382M • Tocqueville 39150 • Spring 2011 Meets T 6:30PM-9:30PM BAT 5.102 show description
See syllabus GOV 370L • President, Congress, And Court 38695 • Fall 2010 Meets TTH 8:00AM-9:30AM UTC 3.112 show description
Course Description: A Study of the political relationship of the President, Congress and Court in the American constitutional order. Has this relationship changed over the course of American history? Is American national politics prone to stalemate or deadlock between the branches regarding major issues of public policy? Do we have a new “imperial presidency?” Should the Court arbitrate disputes between the President and Congress over custody of their respective powers? Has Congress abdicated its constitutional responsibilities? We will examine questions like these in light of practical problems such as executive privilege and secrecy, the war on terror, budget politics and controversies regarding appointments to the Supreme Court. Grading:Three in class essay tests, for which study questions will be distributed in advance. The exam questions will be chosen from the list of study questions. (25% each) One short take home essay (10% each). Class participation and attendance (15%). Tentative Texts: The FederalistFisher, Congressional Abdication on War and SpendingRudalevige, The New Imperial PresidencyBessette and Tulis, The Constitutional PresidencySkowronek, Presidency in Political TimeGoldsmith, The Terror PresidencyA course packet of articles and essays GOV 370L • President, Congress, And Court 38700 • Fall 2010 Meets TTH 5:00PM-6:30PM UTC 3.122 show description
Course Description: A Study of the political relationship of the President, Congress and Court in the American constitutional order. Has this relationship changed over the course of American history? Is American national politics prone to stalemate or deadlock between the branches regarding major issues of public policy? Do we have a new “imperial presidency?” Should the Court arbitrate disputes between the President and Congress over custody of their respective powers? Has Congress abdicated its constitutional responsibilities? We will examine questions like these in light of practical problems such as executive privilege and secrecy, the war on terror, budget politics and controversies regarding appointments to the Supreme Court. Grading:Three in class essay tests, for which study questions will be distributed in advance. The exam questions will be chosen from the list of study questions. (25% each) One short take home essay (10% each). Class participation and attendance (15%). Tentative Texts: The FederalistFisher, Congressional Abdication on War and SpendingRudalevige, The New Imperial PresidencyBessette and Tulis, The Constitutional PresidencySkowronek, Presidency in Political TimeGoldsmith, The Terror PresidencyA course packet of articles and essays GOV 312L • Iss & Policies In Amer Gov-Hon 38698 • Spring 2010 Meets MW 3:30PM-5:00PM UTC 3.104 show description
Government 312L satisfies the second half of the mandated six hours of government that every UT student must take. Course covers analysis of varying topics concerned with American political institutions and policies, including the United States Constitution, and assumes basic knowledge of government from GOV 310L, which is a prerequiste. May be taken for credit only once. GOV 370L • President, Congress, And Court 38966 • Spring 2010 Meets MW 5:00PM-6:30PM MEZ B0.306 show description
Prerequisite: Six semester hours of lower-division coursework in government.
GOV 370L • President, Congress, And Court 39295 • Fall 2009 Meets TTH 2:00PM-3:30PM UTC 3.112 show description
GOV 370L • President, Congress, And Court 39435 • Spring 2008 Meets MW 3:00PM-4:30PM PAR 203 show description
GOV 312L • Iss & Policies In Am Gov-Hon-W 38615-38620 • Spring 2007 Meets MW 11:00AM-12:00PM MEZ B0.306 show description
Government 312L satisfies the second half of the mandated six hours of government that every UT student must take. Course covers analysis of varying topics concerned with American political institutions and policies, including the United States Constitution, and assumes basic knowledge of government from GOV 310L, which is a prerequiste. May be taken for credit only once. GOV 312L • Iss & Policies In Am Gov-Hon-W 37600-37605 • Spring 2006 Meets MW 11:00AM-12:00PM MEZ B0.306 show description
Government 312L satisfies the second half of the mandated six hours of government that every UT student must take. Course covers analysis of varying topics concerned with American political institutions and policies, including the United States Constitution, and assumes basic knowledge of government from GOV 310L, which is a prerequiste. May be taken for credit only once. GOV 312L • Iss & Policies In Am Gov-Hon-W 34900-34905 • Spring 2004 Meets MW 11:00AM-12:00PM BUR 134 show description
Government 312L satisfies the second half of the mandated six hours of government that every UT student must take. Course covers analysis of varying topics concerned with American political institutions and policies, including the United States Constitution, and assumes basic knowledge of government from GOV 310L, which is a prerequiste. May be taken for credit only once. GOV 312L • Iss & Policies In Am Gov-Hon-W 34495-34500 • Spring 2003 Meets MW 11:00AM-12:00PM UTC 1.130 show description
Government 312L satisfies the second half of the mandated six hours of government that every UT student must take. Course covers analysis of varying topics concerned with American political institutions and policies, including the United States Constitution, and assumes basic knowledge of government from GOV 310L, which is a prerequiste. May be taken for credit only once. Publications
Tulis, JK (2011), ""Plausible Futures,"" in Dunn, Charles W. (ed.) The Presidency in the Twenty-First Century, University Press of Kentucky.Tulis, J.K. and Macedo, S. (2010) The Limits of Constitutional Democracy, Princeton University Press.Tulis, J.K. and Macedo, S. (2010) ""Constitutional Boundaries,"" in The Limits of Constitutional Democracy, Princeton University Press.Tulis, JK (2010), ""The Possibility of Constitutional Statesmanship,"" in Tulis, JK and Macedo, S (eds.) The Limits of Constitutional Democracy, Princeton University Press.Tulis, J. (2009) The Constitutional Presidency. Johns Hopkins University Press.Tulis, J. (2009) Impeachment in the Constitutional Order. In J. Tulis & J.M. Bessette (Eds.), The Constitutional Presidency. Johns Hopkins University Press.Tulis, J. & Bessette, J.M. (2009) On the Constitution, Politics, and the Presidency. In J. Tulis & J.M. Bessette (Eds.), The Constitutional Presidency. Johns Hopkins University Press.Tulis, J (and Bessette, J.M) (2010) The Presidency in the Constitutional Order: Historical Perspectives, Reissued Classics Series, Transaction Publishers,Tulis, J and Bessette, J.M. (2010, ""Introduction to the Transaction Edition,"" The Presidency in the Constitutional Order: Historical Perspectives, Transaction Publishers.
Tulis, JK, (2009) ""The Two Constitutional Presidencies,"" in Nelson, Michael (ed.) The Presidency in the Political System, Congressional Quarterly Press.Tulis, J. & Mellow, N. (2007) Andrew Johnson and the Politics of Failure. In S. Skowronek & M. Glassman (Eds.), Formative Acts: Reckoning with Agency in American Politics. Philadelphia: University of Pennsylvania Press.Tulis, J. (2007, September) The Rhetorical Presidency in Retrospect. Critical Review: An Interdisciplinary Journal of Politics and Society, 19(2&3). Curriculum Vitae",['Legacies of Losing in American Politics and an expanded edition of The Rhetorical Presidency in the Princeton Classics series.'],5306,multifieldqa_en,en,,60b19bbfd875f79ca168f6db761d192ab710f9b5f395de89,Legacies of Losing in American Politics and an expanded edition of The Rhetorical Presidency in the Princeton Classics series.,126
What are some fields in which the inverse problem is encountered?,"\section{Introduction}
Given a data set and a model with some unknown parameters, the inverse problem aims to find the values of the model parameters that best fit the data.  
In this work, in which we focus on systems of interacting elements,
 the inverse problem concerns  the statistical inference
 of the underling interaction network and of its coupling coefficients from observed data on the dynamics  of the system. 
 Versions of this problem are encountered in physics, biology (e.g., \cite{Balakrishnan11,Ekeberg13,Christoph14}), social sciences and finance (e.g.,\cite{Mastromatteo12,yamanaka_15}), neuroscience (e.g., \cite{Schneidman06,Roudi09a,tyrcha_13}), just to cite a few, and are becoming more and more important due to the increase in the amount of data available from these fields.\\
 \indent 
 A standard approach used in statistical inference is to predict the interaction couplings by maximizing the likelihood function.
 This technique, however, requires the evaluation of the  
 
   partition function that, in the most general case, concerns a number of computations scaling exponentially with the system size.
  
  
    Boltzmann machine learning uses Monte Carlo sampling to compute the gradients of the Log-likelihood looking for stationary points \cite{Murphy12} but this method is computationally manageable only for small systems. A series of faster approximations, such as naive mean-field, independent-pair approximation \cite{Roudi09a, Roudi09b}, inversion of TAP equations \cite{Kappen98,Tanaka98}, small correlations expansion \cite{Sessak09}, adaptive TAP \cite{Opper01}, adaptive  cluster expansion \cite{Cocco12} or Bethe approximations \cite{Ricci-Tersenghi12, Nguyen12}  have, then, been developed. These techniques take as input means and correlations of observed variables and most of them assume a fully connected graph as underlying connectivity network, or expand  around it by perturbative dilution.  In most cases, network reconstruction turns out to  be not accurate for small data sizes and/or when couplings are  strong or, else, if the original interaction network is sparse.\\
\indent
 A further method, substantially improving performances for small data, is the so-called Pseudo-Likelyhood Method (PLM) \cite{Ravikumar10}. In Ref. \cite{Aurell12} Aurell and Ekeberg performed a comparison between PLM and some of the just mentioned mean-field-based algorithms on the pairwise interacting Ising-spin  ($\sigma = \pm 1$) model, showing how PLM performs sensitively better, especially on sparse graphs and in the high-coupling limit, i.e., for low temperature.
 
 In this work, we aim at performing statistical inference  on a model whose interacting variables are continuous $XY$ spins, i.e., $\sigma \equiv \left(\cos \phi,\sin \phi\right)$ with $\phi \in [0, 2\pi )$. The developed tools can, actually, be also straightforward applied  to the $p$-clock model  \cite{Potts52} where the phase $\phi$ takes discretely equispaced $p$ values  in the $2 \pi$ interval, $\phi_a =  a 2 \pi/p$, with $a= 0,1,\dots,p-1$. The $p$-clock model, else called vector Potts model, gives a hierarchy of discretization of the $XY$ model as $p$ increases. For $p=2$, one recovers the Ising model, for $p=4$ the Ashkin-Teller model \cite{Ashkin43}, for $p=6$ the ice-type model \cite{Pauling35,Baxter82} and the eight-vertex model \cite{Sutherland70,Fan70,Baxter71} for $p=8$.  
It turns out to be very useful also for numerical implementations of the continuous $XY$ model. 
Recent analysis on the multi-body $XY$ model has shown that for a limited number of discrete phase values ($p\sim 16, 32$) the thermodynamic critical properties of the $p\to\infty$ $XY$ limit are promptly recovered \cite{Marruzzo15, Marruzzo16}.  
Our main motivation to study statistical inference is that these kind of models have recently turned out to be rather useful in describing the behavior of optical systems, 
including standard mode-locking lasers \cite{Gordon02,Gat04,Angelani07,Marruzzo15} and random lasers \cite{Angelani06a,Leuzzi09a,Antenucci15a,Antenucci15b,Marruzzo16}. 
In particular, the inverse problem on the pairwise XY model analyzed here might be of help in recovering images from light propagated through random media. 


      This paper is organized as follows: in Sec. \ref{sec:model} we introduce the general model  and we discuss its  derivation also  as a model for light transmission through random scattering media. 
    In Sec. \ref{sec:plm} we introduce the PLM with $l_2$ regularization  and with decimation, two variants of the PLM respectively introduced in Ref. \cite{Wainwright06} and \cite{Aurell12} for the inverse Ising problem. 
    Here, we analyze these techniques for continuous $XY$ spins and we test them on thermalized data generated by Exchange Monte Carlo numerical simulations of the original model dynamics. In Sec. \ref{sec:res_reg} we  present the results related to the PLM-$l_2$. In Sec. \ref{sec:res_dec} the results related to the PLM with decimation are reported and its performances are compared to the PLM-$l_2$ and to a variational mean-field method analyzed in Ref. \cite{Tyagi15}. In Sec. \ref{sec:conc}, we outline conclusive remarks and perspectives.

        \section{The leading $XY$ model}
      \label{sec:model}
 The leading model we are considering is defined, for a system of $N$ angular $XY$ variables, by the Hamiltonian 
 \begin{equation}
  \mathcal{H} = - \sum_{ik}^{1,N} J_{ik} \cos{\left(\phi_i-\phi_k\right)} 
  \label{eq:HXY}
 
  \end{equation} 
  
  The $XY$ model is well known in statistical mechanics, displaying important physical
  insights, starting from the Berezinskii-Kosterlitz-Thouless
  transition in two dimensions\cite{Berezinskii70,Berezinskii71,Kosterlitz72} and moving to, e.g., the
  transition of liquid helium to its superfluid state \cite{Brezin82}, the roughening transition of the interface of a crystal in equilibrium with its vapor \cite{Cardy96}. In presence of disorder and frustration \cite{Villain77,Fradkin78} the model has been adopted to describe synchronization problems as the Kuramoto model \cite{Kuramoto75} and in the theoretical modeling of Josephson junction arrays \cite{Teitel83a,Teitel83b} and arrays of coupled lasers \cite{Nixon13}.
  Besides several derivations and implementations of the model in quantum and classical physics, equilibrium or out of equilibrium, ordered or fully frustrated systems, Eq. (\ref{eq:HXY}), in its generic form,
  has found applications also in other fields. A rather fascinating example being the behavior of starlings flocks  \cite{Reynolds87,Deneubourg89,Huth90,Vicsek95, Cavagna13}.
  Our interest on the $XY$ model resides, though, in optics. Phasor and phase models with pairwise and  multi-body interaction terms can, indeed, describe the behavior of electromagnetic modes in both linear and nonlinear optical systems in the analysis of problems such as light propagation and lasing \cite{Gordon02, Antenucci15c, Antenucci15d}. As couplings are strongly frustrated, these models turn out to be especially useful to the study of optical properties in random media \cite{Antenucci15a,Antenucci15b}, as in the noticeable case of random lasers \cite{Wiersma08,Andreasen11,Antenucci15e}  and they might as well be applied to linear scattering problems, e.g., propagation of waves in opaque systems or disordered fibers. 
  
  
  \subsection{A propagating wave model}
  We briefly mention a derivation of the model as a proxy for the propagation of light through random linear media. 
 Scattering of light is held responsible to obstruct our view and make objects opaque. Light rays, once that they enter the material, only exit  after getting scattered multiple times within the material. In such a disordered medium, both the direction and the phase of the propagating waves are random. Transmitted light 
 yields a disordered interference pattern typically  having low intensity, random phase and almost no resolution, called a speckle. Nevertheless, in recent years it has been realized that disorder is rather a blessing in disguise \cite{Vellekoop07,Vellekoop08a,Vellekoop08b}. Several experiments have made it possible to control the behavior of light and other optical processes in a given random disordered medium,  
 by exploiting, e.g.,  the tools developed for wavefront shaping to control the propagation of light and to engineer the confinement of light \cite{Yilmaz13,Riboli14}.
 \\
 \indent
 In a linear dielectric medium, light propagation can be described through a part of the scattering matrix, the transmission matrix $\mathbb{T}$, linking the outgoing to the incoming fields. 
 Consider the case in which there are $N_I$ incoming channels and $N_O$ outgoing ones; we can indicate with $E^{\rm in,out}_k$ the input/output electromagnetic field phasors of channel $k$. In the most general case, i.e., without making any particular assumptions on the field polarizations, each light mode and its polarization polarization state can be represented by means of the $4$-dimensional Stokes vector. Each $ t_{ki}$ element of $\mathbb{T}$, thus, is a $4 \times 4$ M{\""u}ller matrix. If, on the other hand, we know that the source is polarized and the observation is made on the same polarization, one can use a scalar model and adopt Jones calculus \cite{Goodman85,Popoff10a,Akbulut11}:
   \begin{eqnarray}
 E^{\rm out}_k = \sum_{i=1}^{N_I}  t_{ki} E^{\rm in}_i \qquad \forall~ k=1,\ldots,N_O
 \label{eq:transm}
 \end{eqnarray}
  We recall that the elements of the transmission matrix are random complex coefficients\cite{Popoff10a}. For the case of completely unpolarized modes, we can also use a scalar model similar to Eq. \eqref{eq:transm}, but whose variables are  the intensities of the outgoing/incoming fields, rather than the fields themselves.\\ 
In the following, for simplicity, we will consider Eq. (\ref{eq:transm}) as our starting point,
where $E^{\rm out}_k$, $E^{\rm in}_i$ and $t_{ki}$ are all complex scalars. 
If Eq. \eqref{eq:transm} holds for any $k$, we can write:
  \begin{eqnarray}
  \int \prod_{k=1}^{N_O} dE^{\rm out}_k \prod_{k=1}^{N_O}\delta\left(E^{\rm out}_k - \sum_{j=1}^{N_I}  t_{kj} E^{\rm in}_j \right) = 1
  \nonumber
  \\
  \label{eq:deltas}
  \end{eqnarray}

 Observed data are a noisy representation of the true values of the fields. Therefore, in inference problems it is statistically more meaningful to take that noise into account in a probabilistic way, 
 rather than looking  at the precise solutions of the exact equations (whose parameters are unknown). 
 To this aim we can introduce Gaussian distributions whose limit for zero variance are the Dirac deltas in Eq. (\ref{eq:deltas}).
 Moreover, we move to consider the ensemble of all possible solutions of Eq. (\ref{eq:transm}) at given $\mathbb{T}$, looking at  all configurations of input fields. We, thus, define the function:
 
   \begin{eqnarray}
  Z &\equiv &\int_{{\cal S}_{\rm in}} \prod_{j=1}^{N_I}  dE^{\rm in}_j \int_{{\cal S}_{\rm out}}\prod_{k=1}^{N_O} dE^{\rm out}_k 
  \label{def:Z}
\\
    \times
  &&\prod_{k=1}^{N_O}
   \frac{1}{\sqrt{2\pi \Delta^2}}  \exp\left\{-\frac{1}{2 \Delta^2}\left|
  E^{\rm out}_k -\sum_{j=1}^{N_I}  t_{kj} E^{\rm in}_j\right|^2
\right\} 
\nonumber
 \end{eqnarray}
   We stress that the integral of Eq. \eqref{def:Z} is not exactly a Gaussian integral. Indeed, starting from Eq. \eqref{eq:deltas}, two constraints on the electromagnetic field intensities must be taken into account. 

  The space of solutions is  delimited by the total power ${\cal P}$ received by system, i.e., 
  ${\cal S}_{\rm in}: \{E^{\rm in} |\sum_k I^{\rm in}_k = \mathcal{P}\}$, also implying  a constraint on the total amount of energy that is transmitted through the medium, i. e., 
  ${\cal S}_{\rm out}:\{E^{\rm out} |\sum_k I^{\rm out}_k=c\mathcal{P}\}$, where the attenuation factor  $c<1$ accounts for total losses.
  As we will see more in details in the following, being interested in inferring the transmission matrix through the PLM, we can omit to explicitly include these terms in Eq. \eqref{eq:H_J} since they do not depend on $\mathbb{T}$ not adding any information on the gradients with respect to the elements of $\mathbb{T}$.
  
 Taking the same number of incoming and outcoming channels, $N_I=N_O=N/2$, and  ordering the input fields in the first $N/2$ mode indices and the output fields in the last $N/2$ indices, we can drop the ``in'' and ``out'' superscripts and formally write $Z$  as a partition function
    \begin{eqnarray}
        \label{eq:z}
 && Z =\int_{\mathcal S} \prod_{j=1}^{N} dE_j \left(   \frac{1}{\sqrt{2\pi \Delta^2}} \right)^{N/2} 
 \hspace*{-.4cm} \exp\left\{
  -\frac{ {\cal H} [\{E\};\mathbb{T}] }{2\Delta^2}
  \right\}
  \\
&&{\cal H} [\{E\};\mathbb{T}] =
-  \sum_{k=1}^{N/2}\sum_{j=N/2+1}^{N} \left[E^*_j t_{jk} E_k + E_j t^*_{kj} E_k^* 
\right]
 \nonumber
\\
&&\qquad\qquad \qquad + \sum_{j=N/2+1}^{N} |E_j|^2+ \sum_{k,l}^{1,N/2}E_k
U_{kl} E_l^*
 \nonumber
 \\
 \label{eq:H_J}
 &&\hspace*{1.88cm } = - \sum_{nm}^{1,N} E_n J_{nm} E_m^*
 \end{eqnarray}
 where ${\cal H}$ is a real-valued function by construction, we have introduced the effective input-input coupling matrix
\begin{equation}
U_{kl} \equiv \sum_{j=N/2+1}^{N}t^*_{lj} t_{jk} 
 \label{def:U}
 \end{equation}
 and the whole interaction matrix reads (here $\mathbb{T} \equiv \{ t_{jk} \}$)
 \begin{equation}
 \label{def:J}
 \mathbb J\equiv \left(\begin{array}{ccc|ccc}
 \phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}\\
 \phantom{()}&-\mathbb{U} \phantom{()}&\phantom{()}&\phantom{()}&{\mathbb{T}}&\phantom{()}\\
\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}\\
 \hline
\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}&\phantom{()}\\
 \phantom{()}& \mathbb   T^\dagger&\phantom{()}&\phantom{()}& - \mathbb{I} &\phantom{()}\\
\phantom{a}&\phantom{a}&\phantom{a}&\phantom{a}&\phantom{a}&\phantom{a}\\
 \end{array}\right)
 \end{equation}
 
 Determining the electromagnetic complex amplitude configurations that minimize the {\em cost function} ${\cal H}$, Eq.  (\ref{eq:H_J}),  means to maximize the overall distribution peaked around the solutions of the transmission Eqs. (\ref{eq:transm}). As the variance $\Delta^2\to 0$, eventually, the initial set of Eqs. (\ref{eq:transm}) are recovered. The ${\cal H}$ function, thus, plays the role of an Hamiltonian and  $\Delta^2$ the role of a noise-inducing temperature. The exact numerical problem corresponds to the zero temperature limit of the statistical mechanical problem. Working with real data, though, which are noisy, a finite ``temperature''
  allows for a better representation of the ensemble of solutions to the sets of equations of continuous variables. 
  

  
  
  Now, we can express every phasor in Eq. \eqref{eq:z}  as $E_k = A_k e^{\imath \phi_k}$. As a working hypothesis we will consider the intensities $A_k^2$ as either homogeneous or as \textit{quenched} with respect to phases.
The first condition occurs, for instance, to the input intensities $|E^{\rm in}_k|$ produced by a phase-only spatial light modulator (SLM) with homogeneous illumination \cite{Popoff11}.
With \textit{quenched} here we mean, instead, that the intensity of each mode is the same for every solution of Eq. \eqref{eq:transm} at fixed $\mathbb T$.
We stress that, including intensities in the model does not preclude the inference analysis but it is out of the focus of the present work and will be considered elsewhere. 

If all intensities are uniform in input and in output, this amount to a constant rescaling for each one of the four sectors of matrix $\mathbb J$ in Eq. (\ref{def:J}) that will not change the properties of the matrices.
For instance, if the original transmission matrix is unitary, so it will be the rescaled one and the matrix $\mathbb U$ will be  diagonal.
Otherwise, if intensities are \textit{quenched}, i.e., they can be considered as constants in Eq. (\ref{eq:transm}),
they are inhomogeneous with respect to phases. The generic Hamiltonian element will, therefore, rescale as 
  \begin{eqnarray}
  E^*_n J_{nm} E_m = J_{nm} A_n A_m e^{\imath (\phi_n-\phi_m)} \to J_{nm} e^{\imath (\phi_n-\phi_m)}
  \nonumber
  \end{eqnarray}
  and the properties of the original  $J_{nm}$ components are not conserved  in the rescaled one. In particular, we have no argument, anymore, to possibly set the rescaled $U_{nm}\propto \delta_{nm}$.
  Eventually, we end up with the complex couplings $XY$ model, whose real-valued Hamiltonian is written as
 \begin{eqnarray}
  \mathcal{H}& = &  - \frac{1}{2} \sum_{nm} J_{nm} e^{-\imath (\phi_n - \phi_m)}  + \mbox{c.c.} 
    \label{eq:h_im}
\\    &=&  - \frac{1}{2} \sum_{nm} \left[J^R_{nm} \cos(\phi_n - \phi_m)+
  J^I_{nm}\sin (\phi_n - \phi_m)\right] 
  \nonumber
 \end{eqnarray}
where $J_{nm}^R$ and $J_{nm}^I$ are the real and imaginary parts of $J_{nm}$. Being $\mathbb J$  Hermitian, $J^R_{nm}=J^R_{mn}$ is symmetric and $J_{nm}^I=-J_{mn}^I$ is skew-symmetric.

\begin{comment}
\textcolor{red}{
F: comment about quenched:
I think that to obtain the XY model, it is not necessary that the intensities are strictly quenched (that is also a quite unfeasible situation, I guess).
Indeed eq (2) does not deal with the dynamics of the modes, but just connect the in and out ones.
For this, what it is necessary to have the XY model, it is that the intensities are always the same on the different samples
(so that the matrix $t_{ij}$ is the same for different phase data). If the intensities are fixed, then they can be incorporated in $t_{ij}$ and eq (2) can be written just for phases as described. \\
}
\end{comment}


  \section{Pseudolikelihood Maximization}
  \label{sec:plm}
The inverse problem consists in the reconstruction of the parameters $J_{nm}$ of the Hamiltonian, Eq. (\ref{eq:h_im}).  
Given a set of $M$ data configurations of $N$ spins
 $\bm\sigma = \{ \cos \phi_i^{(\mu)},\sin \phi_i^{(\mu)} \}$, $i = 1,\dots,N$ and $\mu=1,\dots,M$, we want to \emph{infer} the couplings:
 \begin{eqnarray}
\bm \sigma  \rightarrow  \mathbb{J} 
\nonumber
 \end{eqnarray}
 With this purpose in mind,
 in the rest of this section we implement the working equations for the techniques used. 
 In order to test our methods, we generate the input data, i.e., the configurations, by Monte-Carlo simulations of the model.
 The joint probability distribution of the $N$ variables $\bm{\phi}\equiv\{\phi_1,\dots,\phi_N\}$, follows the Gibbs-Boltzmann distribution:
 \begin{equation}\label{eq:p_xy}
 P(\bm{\phi}) = \frac{1}{Z} e^{-\beta \mathcal{H\left(\bm{\phi}\right)}} \quad \mbox{ where } \quad Z = \int \prod_{k=1}^N d\phi_k  e^{-\beta \mathcal{H\left(\bm{\phi}\right)}}  
 \end{equation}
 and where we denote $\beta=\left( 2\Delta^2 \right)^{-1}$ with respect to Eq. (\ref{def:Z}) formalism.
 In order to stick to usual statistical inference notation, in the following we will rescale the couplings by a factor $\beta / 2$: $\beta J_{ij}/2 \rightarrow J_{ij}$. 
 The main idea of the PLM is to work with the conditional probability distribution of one variable $\phi_i$ given all other variables, 
 $\bm{\phi}_{\backslash i}$:
 
  \begin{eqnarray}
	\nonumber
   P(\phi_i | \bm{\phi}_{\backslash i}) &=& \frac{1}{Z_i} \exp \left \{ {H_i^x (\bm{\phi}_{\backslash i})
  	\cos \phi_i + H_i^y (\bm{\phi}_{\backslash i}) \sin \phi_i } \right \}
	\\
 \label{eq:marginal_xy}
	&=&\frac{e^{H_i(\bm{\phi}_{\backslash i}) \cos{\left(\phi_i-\alpha_i(\bm{\phi}_{\backslash i})\right)}}}{2 \pi I_0(H_i)}
  \end{eqnarray}
  where $H_i^x$ and $H_i^y$ are defined as
   \begin{eqnarray}
   H_i^x (\bm{\phi}_{\backslash i})  &=& \sum_{j (\neq i)} J^R_{ij} \cos \phi_j  - \sum_{j  (\neq i) } J_{ij}^{I} \sin \phi_j \phantom{+  h^R_i} \label{eq:26} \\
   H_i^y  (\bm{\phi}_{\backslash i})  &=&  \sum_{j  (\neq i)} J^R_{ij}  \sin \phi_j  + \sum_{j  (\neq i) } J_{ij}^{I}   \cos \phi_j \phantom{ + h_i^{I} }\label{eq:27}
   \end{eqnarray}
and $H_i= \sqrt{(H_i^x)^2 + (H_i^y)^2}$, $\alpha_i = \arctan H_i^y/H_i^x$   and we introduced the modified Bessel function of the first kind:
  \begin{equation}
  \nonumber
   I_k(x) = \frac{1}{2 \pi}\int_{0}^{2 \pi} d \phi e^{x \cos{ \phi}}\cos{k \phi}
  \end{equation}
  
   Given $M$ observation samples $\bm{\phi}^{(\mu)}=\{\phi^\mu_1,\ldots,\phi^\mu_N\}$, $\mu = 1,\dots, M$, the
   pseudo-loglikelihood for the variable $i$ is given by the logarithm of Eq. (\ref{eq:marginal_xy}),
   \begin{eqnarray}
   \label{eq:L_i}
   L_i &=& \frac{1}{M}  \sum_{\mu = 1}^M  \ln P(\phi_i^{(\mu)}|\bm{\phi}^{(\mu)}_{\backslash i})
   \\
   \nonumber
   & =&  \frac{1}{M}  \sum_{\mu = 1}^M  \left[ H_i^{(\mu)} \cos( \phi_i^{(\mu)} - \alpha_i^{(\mu)}) - \ln  2 \pi I_0\left(H_i^{(\mu)}\right)\right] \, .
    \end{eqnarray}
The underlying idea of PLM is that an approximation of the true parameters of the model is obtained for values that maximize the functions $L_i$.
The specific maximization scheme differentiates the different techniques.


   
   
   \subsection{PLM with $l_2$ regularization}
   Especially for the case of sparse graphs, it is useful to add a regularizer, which prevents the maximization routine to move towards high values of 
   $J_{ij}$ and $h_i$ without converging. We will adopt an $l_2$ regularization so that the Pseudolikelihood function (PLF) at site $i$  reads:
   \begin{equation}\label{eq:plf_i}
  {\cal L}_i = L_i
  - \lambda \sum_{i \neq j} \left(J_{ij}^R\right)^2 - \lambda \sum_{i \neq j} \left(J_{ij}^I\right)^2  
   \end{equation}
   with $\lambda>0$.
   Note that the values of $\lambda$ have to be chosen arbitrarily, but not too large, in order not to overcome $L_i$.
   The standard implementation of the PLM consists in maximizing each ${\cal L}_i$, for $i=1\dots N$, separately. The expected values of the couplings are then:
   \begin{equation}
   \{ J_{i j}^*\}_{j\in \partial i} :=  \mbox{arg max}_{ \{ J_{ij} \}}
    \left[{\cal L}_i\right]
   \end{equation}
   In this way, we obtain two estimates for the coupling $J_{ij}$, one from maximization of ${\cal L}_i$, $J_{ij}^{(i)}$, and another one from ${\cal L}_j$, say $J_{ij}^{(j)}$.
    Since the original Hamiltonian of the $XY$ model is Hermitian, we know that the real part of the couplings is symmetric while the imaginary part is skew-symmetric. 
   
     The final estimate for $J_{ij}$ can then be obtained  averaging the two results:
  
  
  
   \begin{equation}\label{eq:symm}
   J_{ij}^{\rm inferred} = \frac{J_{ij}^{(i)} + \bar{J}_{ij}^{(j)}}{2} 
   \end{equation}
   where with $\bar{J}$ we indicate the complex conjugate.
   It is worth noting that the pseudolikelihood $L_i$, Eq. \eqref{eq:L_i}, is characterized by the
   following properties: (i) the normalization term of Eq.\eqref{eq:marginal_xy} can be
   computed analytically at odd with the {\em full} likelihood case that
   in general require a computational time which scales exponentially
   with the size of the systems; (ii) the $\ell_2$-regularized pseudolikelihood
   defined in Eq.\eqref{eq:plf_i} is strictly concave (i.e. it has a single
   maximizer)\cite{Ravikumar10}; (iii) it is consistent, i.e. if $M$ samples are
   generated by a model $P(\phi | J*)$ the maximizer tends to $J*$
   for $M\rightarrow\infty$\cite{besag1975}. Note also that (iii) guarantees that  
   $|J^{(i)}_{ij}-J^{(j)}_{ij}| \rightarrow 0$ for $M\rightarrow \infty$.
   In Secs. \ref{sec:res_reg}, \ref{sec:res_dec} 
   we report the results obtained and we analyze the performances of the PLM having taken the configurations from Monte-Carlo simulations of models whose details are known.
   

   
   \subsection{PLM with decimation}
   Even though the PLM with $l_2$-regularization allows to dwell the inference towards the low temperature region and in the low sampling case with better performances that mean-field methods, in some situations some couplings are overestimated and not at all symmetric. Moreover, in the technique there is the bias of the $l_2$ regularizer.
   Trying to overcome these problems, Decelle and Ricci-Tersenghi introduced a new method \cite{Decelle14}, known as PLM + decimation: the algorithm maximizes the sum of the $L_i$,
   \begin{eqnarray}
    {\cal L}\equiv \frac{1}{N}\sum_{i=1}^N \mbox{L}_i
    \end{eqnarray}  
    and, then, it recursively set to zero couplings which are estimated very small. We expect that as long as we are setting to zero couplings that are unnecessary to fit the data, there should be not much changing on ${\cal L}$. Keeping on with decimation, a point is reached where ${\cal L}$ decreases abruptly indicating  that relevant couplings are being decimated and under-fitting is taking place.
   Let us define  by $x$  the fraction of non-decimated couplings. To have a quantitative measure for the halt criterion of the decimation process, a tilted ${\cal L}$ is defined as,
   \begin{eqnarray}
  \mathcal{L}_t &\equiv& \mathcal{L}  - x \mathcal{L}_{\textup{max}} - (1-x) \mathcal{L}_{\textup{min}} \label{$t$PLF} 
   \end{eqnarray}
   where 
   \begin{itemize}
   \item $\mathcal{L}_{\textup{min}}$ is the pseudolikelyhood of a model with independent variables. In the XY case: $\mathcal{L}_{\textup{min}}=-\ln{2 \pi}$.
   \item
   $\mathcal{L}_{\textup{max}}$ is the pseudolikelyhood in the fully-connected model and it is maximized over all the $N(N-1)/2$ possible couplings. 
   \end{itemize}
   At the first step, when $x=1$, $\mathcal{L}$ takes value $\mathcal{L}_{\rm max}$ and  $\mathcal{L}_t=0$. On the last step, for an empty graph, i.e., $x=0$, $\mathcal{L}$ takes the value $\mathcal{L}_{\rm min}$ and, hence, again $\mathcal{L}_t =0$. 
   In the intermediate steps, during the decimation procedure, as $x$ is decreasing from $1$ to $0$, one observes firstly that $\mathcal{L}_t$ increases linearly and, then, it displays an abrupt decrease indicating that from this point on relevant couplings are being decimated\cite{Decelle14}. In Fig. \ref{Jor1-$t$PLF} we give an instance of this behavior for the 2D short-range XY model with ordered couplings. We notice that the maximum point of $\mathcal{L}_t$ coincides with the minimum point of the reconstruction error, the latter defined as 
   \begin{eqnarray}\label{eq:errj}
   \mbox{err}_J \equiv \sqrt{\frac{\sum_{i<j} (J^{\rm inferred}_{ij} -J^{\rm true}_{ij})^2}{N(N-1)/2}} \label{err}
   \end{eqnarray}
  We stress that the ${\cal L}_t$ maximum is obtained ignoring the underlying graph, while the err$_J$ minimum can be evaluated once the true graph has been reconstructed. 
   
     \begin{figure}[t!]
   	\centering
   	\includegraphics[width=1\linewidth]{Jor1_dec_tPLF_new.eps}
   	\caption{The tilted likelyhood ${\cal L}_t$ curve and the reconstruction error vs the number of decimated couplings  for an ordered, real-valued J on 2D XY model with $N=64$ spins. The peak of ${\cal L}_t$ coincides with the dip of the error.}  
   	\label{Jor1-$t$PLF}
   \end{figure} 
       
   
   In the next sections we will show the results obtained on the  $XY$ model analyzing the performances of the two methods and comparing them also with a mean-field method \cite{Tyagi15}.
   
  
    
   \section{Inferred couplings with PLM-$l_2$}
    \label{sec:res_reg}
    \subsection{$XY$ model with real-valued couplings}
    
    In order to obtain the vector of couplings, $J_{ij}^{\rm inferred}$ the function $-\mathcal{L}_i$ is minimized through the vector of derivatives ${\partial \mathcal{L}_i}/\partial J_{ij}$. The process is repeated for all the couplings obtaining then a fully connected adjacency matrix. The results here presented are obtained with $\lambda = 0.01$.
   For the minimization we have used the MATLAB routine \emph{minFunc\_2012}\cite{min_func}. 
      
      \begin{figure}[t!]
    	\centering
    	\includegraphics[width=1\linewidth]{Jor11_2D_l2_JR_soJR_TPJR}
    	\caption{Top panels: instances of single site coupling reconstruction for the case of $N=64$ XY spins on a 2D lattice with ordered $J$ (left column) and bimodal distributed $J$ (right column). 
    	
    	Bottom panels: sorted couplings.}
    	\label{PL-Jor1}
    \end{figure}

   
To produce the data by means of numerical Monte Carlo simulations a system with $N=64$  spin variables  is considered on a deterministic 2D lattice with periodic boundary conditions. 
Each spin has then connectivity $4$, i.e., we expect to infer an adjacency matrix with $N c = 256$ couplings different from zero. 
The dynamics of the simulated model is based on the Metropolis algorithm and parallel tempering\cite{earl05} is used to speed up the thermalization of the system.
The thermalization is tested looking at the average energy over logarithmic time windows and
the acquisition of independent configurations
starts only after the system is well thermalized.

   For the values of the couplings we considered two cases: an ordered case, indicated in the figure as  $J$ ordered (e.g., left column of Fig. \ref{PL-Jor1}) where the couplings can take values $J_{ij}=0,J$, with $J=1$, 
   and a quenched disordered case, indicated in the figures as  $J$ disordered (e.g., right column of Fig. \ref{PL-Jor1})
   where the couplings can take also  negative values, i.e., 
    $J_{ij}=0,J,-J$, with a certain probability.  The results here presented were obtained with bimodal distributed $J$s: 
   
    $P(J_{ij}=J)=P(J_{ij}=-J)=1/2$.  The performances of the PLM have shown not to depend on $P(J)$. 
   
    We recall that in Sec. \ref{sec:plm} we used the temperature-rescaled notation, i.e., $J_{ij}$ stands for $J_{ij}/T$. 
   
    To analyze the performances of the PLM, in Fig. \ref{PL-Jor1} the inferred couplings, $\mathbb{J}^R_{\rm inf}$, are shown on top of the original couplings,  $\mathbb{J}^R_{\rm true}$.
     The first figure (from top) in the left column shows  the $\mathbb{J}^R_{\rm inf}$ (black) and the $\mathbb{J}^R_{\rm tru}$ (green) for a given spin
     at temperature $T/J=0.7$ and number of samples  $M=1024$. PLM appears to reconstruct the correct couplings, though zero couplings are always given a small inferred non-zero value. 
     In the left column of Fig.  \ref{PL-Jor1},  both the $\mathbb{J}^R_{\rm{inf}}$ and the $\mathbb{J}^R_{\rm{tru}}$ are sorted in decreasing order and plotted on top of each other. 
     We can clearly see that $\mathbb{J}^R_{\rm inf}$ reproduces the expected step function. Even though the jump is smeared, the difference between inferred couplings corresponding to the set of non-zero couplings 
     and to the set of zero couplings can be clearly appreciated.
     Similarly, the plots in the right column of Fig. \ref{PL-Jor1} show the results obtained for the case with  bimodal disordered couplings, for the same working temperature and number of samples. 
     In particular, note that the algorithm infers half positive and half negative couplings, as expected.
     
     
\begin{figure}
\centering
\includegraphics[width=1\linewidth]{Jor11_2D_l2_errJ_varT_varM}
\caption{Reconstruction error $\mbox{err}_J$, cf. Eq. (\ref{eq:errj}), plotted as a function of temperature (left) for three values of the number of samples $M$ and  as a function $M$ (right) for three values of temperature in the ordered system, i.e., $J_{ij}=0,1$. 
The system size is $N=64$.}
\label{PL-err-Jor1}
\end{figure}

In order to analyze the effects of the number of samples and of the temperature regimes, we plot in Fig. \ref{PL-err-Jor1} the reconstruction error, Eq. (\ref{err}), as a function of temperature for three different sample sizes $M=64,128$ and $512$. 
The error is seen to sharply rise al low temperature, incidentally, in the ordered case, for  $T<T_c \sim 0.893$, which is the Kosterlitz-Thouless transition temperature of the 2XY model\cite{Olsson92}. 
 However, we can see that if only $M=64$ samples are considered, $\mbox{err}_J$ remains high independently on the working temperature. 
 In the right plot of Fig. \ref{PL-err-Jor1},  $\mbox{err}_J$ is plotted as a function of $M$ for three different working temperatures $T/J=0.4,0.7$ and $1.3$. As we expect, 
 $\mbox{err}_J$  decreases as $M$ increases. This effect was observed also with mean-field inference techniques on the same model\cite{Tyagi15}.

To better understand the performances of the algorithms, in Fig. \ref{PL-varTP-Jor1} we show several True Positive (TP) curves obtained for various values of $M$ at three different temperatures $T$. As $M$ is large and/or temperature is not too small,  we are able to reconstruct correctly all the couplings present in the system (see bottom plots).
The True Positive curve displays how many times the inference method finds a true link of the original network  as a function of the index of the vector of sorted absolute value of reconstructed couplings $J_{ij}^{\rm inf}$. 
The index $n_{(ij)}$ represents the related spin couples $(ij)$. The TP curve is obtained as follows: 
first  the values $|J^{\rm inf}_{ij}|$ are sorted in descending order and  the spin pairs $(ij)$ are ordered according to the sorting position of $|J^{\rm inf}_{ij}|$. Then,
     	a cycle over the ordered set of pairs $(ij)$, indexed by $n_{(ij)}$, is performed, comparing with the original network coupling $J^{\rm true}_{ij}$ and verifying whether it is zero or not. The true positive curve is computed as
\begin{equation}
\mbox{TP}[n_{(ij)}]= \frac{\mbox{TP}\left[n_{(ij)}-1\right] (n_{ij}-1)+ 1 -\delta_{J^{\rm true}_{ij},0}}{n_{(ij)}}
\end{equation}
As far as $J^{\rm true}_{ij} \neq 0$, TP$=1$. As soon as the true coupling of a given $(ij)$ couple in the sorted list is zero, the TP curve departs from one. 
In our case, where the connectivity per spin of the original system is  $c=4$ and there are $N=64$ spins, we know that we will have  $256$ non-zero couplings.  
     	If the inverse problem is successful, hence, we expect a steep decrease of the TP curve when  $n_{ij}=256$ is overcome.

In Fig. \ref{PL-varTP-Jor1}
it is  shown that,  almost independently of $T/J$, the TP score improves as $M$ increases. Results are plotted for three different temperatures, $T=0.4,1$ and $2.2$, with increasing number of samples $M = 64, 128,512$ and $1024$ (clockwise). 
We can clearly appreciate the improvement in temperature if the size of the data-set is not very large: for small $M$, $T=0.4$ performs better. 
When $M$ is high enough (e.g., $M=1024$), instead, the TP curves do not appear to be strongly influenced by the temperature.

\begin{figure}[t!]
	\centering
	\includegraphics[width=1\linewidth]{Jor11_2D_l2_TPJR_varT_varM}
	\caption{TP  curves  for 2D short-range ordered $XY$ model with $N=64$ spins at three different values of $T/J$ with increasing - clockwise from top - $M$.}
	\label{PL-varTP-Jor1} 
\end{figure} 

\subsection{$XY$ model with complex-valued couplings}
For the complex $XY$ we have to contemporary  infer $2$ apart coupling matrices,  $J^R_{i j}$ and $J^I_{i j}$.  As before, a system of $N=64$ spins is considered on a 2D lattice.
For the couplings we have considered both ordered and bimodal disordered cases.
In Fig. \ref{PL-Jor3}, a single row of the matrix $J$ (top) and the whole sorted couplings (bottom) are displayed for the ordered model (same legend as in Fig. \ref{PL-Jor1}) for the real, $J^R$ (left column), and the imaginary part, $J^I$.  

\begin{figure}[t!]
	\centering
\includegraphics[width=1\linewidth]{Jor3_l2_JRJI_soJRJI_TPJRJI}
	\caption{Results related to the ordered complex XY model with $N=64$ spins on a  2D lattice. Top: instances of  single site reconstruction for the real, JR (left column), and
		the imaginary, JI (right column), part of $J_{ij}$. Bottom: sorted values of JR (left) and JI (right).}
		
	\label{PL-Jor3}
\end{figure}
 
 
  \section{PLM with Decimation}
 \label{sec:res_dec}
 

\begin{figure}[t!]
   	\centering
   	\includegraphics[width=1\linewidth]{Jor1_dec_tPLF_varT_varM}
    	\caption{Tilted Pseudolikelyhood, ${\cal L}_t$, plotted as a function of decimated couplings. Top: Different ${\cal L}_t$ curves obtained for different values of $M$ plotted on top of each other. Here $T=1.3$. The black line indicates the expected number of decimated couplings, $x^*=(N (N-1) - N c)/2=1888$. As we can see, as $M$ increases, the maximum point of ${\cal L}_t$ approaches $x^*$. Bottom: Different ${\cal L}_t$ curves obtained for different values of T with $M=2048$. We can see that, with this value of $M$, no differences can be appreciated on the maximum points of the different  ${\cal L}_t$ curves.}
   	\label{var-$t$PLF}
   \end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{Jor1_dec_tPLF_peak_statistics_varM_prob.eps}
    	\caption{Number of most likely decimated couplings, estimated by the maximum point of $\mathcal{L}_t$, as a function of the number of samples $M$. We can clearly see that the maximum point of $\mathcal{L}_t$ tends toward $x^*$, which is the right expected number of zero couplings in the system.}  
    	\label{PLF_peak_statistics}
    \end{figure}
      
      For the ordered real-valued XY model we show in Fig. \ref{var-$t$PLF}, top panel, the outcome on the tilted pseudolikelyhood, $\mathcal{L}_t$ Eq. \eqref{$t$PLF}, of the progressive decimation: from a fully connected lattice  down to an empty lattice. The figure shows the behaviour of $\mathcal{L}_t$ for three different data sizes $M$. A clear data size dependence of the maximum point of  $\mathcal{L}_t$, signalling the most likely value for decimation, is shown. For small $M$ the most likely number of  couplings is overestimated and for increasing $M$ it tends to the true value, as displayed in Fig. \ref{PLF_peak_statistics}.  In the bottom panel of Fig. \ref{var-$t$PLF} we display instead different 
 $\mathcal{L}_t$ curves obtained for three different values of $T$.
  Even though the values of $\mathcal{L}_t$ decrease with increasing temperature, the value of the most likely number of decimated couplings appears to be quite independent on $T$ with $M=2048$ number of samples.
In Fig. \ref{fig:Lt_complex} we eventually display the tilted pseudolikelyhood for a 2D network with complex valued ordered couplings, where the decimation of the real and imaginary coupling matrices proceeds in parallel, that is, 
when a real coupling is small enough to be decimated its imaginary part is also decimated, and vice versa.
One can see that though the apart errors for the real and imaginary parts are different in absolute values, they display the same dip, to be compared with the maximum point of $\mathcal{L}_t$.
     
       \begin{figure}[t!]
    	\centering
      	\includegraphics[width=1\linewidth]{Jor3_dec_tPLF_new}
      	\caption{Tilted Pseudolikelyhood, ${\cal L}_t$, plotted with the reconstruction errors for the XY model with $N=64$ spins on a 2D lattice. These results refer to the case of  ordered and complex valued couplings. The full (red) line indicates ${\cal L}_t$. The dashed (green) 
      		and the dotted (blue) lines show the reconstruction errors (Eq. \eqref{eq:errj}) obtained for the real and the imaginary couplings respectively. We can see that both ${\rm err_{JR}}$ and ${\rm err_{JI}}$ have a minimum at $x^*$.}
          	\label{fig:Lt_complex}
    \end{figure}

\begin{figure}[t!]
   	\centering
   	\includegraphics[width=1\linewidth]{Jor1_dec_JR_soJR_TPJR}
   	\caption{XY model on a 2D lattice with $N=64$ sites and real valued couplings. The graphs show the inferred (dashed black lines) and true couplings (full green lines) plotted on top of each other. The left and right columns refer to the
   		 cases of ordered and bimodal disordered couplings, respectively. Top figures: single site reconstruction, i.e., one row of the matrix $J$. Bottom figures: couplings are plotted sorted in descending order.}  
   	\label{Jor1_dec}
   \end{figure}
   
\begin{figure}[t!]
    	\centering
    	\includegraphics[width=1\linewidth]{Jor3_dec_JRJI_soJRJI_TPJRJI}
    	\caption{XY model on a 2D lattice with $N=64$ sites and ordered complex-valued couplings.
    		The inferred and true couplings are plotted on top of each other. The left and right columns show the real and imaginary parts, respectively, of the couplings. Top figures refer to a single site reconstruction, i.e., one row of the matrix $J$. Bottom figures report the couplings sorted in descending order.}
    	\label{Jor3_dec}
    \end{figure}
    
       







       \begin{figure}[t!]
     	\centering
     	\includegraphics[width=1\linewidth]{MF_PL_Jor1_2D_TPJR_varT}
     	\caption{True Positive curves obtained with the three techniques: PLM with decimation, (blue) dotted line,  PLM with $l_2$ regularization, (greed) dashed line, and mean-field, (red) full line.  These results refer to real valued ordered couplings with $N=64$ spins on a 2D lattice. The temperature is here $T=0.7$ while the four graphs refer to different sample sizes: $M$ increases clockwise.}
     	\label{MF_PL_TP}
     \end{figure}
    
    \begin{figure}[t!]
    	\centering
    	\includegraphics[width=1\linewidth]{MF_PL_Jor1_2D_errJ_varT_varM}
    	\caption{Variation of reconstruction error, ${\rm err_J}$, with respect to temperature as obtained with the three different techniques, see Fig. \ref{MF_PL_TP}, for four different sample size:  clockwise from top   $M=512,1024, 2048$ and $4096$.} 
    	\label{MF_PL_err}
    \end{figure}
    
     Once the most likely network has been identified through the decimation procedure, we perform the same analysis displayed in Fig. \ref{Jor1_dec}  for ordered and then quenched disordered real-valued couplings
and in Fig. \ref{Jor3_dec} for  complex-valued ordered couplings.  In comparison to the results shown in Sec. \ref{sec:res_reg},
  the PLM with decimation leads to rather cleaner results. In Figs. \ref{MF_PL_err} and \ref{MF_PL_TP} we compare the performances of the PLM with decimation in respect to ones of the PLM with $l_2$-regularization. These two techniques are also analysed in respect to a mean-field technique previously implemented on the same XY systems\cite{Tyagi15}.
  
    For what concerns the network of connecting links, in Fig. \ref{MF_PL_TP} we compare the TP curves obtained with the three techniques. The results refer to the case of ordered and real valued couplings, but similar behaviours were obtained for the other cases analysed. 
  The four graphs are related to different sample sizes, with $M$ increasing clockwise. When $M$ is high enough, all techniques reproduce the true network. 
  However, for lower values of $M$ the performances of the PLM with $l_2$ regularization and with decimation drastically overcome those ones of the previous mean field technique. 
  In particular, for $M=256$ the PLM techniques still reproduce the original network while the mean-field method fails to find more than half of the couplings. 
  When $M=128$, the network is clearly reconstructed only through the PLM with decimation while the PLM with $l_2$ regularization underestimates the couplings. 
  Furthermore, we notice that the PLM method with decimation is able to clearly infer the network of interaction even when $M=N$ signalling that it could be considered also in the under-sampling regime $M<N$.  
 
 
In Fig. \ref{MF_PL_err} we compare the temperature behaviour of the reconstruction error.
In can be observed that for all temperatures and for all sample sizes  the reconstruction error, ${\rm err_J}$, (plotted here in log-scale) obtained with the PLM+decimation is always smaller than 
that one obtained with the other techniques. The temperature behaviour of ${\rm err_J}$ agrees with the one already observed for Ising spins in \cite{Nguyen12b} and for XY spins  in \cite{Tyagi15} with a mean-field approach:  ${\rm err_J}$ displays a minimum around $T\simeq 1$ and then it increases for very lower $T$; however,
 the error obtained with the PLM with decimation is several times smaller  than the error estimated by the other methods.



 
 

     
     \section{Conclusions}
     \label{sec:conc}


Different statistical inference methods have been applied to the inverse problem of the XY model.
After a short review of techniques based on pseudo-likelihood and their formal generalization to the model we have tested their performances against data generated by means of Monte Carlo numerical simulations of known instances
with diluted, sparse, interactions.

The main outcome is that the best performances are obtained by means of the  pseudo-likelihood method combined with decimation. Putting to zero (i.e., decimating) very weak bonds, this technique turns out to be very precise for  problems whose real underlying interaction network is sparse, i.e., the number of couplings per variable does not scale with number of variables.
The PLM + decimation method is compared to the PLM + regularization method, with $\ell_2$ regularization and to a mean-field-based method. The behavior of the quality of the network reconstruction is analyzed by looking at the overall sorted couplings and at the single site couplings, comparing them with the real network, and at the true positive curves in all three approaches. In the PLM +decimation method, moreover, the identification of the number of decimated bonds at which the tilted pseudo-likelihood is maximum allows for a precise estimate of the total number of bonds. Concerning this technique, it is also shown that the network with the most likely number of bonds is also the one of least reconstruction error, where not only the prediction of the presence of a bond is estimated but also its value.

The behavior of the inference quality in temperature and in the size of data samples is also investigated, basically confirming the low $T$ behavior hinted by Nguyen and Berg \cite{Nguyen12b} for the Ising model. In temperature, in particular, the reconstruction error curve displays a minimum at a low temperature, close to the critical point in those cases in which a critical behavior occurs, and a sharp increase as temperature goes to zero. The decimation method, once again, appears to enhance this minimum of the reconstruction error of almost an order of magnitude with respect to other methods.
 
The techniques displayed and the results obtained in this work can be of use in any of the many systems whose theoretical representation is given by Eq. \eqref{eq:HXY} or Eq. \eqref{eq:h_im}, some of which are recalled in Sec. \ref{sec:model}. In particular, a possible application can be the field of light waves propagation through random media and the corresponding problem of the  reconstruction of an object seen through an opaque medium or a disordered optical fiber \cite{Vellekoop07,Vellekoop08a,Vellekoop08b, Popoff10a,Akbulut11,Popoff11,Yilmaz13,Riboli14}.

 	","['Physics, biology, social sciences, finance, and neuroscience are some fields in which the inverse problem is encountered.']",6308,multifieldqa_en,en,,c18fe208b75f8f5b4214e2cb17420d75bf525e176e425fc0,"Physics, biology, social sciences, finance, and neuroscience are some fields in which the inverse problem is encountered.",121
What is the name of the generative interactive model used in the method?,"\section{Introduction}
In recent years, vehicular technology has attracted significant attention from the automotive and telecommunication industries, leading to the emergence of vehicle-to-everything (V2X) communications for improving road safety, traffic management services and driving comfort.
V2X supported by the sixth generation (6G) is envisioned to be a key enabler of future connected autonomous vehicles \cite{9779322}. Although its transformative benefits for leveraging intelligent transportation systems, V2X still face several technical issues mainly related to performance and security.

The integration of sensing and communication (ISAC) has emerged very recently as a revolutionary element of 6G that could potentially help enabling adaptive learning and intelligent decision-making in future V2X applications.
The combination of sensing and communication allows vehicles to perceive their surroundings better, predict manoeuvres from nearby users and make intelligent decisions, thus paving the way toward a safer transportation system \cite{9665433}.
Modernized vehicles are augmented with various types of sensors divided into exteroceptive to observe their surrounding environment and proprioceptive to observe their internal states.
The former like GPS, Lidar, and Cameras are conveyed to improve situational awareness, while latter sensors, such as steering, pedal, and wheel speed, convey to improve self-awareness. 

While sensing the environment, vehicles can exchange messages that assist in improving situational- and self-awareness and in coordinating maneuvers with other vehicles.
Those messages like the basic safety (BSMs) and cooperative awareness messages (CAMs) are composed of transmitting vehicle's states such as position and velocity and other vehicles' states in the vicinity. Vehicles might use their sensors, such as cameras and Lidar, to detect road users (e.g., pedestrians), which can be communicated with other road users via the V2X messages to improve the overall performance. However, V2X communication links carrying those messages are inherently vulnerable to malicious attacks due to the open and shared nature of the wireless spectrum among vehicles and other cellular users \cite{8336901}. For instance, a jammer in the vicinity might alter the information to be communicated to nearby vehicles/users or can intentionally disrupt communication between a platoon of vehicles making the legitimate signals unrecognizable for on-board units (OBUs) and/or road side units (RSUs) that endanger vehicular safety 
\cite{8553649}.

In addition, the integrity of GPS signals and the correct acquisition of navigation data to compute position, velocity and time information is critical in V2X applications for their safe operation. However, since civil GPS receivers rely on unencrypted satellite signals, spoofers can easily replicate them by deceiving the GPS receiver to compute falsified positions \cite{9226611}.
Also, the long distance between satellites and terrestrial GPS receivers leads to an extremely weak signal that can be easily drowned out by a spoofer. 
Thus, GPS sensors' vulnerability to spoofing attacks poses a severe threat that might be causing vehicles to be out of control or even hijacked and endanger human life \cite{9881548}.
Therefore, GPS spoofing attacks and jamming interference needs to be controlled and detected in real-time to reach secured vehicular communications allowing vehicles to securely talk to each other and interact with the infrastructure (e.g., roadside terminals, base stations) \cite{9860410}.

Existing methods for GPS spoofing detection include GPS signal analysis methods and GPS message encryption methods \cite{9845684}. However, the former requires the ground truth source during the detection process, which is not always possible to collect. In contrast, the latter involves support from a secured infrastructure and advanced computing resources on GPS receivers, which hinders their adoption in V2X applications. On the other hand, existing methods for jammer detection in vehicular networks are based on analysing the packet drop rate as in \cite{9484071}, making it difficult to detect an advanced jammer manipulating the legitimate signal instead of disrupting it.
In this work, we propose a method to jointly detect GPS spoofing and jamming attacks in the V2X network. A coupled generalized dynamic Bayesian network (C-GDBN) is employed to learn the interaction between RF signals received by the RSU from multiple vehicles and their corresponding trajectories. This integration of vehicles' positional information with vehicle-to-infrastructure (V2I) communications allows semantic learning while mapping RF signals with vehicles' trajectories and enables the RSU to jointly predict the RF signals it expects to receive from the vehicles from which it can anticipate the expected trajectories.

The main contributions of this paper can be summarized as follows: \textit{i)} A joint GPS spoofing and jamming detection method is proposed for the V2X scenario, which is based on learning a generative interactive model as the C-GDBN. Such a model encodes the cross-correlation between the RF signals transmitted by multiple vehicles and their trajectories, where their semantic meaning is coupled stochastically at a high abstraction level. \textit{ii)} A cognitive RSU equipped with the acquired C-GDBN can predict and estimate vehicle positions based on real-time RF signals. This allows RSU to evaluate whether both RF signals and vehicles' trajectories are evolving according to the dynamic rules encoded in the C-GDBN and, consequently, to identify the cause (i.e., a jammer attacking the V2I or a spoofer attacking the satellite link) of the abnormal behaviour that occurred in the V2X environment. \textit{iii)} Extensive simulation results demonstrate that the proposed method accurately estimates the vehicles' trajectories from the predicted RF signals, effectively detect any abnormal behaviour and identify the type of abnormality occurring with high detection probabilities.
To our best knowledge, this is the first work that studies the joint detection of jamming and spoofing in V2X systems.

\section{System model and problem formulation}
The system model depicted in Fig.~\ref{fig_SystemModel}, includes a single cell vehicular network consisting of a road side unit (RSU) located at $\mathrm{p}_{R}=[{x}_{R},{y}_{R}]$, a road side jammer (RSJ) located at $\mathrm{p}_{J}=[{x}_{J},{y}_{J}]$, a road side spoofer (RSS) located at $\mathrm{p}_{s}=[{x}_{s},{y}_{s}]$ and $N$ vehicles moving along multi-lane road in an urban area. The time-varying positions of the $n$-th vehicle is given by $\mathrm{p}_{n,t}=[{x}_{n,t},{y}_{n,t}]$ where $n \in N$. Among the $K$ orthogonal subchannels available for the Vehicle-to-Infrastructure (V2I) communications, RSU assigns one V2I link to each vehicle. Each vehicle exchanges messages composed of the vehicle's state (i.e., position and velocity) with RSU through the $k$-th V2I link by transmitting a signal $\textrm{x}_{t,k}$ carrying those messages at each time instant $t$ where $k \in K$. We consider a reactive RSJ that aims to attack the V2I link by injecting intentional interference to the communication link between vehicles and RSU to alter the transmitted signals by the vehicles. In contrast, the RSS purposes to mislead the vehicles by spoofing the GPS signal and so registering wrong GPS positions. RSU aims to detect both the spoofer on the satellite link and the jammer on multiple V2I links in order to take effective actions and protect the vehicular network. 
The joint GPS spoofing and jamming detection problem can be formulated as the following ternary hypothesis test:
\begin{equation}
    \begin{cases}
        \mathcal{H}_{0}: \mathrm{z}_{t,k} = \mathrm{g}_{t,k}^{nR} \mathrm{x}_{t,k} + \mathrm{v}_{t,k}, \\
        \mathcal{H}_{1}: \mathrm{z}_{t,k} = \mathrm{g}_{t,k}^{nR} \mathrm{x}_{t,k} + \mathrm{g}_{t,k}^{JR} \mathrm{x}_{t,k}^{j} + \mathrm{v}_{t,k}, \\
        \mathcal{H}_{2}: \mathrm{z}_{t,k} = \mathrm{g}_{t,k}^{nR} \mathrm{x}_{t,k}^{*} + \mathrm{v}_{t,k},
    \end{cases}
\end{equation}
where $\mathcal{H}_{0}$, $\mathcal{H}_{1}$ and $\mathcal{H}_{2}$ denote three hypotheses corresponding to the absence of both jammer and spoofer, the presence of the jammer, and the presence of the spoofer, respectively. $\textrm{z}_{t,k}$ is the received signal at the RSU at $t$ over the $k$-th V2I link, $\textrm{g}_{t,k}^{nR}$ is the channel power gain from vehicle $n$ to the RSU formulated as: $\textrm{g}_{t,k}^{nR} = \alpha_{t,k}^{nR} \mathrm{h}_{t,k}^{nR}$, where $\alpha_{t,k}^{nR}$ is the large-scale fading including path-loss and shadowing modeled as \cite{8723178}: $\alpha_{t,k}^{nR}=G\beta d_{t,nR}^{-\gamma}$.
\begin{figure}[t!]
    \centering
    \includegraphics[height=5.3cm]{Figures/SystemModel_V1.pdf}
    \caption{An illustration of the system model.}
    \label{fig_SystemModel}
\end{figure}
$G$ is the pathloss constant, $\beta$ is a log normal shadow fading random variable, $d_{t,nR}=\sqrt{({x}_{n,t}-x_{R})^{2}+({y}_{n,t}-y_{R})^{2}}$ is the distance between the $n$-th vehicle and the RSU. $\gamma$ is the power decay exponent and
$\mathrm{h}_{t,k}$ is the small-scale fading component distributed according to $\mathcal{CN}(0,1)$. In addition, $\mathrm{x}_{t,k}$ is the desired signal transmitted by the $n$-th vehicle, and $\mathrm{v}_{t,k}$ is an additive white Gaussian noise with variance $\sigma_{n}^{2}$. $\mathrm{x}_{t,k}^{J}$ is the jamming signal, $\mathrm{x}_{t,k}^{*}$ is the spoofed signal (i.e., the signal that carries the bits related to the wrong GPS positions), $\mathrm{g}_{t,k}^{JR} = \alpha_{t,k}^{JR} \mathrm{h}_{t,k}^{JR}$ is the channel power gain from RSJ to RSU where $\alpha_{t,k}^{JR}=G\beta d_{t,JR}^{-\gamma}$ such that $d_{t,JR}=\sqrt{({x}_{J}-x_{R})^{2}+({y}_{J}-y_{R})^{2}}$.
We assume that the channel state information (CSI) of V2I links is known and can be estimated at the RSU as in \cite{8345717}. 
The RSU is equipped with an RF antenna which can track the vehicles' trajectories after decoding the received RF signals. RSU aims to learn the interaction between the RF signals received from multiple vehicles and their corresponding trajectories.

\section{Proposed method for joint detection of GPS spoofing and jamming}

\subsection{Environment Representation}
The RSU is receiving RF signals from each vehicle and tracking its trajectory (which we refer to as GPS signal) by decoding and demodulating the received RF signals. 
The Generalized state-space model describing the $i$-th signal evolvement at multiple levels embodies the following equations: 
\begin{equation} \label{eq_discreteLevel}
    \mathrm{\Tilde{S}_{t}}^{(i)} = \mathrm{f}(\mathrm{\Tilde{S}_{t-1}}^{(i)}) + \mathrm{\tilde{w}}_{t},
\end{equation}
\begin{equation} \label{eq_continuousLevel}
    \mathrm{\Tilde{X}_{t}}^{(i)} = \mathrm{A} \mathrm{\Tilde{X}_{t-1}}^{(i)} + \mathrm{B} \mathrm{U}_{\mathrm{\Tilde{S}_{t}}^{(i)}} + \mathrm{\tilde{w}}_{t},
\end{equation}
\begin{equation} \label{eq_observationLevel}
    \mathrm{\Tilde{Z}_{t}}^{(i)} = \mathrm{H} \mathrm{\Tilde{X}_{t}}^{(i)} + \mathrm{\tilde{v}}_{t},
\end{equation}
where $i \in \{$RF, GPS$\}$ indicates the type of signal received by the RSU. The transition system model defined in \eqref{eq_discreteLevel} explains the evolution of the discrete random variables $\mathrm{\Tilde{S}_{t}}^{(i)}$ representing the clusters of the RF (or GPS) signal dynamics, $\mathrm{f}(.)$ is a non linear function of its argument and the additive term $\mathrm{\tilde{w}}_{t}$ denotes the process noise. The dynamic model defined in \eqref{eq_continuousLevel} explains the RF signal dynamics evolution or the motion dynamics evolution of the $n$-th vehicle, where $\mathrm{\Tilde{X}_{t}}^{(i)}$ are hidden continuous variables generating sensory signals, $\mathrm{A} \in \mathbb{R}^{2d}$ and $\mathrm{B} \in \mathbb{R}^{2d}$ are the dynamic and control matrices, respectively, and $\mathrm{U}_{\mathrm{\Tilde{S}_{t}}^{(i)}}$ is the control vector representing the dynamic rules of how the signals evolve with time. The measurement model defined in \eqref{eq_observationLevel} describes dependence of the sensory signals $\mathrm{\Tilde{Z}_{t}}^{(i)}$ on the hidden states $\mathrm{\Tilde{X}_{t}}^{(i)}$ that is parametrized by the measurement matrix $\mathrm{B} \in \mathbb{R}^{2d}$ where $d$ stands for the data dimensionality and $\mathrm{\tilde{v}}_{t}$ is a random noise. 

\subsection{Learning GDBN}
The hierarchical dynamic models defined in \eqref{eq_discreteLevel}, \eqref{eq_continuousLevel} and \eqref{eq_observationLevel} are structured in a Generalized Dynamic Bayesian Network (GDBN) \cite{9858012} as shown in Fig.~\ref{fig_GDBN_CGDBN}-(a) that provides a probabilistic graphical model expressing the conditional dependencies among random hidden variables and observable states. The generative process explaining how sensory signals have been generated can be factorized as:
\begin{equation} \label{eq_generative_process}
\begin{split}
    \mathrm{P}(\mathrm{\tilde{Z}}_{t}^{(i)}, \mathrm{\tilde{X}}_{t}^{(i)}, \mathrm{\tilde{S}}_{t}^{(i)}) = \mathrm{P}(\mathrm{\tilde{S}}_{0}^{(i)}) \mathrm{P}(\mathrm{\tilde{X}}_{0}^{(i)}) \\ \bigg[ \prod_{t=1}^{\mathrm{T}} \mathrm{P}(\mathrm{\tilde{Z}}_{t}^{(i)}|\mathrm{\tilde{X}}_{t}^{(i)}) \mathrm{P}(\mathrm{\tilde{X}}_{t}^{(i)}|\mathrm{\tilde{X}}_{t-1}^{(i)}, \mathrm{\tilde{S}}_{t}^{(i)}) \mathrm{P}(\mathrm{\tilde{S}}_{t}^{(i)}|\mathrm{\tilde{S}}_{t-1}^{(i)}) \bigg],
\end{split}
\end{equation}
where $\mathrm{P}(\mathrm{\tilde{S}}_{0}^{(i)})$ and $\mathrm{P}(\mathrm{\tilde{X}}_{0}^{(i)})$ are initial prior distributions, $\mathrm{P}(\mathrm{\tilde{Z}}_{t}^{(i)}|\mathrm{\tilde{X}}_{t}^{(i)})$ is the likelihood, $\mathrm{P}(\mathrm{\tilde{X}}_{t}^{(i)}|\mathrm{\tilde{X}}_{t-1}^{(i)}, \mathrm{\tilde{S}}_{t}^{(i)})$ and $\mathrm{P}(\mathrm{\tilde{S}}_{t}^{(i)}|\mathrm{\tilde{S}}_{t-1}^{(i)})$ are the transition densities describing the temporal and hierarchical dynamics of the generalized state-space model.
The generative process defined in \eqref{eq_generative_process} indicates the cause-effect relationships the model impose on the random variables $\mathrm{\tilde{S}}_{t}^{(i)}$, $\mathrm{\tilde{X}}_{t}^{(i)}$ and $\mathrm{\tilde{Z}}_{t}^{(i)}$ forming a chain of causality describing how one state contributes to the production of another state which is represented by the link $\mathrm{\tilde{S}}_{t}^{(i)} \rightarrow \mathrm{\tilde{X}}_{t}^{(i)} \rightarrow \mathrm{\tilde{Z}}_{t}^{(i)}$.

The RSU starts perceiving the environment using a static assumption about the environmental states evolution by assuming that sensory signals are only subject to random noise. Hence, RSU predicts the RF signal (or vehciles trajectory) using the following simplified model:
$\mathrm{\tilde{X}}_{t}^{(i)} = \mathrm{A} \mathrm{\tilde{X}}_{t-1}^{(i)} + \mathrm{\tilde{w}}_{t}$, 
that differs from \eqref{eq_continuousLevel} in the control vector $\mathrm{U}_{\mathrm{\Tilde{S}_{t}}^{(i)}}$ which is supposed to be null, i.e., $\mathrm{U}_{\mathrm{\Tilde{S}_{t}}^{(i)}} = 0$ as the dynamic rules explaining how the environmental states evolve with time are not discovered yet.
Those rules can be discovered by exploiting the generalized errors (GEs), i.e., the difference between predictions and observations. The GEs projected into the measurement space are calculated as:
$\tilde{\varepsilon}_{\mathrm{\tilde{Z}}_{t}^{(i)}}^{} = \mathrm{\tilde{Z}}_{t}^{(i)} - \mathrm{H} \mathrm{\tilde{X}}_{t}^{(i)}$.
Projecting $\tilde{\varepsilon}_{\mathrm{\tilde{Z}}_t}^{}$ back into the generalized state space can be done as follows:
\begin{equation}\label{GE_continuousLevel_initialModel}
    \tilde{\varepsilon}_{\mathrm{\tilde{X}}_t}^{(i)} = \mathrm{H}^{-1}\tilde{\varepsilon}_{\mathrm{\tilde{Z}}_{t}^{(i)}}^{}=\mathrm{H}^{-1}(\mathrm{\tilde{Z}}_{t}^{(i)}-\mathrm{H}\mathrm{\tilde{X}}_{t}^{(i)}) = \mathrm{H}^{-1}\mathrm{\tilde{Z}}_{t}^{(i)} - \mathrm{\tilde{X}}_{t}^{(i)}.
\end{equation}
The GEs defined in \eqref{GE_continuousLevel_initialModel} can be grouped into discrete clusters in an unsupervised manner by employing the Growing Neural Gas (GNG). The latter produces a set of discrete variables (clusters) denoted by:
$\mathbf{\tilde{S}^{(i)}}=\{\mathrm{\tilde{S}}_{1}^{(i)},\mathrm{\tilde{S}}_{2}^{(i)},\dots,\mathrm{\tilde{S}}_{M_{i}}^{(i)}\}$,
where $M_{i}$ is the total number of clusters and each cluster $\mathrm{\tilde{S}}_{m}^{(i)} \in \mathbf{\tilde{S}^{(i)}}$ follows a Gaussian distribution composed of GEs with homogeneous properties, such that $\mathrm{\tilde{S}}_{m}^{(i)} \sim \mathcal{N}(\tilde{\mu}_{\mathrm{\tilde{S}}_{m}^{(i)}}=[\mu_{\tilde{S}_{m}^{(i)}}, \Dot{\mu}_{\tilde{S}_{m}^{(i)}}], \Sigma_{\mathrm{\tilde{S}}_{m}^{(i)}})$.
\begin{figure}[t!]
    \begin{center}
        \begin{minipage}[b]{.40\linewidth}
        \centering
           \includegraphics[width=2.5cm]{Figures/GDBN.pdf}
        \\[-1.0mm]
        {\scriptsize (a)}
        \end{minipage}
        \begin{minipage}[b]{.50\linewidth}
            \centering
            \includegraphics[width=5.0cm]{Figures/C_GDBN.pdf}
           
            {\scriptsize (b)}
        \end{minipage}
        \caption{(a) The GDBN. (b) The coupled GDBN (C-GDBN) composed of two GDBNs representing the two signals received at the RSU where their discrete hidden variables are stochastically coupled.}
        \label{fig_GDBN_CGDBN}
    \end{center}
\end{figure}
The dynamic transitions of the sensory signals among the available clusters can be captured in a time-varying transition matrix ($\Pi_{\tau}$) by estimating the time-varying transition probabilities $\pi_{ij}=\mathrm{P}(\mathrm{\tilde{S}}_{t}^{(i)}=i|\mathrm{\tilde{S}}_{t-1}^{(i)}=j, \tau)$ where $\tau$ is the time spent in $\mathrm{\tilde{S}}_{t-1}^{(i)}=j$ before transition to $\mathrm{\tilde{S}}_{t}^{(i)}=i$.

\subsection{Learning Coupled GDBN (C-GDBN)}
The learning procedure described in the previous section can be executed for each signal type, i.e., RF and GPS. After learning a separated GDBN model for each signal type, we analyse the interaction behaviour between RF signal and GPS signal received at the RSU by tracking the cluster firing among $\mathbf{\tilde{S}^{(1)}}$ and $\mathbf{\tilde{S}^{(2)}}$ during a certain experience. Such an interaction can be encoded in a Coupled GDBN (C-GDBN) as shown in Fig.\ref{fig_GDBN_CGDBN}-(b) composed of the two GDBNs representing the two signals where their hidden variables at the discrete level are stochastically coupled (in $\mathrm{\tilde{C}}_{t}{=}[\mathrm{\tilde{S}}_{t}^{(1)},\mathrm{\tilde{S}}_{t}^{(2)}]$) as those variables are uncorrelated but have coupled means.
The interactive matrix $\Phi \in \mathbb{R}^{M_{1},M_{2}}$ encodes the firing cluster pattern allowing to predict the GPS signal from RF signal is defined as follows:
\begin{equation} \label{interactiveTM_fromRFtoGPS}
\Phi = 
        \begin{bmatrix} 
            \mathrm{P}(\mathrm{\Tilde{S}_{1}}^{(2)}|\mathrm{\Tilde{S}_{1}}^{(1)}) & \mathrm{P}(\mathrm{\Tilde{S}_{2}}^{(2)}|\mathrm{\Tilde{S}_{1}}^{(1)}) & \dots & \mathrm{P}(\mathrm{\Tilde{S}_{M_{2}}}^{(2)}|\mathrm{\Tilde{S}_{1}}^{(1)}) \\
            \mathrm{P}(\mathrm{\Tilde{S}_{1}}^{(2)}|\mathrm{\Tilde{S}_{2}}^{(1)}) & \mathrm{P}(\mathrm{\Tilde{S}_{2}}^{(2)}|\mathrm{\Tilde{S}_{2}}^{(1)}) & \dots & \mathrm{P}(\mathrm{\Tilde{S}_{M_{2}}}^{(2)}|\mathrm{\Tilde{S}_{2}}^{(1)}) \\
            \vdots & \vdots & \ddots & \vdots \\
            \mathrm{P}(\mathrm{\Tilde{S}_{1}}^{(2)}|\mathrm{\Tilde{S}_{M_{1}}}^{(1)}) & \mathrm{P}(\mathrm{\Tilde{S}_{2}}^{(2)}|\mathrm{\Tilde{S}_{M_{1}}}^{(1)}) & \dots & \mathrm{P}(\mathrm{\Tilde{S}_{M_{2}}}^{(2)}|\mathrm{\Tilde{S}_{M_{1}}}^{(1)}) 
        \end{bmatrix}.
\end{equation}

\subsection{Joint Prediction and Perception}
RSU starts predicting the RF signals it expects to receive from each vehicle based on a Modified Markov Jump Particle Filter (M-MJPF) \cite{9858012} that combines Particle filter (PF) and Kalman filter (KF) to perform temporal and hierarchical predictions. Since the acquired C-GDBN allows predicting a certain signal's dynamic evolution based on another's evolution, it requires an interactive Bayesian filter capable of dealing with more complicated predictions. To this purpose, we propose to employ an Interactive M-MJPF (IM-MJPF) on the C-GDBN. The IM-MJPF consists of a PF that propagates a set of $L$ particles equally weighted, such that $\{\mathrm{\tilde{S}}_{t,l}^{(1)}, \mathrm{W}_{t,l}^{(1)}\}{\sim}\{\pi(\mathrm{\tilde{S}}_{t}^{(1)}), \frac{1}{L}\}$, where $\mathrm{\tilde{S}}_{t,l}^{(1)}$, $l \in L$ and $(.^{(1)})$ is the RF signal type. In addition, RSU relies on $\Phi$ defined in \eqref{interactiveTM_fromRFtoGPS} to predict $\mathrm{\tilde{S}}_{t}^{(2)}$ realizing the discrete cluster of vehicle's trajectory starting from the predicted RF signal according to: $\{\mathrm{\tilde{S}}_{t}^{(2)},\mathrm{W}_{t,l}^{(2)}\}{\sim} \{\Phi(\mathrm{\tilde{S}}_{t,l}^{(1)}){=}\mathrm{P}(.|\mathrm{\tilde{S}}_{t,l}^{(1)}), \mathrm{W}_{t,l}^{(2)}\}$. For each predicted discrete variable $\mathrm{\tilde{S}}_{t,l}^{(i)}$, a multiple KF is employed to predict multiple continuous variables which guided by the predictions at the higher level as declared in \eqref{eq_continuousLevel} that can be represented probabilistically as $\mathrm{P}(\mathrm{\tilde{X}}_{t}^{(i)}|\mathrm{\tilde{X}}_{t-1}^{(i)}, \mathrm{\tilde{S}}_{t}^{(i)})$. The posterior probability that is used to evaluate expectations is given by:
\begin{multline} \label{piX}
    \pi(\mathrm{\tilde{X}}_{t}^{(i)})=\mathrm{P}(\mathrm{\tilde{X}}_{t}^{(i)},\mathrm{\tilde{S}}_{t}^{(i)}|\mathrm{\tilde{Z}}_{t-1}^{(i)})= \\ \int \mathrm{P}(\mathrm{\tilde{X}}_{t}^{(i)}|\mathrm{\tilde{X}}_{t-1}^{(i)}, \mathrm{\tilde{S}}_{t}^{(i)}) \lambda(\mathrm{\tilde{X}}_{t-1}^{(i)})d\mathrm{\tilde{X}}_{t-1}^{(i)},
\end{multline}
where $\lambda(\mathrm{\tilde{X}}_{t-1}^{(i)}){=}\mathrm{P}(\mathrm{\tilde{Z}}_{t-1}^{(i)}|\mathrm{\tilde{X}}_{t-1}^{(i)})$. 
The posterior distribution can be updated (and so representing the updated belief) after having seen the new evidence $\mathrm{\tilde{Z}}_{t}^{(i)}$ by exploiting the diagnostic message $\lambda(\mathrm{\tilde{X}}_{t}^{(i)})$ in the following form: $\mathrm{P}(\mathrm{\tilde{X}}_{t}^{(i)}, \mathrm{\tilde{S}}_{t}^{(i)}|\mathrm{\tilde{Z}}_{t}^{(i)}) {=} \pi(\mathrm{\tilde{X}}_{t}^{(i)})\lambda(\mathrm{\tilde{X}}_{t}^{(i)})$. Likewise, belief in discrete hidden variables can be updated according to: $\mathrm{W}_{t,l}^{(i)}{=}\mathrm{W}_{t,l}^{(i)}\lambda (\mathrm{\tilde{S}}_{t}^{(i)})$ where:
$\lambda (\mathrm{\tilde{S}}_{t}^{(i)}) {=} \lambda (\mathrm{\Tilde{X}}_{t}^{(i)})\mathrm{P}(\mathrm{\Tilde{X}}_{t}^{(i)}|\mathrm{\tilde{S}}_{t}^{(i)}) {=} \mathrm{P}(\mathrm{\tilde{Z}}_{t}^{(i)}|\mathrm{\Tilde{X}}_{t}^{(i)})\mathrm{P}(\mathrm{\Tilde{X}}_{t}^{(i)}|\mathrm{\tilde{S}}_{t}^{(i)})$.

\subsection{Joint GPS spoofing and jamming detection}
RSU can evaluate the current situation and identify if V2I is under attack, or the satellite link is under spoofing based on a multiple abnormality indicator produced by the IM-MJPF. The first indicator calculates the similarity between the predicted RF signal and the observed one, which is defined as:
\begin{equation}\label{eq_CLA1}
    \Upsilon_{\mathrm{\tilde{X}}_{t}^{(1)}} = -ln \bigg( \mathcal{BC} \big(\pi(\mathrm{\tilde{X}}_{t}^{(1)}),\lambda(\mathrm{\tilde{X}}_{t}^{(1)}) \big) \bigg),
\end{equation}
where $\mathcal{BC}(.){=}\int \sqrt{\pi(\mathrm{\tilde{X}}_{t}^{(1)}),\lambda(\mathrm{\tilde{X}}_{t}^{(1)}})d\mathrm{\tilde{X}}_{t}^{(1)}$ is the Bhattacharyya coefficient.
The second indicator calculates the similarity between the predicted GPS signal (from the RF signal) and the observed one after decoding the RF signal which is defined as:
\begin{equation}\label{eq_CLA2}
    \Upsilon_{\mathrm{\tilde{X}}_{t}^{(2)}} = -ln \bigg( \mathcal{BC} \big(\pi(\mathrm{\tilde{X}}_{t}^{(2)}),\lambda(\mathrm{\tilde{X}}_{t}^{(2)}) \big) \bigg),
\end{equation}
where $\mathcal{BC}(.){=}\int \sqrt{\pi(\mathrm{\tilde{X}}_{t}^{(2)}),\lambda(\mathrm{\tilde{X}}_{t}^{(2)}})d\mathrm{\tilde{X}}_{t}^{(2)}$.
Different hypotheses can be identified by the RSU to understand the current situation whether there is: a jammer attacking the V2I link, or a spoofer attacking the link between the satellite and the vehicle or both jammer and spoofer are absent according to:
\begin{equation}
    \begin{cases}
        \mathcal{H}_{0}: \text{if} \ \ \Upsilon_{\mathrm{\tilde{X}}_{t}^{(1)}} < \xi_{1} \ \text{and} \ \Upsilon_{\mathrm{\tilde{X}}_{t}^{(2)}} < \xi_{2}, \\
        \mathcal{H}_{1}: \text{if} \ \ \Upsilon_{\mathrm{\tilde{X}}_{t}^{(1)}} \geq \xi_{1} \ \text{and} \ \Upsilon_{\mathrm{\tilde{X}}_{t}^{(2)}} \geq \xi_{2}, \\
        \mathcal{H}_{2}: \text{if} \ \ \Upsilon_{\mathrm{\tilde{X}}_{t}^{(1)}} < \xi_{1} \ \text{and} \ \Upsilon_{\mathrm{\tilde{X}}_{t}^{(2)}} \geq \xi_{2},
    \end{cases}
\end{equation}
where $\xi_{1} = \mathbb{E}[\Bar{\Upsilon}_{\mathrm{\tilde{X}}_{t}^{(1)}}] + 3\sqrt{\mathbb{V}[\Bar{\Upsilon}_{\mathrm{\tilde{X}}_{t}^{(1)}}]}$, and $\xi_{2} = \mathbb{E}[\Bar{\Upsilon}_{\mathrm{\tilde{X}}_{t}^{(2)}}] + 3\sqrt{\mathbb{V}[\Bar{\Upsilon}_{\mathrm{\tilde{X}}_{t}^{(2)}}]}$. In $\xi_{1}$ and $\xi_{2}$, $\Bar{\Upsilon}_{\mathrm{\tilde{X}}_{t}^{(1)}}$ and $\Bar{\Upsilon}_{\mathrm{\tilde{X}}_{t}^{(2)}}$ stand for the abnormality signals during training (i.e., normal situation when jammer and spoofer are absent).

\subsection{Evaluation metrics}
In order to evaluate the performance of the proposed method to jointly detect jammer and GPS spoofer, we adopt the jammer detection probability ($\mathrm{P}_{d}^{j}$) and the spoofer detection probability ($\mathrm{P}_{d}^{s}$), respectively, which are defined as:
\begin{equation}
    \mathrm{P}_{d}^{j} = \mathrm{Pr}(\Upsilon_{\mathrm{\tilde{X}}_{t}^{(1)}}\geq \xi_{1}, \Upsilon_{\mathrm{\tilde{X}}_{t}^{(2)}} \geq \xi_{2}|\mathcal{H}_{1}),
\end{equation}
\begin{equation}
    \mathrm{P}_{d}^{s} = \mathrm{Pr}(\Upsilon_{\mathrm{\tilde{X}}_{t}^{(1)}}< \xi_{1}, \Upsilon_{\mathrm{\tilde{X}}_{t}^{(2)}} \geq \xi_{2}|\mathcal{H}_{2}).
\end{equation}
Also, we evaluate the accuracy of the proposed method in predicting and estimating the vehicles' trajectories and the expected RF signals by adopting the root mean square error (RMSE) defined as:
\begin{equation}
    RMSE = \sqrt{ \frac{1}{T} \sum_{t=1}^{T}\bigg( \mathrm{\tilde{Z}}_{t}^{(i)}-\mathrm{\tilde{X}}_{t}^{(i)} \bigg)^{2} },
\end{equation}
where $T$ is the total number of predictions.

\section{Simulation Results}
In this section, we evaluate the performance of the proposed method to jointly detect the jammer and the spoofer using extensive simulations. We consider $\mathrm{N}=2$ vehicles interacting inside the environment and exchanging their states (i.e., position and velocity) with the RSU. The vehicles move along predefined trajectories performing various maneuvers which are picked from the \textit{Lankershim} dataset proposed by \cite{5206559}. The dataset depicts a four way intersection and includes about $19$ intersection maneuvers. RSU assigns one subchannel realizing the V2I link for each vehicle over which the vehicles' states are transmitted. The transmitted signal carrying the vehicle's state and the jamming signal are both QPSK modulated. 
The simulation settings are: carrier frequency of $2$GHz, BW${=}1.4$MHz, cell radius of $500$m, RSU antenna height and gain is $25$m and $8$ dBi, receiver noise figure of $5$dB, vehicle antenna height and gain is $1.5$m and $3$dBi, vehicle speed is $40$Km/h, V2I transmit power is $23$dBm, jammer transmit power ranging from $20$dBm to $40$dBm, SNR of $20$dB, path loss model ($128.1{+}37.6log d$), Log-normal shadowing with $8$dB standard deviation and a fast fading channel following the Rayleigh distribution.
\begin{figure}[ht!]
    \begin{center}
        \begin{minipage}[b]{.55\linewidth}
        \centering
            \includegraphics[width=5.0cm]{Results/ObservedTrajectories_reference}
        \\[-1.5mm]
        {\scriptsize (a)}
        \end{minipage}
        \begin{minipage}[b]{.49\linewidth}
            \centering
            \includegraphics[width=4.9cm]{Results/ObservedRFsignal_Veh1_reference}
            \\[-1.5mm]
            {\scriptsize (b)}
        \end{minipage}
        \begin{minipage}[b]{.49\linewidth}
            \centering
            \includegraphics[width=4.9cm]{Results/ObservedRFsignal_Veh2_reference}
            \\[-1.5mm]
            {\scriptsize (c)}
        \end{minipage}
        \caption{An example visualizing the received RF signals from the two vehicles and the corresponding trajectories: (a) Vehicles' trajectories, (b) received RF signal from vehicle 1, (c) received RF signal from vehicle 2.}
        \label{fig_receivedRFsignalandTrajectory}
    \end{center}
\end{figure}
\begin{figure}[t!]
    \begin{center}
        \begin{minipage}[b]{.49\linewidth}
            \centering
                \includegraphics[width=4.5cm]{Results/clusters_trajectory_veh1}
            \\[-1.5mm]
            {\scriptsize (a)}
        \end{minipage}
        \begin{minipage}[b]{.49\linewidth}
            \centering
                \includegraphics[width=4.5cm]{Results/clusters_trajectory_veh2}
            \\[-1.5mm]
            {\scriptsize (b)}
        \end{minipage}
        \begin{minipage}[b]{0.49\linewidth}
            \centering
            \includegraphics[width=4.5cm]{Results/clusters_RFsignal_veh1}
            \\[-1.5mm]
            {\scriptsize (c)}
        \end{minipage}
        \begin{minipage}[b]{0.49\linewidth}
            \centering
            \includegraphics[width=4.5cm]{Results/clusters_RFsignal_veh2}
            \\[-1.5mm]
            {\scriptsize (d)}
        \end{minipage}
       
        \caption{GNG output after clustering the generalized errors obtained from different experiences: (a) clustered trajectory of vehicle 1, (b) clustered trajectory of vehicle 2, (c) clustered RF signal received from vehicle 1, (d) clustered RF signal received from vehicle 2.}
        \label{fig_GNG_of_receivedRFsignalandTrajectory}
    \end{center}
\end{figure}

The RSU aims to learn multiple interactive models (i.e., C-GDBN models) encoding the cross relationship between the received RF signal from each vehicle and its corresponding trajectory. These models allow the RSU to predict the trajectory the vehicle will follow based on the received RF signal and evaluate whether the V2I is under jamming attacks or the satellite link is under spoofing. It is to note that the RSU is receiving only the RF signals from the two vehicles and obtaining their positions after decoding the RF signals. Thus, the RSU should be able to evaluate if the received RF signals are evolving according to the dynamic rules learned so far and if the vehicles are following the expected (right) trajectories to decide whether the V2I links are really under attack or whether the satellite link is under spoofing.

Fig.~\ref{fig_receivedRFsignalandTrajectory}-(a) illustrates an example of the interaction between the two vehicles performing a particular manoeuvre, and Fig.~\ref{fig_receivedRFsignalandTrajectory}-(b) shows the received RF signals by the RSU from the two vehicles. At the beginning of the learning process, RSU performs predictions according to the simplified model defined in \eqref{eq_continuousLevel} where $\mathrm{U}_{\mathrm{\Tilde{S}_{t}}^{(i)}} {=} 0$.
After obtaining the generalized errors as pointed out in \eqref{GE_continuousLevel_initialModel}, RUS clusters those errors using GNG to learn two GDBN models encoding the dynamic rules of how the RF signal and the GPS signal evolve with time, respectively, as showed in Fig.~\ref{fig_GNG_of_receivedRFsignalandTrajectory} and Fig.~\ref{fig_graphicalRep_transitionMatrices}. RSU can couple the two GDBNs by learning the interactive transition matrix that is encoded in a C-GDBN as shown in Fig.~\ref{fig_interactiveMatrices}.
\begin{figure}[t!]
    \begin{center}
        \begin{minipage}[b]{.49\linewidth}
            \centering
                \includegraphics[width=4.5cm]{Results/graphTransition_Trajectory_veh1}
            \\[-1.5mm]
            {\scriptsize (a)}
        \end{minipage}
        \begin{minipage}[b]{.49\linewidth}
            \centering
                \includegraphics[width=4.5cm]{Results/graphTransition_Trajectory_veh2}
            \\[-1.5mm]
            {\scriptsize (b)}
        \end{minipage}
        \begin{minipage}[b]{0.49\linewidth}
            \centering
            \includegraphics[width=4.5cm]{Results/graphTransition_RFsignal_veh1}
            \\[-1.5mm]
            {\scriptsize (c)}
        \end{minipage}
        \begin{minipage}[b]{0.49\linewidth}
            \centering
            \includegraphics[width=4.5cm]{Results/graphTransition_RFsignal_veh2}
            \\[-1.5mm]
            {\scriptsize (d)}
        \end{minipage}
        \caption{Graphical representation of the transition matrices (TM): (a) TM related to the trajectory of vehicle 1, (b) TM related to the trajectory of vehicle 2, (c) TM related to the RF signal received from vehicle 1, (d) TM related to the RF signal received from vehicle 2.}
        \label{fig_graphicalRep_transitionMatrices}
    \end{center}
\end{figure}
\begin{figure}[t!]
    \begin{center}
        \begin{minipage}[b]{.49\linewidth}
            \centering
                \includegraphics[width=3.8cm]{Results/interactiveMatrix_RFtoGPS_Neu5_veh1}
            \\[-1.0mm]
            {\scriptsize (a)}
        \end{minipage}
        \begin{minipage}[b]{0.49\linewidth}
            \centering
            \includegraphics[width=3.8cm]{Results/interactiveMatrix_RFtoGPS_Neu25_veh1}
            \\[-1.0mm]
            {\scriptsize (d)}
        \end{minipage}
       
        \caption{Interactive transition matrix defined in \eqref{interactiveTM_fromRFtoGPS} using different configurations: (a) $\mathrm{M_{1}}=5$, $\mathrm{M_{2}}=5$, (b) $\mathrm{M_{1}}=25$, $\mathrm{M_{2}}=25$.}
        \label{fig_interactiveMatrices}
    \end{center}
\end{figure}
\begin{figure}[t!]
    \begin{center}
        \begin{minipage}[b]{.49\linewidth}
        \centering
            \includegraphics[width=4.9cm]{Results/RF_situation1_best_veh1}
        \\[-1.5mm]
        {\scriptsize (a)}
        \end{minipage}
        \begin{minipage}[b]{0.49\linewidth}
            \centering
            \includegraphics[width=4.9cm]{Results/RF_situation1_worst_veh1}
            \\[-1.5mm]
            {\scriptsize (b)}
        \end{minipage}
        \begin{minipage}[b]{0.49\linewidth}
            \centering
            \includegraphics[width=4.9cm]{Results/RF_situation1_best_veh2}
            \\[-1.5mm]
            {\scriptsize (c)}
        \end{minipage}
        \begin{minipage}[b]{0.49\linewidth}
            \centering
            \includegraphics[width=4.9cm]{Results/RF_situation1_worst_veh2}
            \\[-1.5mm]
            {\scriptsize (d)}
        \end{minipage}
        \caption{An example visualizing the predicted and observed RF signals transmitted by the 2 vehicles using different configurations. Predicted RF signal from: (a) vehicle 1 using $\mathrm{M_{1}}{=}5$, $\mathrm{M_{2}}{=}5$, (b) vehicle 1 using $\mathrm{M_{1}}{=}25$, $\mathrm{M_{2}}{=}25$, (c) vehicle 2 using $\mathrm{M_{1}}{=}5$, $\mathrm{M_{2}}{=}5$, (d) vehicle 2 using $\mathrm{M_{1}}{=}25$, $\mathrm{M_{2}}{=}25$.}
            \label{fig_situation1_PredictedRF}
    \end{center}
\end{figure}

\begin{figure}[t!]
    \begin{center}
        \begin{minipage}[b]{.49\linewidth}
        \centering
            \includegraphics[width=4.8cm]{Results/GPSfromRF_situation1_best}
        \\[-1.0mm]
        {\scriptsize (a)}
        \end{minipage}
        \begin{minipage}[b]{0.49\linewidth}
            \centering
            \includegraphics[width=4.8cm]{Results/GPSfromRF_situation1_worst}
            \\[-1.0mm]
            {\scriptsize (b)}
        \end{minipage}
        %
        \caption{An example visualizing the predicted and observed trajectories of two vehicles interacting in the environment. (a) $\mathrm{M_{1}}{=}5$, $\mathrm{M_{2}}{=}5$, (b) $\mathrm{M_{1}}{=}25$, $\mathrm{M_{2}}{=}25$.}
            \label{fig_situation1_VehiclesTrajectories}
    \end{center}
\end{figure}

\begin{figure}[ht!]
    \begin{center}
        \begin{minipage}[b]{.49\linewidth}
        \centering
            \includegraphics[width=4.8cm]{Results/rmse_on_trajectory}
        \\[-1.0mm]
        {\scriptsize (a)}
        \end{minipage}
        \begin{minipage}[b]{0.49\linewidth}
            \centering
            \includegraphics[width=4.8cm]{Results/rmse_on_RFSignal}
            \\[-1.0mm]
            {\scriptsize (b)}
        \end{minipage}
        \caption{The average RMSE after testing different experiences and examples of: (a) trajectories and (b) RF signals.}
            \label{fig_rmse_onTraj_onSig}
    \end{center}
\end{figure}

Fig.~\ref{fig_situation1_PredictedRF} illustrates an example comparing between predicted RF signals and observed ones based on two different configurations in learning the interactive matrix (as shown in Fig.~\ref{fig_interactiveMatrices}). Also, Fig.~\ref{fig_situation1_VehiclesTrajectories} illustrates an example comparing between the predicted and observed trajectories of the two vehicles using the two interactive matrices depicted in Fig.~\ref{fig_interactiveMatrices}. From Fig.~\ref{fig_situation1_PredictedRF} and Fig.~\ref{fig_situation1_VehiclesTrajectories} we can see that using an interactive matrix with less clusters allows to perform better predictions compared to that with more clusters. This can be validated by observing Fig.~\ref{fig_rmse_onTraj_onSig} that illustrates the RMSE values versus different number of clusters related to the two models representing the dynamics of the received RF signals and the vehicles' trajectories. It can be seen that as the number of clusters increases the RMSE error increases, since adding more clusters decreases the firing probability that explains the possibility to be in one of the $M_{2}$ clusters of the second model conditioned in being in a certain cluster of the first model.

Fig.~\ref{fig_exNormal_Spoofed_JammedTrajectories} illustrates an example of vehicle's trajectory under normal situation (i.e., jammer and spoofer are absent), under jamming attacks and under spoofing attacks. Also the figure shows the predicted trajectory which should follow the same dynamic rules learned during a normal situation. After that, we implemented the IM-MJPF on the learned C-GDBN to perform multiple predictions, i.e., to predict the RF signal that the RSU is expecting to receive from a certain vehicle and the corresponding trajectory that the vehicle is supposed to follow. IM-MJPF through the comparison between multiple predictions and observations, produces multiple abnormality signals as defined in \eqref{eq_CLA1} and \eqref{eq_CLA2} which are used to detect the jammer and the spoofer.

Fig.~\ref{fig_abnormalitySignals_JammerSpoofer} illustrates the multiple abnormality signals related to the example shown in Fig.~\ref{fig_exNormal_Spoofed_JammedTrajectories}. We can observe that the abnormal signals related to both RF signal (Fig.~\ref{fig_abnormalitySignals_JammerSpoofer}-(a)) and trajectory (Fig.~\ref{fig_abnormalitySignals_JammerSpoofer}-(b)) are below the threshold under normal situations. This proves that RSU learned the correct dynamic rules of how RF signals and trajectories evolve when the jammer and spoofer are absent (i.e., under normal situations). Also, we can see that the RSU can notice a high deviation on both the RF signal and the corresponding trajectory due to a jamming interference from what it has learned so far by relying on the abnormality signals. In contrast, we can see that under spoofing attacks, RSU notice a deviation only on the trajectory and not on the RF signal since the spoofer has affected only the positions without manipulating the RF signal. In addition, it is obvious how the proposed method allows the RSU to identify the type of abnormality occurring and to explain the cause of the detected abnormality (i.e., understanding if it was because of a jammer attacking the V2I link or a spoofer attacking the satellite link).
\begin{figure}[t!]
    \centering
    \includegraphics[width=6.5cm]{Results/trajectories_underJamming_andSpoofing}
   
    \caption{Vehicle's trajectory under: normal situation, jamming and spoofing.}
    \label{fig_exNormal_Spoofed_JammedTrajectories}
\end{figure}
\begin{figure}[t!]
    \begin{center}
        \begin{minipage}[b]{.92\linewidth}
        \centering
            \includegraphics[height=2.6cm]{Results/abnSignal_onRF}
        \\[-1.5mm]
        {\scriptsize (a)}
        \end{minipage}
        \begin{minipage}[b]{.92\linewidth}
            \centering
            \includegraphics[height=2.6cm]{Results/abnSignal_onGPS}
            \\[-1.5mm]
            {\scriptsize (b)}
        \end{minipage}
        %
        \caption{Abnormality Signals related to the example shown in Fig.\ref{fig_exNormal_Spoofed_JammedTrajectories}: (a) abnormality indicators related to the RF signal, (b) abnormality indicators related to the trajectory.}
            \label{fig_abnormalitySignals_JammerSpoofer}
    \end{center}
\end{figure}
\begin{figure}[t!]
    \centering
    \includegraphics[height=3.2cm]{Results/Detection_Probability_RFfromGPS_versusPj}
    \caption{Detection probability ($\mathrm{P_{d}}$) versus jammer's power ($\mathrm{P_{J}}$) using different number of clusters $\mathrm{M}_{2}$.}
    \label{fig_jammerDetectionProb}
\end{figure}
\begin{figure}[t!]
    \centering
    \includegraphics[height=3.2cm]{Results/spoofingDetectionProbability_falseAlarm_versusM2}
    \caption{Spoofing detection probability ($\mathrm{P}_{d}^{s}$) and spoofing false alarm ($\mathrm{P}_{f}^{s}$) versus the number of clusters $\mathrm{M}_{2}$.}
    \label{fig_spooferDetectionProb}
\end{figure}

Fig.~\ref{fig_jammerDetectionProb} shows the overall performance of the proposed method in detecting the jammer by testing many situations and examples and by considering different jamming powers which ranges from $20$dBm to $40$dBm. It can be seen that the proposed method is able to detect the jammer with high probabilities (near $1$) and by considering low and high jamming powers. Also, the figure compares the performance in detecting the jammer by varying the number of clusters ($M_{2}$).
Fig.~\ref{fig_spooferDetectionProb} shows the overall performance of the proposed method in detecting the spoofer by testing different different examples of driving maneuvers. It can be seen that the RSU is able to detect the spoofer with high detection probability and null false alarm versus different number of clusters.

\section{Conclusion}
A joint detection method of GPS spoofing and jamming attacks is proposed. The method is based on learning a dynamic interactive model encoding the cross-correlation between the received RF signals from multiple vehicles and their corresponding trajectories. Simulation results show the high effectiveness of the proposed approach in jointly detecting the GPS spoofer and jammer attacks. 
Subsequent work will extend the system model to consider more than two vehicles with different channel conditions and various modulation schemes to evaluate the effectiveness of the proposed method.

\bibliographystyle{IEEEtran}
",['The generative interactive model used in the method is called the Coupled Generalized Dynamic Bayesian Network (C-GDBN).'],4482,multifieldqa_en,en,,2d0e8d88c8dcd187eb0590fb072f6a69bd774c9aa029246b,The generative interactive model used in the method is called the Coupled Generalized Dynamic Bayesian Network (C-GDBN).,120
What was the conclusion of the study?,"consumption influences mercury: Topics by WorldWideScience.org
Sample records for consumption influences mercury
Epidemiologic confirmation that fruit consumption influences mercury exposure in riparian communities in the Brazilian Amazon
Sousa Passos, Carlos Jose; Mergler, Donna; Fillion, Myriam; Lemire, Melanie; Mertens, Frederic; Guimaraes, Jean Remy Davee; Philibert, Aline
Since deforestation has recently been associated with increased mercury load in the Amazon, the problem of mercury exposure is now much more widespread than initially thought. A previous exploratory study suggested that fruit consumption may reduce mercury exposure. The objectives of the study were to determine the effects of fruit consumption on the relation between fish consumption and bioindicators of mercury (Hg) exposure in Amazonian fish-eating communities. A cross-sectional dietary survey based on a 7-day recall of fish and fruit consumption frequency was conducted within 13 riparian communities from the Tapajos River, Brazilian Amazon. Hair samples were collected from 449 persons, and blood samples were collected from a subset of 225, for total and inorganic mercury determination by atomic absorption spectrometry. On average, participants consumed 6.6 fish meals/week and ate 11 fruits/week. The average blood Hg (BHg) was 57.1Â±36.3 Î¼g/L (median: 55.1 Î¼g/L), and the average hair-Hg (HHg) was 16.8Â±10.3 Î¼g/g (median: 15.7 Î¼g/g). There was a positive relation between fish consumption and BHg (r=0.48; P 2 =36.0%) and HHg levels (fish: Î²=1.2, P 2 =21.0%). ANCOVA models showed that for the same number of fish meals, persons consuming fruits more frequently had significantly lower blood and HHg concentrations. For low fruit consumers, each fish meal contributed 9.8 Î¼g/L Hg increase in blood compared to only 3.3 Î¼g/L Hg increase for the high fruit consumers. In conclusion, fruit consumption may provide a protective effect for Hg exposure in Amazonian riparians. Prevention strategies that seek to maintain fish consumption while reducing Hg exposure in fish-eating communities should be pursued
Influence of mercury bioaccessibility on exposure assessment associated with consumption of cooked predatory fish in Spain.
Torres-Escribano, Silvia; Ruiz, Antonio; Barrios, Laura; VÃ©lez, Dinoraz; Montoro, Rosa
Predatory fish tend to accumulate high levels of mercury (Hg). Food safety assessment of these fish has been carried out on the raw product. However, the evaluation of the risk from Hg concentrations in raw fish might be modified if cooking and bioaccessibility (the contaminant fraction that solubilises from its matrix during gastrointestinal digestion and becomes available for intestinal absorption) were taken into account. Data on Hg bioaccessibility in raw predatory fish sold in Spain are scarce and no research on Hg bioaccessibility in cooked fish is available. The aim of the present study was to evaluate Hg bioaccessibility in various kinds of cooked predatory fish sold in Spain to estimate their health risk. Both Hg and bioaccessible Hg concentrations were analysed in raw and cooked fish (swordfish, tope shark, bonito and tuna). There were no changes in Hg concentrations during cooking. However, Hg bioaccessibility decreased significantly after cooking (42 Â± 26% in raw fish and 26 Â± 16% in cooked fish), thus reducing in swordfish and tope shark the Hg concentration to which the human organism would be exposed. In future, cooking and bioaccessibility should be considered in risk assessment of Hg concentrations in predatory fish. Copyright Â© 2011 Society of Chemical Industry.
Intake of mercury through fish consumption
Sarmani, S.B.; Kiprawi, A.Z.; Ismail, R.B.; Hassan, R.B.; Wood, A.K.; Rahman, S.A.
Fish has been known as a source of non-occupational mercury exposure to fish consuming population groups, and this is shown by the high hair mercury levels. In this study, hair samples collected from fishermen and their families, and commercial marine fishes were analyzed for mercury and methylmercury by neutron activation and gas chromatography. The results showed a correlation between hair mercury levels and fish consumption patterns. The levels of mercury found in this study were similar to those reported by other workers for fish consuming population groups worldwide. (author)
Fish consumption limit for mercury compounds
Abbas Esmaili-Sari
Full Text Available Background and objectives: Methyl mercury can carry out harmful effects on the reproductive, respiratory, and nervous system of human. Moreover, mercury is known as the most toxic heavy metal in nature. Fish and seafood consumption is the major MeHg exposure route for human. The present study tries to cover researches which have been conducted on mercury levels in 21 species of fish from Persian Gulf, Caspian Sea and Anzali Wetland during the past 6 years, and in addition to stating mercury level, it provides recommendations about the restriction of monthly fish consumption for each species separately. Material and methods: Fish samples were transferred to the laboratory and stored in refrigerator under -20oC until they were dissected. Afterwards, the muscle tissues were separated and dried. The dried samples were ground and changed into a homogenous powder and then the mercury concentration rate has been determined by advanced mercury analyzer, model 254. Results: In general, mercury contamination in fishes caught from Anzali Wetland was much more than fishes from Caspian Sea. Also, from among all studied fishes, oriental sole (Euryglossa orientalis, caught from Persian Gulf, allocated the most mercury level to itself with the rate of 5.61ml per kg., therefore, it exercises a severe consumption restriction for pregnant women and vulnerable groups. Conclusion: Based on the calculations, about 50% of fishes, mostly with short food chain, can be easily consumed during the year. However, with regard to Oriental sole (Euryglossa orientalis and shark (Carcharhinus dussumieri, caught from Persian Gulf, special consideration should be taken in their consumption. On the other hand, careful planning should be made for the high rate of fish consumption among fishing community.
Hair Mercury Concentrations and Fish Consumption Patterns in Florida Residents
Adam M. Schaefer
Full Text Available Mercury exposure through the consumption of fish and shellfish represents a significant public health concern in the United States. Recent research has demonstrated higher seafood consumption and subsequent increased risk of methylmercury exposure among subpopulations living in coastal areas. The identification of high concentrations of total mercury in blood and skin among resident Atlantic bottlenose dolphins (Tursiops truncatus in the Indian River Lagoon (IRL, a coastal estuary in Florida, alerted us to a potential public health hazard in the contiguous human population. Therefore, we analyzed hair mercury concentrations of residents living along the IRL and ascertained their sources and patterns of seafood consumption. The total mean mercury concentration for 135 residents was 1.53 Â± 1.89 Âµg/g. The concentration of hair mercury among males (2.02 Â± 2.38 Âµg/g was significantly higher than that for females (0.96 Â± 0.74 Âµg/g (p < 0.01. Log transformed hair mercury concentration was significantly associated with the frequency of total seafood consumption (p < 0.01. Individuals who reported consuming seafood once a day or more were 3.71 (95% CI 0.84â€“16.38 times more likely to have a total hair mercury concentration over 1.0 Âµg/g, which corresponds approximately to the U.S. EPA reference dose, compared to those who consumed seafood once a week or less. Hair mercury concentration was also significantly higher among individuals who obtained all or most of their seafood from local recreational sources (p < 0.01. The elevated human mercury concentrations mirror the elevated concentrations observed in resident dolphins in the same geographical region. The current study is one of the first to apply the concept of a sentinel animal to a contiguous human population.
Fish consumption and bioindicators of inorganic mercury exposure
Sousa Passos, Carlos Jose; Mergler, Donna; Lemire, Melanie; Fillion, Myriam; Guimaraes, Jean Remy Davee
Background: The direct and close relationship between fish consumption and blood and hair mercury (Hg) levels is well known, but the influence of fish consumption on inorganic mercury in blood (B-IHg) and in urine (U-Hg) is unclear. Objective: Examine the relationship between fish consumption, total, inorganic and organic blood Hg levels and urinary Hg concentration. Methods: A cross-sectional study was carried out on 171 persons from 7 riparian communities on the Tapajos River (Brazilian Amazon), with no history of inorganic Hg exposure from occupation or dental amalgams. During the rising water season in 2004, participants responded to a dietary survey, based on a seven-day recall of fish and fruit consumption frequency, and socio-demographic information was recorded. Blood and urine samples were collected. Total, organic and inorganic Hg in blood as well as U-Hg were determined by Atomic Absorption Spectrometry. Results: On average, participants consumed 7.4 fish meals/week and 8.8 fruits/week. Blood total Hg averaged 38.6 Â± 21.7 Î¼g/L, and the average percentage of B-IHg was 13.8%. Average organic Hg (MeHg) was 33.6 Â± 19.4 Î¼g/L, B-IHg was 5.0 Â± 2.6 Î¼g/L, while average U-Hg was 7.5 Â± 6.9 Î¼g/L, with 19.9% of participants presenting U-Hg levels above 10 Î¼g/L. B-IHg was highly significantly related to the number of meals of carnivorous fish, but no relation was observed with non-carnivorous fish; it was negatively related to fruit consumption, increased with age, was higher among those who were born in the Tapajos region, and varied with community. U-Hg was also significantly related to carnivorous but not non-carnivorous fish consumption, showed a tendency towards a negative relation with fruit consumption, was higher among men compared to women and higher among those born in the region. U-Hg was strongly related to I-Hg, blood methyl Hg (B-MeHg) and blood total Hg (B-THg). The Odds Ratio (OR) for U-Hg above 10 Î¼g/L for those who ate > 4 carnivorous fish
Methyl mercury exposure in Swedish women with high fish consumption
Bjoernberg, Karolin Ask [Division of Metals and Health, Institute of Environmental Medicine, Karolinska Institutet, Box 210, SE-171 77, Stockholm (Sweden); Vahter, Marie [Division of Metals and Health, Institute of Environmental Medicine, Karolinska Institutet, Box 210, SE-171 77, Stockholm (Sweden); Grawe, Kierstin Petersson [Toxicology Division, National Food Administration, Box 622, SE-751 26 Uppsala (Sweden); Berglund, Marika [Division of Metals and Health, Institute of Environmental Medicine, Karolinska Institutet, Box 210, SE-171 77, Stockholm (Sweden)]. E-mail: Marika.Berglund@imm.ki.se
We studied the exposure to methyl mercury (MeHg) in 127 Swedish women of childbearing age with high consumption of various types of fish, using total mercury (T-Hg) in hair and MeHg in blood as biomarkers. Fish consumption was assessed using a food frequency questionnaire (FFQ), including detailed information about consumption of different fish species, reflecting average intake during 1 year. We also determined inorganic mercury (I-Hg) in blood, and selenium (Se) in serum. The average total fish consumption, as reported in the food frequency questionnaire, was approximately 4 times/week (range 1.6-19 times/week). Fish species potentially high in MeHg, included in the Swedish dietary advisories, was consumed by 79% of the women. About 10% consumed such species more than once a week, i.e., more than what is recommended. Other fish species potentially high in MeHg, not included in the Swedish dietary advisories, was consumed by 54% of the women. Eleven percent never consumed fish species potentially high in MeHg. T-Hg in hair (median 0.70 mg/kg; range 0.08-6.6 mg/kg) was associated with MeHg in blood (median 1.7 {mu}g/L; range 0.30-14 {mu}g/L; r {sub s}=0.78; p<0.001). Hair T-Hg, blood MeHg and serum Se (median 70 {mu}g/L; range 46-154 {mu}g/L) increased with increasing total fish consumption (r {sub s}=0.32; p<0.001, r {sub s}=0.37; p<0.001 and r {sub s}=0.35; p=0.002, respectively). I-Hg in blood (median 0.24 {mu}g/L; range 0.01-1.6 {mu}g/L) increased with increasing number of dental amalgam fillings. We found no statistical significant associations between the various mercury species measured and the Se concentration in serum. Hair mercury levels exceeded the levels corresponding to the EPA reference dose (RfD) of 0.1 {mu}g MeHg/kg b.w. per day in 20% of the women. Thus, there seems to be no margin of safety for neurodevelopmental effects in fetus, for women with high fish consumption unless they decrease their intake of certain fish species.
Bjoernberg, Karolin Ask; Vahter, Marie; Grawe, Kierstin Petersson; Berglund, Marika
We studied the exposure to methyl mercury (MeHg) in 127 Swedish women of childbearing age with high consumption of various types of fish, using total mercury (T-Hg) in hair and MeHg in blood as biomarkers. Fish consumption was assessed using a food frequency questionnaire (FFQ), including detailed information about consumption of different fish species, reflecting average intake during 1 year. We also determined inorganic mercury (I-Hg) in blood, and selenium (Se) in serum. The average total fish consumption, as reported in the food frequency questionnaire, was approximately 4 times/week (range 1.6-19 times/week). Fish species potentially high in MeHg, included in the Swedish dietary advisories, was consumed by 79% of the women. About 10% consumed such species more than once a week, i.e., more than what is recommended. Other fish species potentially high in MeHg, not included in the Swedish dietary advisories, was consumed by 54% of the women. Eleven percent never consumed fish species potentially high in MeHg. T-Hg in hair (median 0.70 mg/kg; range 0.08-6.6 mg/kg) was associated with MeHg in blood (median 1.7 Î¼g/L; range 0.30-14 Î¼g/L; r s =0.78; p s =0.32; p s =0.37; p s =0.35; p=0.002, respectively). I-Hg in blood (median 0.24 Î¼g/L; range 0.01-1.6 Î¼g/L) increased with increasing number of dental amalgam fillings. We found no statistical significant associations between the various mercury species measured and the Se concentration in serum. Hair mercury levels exceeded the levels corresponding to the EPA reference dose (RfD) of 0.1 Î¼g MeHg/kg b.w. per day in 20% of the women. Thus, there seems to be no margin of safety for neurodevelopmental effects in fetus, for women with high fish consumption unless they decrease their intake of certain fish species
Fish Consumption and Mercury Exposure among Louisiana Recreational Anglers
Lincoln, Rebecca A; Shine, James P; Chesney, Edward J
Background: Methylmercury (MeHg) exposure assessments among average fish consumers in the U.S. may underestimate exposures among U.S. subpopulations with high intakes of regionally specific fish. Objectives: We examined relationships between fish consumption, estimated mercury (Hg) intake......, and measured Hg exposure among one such potentially highly-exposed group, recreational anglers in Louisiana USA. Methods: We surveyed 534 anglers in 2006 using interviews at boat launches and fishing tournaments combined with an internet-based survey method. Hair samples from 402 of these anglers were...... collected and analyzed for total Hg. Questionnaires provided information on species-specific fish consumption over 3 months prior to the survey. Results: Anglers' median hair-Hg concentration was 0.81 Âµg/g (n=398; range: 0.02-10.7 Âµg/g), with 40% of participants above 1 Âµg/g, the level that approximately...
Umbilical cord blood and placental mercury, selenium and selenoprotein expression in relation to maternal fish consumption
Gilman, Christy L.; Soon, Reni; Sauvage, Lynnae; Ralston, Nicholas V.C.; Berry, Marla J.
Seafood is an important source of nutrients for fetal neurodevelopment. Most individuals are exposed to the toxic element mercury through seafood. Due to the neurotoxic effects of mercury, United States government agencies recommend no more than 340 g (12 oz) per week of seafood consumption during pregnancy. However, recent studies have shown that selenium, also abundant in seafood, can have protective effects against mercury toxicity. In this study, we analyzed mercury and selenium levels an...
Factors that negatively influence consumption of traditionally ...
Factors that negatively influence consumption of traditionally fermented milk ... in various countries of sub-Saharan Africa and a number of health benefits to human ... influence consumption of Mursik, a traditionally fermented milk product fromÂ ...
Mercury exposure as a function of fish consumption in two Asian communities in coastal Virginia, USA.
Xu, Xiaoyu; Newman, Michael C
Fish consumption and associated mercury exposure were explored for two Asian-dominated church communities in coastal Virginia and compared with that of two non-Asian church communities. Seafood-consumption rates for the Chinese (36.9Â g/person/day) and Vietnamese (52.7Â g/person/day) church communities were greater than the general United States fish-consumption rate (12.8Â g/person/day). Correspondingly, hair mercury concentrations for people from the Chinese (0.52Â Âµg/g) and the Vietnamese church (1.46Â Âµg/g) were greater than the overall level for United States women (0.20Â Âµg/g) but lower than the published World Health Organization exposure threshold (14Â Âµg/g). A conventional regression model indicated a positive relationship between seafood consumption rates and hair mercury concentrations suggesting the importance of mercury exposure through seafood consumption. The annual-average daily methylmercury intake rate for the studied communities calculated by Monte Carlo simulations followed the sequence: Vietnamese communityÂ >Â Chinese communityÂ >Â non-Asian communities. Regardless, their daily methylmercury intake rates were all lower than the United States Environmental Protection Agency reference dose of 0.1Â Âµg/kg body weight-day. In conclusion, fish-consumption patterns differed among communities, which resulted in different levels of mercury exposure. The greater seafood and mercury ingestion rates of studied Asian groups compared with non-Asian groups suggest the need for specific seafood consumption advice for ethnic communities in the United States. Otherwise the health benefits from fish consumption could be perceived as trivial compared with the ill-defined risk of mercury exposure.
Feather growth influences blood mercury level of young songbirds.
Condon, Anne M; Cristol, Daniel A
Dynamics of mercury in feathers and blood of free-living songbirds is poorly understood. Nestling eastern bluebirds (Sialia sialis) living along the mercury-contaminated South River (Virginia, USA) had blood mercury levels an order of magnitude lower than their parents (nestling: 0.09 +/- 0.06 mg/kg [mean +/- standard deviation], n = 156; adult: 1.21 +/- 0.57 mg/kg, n = 86). To test whether this low blood mercury was the result of mercury sequestration in rapidly growing feathers, we repeatedly sampled free-living juveniles throughout the period of feather growth and molt. Mean blood mercury concentrations increased to 0.52 +/- 0.36 mg/kg (n = 44) after the completion of feather growth. Some individuals had reached adult blood mercury levels within three months of leaving the nest, but levels dropped to 0.20 +/- 0.09 mg/kg (n = 11) once the autumn molt had begun. Most studies of mercury contamination in juvenile birds have focused on recently hatched young with thousands of rapidly growing feathers. However, the highest risk period for mercury intoxication in young birds may be during the vulnerable period after fledging, when feathers no longer serve as a buffer against dietary mercury. We found that nestling blood mercury levels were not indicative of the extent of contamination because a large portion of the ingested mercury ended up in feathers. The present study demonstrates unequivocally that in songbirds blood mercury level is influenced strongly by the growth and molt of feathers.
High mercury seafood consumption associated with fatigue at specialty medical clinics on Long Island, NY
Shivam Kothari
Full Text Available We investigated the association between seafood consumption and symptoms related to potential mercury toxicity in patients presenting to specialty medical clinics at Stony Brook Medical Center on Long Island, New York. We surveyed 118 patients from Aprilâ€“August 2012 about their seafood consumption patterns, specifically how frequently they were eating each type of fish, to assess mercury exposure. We also asked about symptoms associated with mercury toxicity including depression, fatigue, balance difficulties, or tingling around the mouth. Of the 118 adults surveyed, 14 consumed high mercury seafood (tuna steak, marlin, swordfish, or shark at least weekly. This group was more likely to suffer from fatigue than other patients (pÂ =Â 0.02. Logistic regression confirmed this association of fatigue with frequent high mercury fish consumption in both unadjusted analysis (ORÂ =Â 5.53; 95% CI: 1.40â€“21.90 and analysis adjusted for age, race, sex, income, and clinic type (ORÂ =Â 7.89; 95% CI: 1.63â€“38.15. No associations were observed between fish intake and depression, balance difficulties, or tingling around the mouth. Findings suggest that fatigue may be associated with eating high mercury fish but sample size is small. Larg",['The conclusion was that fruit consumption may provide a protective effect for mercury exposure in Amazonian riparians.'],3247,multifieldqa_en_e,en,,ac42a5a54b029a05be6e6e13d9001a81761d7b40ad1edfe2,The conclusion was that fruit consumption may provide a protective effect for mercury exposure in Amazonian riparians.,118
"Besides the Boeing C-17, what other transport aircraft is the IAF considering for acquisition?","Transport Aircraft for IAF - Page 67 - Bharat Rakshak
Transport Aircraft for IAF
Re: Transport Aircraft for IAF
Postby abhik » 17 Nov 2014 05:55
+1, Air India recently sold their entire fleet of Boeing 777s.
afaik the A330 MRTT does not make any structural mods or add anything internally in cargo or passenger cabin. it just relies on the intrinsic 110 tons of fuel. external refueling pods are added and internally the control station and cameras for the operator i guess.
so its a easy conversion from a passenger layout to the AAR mode - mostly ripping out the passenger cabin of all extra stuff and retuning the FCS for any changes in COG.
this should have been pursued years ago
the IL78 adds a palletized drum tank system inside its cargo bay due to paucity of intrinsic fuel but it can be removed and a/c converted back to cargo hauling or send off to russia for Phalcon structural mods if we want it that way. they will however need to change engines to PS90 as they have the old engines
http://www.airplane-pictures.net/images ... 7/5616.jpg
the RAF is already gone that route in 2011
http://www.defensenews.com/article/2011 ... -Refuelers
LONDON - Airbus Military has delivered the first of 12 A330-200 airliners due to be converted into in-flight refueling planes for the British Royal Air Force by Cobham Aviation Services.
The aircraft, part of an order of 14 jets, will be modified with aerial refueling pods and other equipment at Cobham's newly refurbished facility in Bournemouth, England. The first two aircraft have already been converted by Airbus in Spain.
The multirole tanker aircraft are being provided to the RAF under a private finance initiative service deal led by Airbus parent EADS.
Seven of the planes will be operated full time by the RAF. The remainder will be available for lease in the third-party market, with the proviso that they can be returned to British military service to meet any surge in demand.
All of the aircraft, to be known as the Voyager in RAF service, will be fitted with two wing-mounted refueling pods, while half the fleet will also be fitted for, but not necessarily with, a center-line mounted unit. The refueling units are being supplied by Cobham.
The first aircraft will become operational in a passenger and freight transport role by the end of this year to start relieving pressure on the RAF's hard-pressed assets.
Despite the increasing fragility of current RAF in-flight refueling operations, the new capability is not contracted to start being used in this role until 2015.
All 14 Voyagers are scheduled to be available for RAF operations by the middle of the decade. The A330 will replace the increasingly ancient Tristar and VC-10 refuelers now in service.
Push the 6 Il-476 from refueler to AEW duty. Phalcon them up
Not sure if that is a good path to follow. For one they all should be sent to pasture in about 8 years. Then if the are to be phalconed up - the requires major structural changes. Not worth that cost.
Whatever happened ot the two new ones that were supposed ot be ordered?
the IL78 can be easily converted back to IL76 cargo hauling. only the fuel tank inside cargo bay needs removal...infact that was even mentioned in initial days as swing role fuel/cargo.
Postby Cybaru » 17 Nov 2014 07:55
I am talking about the new il78 that we ordered recently in refueling role. Sorry for the mix up. They are the same platform, that I why i used 476 or 76 to identify it.
777 carries more internal fuel than the A330. We suck!
From the KC-777 program.
http://www.globalsecurity.org/military/ ... kc-777.htm
""the KC-777 would be 209 feet long with a wingspan of 212 feet, 7 inches. That's the same size as the 777-200LR commercial jet. The KC-777 would be able to carry far more fuel, cargo and passengers than either the KC-767 or the Airbus A330 tanker. The KC-767 offers more operational flexibility, while the KC-777 would be better suited for long-range strategic missions in which more cargo needs to be delivered. The KC-777 would be able to carry more than 350,000 pounds (160,000 kilograms) of fuel and offload more than 220,000 pounds (100,000 kg) of it on a mission of 500 nautical miles (900 kilometers). On the other hand, the KC-767 can lift off with more than 200,000 pounds (90,000 kg) of fuel and offload more than 130,000 pounds (60,000 kg) in a similar mission. The KC-777 would be able to deliver 200 percent more fuel after flying 1,000 nautical miles than older Air Force KC-135s. The KC-777 could carry up to 37 pallets of cargo, compared to the 19 pallets for the KC-767.""
Postby Cosmo_R » 18 Nov 2014 04:31
Viv S wrote: From Ajai Shukla's article -
HAL points out that, since each Avro flies barely 350 hours every year, most of them have a residual life of about 80,000 hours. In a request for information (RFI) released on August 15, HAL has proposed replacing the aircraft’s engines (Rolls Royce Dart) with “modern fuel efficient engines”.
So, the IAF's Avros have a residual life of 228 years at the current rate of usage. Ain't life grand?
At zero up time, it could reach infinity.
Relax Cy. Kc777 has no client. Usaf is going with kc767 and almost everyone else with a330.
We don't have the number of heavies and long missions of usaf else I would say convert an124.
KC777 will be extremely expensive given the demand/backlog for the 777 and the 777x. Any buyer would have to virtually pay for the increase in capacity.
I think the 767 production line is closed. so the proposed KC767 Boeing is supposed to deliver 18 by 2017..that can be managed from mothballed and cargo hauler airframes on the market.
but to meet the final order of around 180 will they not have to open the production line unless such a huge number were available on the market?
I do get the spider feel this program again will be cancelled in favour of a in-production plane like the 777X ?
I wasn't suggesting we get the KC777. All I was doing was comparing what possibly the 777 could offload compared to A330. It carries 171000 liters of fuel versus 130000 liters that the A330 carries. If we had older 777s in stock, we could have quite easily converted them to this config. The cost to us would be miniscule just the refurbishing cost vs acquiring a new type.
Singha wrote: I think the 767 production line is closed. so the proposed KC767 Boeing is supposed to deliver 18 by 2017..that can be managed from mothballed and cargo hauler airframes on the market.
The Line is open, they have a backlog of around 50 (All Fed ex), with Fed Ex placing a small order this year. The Pegasus order is for all new builds, and so will the follow on order. The only reason for any nation to buy the 767 tanker is going to be because of the ability to hard bargain with Boeing given that the commercial future of the 767 is dead. This also allows a potential buyer to purchase cheap spares from the open market, or club its logistical and inventory purchase with that of the USAF. Other than that and perhaps availability (which would be doubtful once USAF pushes through a larger order) there is really no technical reason to purchase the this tanker over the A330 which by all accounts is a superior tanker in addition to being a much much better airliner in general.
IAI is doing conversations for the 767 and its called the 767 MMTT
http://www.iai.co.il/sip_storage/FILES/1/38471.pdf
Cybaru wrote: I wasn't suggesting we get the KC777. All I was doing was comparing what possibly the 777 could offload compared to A330. It carries 171000 liters of fuel versus 130000 liters that the A330 carries. If we had older 777s in stock, we could have quite easily converted them to this config. The cost to us would be miniscule just the refurbishing cost vs acquiring a new type.
The cost of converting a commercial airliner to a tanker, certifying it and running a full fledged test program is by no means small. There is absolutely no justification for that sort of cost over and above the capability that that A330 provides. If it were a certified and tested conversion, that would be a different matter.
Postby Kartik » 21 Nov 2014 12:27
Cybaru wrote:
Why? If the airframe can handle more flight hours, why not?
because it is a very very old airframe as is. Maintenance spares won't be available easily even as of now, then imagine how it'll be 20-30 years from now.. and as things stood anyway, the HS-748 offered very little in terms of payload and range versus a C-295 class aircraft. The C-295 offers a very credible light transport, whereas the HS-748's role in the IAF was more akin to a transport trainer and for communication duties with little operational use. Having seen a dozen or so HS-748s parked at Vadodara airport all through my childhood, I never once saw one in the air. They just seemed to be stored out in the open. Upon asking an IAF transport pilot who was my friend's father, he remarked ""zyaada kaam ke nahi hain yeh"".
Why would you expend more capital on what is essentially an obsolete airframe, even if theoretically it had not yet reached its service life? You'd have to re-engine it, put new avionics on board and even that wouldn't suffice for para dropping requirements..it was operationally never suitable for para dropping, which is an important mission for transport aircraft and had deficiencies in hot and high climes as well.
Unfortunately, the 748 was never meant to be a military transport. At the request of IAF, its door was enlarged to enable larger cargo items to be loaded and to allow para dropping without hitting the tail plane. However, to load a jeep in it, a 30-ft long ramp was required. The jeep would drive in and insert its front wheels into the aircraft. Then it had to be manually lifted and turned to get it in. Unloading it was just as difficult. Para dropping of troops or cargo even from the aircraft with the enlarged door was considered too dangerous with the risk of hitting the tail plane. The aircraft's performance at hot and high airfields was hopelessly inadequate. Eventually IAF acquired the tail-loading An-32s which were powered specifically for IAF's need for operating in the Himalayas.
BRF article -Avro in IAF service
Now unless you want to overcome all these through a costly, time consuming engineering re-design program, that too without access to original documents since this airplane was designed in the 1960s, there is no question of keeping them going for another 40 years. By which time the original design would be over 80 years old and with no one on earth but the IAF as an operator and HAL as the agency supporting it. Hardly a situation anyone would want.
abhik wrote: +1, Air India recently sold their entire fleet of Boeing 777s.
Only 5 of the Boeing 777-200LR, to Etihad Airways, which IMO was a bad decision..they could have reconfigured the airplanes with just 2 classes and continued to fly them to the US, non-stop.
The remaining 3 777-200LR were offered for lease but are still a part of AI's fleet since they didn't find any takers. This particular model hardly sold much and was developed for ultra-long range flights..it was the least successful 777 model and clearly AI goofed up on the configuration by going for these in place of the 300ER. The economics however didn't make too much sense for AI eventually.
there are 13 777-300ER as a part of their fleet ahd their economics is much better.
Govt. to decide tomorrow on whether to go ahead and allow the IAF to verify the technical details of the C-295 bid by Tata-Airbus instead of scrapping the tender due to single vendor situation.
The government will decide on Saturday whether to press ahead with the Rs 13,000 crore mega project for the private sector to supply 56 medium transport aircraft to the IAF despite only a single bidder, the Tata-Airbus consortium, being in the fray.
Though the defence acquisitions council (DAC) chaired by Manohar Parrikar will take the final decision, MoD sources on Tuesday said the ""emerging dominant view"" is that green signal should be given to the crucial project designed to promote Indian private sector's entry into the domestic aerospace arena with foreign collaboration.
""The Tata-Airbus technical and commercial bid is a credible offer submitted in a competitive environment. The other seven contenders backed out for one reason or the other,"" said a source.
IAF has now sought the clearance of the DAC -- the first such meeting to be chaired by Parrikar after becoming defence minister on November 10 -- to begin technical evaluation of the C-295 aircraft offered by Airbus Defence & Space and Tata Advanced Systems.
Though it has become a single-vendor situation, the DAC can approve it if it wants as per existing procurement procedures. Of the eight foreign aviation majors that got the global tender, American Boeing and Lockheed-Martin as well as Brazilian Embraer said they did not manufacture the class of aircraft being sought by IAF.
Refusing to take part in the tender, Russian Rosoboronexport said it wanted a fresh design and development project. Antonov of Ukraine wanted yet another extension of the bid submission deadline due to the ongoing conflict in Crimea. Swedish Saab said it had shut down its assembly line for such aircraft.
Then, Alenia Aermacchi was linked to Italian conglomerate Finmeccanica, which has been slapped with ""a partial ban"" after the infamous VVIP helicopter scandal. ""All this left only the European consortium Airbus. The DAC will have to take a call since re-tendering may lead to the same situation,"" said the source.
Incidentally, it was the Modi government's first DAC in July -- then headed by Arun Jaitley - which revived the Avro replacement project after it was put on hold by the UPA-2 regime last year due to strong opposition from the powerful PSU lobby and ministers like Praful Patel, as reported by TOI earlier.
Apart from the critical need to encourage the private sector to enter defence production in a big way, especially in the aerospace arena where Hindustan Aeronautics enjoys a monopoly, its felt the defence PSU's order books are already overflowing with projects.
Fingers crossed. Hopefully sense will prevail.
Why was lr got? Er is capable of Dubai to sfo nonstop.
Lr is overkill unless we want Delhi to Peru .
Singha wrote: Why was lr got? Er is capable of Dubai to sfo nonstop.
they wanted it for non-stop routes from India to the west coast of the US. But with fuel prices going higher and with the lower seat count on the 777-200LR, the seat mile costs grew too high. A 3 class configuration only made matters worse. A higher density configuration with more economy class seats and just 12-15 Business class seats would have been better perhaps, especially if they didn't have very high First Class load factors.
LR and ER is better if you want to have a better payload down below for long haul. Ultimately, the best bet is going to come form the 787's that take a fewer people (so you can do the longer routes) with still a competitive CASM, and the B and F class folks will pay good money for newer aircraft.
Postby Kartik » 04 Dec 2014 12:55
Lets see if there is any forward movement on the stalled MTA project once Putin arrives in New Delhi
Major defence deals to be signed during Putin-Modi summit
In this connection, it is expected that during the summit, Russia and India may ultimately resolve several long-delayed agreements on military-technical cooperation projects between the two countries and sign them finally for their implementation. These agreements, above all, include joint Fifth Generation Fighter Aircraft (FGFA) project and joint development of Multi-role Transport Aircraft (MTA).
A final deal on FGFA for production has been delayed because the Indian Air Force (IAF) did not approve the design and work-share. Now Russia has reportedly agreed that the jet would be a two-seat design, not a one-seater. India’s work-share would also be increased from18 percent to 25 percent, and even up to 40-50 percent in the near future, in view of the steady development of the Indian aviation industry.
Defence and SecurityAccording to the agreement, India’s stealth air-to-air missile “Astra” along with Indo-Russian BrahMos supersonic cruise missile will be mounted on the FGFA.
The preliminary design agreement on FGFA had been signed in 2010 between Indian HAL and Russian Sukhoi Design Bureau to build the jet for the use by both countries. The final design contract was to be signed in July-August 2012. But the deadline has already passed. According to the Indian media reports, under the programme, India is expected to build 200 fighter jets at the cost of $30 billion.
FGFA is not the only Indo-Russia joint project. The two countries also signed an agreement on the joint development of MTA in 2007, based on Il-214 Russian plane. The cost of the $600 million project is being equally shared by the two countries. The MTA, when developed, will have ready market for 205 aircraft - 45 for the Indian Air Force, 100 for the Russian Air Force, and 60 more for exporting to friendly countries. The international market for MTA is estimated at 390 planes. Under the agreement, thirty percent of the annual production of planes could be exported to third countries.
The MTA was expected to go in service with the Russian and Indian Air Forces in 2015. But the project faced a number of problems, delaying the development of the MTA. The project got into rough weather after India felt there was nothing much for Indian engineers and scientists to do in the design and development of the MTA.
However, all the issues related to the project were resolved with the Russians when the HAL undertook to carry out design and development of its work-share of MTA at Aircraft R&D Centre at Bangalore. Russian Ilyushin Design Bureau and the Irkut Corporation and HAL are participating in the project. The first flight is expected to take place in 2017-18.
The MTA would replace the AN- 32 aircraft being used by the IAF. It will be used for both cargo and troop transportation, para-drop and air drop of supplies, including low-altitude parachute extraction system.
BrahMos missile exports a challenging proposition
Another key deal expected to be signed during the summit, is for the development of “BrahMos mini missile” by the Indo-Russian joint venture BrahMos Aerospace which manufactures supersonic cruise missile. BrahMos’ new CEO Sudhir Mishra recently said he was hopeful that a deal to develop the mini version of the missile will be signed during Putin’s summit with Modi.
“We are hoping to sign a tripartite agreement between DRDO, NPOM lab and BrahMos Aerospace during the planned visit of Russian President in December,” Mishra said.
He said that the new missile will have a speed of 3.5 mach and carry a payload of 300 km up to a range of 290 km. In size, it will be about half of the present missile, which is around 10 metres long. The missile can be integrated with different platforms, including submarines and FGFA. It is planned to be inducted into service by 2017.
Modi-Abbott to upgrade defence ties
A new dimension:
In a first, India and Australia will also set up a mechanism to discuss “synergies in integrating defence system”, including research and development cooperation on integrating defence equipment that both countries currently purchase, for example, U.S’s C-17 Globemaster III, according to officials.
^^That report about MTA is fishy. First it says that India has nothing to learn from an existing design (duh) and then says the issue has been resolved. How? Next it says India's need is 45 planes to replace over 100 An-32s. It also speculates about the export potential which may be nonexistent unless we sell it for peanuts.
This is a scam which only aims to create screwdriver jobs at HAL, stall any attempt to introduce private players into the aviation market and continue the Russian gravy train. My fear is the Russkies have our testiments in a firm grip with key components of Brahmos, nuke subs, Su30mki etc and we may be jerked around.
(They need to be more definitive about ""MTA"" - Multirole vs. Medium)
The Indians had not selected an engine (among other things) for the MTA with the Russians. Perhaps that has been resolved now.
On export numbers, IIRC, it was the responsibility of Rosoboronexport. ?????
Kartik wrote: The MTA would replace the AN- 32 aircraft being used by the IAF. It will be used for both cargo and troop transportation, para-drop and air drop of supplies, including low-altitude parachute extraction system.
Pardon my ignorance. The Avro and An-32 have different upgrade paths. How are the replacements for these venerable aircraft different in terms of use cases in IAF. Cannot one platform replace both these types? (Either MTA or C-295)
In this case, I feel they should have just gone with screwdrivergiri (production tech) and got to market first. There is no jet-powered transporter in this range! Just license produce the IL-214 with the PD-14M, glass cockpit and a state-of-the-art COTS avionics computer.
In my view, it was a low hanging fruit, which they completely messed up! They could have learnt on how to adopt the plane for the 160-200 seater.
indranilroy wrote: They could have learnt on how to adopt the plane for the 160-200 seater.
Yes, the MTA project should fold the Avro, An-32 and the regional transport role and become a conversion project rather a development one. The driving numbers will come from the regional transport (thousands in India itself) rather than the Avro or medium transport roles (max 300 between them). This changes the ball game and introduces all kinds of possibilities. But I'm pretty sure that the Il-214/MTA is not the way to go because it will take a decade or more to arrive. A good possibility was another Antonov, the An-148 but it has some mechanical glitches apparently besides being bogged down in the Ukraine mess. Maybe the Russians can ""relocate"" the aircraft to Russia? The other possibility is the BAe-146 which is ironically another Avro. We should remember that both the HS-748 ""Avro"" and An-32 were regional airliners that were converted to military use, not the other way around. HAL or a private firm will pick up a lot of experience in the conversion process itself.
The Sukhoi Superjet is already in production/orders,with over 100+ for Russian and intl. customers. It is ideal for regional transport,perfect for flights to smaller Tier-2/3 cities from metros. If we really want a regional jet this is the fastest way toi go,we can set up a manufacturing unti here for the same at an HAL unit.
Postby shaun » 05 Dec 2014 15:24
Its an international projects, with components outsourced from different international vendors . Over 30 foreign partnership companies are involved in the project and partly financed by Italy.
Sukhoi is good for passenger use but wont be suitable for military, rough field use. The shoulder wing jets like the An-148 have slower speeds and better ground clearance. The Bae-146 was usedby Druk Air in Bhutan so it should do OK in the ALGs. If we don't fold our requirements then we should go with something like the Superjet which we will at least be able to make in India and also modify to stretched versions. Unless we have a clear path to operational clearance within 10 yrs for the RTA project vetted by our top industrial houses, it is pie-in-the-sky and should be dropped. The RTA will be big enough to keep 2-3 factories humming and leapfrog our capabilities. If we don't get our act together almost immediately, we will miss the boat, just like our trainer fiascos.
I don't think Superjet fits into our scheme of things. We should think as a country and see to it that our programs don't trample on each other.
First, the more certain ones:
1. Mahindras NM5 and Airvans can care of the low-cost but sturdy 5,8,10 and 18-seater section.
2. Saras had such great potential for being the high performance 14-18 seater. But I have almost given up on it. This section will most probably be taken up by the Tata-built Do-228 NG.
3. We should standardize the C-295 as the Avro/An-32 replacement and create a 70-80 seater variant out of it.
And then the more wishful ones:
1. If the RTA is going to be a jet, then make it a 100-130 seater. I don't expect the first prototype to take the sky before 2025. I feel it is too big of a jump where we don't even have a base. With LCA, at least we were at least license producing other fighters.
4. Building on the IL-214, the MTA was on a more sure footing. But, I can't see how the first prototype can to take to the sky before 2019(more than 10 years since MTAL was formed)! If the transport plane materializes, then one can imagine making a civilian 150-200 seater version of the same. But this program needs a push. Will Putin's visit be able to galvanize this into the next symbol of Indo-Russian cooperation. Probably not!
Postby GeorgeWelch » 12 Dec 2014 23:39
http://www.ctvnews.ca/canada/defence-de ... -1.2144472
The Defence Department intends to purchase a Boeing C-17 Globemaster III, a large military transport plane that comes with a price tag of just under $200 million, CTV News has learned
It's difficult to get a good count, but by some sources, if this and the 4 Australia planes go through, there will only be 5 left.
X-Posting from FGFA thread.
Despite Putin’s visit, two pacts on military aircraft still in doldrums
President Vladimir Putin may have come and gone but stalemate largely persists over two key long-pending India-Russian defence projects, the fifth-generation fighter aircraft (FGFA) and military multirole transport aircraft (MTA).
The deadlock over the MTA, which were initially envisaged to gradually replace IAF's ageing fleet of the medium-lift AN-32 aircraft, seems to be much more serious. India now wants to ascertain the cost viability of the twin-engine transport aircraft in comparison to similar planes available in the market.
There are also questions about the MTA's ""predicted timelines for delivery"" as well as its failure to meet the high-altitude requirements, which need to be answered before India even thinks of inking the full-scale contract for the project, said sources.
Postby Gyan » 13 Dec 2014 12:29
indranilroy wrote: I don't think Superjet fits into our scheme of things. We should think as a country and see to it that our programs don't trample on each other.
1. Mahindras NM5 and Airvans can care of the low-cost but sturdy 5,8,10 and 18-seater section. Righto
2. Saras had such great potential for being the high performance 14-18 seater. But I have almost given up on it. This section will most probably be taken up by the Tata-built Do-228 NG. We need future extended variants of presurrized aircraft like 30 seater Saras and say 30 seater unpressurized Do-328 NG.
3. We should standardize the C-295 as the Avro/An-32 replacement and create a Civilian turboprop pressurized cabin 70-80 seater variant out of it.
1. If the RTA is going to be a jet, then make it a 100-130 seater. Agreeeeeed I don't expect the first prototype to take the sky before 2025. I feel it is too big of a jump where we don't even have a base. With LCA, at least we were at least license producing other fighters. Though I think that we should participate in Russian MS-21 and also the wide body follow on.
4. Building on the IL-214, the MTA was on a more sure footing. But, I can't see how the first prototype can to take to the sky before 2019(more than 10 years since MTAL was formed)! If the transport plane materializes, then one can imagine making a civilian 150-200 seater version of the same. Though I think that we should participate in Russian MS-21 and also the wide body follow on. But this program needs a push. Will Putin's visit be able to galvanize this into the next symbol of Indo-Russian cooperation. Probably not!
Absence of any specifics on Sukhoi Superjet, MS-21, Wide body aircraft, Mi-38, MRTA, FGFA, even after Putin visit is very disappointing.
FlightGlobal- Boeing sitting on 8 unsold C-17s
By: Dan ParsonsWashington DCSource: Flightglobal.com
This story is sourced from Flightglobal.com 12 hours agoBoeing has sold two more C-17 transports to an undisclosed customer, but it will likely end the year with eight unsold white tails.
There are 10 Boeing C-17 airlifters in various stages of assembly at the company’s Long Beach, California, production facility.
Two of the aircraft are spoken for by an unnamed customer, Boeing says. Boeing is trying to sell off the other eight white tails, which will be the last produced before the factory is shuttered sometime in the summer of 2015.
The 279th – and final – C-17 fuselage will be mated to its wings in January or February, programme spokeswoman Tiffany Pitts tells Flightglobal. The operation is California’s last remaining aircraft production line and the lone widebody military aircraft production line in the USA, according to Boeing.
At least two countries – Australia and Canada – have publicly announced an intention to purchase a C-17, though neither factor into Boeing’s future planning, Pitts says. Until contracts are finalised, the number available remains eight, she says. The Royal Canadian Air Force already has four C-17As, according to Flightglobal’s World Air Forces 2014 directory.
Canadian news outlets reported earlier in December that the air force would buy one C-17 with money left over at the end of 2015.
Australia is further along with its bid to purchase C-17s. The US Defense Security Cooperation Agency in November announced Australia was approved to buy up to four C-17s and support equipment for $1.6 billion.
Boeing has plans to store any unsold C-17s following closure of its production line, Pitts says.
“I’m hoping they all will be sold before then, but we’ve had plans in place for a very long time to store and maintain the aircraft if that doesn’t happen,” she says.
the IAF will need to factor in the demand vs availability of C-17s and stock up with a follow-on order quickly. The initial plan to have 16 C-17s may not fructify, considering that there are just 8 left now, with Australia having announced plans to buy 4 more.
why are they closing the line if it has demands ???
Real estate sales tactics probably. Buy now last 8 3bhk flats Saar.
krishnan wrote: why are they closing the line if it has demands ???
It requires 3 years lead time to order raw materials/parts from all of its sub-vendors. All current firm orders have been fulfilled, and no new orders have come. Anticipating a need for a few more aircrafts, they produced 10 extra (self-funded) units before production winded down. Bottom line is they don't make money keeping an idle plant around with all its employees and infrastructure. At most what they will likely do is keep a limited infrastructure around for a few more years in case a bunch of new orders come. They can then see if it makes business sense to re-open the plant.
Postby Aditya_V » 17 Dec 2014 12:19
Wish this can be brought to the notice of Journos/ Poster when slamming LCA/ Arjun and other indigenous projects. If there are no orders there will be no efficiency.
Dec 10, 2014 :: Russia launches Il-76MDM upgrade programme
Russia's Ilyushin has started to upgrade a first Russian Air Force (VVS) Ilyushin Il-76MD 'Candid' military transport aircraft to Il-76MDM standard, company officials have told IHS Jane's . The main features of the upgrade include refurbished engines and upgraded avionics.
The modernisation is being conducted at the VVS's Military Transport Aviation (MTA) maintenance facility based at the Ilyushin division in Zhukovsky city near Moscow.
A senior Ilyushin official told IHS Jane's that the upgrade of the first aircraft will be finished in 18 months. Subsequent aircraft will take less time to complete the process, however. When the modernisation is finished the initial Il-76MDM will undergo state trials. The upgrade process for subsequent aircraft will begin when the trials programme is completed.
IHS Jane's was previously told by a VVS senior official that the modernisation of 41 MTA Il-76MDs is planned by 2020. While the Il-76MDM upgrade retains the old D-30KP engine (compared with the PS-90A engine equipping the new Il-76MD-90A/Il-476), the modernisation effort should match the aircraft's onboard electronics with those of the newbuild Il-76MD-90A. This and other efforts mean the cost of modernising the Il-76MD to Il-76MDM is only a third of that of a newbuild Il-76MD-90A.
The existing D-30KP engines are to be enhanced to increase their service life. The overall aircraft's service life will be extended by 15 years.
The upgrade works are planned to be conducted in an aviation repair factory or in the MTA's aircraft maintenance facility. As a result, the Ulyanovsk-based Aviastar-SP plant, which is building the Il-76MD-90A, is not involved in the Il-76MD to Il-76MDM modernisation programme.
Users browsing this forum: Jaeger, Manish_Sharma, rajkumar, VikramA and 43 guests",['The IAF is considering the acquisition of the Airbus A330 MRTT (Multi-Role Tanker Transport) besides the Boeing C-17.'],5660,multifieldqa_en_e,en,,8d72c6709ee1da38a65cdfb4f9d90e7348fd8e356c0a8165,The IAF is considering the acquisition of the Airbus A330 MRTT (Multi-Role Tanker Transport) besides the Boeing C-17.,117
"According to the text, what is Toby Schindelbeck's observation about the police?","July | 2012 | Chico Taxpayers Association
Keep a Knockin’ but you can’t come in! Come back next Tuesday night and try it again! And be sure to bring plenty of your friends.
Toby Schindelbeck has finally been rewarded for his persistence – he’s been going before Chico City Council, asking that Finance MisDirector Jennifer Hennessy comply with city code and give a budget report at every meeting. City clerk Debbie Presson has informed him that this subject will be “discussed” at the August 7 council meeting.
But we know, it won’t be a very good “discussion” unless a bunch of people come in and demand some action. Toby has observed that issues like Corporate Personhood and the “single-use” plastic bag ban have drawn fairly small crowds – he estimates 25 – 30 people, and I’d say he’s being generous. The city has acted on these issues, with only that small fraction of the population in support. So, Toby believes there needs to be an even stronger presence to get a decent discussion on this matter, and I agree.
Like Toby and Stephanie Taber and others have been saying, the city code calls for a monthly budget report, with sticky details like receipts, etc, and Jennifer Hennessy admits she has not made such a report in the seven years she’s been with the city of Chico. Try not paying your taxes for seven years – you’ll get the same treatment as the man from Touch of Class Florist – 68 years old, and he’s being sent to PRISON. But Jennifer Hennessy and her boss Dave Burkland, and their overseer, Mayor Ann Schwab, get to flog the law right in front of everybody, and Ann just steps right into that little red convertible and drives off to her palatial estate in Forest Ranch.
The law is a piece of paper. It takes people to demand law enforcement. We’ve got a serious law enforcement problem in our town. The police say they aren’t paid enough to enforce the laws in the streets, and now Dave Burkland says, he just doesn’t have to.
And your mayor won’t make him either. He’s retiring, on more than $150,000 a year, for the rest of his life, but she’s up for election in November – time to take out the trash.
That meeting is scheduled for August 7, the usual time, the usual place. I’ll keep you posted.
Tags: Ann Schwab Chico CA, Ann Schwab for city council, Dave Burkand Chico Ca, Friends of Ann Schwab, Jennifer Hennessy Chico Ca
Stephanie Taber answers Quentin Colgan’s letter to the News and Review
I get complaints from friends and strangers, and it has also been my own experience, that the editor of the Chico News and Review is not always objective in deciding which letters received from the public will be printed in the paper and which ones won’t. Robert Speer has offered me excuses, but I have always found him to be disingenuous. For example – he told me he would only run letters that referenced an article or letter recently printed in the paper – untrue a million times over. He also told me he wouldn’t print letters that had already run in the Enterprise Record – also untrue a million times over. The man has his own reasons for running or not running letters.
David Little is more objective, but he’s got his faults too – once he threw out a letter from my husband and later admitted he had thought I’d written it and used my old man’s name. He just threw it out without even calling the phone number or e-mailing, just assumed I’d do something like that when I’d never done anything like that before, because he was mad at me over a snit we were having at the time.
I think Little gets his nose out at people personally, and Hell hath no fury, know what I mean? With Speer it can personal but I think it’s most often political. Suffice to say, they both carry what my dad used to call a “Shit List,” and if you’re on it, you don’t get ink in their rag.
Of course either paper is equally likely to print a total wad of lies or misinformation without so much as a google fact check. I will never forget the time Dave Little printed a letter saying the cops had been called to my house on a dog complaint. The letter writer insinuated that this was why I often wrote letters complaining about the cop contracts. I called Little and told him the letter was false, nothing like that had ever happened – but he wouldn’t retract it. I had to look the old man up in the phone book and call him myself, tell him he had been misinformed, and ask him to write a retraction. He apologized profusely and the apology was in the paper within three days. He wouldn’t tell me where he got the information, but later I found out he was a member of VIPS, and he still is. I think that’s something Dave Little could have looked into before he printed a story like that about me and my family, not to mention my dogs, but he didn’t see it that way. Poor journalism, is how I see it, and that’s what I’ve come to expect out of both the daily and the weekly.
So, pardon me if I was not surprised when my friend Stephanie mentioned to me that she didn’t think Speer would run her response to a letter from Quentin Colgan, regarding our current fiscal morass. QC made an argument he has been swinging around town lately – that Fire Station 5 had to be closed recently because the Tea Party forced the city to have a $150,000 election over Measure A.
The first problem I have with this argument is, the city is out a heck of a lot more than $150,000. The second problem I have is, I happen to know that over 8,000 Chicoans signed that petition, and there’s not more than 600 active members of the Tea Party. I also know the Tea Party didn’t sponsor the petition drive, nor were they the only people that marched out with those petitions. Colgan’s argument doesn’t make sense to me, but it’s amazing what kind of “facts” the general populace will believe if you just keep repeating them.
Some folks are trying to use the Tea Party as a target to rile up their peanut gallery, using Measure A as their rally call. They keep banging the same old drum. They refuse to have a rational discussion about the situation we’re facing, because it’s going to mean some sour beans for them and their trough-dwelling friends.
So, it’s up to a rational person like Stephanie Taber to lay it out straight for those who like facts. Stephanie attends the meetings, she reads the reports, she goes to the trouble of putting questions in writing for $taff, and then waiting persistently for an answer that practically has to be deciphered by a lawyer. She has followed this budget conversation since the day then-city-manager and first rat to jump, Greg Jones, expressed his grave concerns that we were headed straight for bankruptcy. She has followed the figures and checked the facts until she has forced these rats right to the wall – they have lately begun to dig their feet in and refuse to obey the sunshine laws, refusing to give the fiscal reports demanded by the city charter. Some people can try to run their little smokescreen of repetitive nonsense, but more rational people are finding out the truth. Thanks to Stephanie Taber for writing this letter below, which may or may not run in the Chico News and Review:
I’d like to take this opportunity to respond to Quentin Colgan’s letter of July 12th; primarily because the costs surrounding the Special Election held regarding Measure A have been distorted. Yes, it did cost $150,000, but why? That’s the elephant in the room. The progressives on the City Council chose the method by which the election would be held. Per the City Charter (which is the City’s Constitution) Section 501 clearly states “The City Council may determine that any Special Election shall be held by mailed ballot” etc. That would have cut the cost by half, at least. But the Council chose the most expensive means possible, voting at the precinct. They were afraid that just telling the students they were being disenfranchised, which was an obvious lie, would not be sufficient to defeat it.
As to “it’s all the Tea Party’s fault”; I was the only signature to the Measure. I felt no need to consult the Tea Party before I took that action; but did enlist the help of many concerned citizens to gather the more than 8,000 signature required to put it on the ballot.
Toby Schindelbeck has called upon our Finance Director to adhere to Section 908 of the City’s Charter which states “(the) Finance Director shall submit to the Council through the City Manager monthly statements of receipts, disbursements and balances in such form as to show the exact financial condition of the City”. It does not state when you may want to or if you have time to; it says “shall”. No one on the Council or otherwise can remember when that may have happened last. If it was being done as the Charter states it would have been recognize that the City was facing a financial Armageddon and steps could have been taken much earlier in the fiscal year to avoid the closing of Fire Station 5.
Tags: Ann Sc hwab Chico Ca, Ann Schwab for city council, Chico Enterprise Record, Chico News and Review, Chico Tea Party Patriots, City of Chico, David Little, Friends of Ann Schwab, Quentin Colgan, Robert Speer, Stephanie Taber
City Art Director Mary Gardner is foisting a new “Art Tax” on us to pay her own salary
To mgardner@ci.chico.ca.us, gerimahood@yahoo.com, mcbergarts@gmail.com
(Mary Gardner, city of Chico public arts director, city of Chico, Geraldine Mahood and Monica Berg of the Arts Commission)
I recently read your memo here
Chico-Arts-Building-Tax.pdf
I think it’s despicable Ms. Gardner that you are trying raise revenues for your own salary by foisting a new “Art Tax” on new development.
Ms. Mahood, Ms. Berg, nobody wants eggsuckers like you telling them how to spend their money or what’s “art”. You people make me sick.
The Chico Taxpayers Association will fight this grab, as will other civic groups through the area. That’s why you’ve kept your efforts “under the radar” I assume – you don’t want people to know about this, because you don’t want to hear what they think about it. Or YOU!
You people need to get real jobs and quit sucking off the public teat.
http://www.norcalblogs.com/adhoc/
Sincerely, Juanita Sumner, Chico CA
Tags: Ann Schwab Chico CA, Ann Schwab for city council, Chico Arts Commission, City of Chico ""Art Tax"", City of Chico Arts Policy Manual, Friends of Ann Schwab, Geraldine Mahood, Mary Gardner, Monica Berg
Jennifer Hennessy is incompetent – she can’t do her job and Burkland says she doesn’t have to
I’ll never forget my first real job – a clerical position at a manufacturing plant. I would compare it to the story of the miller’s daughter. On the first day, I was told that the employee I was to be replacing would stick around for a week to train me. At noon that day, having shown me where everything was and how to use the coffee maker, she got up from her chair, smiled, and told me she thought I could “handle it,” then left. At one o’clock, the plant manager came over to my desk followed by several “production” workers. They brought cart loads of microfilm, on rolls, in little white boxes. I was to label all of those boxes, three carts, piled high. This job had gotten held up, he explained, it would be “great!” if it could go out today. Did I think I could get them done by 4 o’clock? I wanted to make everybody happy, so said I yes without thinking, and set to work loading the labels into the typewriter.
It was a disaster. I had never typed anything like those labels before – typing class had been all about letters and envelopes, columns and reports. The labels skittered all over the platen, getting glue all over the inside of the typewriter. About every 50 or so labels, the platen had to be taken out and cleaned with alcohol. I typed and typed. By 3 o’clock I knew I was in trouble. The production workers had come over to my desk to help me affix the sticky labels. We were nervous, labels were getting screwed up. At 3:30 the office manager and receptionist came back to my desk to help with the labels. I typed and typed, and tried not to cry.
We didn’t make it. The plant manager was flustered. The salesman who’d promised the job was really pissed off, he said mean things. I apologized again and again, they told me it wasn’t all my fault, but could I please be more careful what I committed myself to in future. I could tell they also expected me to get a hell of a lot faster, but they were just trying to be nice.
So, I got faster. I came in early in the morning and worked through lunch until I got better at my job. I had signed up for a typing job, nobody had described all the weird stuff they expected me to type. It started with typing and labeling, not only sticky labels, but microfiche jackets. They have a little quarter inch tall label strip across the top that chips and peels if you aren’t careful loading them into the typewriter, and strips or frames of 35 and 16 mm film that falls out in your typewriter. Then there were the three-part work orders, with carbon paper, and the three-part shipping labels, also with carbon paper. There were the mistakes – whole orders that had been indexed incorrectly, and therefore typed incorrectly, and therefore had to be corrected and typed all over again. I won’t describe what I had to go through to correct microfiche labels, it was too stupid. I hated doing that, so I asked for my own little “eye-loup” – a little magnifier that you hold up to a light to look at the tiny little page numbers on the film – to make sure the cards had been indexed correctly before I typed them.
I’m not perfect, but I know I’m competent, cause I kept that job for five years while I watched others get fired, for everything from showing up late to breaking expensive equipment to stealing. I was given new jobs and increased responsibility as time went by. I got good job reviews from my supervisors, and good raises. Morale was high, we liked our co-workers and our managers, we felt like a team. Our customers were nice to us too. We worked for cities and counties, hospitals, banks – anybody who needed to keep records. We were trusted to handle confidential records, like people’s medical records. As we handled these confidential files we were simply told, “Don’t look at them,” so we didn’t.
I left in 1984 in finish school. Over the next decade computers killed the microfilm industry, and the company went out of business.
Excuse me if I compare my experiences in the private sector with stuff I’ve seen coming out of our city $taff. I keep waiting for some professional behavior, some professional accountability out of the people who run our town, and I start to wonder if I will ever get it. For a couple of months now, Toby Schindelbeck and Stephanie Taber, among others, have been asking council and Finance MisDirector Jennifer Hennessy to provide a simple accounting of city finances, as is required by the city charter, and she just plain refuses to give it. City Mangler Dave Burkland won’t make her.
Last month she actually admitted, she is UNABLE to do it. At the June 5 meeting she admitted that she is incompetent to follow the city charter. She said that when she came to her position seven years ago, she “struggled” with doing such a report – something every house wife does – and went whining to then-city-manager Tom Lando, who apparently patted her on the head and told her she didn’t have to do it anymore.
I don’t know about you guys, but I go over my check book every month, just to make sure everything is straight. I’ve found big, dumb mistakes, in the 100’s column even, that could have caused big, dumb problems down the road. I’m no math instructor, like Mary Goloff, but it’s not exactly rocket science – you just add your deposits and subtract your checks and withdrawals. I’ll admit, when my kids were little, I felt like I never had time to do that, and stuff would get screwed up. So now that I’ve got time, I make it a regularly scheduled event, and it’s amazing how much easier it is. And, I can keep the figures in my head, I know essentially how much I can afford to spend when I’m at the grocery store, or what kind of activities we can plan. My husband and son are enjoying a weekend trip right now that is already paid for, thankyouverymuch.
But Jennifer Hennessy is unable to do that? And she has expectable stuff – over 80 percent of her budget is payroll. She doesn’t have that many emergencies. The biggest emergency she’s had lately, is that the state has taken back the fund she’s been mis-using – the RDA. She was paying salaries and benefits out of a fund that’s supposed to be reserved for emergency public works projects. In other words, she’s been dipping into the till to pay her own salary!
The mayor is to blame here, she’s the captain of our ship. Unfortunately, like the captain of the Costa Concordia, she’s abandoned ship for a party onshore. While she and her college chums bully their bag ban down our throats, our ship is sinking. We have less than $200,000 in our reserve fund, we have un-secured pension obligations totaling in the millions and growing every day, and we have $taff who are using blackmail to get their way – they are just refusing to do their jobs. Hennessy won’t give the report she’s required to give because it’s BAD. I think the mayor is completely behind her on this – Ann Schwab doesn’t want us to hear that report either. Would you?
Please write a letter to council demanding that Hennessy do her job, or get out.
Tags: Ann Schwab Chico CA, Ann Schwab for city council, bankruptcy, City of Chico, Dave Burkland, embezzlement, Friends of Ann Schwab, Jennifer Hennessy, malfeasance
Scranton, Pennsylvania cuts workers to minimum wage – only $130,000 in their cash reserves
I finally got a chance to watch the video of last Tuesday’s council meeting. It cut on me during the meeting, just after Walker and Goloff were mopping up their attack on Sorensen, and I didn’t get it back til yesterday. I have watched the video in bits and snatches. I made it to the noise ordinance conversation last night, but had to turn it off after Jessica Allen and a couple of her friends got up to demand their rights to be bad neighbors.
One thing I learned is that the city of Chico has less than $200,000 in the reserve fund. No, I did not forget a zero on that figure, that’s it – less than $200,000. Read it and weep – and then call them to ask what they did with that property tax check you just sent in.
You can look at the budget report here: http://www.chico.ca.us/finance/budget.asp
You see the millions the city takes in, in sales tax (over $17 million) property tax (over $11 million), even taxes on your PG&E, phone and water (almost $7 million), and your visitors’ motel rooms (over $2 million). To me that seems petty – “bed tax”? Some people think it’s a good idea to shake down the visitors of your town, as if it’s not enough that they spend money on your motels, restaurants and shopping centers. It’s a common grab all over California, every city does it. A lot of distasteful things become “common” when no decent person stands up to say “enough is enough.”
In Chico, as has been oft repeated, over 80 percent of our budget is in salaries and benefits. That’s the elephant in the room, and everybody’s getting pretty hip deep in elephant shit around here. It’s a simple concept, no matter how convoluted $taff and council try to make it: if they spend all the money on salaries, benefits, and the Great Pension Stock Market Disaster, there’s no money left to pay for supplies to say, clean up leaks in the sewer and water lines that are causing the state to fine us by the day, widen the roads that we are required to widen because of the permitting of Meriam Park, etc. And you can just get used to those pot holes in the street out front of your house. Got bad neighbors? Get a lawyer.
What’s really frustrating are the reactions of the cops and fire – they act like they don’t get paid at all. Those guys take most of the 80 percent. They get overtime written into their schedules. According to Hennessy, both fire and the cops are over budget on their workman’s comp claims for at least the third year in a row. The city just slammed another cop contract past us without public review, and signed the new chief’s contract three days before it was made available to the public, and then only by request and a direct visit to the clerk’s office Downtown.
So, we will get another year of poor response times, bitching and moaning from cops and fire. Get ready for your homeowners and your car insurance to go up – the insurance companies know when your local police and fire departments are a pile of shit.
And don’t think I’m not wondering about all those suspicious house fires.
You can just forget about any of the services a city is supposed to offer. Try to get something out of the city clerk these days – if you can catch her in the office!
Well, here’s the story of Scranton, Pennsylvania – home of Michael Scott!
http://bottomline.msnbc.msn.com/_news/2012/07/10/12659748-scranton-pa-slashes-workers-pay-to-minimum-wage?lite
The mayor of Scranton, when faced with a situation similar to Chico’s mess, did what needed to be done. Unfortunately, he waited until it was too late to do something rational. I’m afraid it’s come to that with our city council – if you think that scene between Goloff and Sorensen was rational, well, you deserve to live here.
Tags: Ann Schwab for city council, Bob Evans for city council, Chico City council eletions 2012, cities declare bankruptcy, Friends of Ann Schwab, pensions, phone tax, salaries, sales tax increase
Marysville council rejects sales tax ploy by retiring city administrator – where’s Chico’s knight in shining armor?
I am not a member of the Chico Chamber of Commerce, but I check in to their website regularly to see what they’re up to. Sometimes I believe, they are the real Chico City Council. While our elected leaders frolic and cavort in their stupid committee meetings, the Chamber is working on a “Top 10 Economic Development Action List”.
Yeah, sounds great, until you consider, one of their “Top 10” is a proposal to raise the local sales tax.
One prominent member of the Chamber who might be able to fill us in on the discussion is Bob Evans. I’ve asked Bob where he stands on this tax increase, but he just keeps saying he hasn’t seen a proposal yet. Lately I have asked him if he would require Lando and the other sales tax increase proponents to get the legal number of signatures on a petition before he votes to put this proposal on the ballot, but he won’t answer me. His downright refusal to discuss the tax increase is frustrating to me – I want to believe Bob is a “fiscal conservative.” After all, he had some high and mighty things to say about his opposition to the phone tax. But, he knew the phone tax didn’t need his support to get on the ballot. It’s easy to posture as the good guy when you know others will achieve the end result you really want. Evans’ resistance to making a pledge against a sales tax increase is screaming in my ear like a fire alarm.
In Marysville, Mayor Bill Harris had no trouble making himself clear when his city mangler proposed a half-cent sales tax increase: “This will be viewed as the City Council coming to them wanting more money again.”
Well, the article mentioned, the city mangler is retiring, so I would also see it as his way of securing his f-ing pension, but nobody mentions that.
City councilwoman Christina Billeci echoed a sentiment I’ve been hearing increasingly in Chico – “We need to balance the budget with the revenues we have,” she said.
Other council members cited lack of support from citizens, including one councillor who claimed to have got “angry reactions” to the proposal. One council member said he might have supported the move before the June election, “But the cigarette tax was voted down, and that should have been a slam dunk,” he said. “I would see this as a waste of effort and money.”
The only council member who supported the notion, Head Start administrator Ricky Samayoa, made some pretty disparaging remarks about the town.
“There’s a lot of people that know there’s a lack of resources here for us to have a proper city and manage it,” he said. Oooo! A “proper city”! What a bitch! Does he have letters from constituents to support this statement, or is he just using “a lot of people” to describe himself and his co-workers? Not enough drive through coffee stands for you Ricky? Not enough 5 Star restaurants or pink boutiques? Sorry, we’ve never been ones for putting on the Ritz here in the North State, better get in your zip car and drive back to the Bay Area.
In the Enterprise Record story, Samoyoa further claimed that “continued cuts to maintenance and other aspects of the city’s budget hurt chances for an economic recovery.” I imagine Marysville has the same problem Chico has – too many $100,000+ salaries and not enough $20,000 – $50,000 workers. While he’s sitting down there under the air conditioner vent at Head Start in a fresh shirt and manicure, the streets are going unmaintained, the classrooms overcrowded, the police and fire departments underfunded – is that the problem Mr. Samayoa?
“The way we’re continuing to go, it’s just going to be a dying city, even if the economy picks up,” he said. Now, that statement doesn’t even make sense. This is a typical example of scare tactics. “The way we’re continuing to go…” You mean, paying $100,000+ salaries to fat bureaucrats, while cutting services to the public? Somehow I don’t think that’s what he’s talking about. ” …it’s just going to be a dying city…” Wow, what an idiot – obviously no knowledge of local history. Marysville has been through so many booms and busts, it ought to be called “Bouncyville.” If you get to know Marysville, you see it has everything needed to be a wonderful place to live, in good times and bad, regardless of carpetbaggers like Samayoa.
“Give folks the opportunity to have this debate,” Mr. Samayoa suggests. Sounds like the rhetoric coming from Andy Holcombe and the rest of the sales tax increase proponents. Hey, that’s a swell idea! People should talk about these things, hash them out. And then, if enough of them sign a petition to put such a proposal on a legal ballot, well, they can VOTE on it! But that costs alot of money – best for those who really believe in this cockamamie idea to get the petition first, show the need to spend all that money on an election. That’s what rational people would do, anyway.
But if you ask Holcombe to discuss the pending proposal, he denies there is any such thing. The only member of Chico City Council who is willing to discuss this proposal at all has been Mark Sorensen – thanks Mark. At least Mark has been good enough to answer our questions about the mechanics of such a proposal and getting it onto the ballot. Evans and Holcombe have both denied knowing anything about it, although Holcombe has made it good and clear he’d support raising the sales tax and Evans has been seen at Chamber discussions on the matter. The others have been mum to the public, but I’m guessing they will support it. Holcombe, Schwab, Goloff, Walker, Gruendl – and Evans? – are all banking on more revenues to rescue the city from the Shit Creek they’ve floated us up. Evans, while he will admit we’re in deep shit, will not offer so much as a suggestion of a paddle. He seems to be holding back until after he gets himself safely re-elected in November. Then he’s got a year to get that sales tax voted in and three years to make the public forget he had anything to do with it.
Well Bob, is that what you’re up to?
I’ll say, if he were at least honest, I might be able to hold my nose and support him, but this game he’s playing is a real turn-off.
Tags: Ann Schwab Chico CA, Ann Schwab for city council, Bob Evans Chico Ca, Bob Evans for city council, chico city council race 2012, city of Chico bankruptcy, city of Chico sales tax increase, Friends of Ann Schwab, Ricky Samayoa Marysville Ca
Council video feed still not available – $taff seems to have taken the Summer off!
I know, there’s probably a perfectly legitimate explanation for this. Debbie Presson isn’t sure why the feed is off, but she’s got somebody working on it. Not yesterday though, cause she was out of her office.
I’ll tell you what else is interesting – there haven’t been any of those morning meetings lately – in fact, it looks like all the committee meetings for July are CANCELLED. In fact, there hasn’t been an “Economic Development” committee meeting for months that I’m aware. For all intents and purposes, the city of Chico seems to be on Summer Vacation! How nice for them!
But, as you see, the town runs along without them. In fact, I’m wishing the public works department would also take a hike – they’re TOO BUSY right now, tearing up the streets Downtown. Oh well, the college students have “gone home” – what do we need Downtown for when the college students have gone home?
That seems to be the gist of if – the city of Chico is here to serve the college students. The rest of us can just get along – as long as we keep paying our taxes, nobody will bother us!
I just have to wonder, what are these $85,000, $95,000, $134,000 $taffers doing right now, and why do we need to keep paying them?
Tags: Ann Schwab Chico CA, Ann Schwab for city council, City of Chico, embezzlers, Friends of Ann Schwab, malfeasance
New police chief’s contract signed last Tuesday, made available to the public Friday – gotta love that “sunshine”!
Last Tuesday night we got a new police chief – Kirk Trostle. Only a month ago city manager Dave Burkland issued a statement – “police chief candidates not knockouts” according to the Enterprise Record. Trostle is a refugee from the Oroville police department, where, as chief, he certainly had his critics. He came to Chico only about a year and a half ago, from a department that was not without it’s problems. The council made their appointment without any elaboration – he was essentially the best thing they could come up with on short notice.
But shouldn’t we be able to negotiate a better contract with this man? Retiring Chief Porky Mike Maloney is getting over $165,000 a year, just in salary. He will be getting over $100,000 to retire, for the rest of his life, plus medical benefits. Frankly, I predict he’s carrying a colostomy bag within five years.
Have you seen Trostle’s contract? They signed it at council last Tuesday. But when we asked for it, they said we wouldn’t be able to look at it until Friday. I was invited to go down to the clerk’s office, at her convenience, 9 – 5, during MY WORK DAY, to look at a contract that had already been signed. Why in the hell would I want to do that? They don’t even offer you a decent cup of coffee.
So no, I haven’t seen it yet, but I’m guessing, it’s worse than Maloney’s contract. A fellow taxpayer went down Friday and reports he has the contracts, but has not given me any details. I don’t know if he had to pay for paper copies or what, but you can view it for free if you want to go down there. I’ll get back to you when I got something.
Tags: Ann Schwab Chico CA, Ann Schwab for city council, Chico Police Department, Chico Police Officers Association, City of Chico, Friends of Ann Schwab, Kirk Trostle chief of police chico ca, mike maloney retires at 50 what a pig
Mary Goloff and Jim Walker gang jump Mark Sorensen on the dais – just another lovely Chico city council meeting!
I’m sitting here in disbelief of the attack I just watched Mary Goloff and Jim Walker wage on Mark Sorensen at city council tonight. I couldn’t make the meeting, so I have been watching it via computer.
Sorensen had been challenged by a smarmy Jim Walker to list what changes he would make to balance the budget. Sorensen carefully began to explain that city funds had been depleted by millions over the last few years, with escalating costs leaving revenues in the dirt. He also explained that the lion’s share of our expenses are “operating costs,” meaning, salaries. He also carefully explained that there were programs we simply could not afford anymore, meaning, salaries.
Mary Goloff could be heard heckling him off microphone. If you or I did what she was doing we’d be asked to leave the room, possibly with police escort. But Mayor Schwab just sat there looking at Goloff, saying nothing. Goloff finally got on mike, interrupted Sorensen, and asked him to be specific. So, Sorensen offered housing, saying it had been a mistake to undertake so many housing projects, and he also specified the arts programs – such as the requirement that any capital project include one percent of the total cost of that project be added for art.
At this point Goloff began to interrupt Sorensen. She started heckling him about how “we all agree” that the arts are important, yadda, yadda. She just kept at Sorensen, not allowing him to answer any of her out-there questions, until Sorensen asked her to stop interrupting him.
After a quick exchange Walker butted in to attack Sorensen. Out of nowhere, Walker bashed Sorensen about wanting to spend more money on the police department, asking Sorensen where he would get the money to hire more police. This question was off base, Sorensen hadn’t even gotten that far before Goloff had completely derailed him.
Jim Walker is just sitting out his time, he seems to be enjoying himself at all of our expense. He, like so many “public servants,” seems to think he is elected to do what he wants, what seems like “the right thing” in his fairy tale mind, instead of carry out the law.
Mary Goloff seems to think she has been anointed Queen in some farcical aquatic ceremony to lead us all in the light of her cough syrup-induced wisdom. She seems to love the sound of her own voice, while here at my house, it sets off the hounds for blocks.
My computer started failing at this point, and I was unable to watch the rest of the meeting. I am going on vacation tomorrow, I’ll see you folks on the flip flop.
Tags: Ann Schwab Chico CA, Ann Schwab for city council, Friends of Ann Schwab
Turn that S*** UP!
We had a lively discussion down at the library yesterday about how we are going to fight the phone tax increase in November.
The key here is to inform the public. $taff has already done their best to make this measure confusing and deceptive, actually writing into the measure that it will lower taxes. They mean, they are lowering the rate half a cent, but of course, this half-cent will be an ice cube in hell when they apply the tax to all the new stuff this measure allows – starting with cell phones, texting, paging, and adding whatever new technology comes along. All the voter needs to know is, this measure will raise his/her taxes, noticeably.
Even people on welfare will pay this tax, even though they qualify for the rate-assistance plans offered by the phone companies – utility tax is based on the total bill, before the adjustment for the rate assistance. And, this tax includes those prepaid phone cards.
The hardest hit will be commercial customers. A friend of mine who owns a little manufacturing business in town tells me the city of Chico thinks all business owners are “rich sugar daddies”.
My friend always tells me, that while I am in these meetings Downtown, he is in Oroville or Redding or Modesto or some other town, dealing with his business. He says these towns have better, more workable $taff. He is among the business owners who have used the word “hostile” to describe Dave Burkland, and the city business climate in general.
We have to get the word out to people like my friend that NOW IS THE TIME to get involved. I like that band, Rage Against the Machine – they say, “it has to start somewhere, it has to start sometime. What better place than here, what better time than NOW!”
We’re fighting the city, which will use public money to fund this tax increase initiative. For example, they have already used $taff time to research and write the measure, and now council members and $taff will create the “for” argument to be placed on the ballot. Our city attorney makes over $190,000 a year in salary alone – Mark Sorensen figured the cost of an hour of her time, but I forget the figure. More than most people make in a day, is all I remember.
The city will turn over their arguments in favor in August – at that point we can take this dog and pony show on the road. Until then, let’s keep working. Thanks all!
","[""Toby Schindelbeck's observation is that the police say they aren't paid enough to enforce the laws in the streets.""]",6599,multifieldqa_en_e,en,,3a63a9ca3248cecdeef9f43282a5162ff154b0bcdcf4ba81,Toby Schindelbeck's observation is that the police say they aren't paid enough to enforce the laws in the streets.,114
What are some reasons for the lack of data sharing in archaeobotany?,"Sowing the Seeds of Future Research: Data Sharing, Citation and Reuse in Archaeobotany
Reading: Sowing the Seeds of Future Research: Data Sharing, Citation and Reuse in Archaeobotany
University of Oxford, GB
Lisa is a post-doctoral research fellow at All Souls College, University of Oxford. Her publications include the co-authored volume The Rural Economy of Roman Britain (Britannia Monographs, 2017). Her research interests are focussed on agricultural practices in the later prehistoric and Roman period and the utilisation of archaeobotanical data to investigate human-plant relationships.
The practices of data sharing, data citation and data reuse are all crucial aspects of the reproducibility of archaeological research. This article builds on the small number of studies reviewing data sharing and citation practices in archaeology, focussing on the data-rich sub-discipline of archaeobotany. Archaeobotany is a sub-discipline built on the time-intensive collection of data on archaeological plant remains, in order to investigate crop choice, crop husbandry, diet, vegetation and a wide range of other past human-plant relationships. Within archaeobotany, the level and form of data sharing is currently unknown. This article first reviews the form of data shared and the method of data sharing in 239 articles across 16 journals which present primary plant macrofossil studies. Second, it assesses data-citation in meta-analysis studies in 107 articles across 20 journals. Third, it assesses data reuse practices in archaeobotany, before exploring how these research practices can be improved to benefit the rigour and reuse of archaeobotanical research.
Keywords: Archaeobotany, Data reuse, Data sharing, Open science
How to Cite: Lodwick, L., 2019. Sowing the Seeds of Future Research: Data Sharing, Citation and Reuse in Archaeobotany. Open Quaternary, 5(1), p.7. DOI: http://doi.org/10.5334/oq.62
Accepted on 29 May 2019 Submitted on 25 Mar 2019
Archaeology is a discipline built on the production and analysis of quantitative data pertaining to past human behaviour. As each archaeological deposit is a unique occurrence, ensuring that the data resulting from excavation and analysis are preserved and accessible is crucially important. Currently, there is a general perception of a low level of data sharing and reuse. Such a low level of data availability would prevent the assessment of research findings and the reuse of data in meta-analysis (Kansa & Kansa 2013; Moore & Richards 2015). As observed across scientific disciplines, there is a major problem in the reproduction of scientific findings, commonly known as the ‘replication crisis’ (Costello et al. 2013). A range of intersecting debates contribute to this, including access to academic findings (open access), open data, access to software and access to methodologies, which can be broadly grouped as open science practices. Without these, the way that scientific findings can be verified and built upon is impaired. Questions of reproducibility have been raised in recent years in archaeology, with considerations of a range of practices which can improve the reproducibility of findings, and a recent call for the application of open science principles to archaeology (Marwick et al. 2017). Discussion has so far focussed on access to grey literature (Evans 2015), data sharing (Atici et al. 2013), data citation practices (Marwick & Pilaar Birch 2018) and computational reproducibility (Marwick 2017), with a focus on lithics, zooarchaeological evidence, and archaeological site reports.
Quantitative assessments of current levels of data sharing, data citation and reuse remain limited in archaeology. The focus of evaluation has been on the uptake of large-scale digital archives for the preservation and dissemination of digital data, such as the Archaeology Data Service (ADS), utilised by developer-led and research projects, and recommended for use by many research funders in the UK (Richards 2002; Wright and Richards 2018). Much less focus has been paid to the data-sharing practices of individuals or small-groups of university-based researchers who may be disseminating their research largely through journal articles. Recent work on the availability of data on lithics assemblages found a low level of data sharing (Marwick & Pilaar Birch 2018) and there are perceptions of low levels of data reuse (Huggett 2018; Kintigh et al. 2018). Within zooarchaeology numerous studies have explored issues of data sharing and reuse (Kansa & Kansa 2013, 2014), and the sub-discipline is seen as one of the most advanced areas of archaeology in regards to open science (Cooper & Green 2016: 273). Beyond zooarchaeology, however, explicit discussion has remained limited.
This paper assesses data sharing and reuse practices in archaeology through the case study of archaeobotany – a long established sub-discipline within archaeology which has well-established principles of data recording. Archaeobotany is an interesting case study for data sharing in archaeology as it straddles the division of archaeology between scientific and more traditional techniques. Quantitative data on archaeological plant remains are also of interest to a range of other fields, including ecology, environmental studies, biology and earth sciences. The key issues of data sharing and data reuse (Atici et al. 2013) have been touched upon in archaeobotany over the past decade within broader discussions on data quality (Van der Veen, Livarda & Hill 2007; Van der Veen, Hill & Livarda 2013). These earlier studies focussed on the quality and availability of archaeobotanical data from developer-funded excavations in Britain and Cultural Resource Management in North America (Vanderwarker et al. 2016: 156). However, no discussion of data-sharing and reuse in academic archaeobotany occurred. A recent review of digital methods in archaeobotany is the notable exception, with discussions of the challenges and methods of data sharing (Warinner & d’Alpoim Guedes 2014).
Currently, we have no evidence for the levels of data sharing and reuse within archaeobotany. This article provides the first quantitative assessment of 1) data publication in recent archaeobotanical journal articles 2) data citation in recent archaeobotanical meta-analysis 3) the reuse of archaeobotanical datasets, in order to assess whether practices need to change and how such changes can take place.
2. Data Publication and Re-use Practices in Archaeobotany
2.1. History of data production and publication
Archaeobotanical data falls within the category of observational data in archaeology (Marwick & Pilaar Birch 2018). Archaeobotanical data is considered as the quantitative assessment of plant macrofossils present within a sample from a discrete archaeological context, which can include species identification, plant part, levels of identification (cf. – confer or “compares to”), and a range of quantification methods including count, minimum number of individuals, levels of abundance and weight (Popper 1988). Archaeobotanical data is usually entered into a two-way data table organised by sample number. Alongside the counts of individual taxa, other information is also necessary to interpret archaeobotanical data, including sample volume, flot volume, charcoal volume, flot weight, level of preservation, sample number, context number, feature number, feature type and period. Beyond taxonomic identifications, a range of other types of data are increasingly gathered on individual plant macrofossils (morphometric measurements, isotopic values, aDNA).
Archaeobotanical training places a strong emphasis on recording data on a sample-by-sample basis (Jacomet & Kreuz 1999: 138–139; Jones & Charles 2009; Pearsall 2016: 97–107). Time-consuming methodologies utilised in the pursuit of accurate sample-level data recording include sub-sampling and splitting samples into size fractions and counting a statistically useful number of items per sample (Van der Veen & Fieller 1982). The creation of sample-level data means analysis is often undertaken on the basis of individual samples, for instance the assessment of crop-processing stages and weed ecological evidence for crop husbandry practices. The analysis of sample level data also enables archaeobotanical finds to be integrated alongside contextual evidence from archaeological sites. Requirements for the publication of this data are in place in some archaeological guidelines, for instance current Historic England guidelines for archaeological practice in England (Campbell, Moffett & Straker 2011: 8).
From the earliest archaeobotanical reports, such as Reid’s work at Roman Silchester, the sample from which plant remains were recovered was noted (Lodwick 2017a), but often results were reported as a list of taxa, or long catalogues of detailed botanical descriptions with seed counts, such as Knörzer’s work at Neuss (Knörzer 1970). Early systematic archaeobotanical reports displayed data within in-text tables, for example Jones’s work at Ashville (Jones 1978) and the two-way data table has been the standard form of reporting archaeobotanical data ever since. Often data tables are presented within book chapters or appendices, but the financial, space and time constraints of book publishing are limiting. Furthermore, there is the perception that specialist data was not necessary for publication (Barker 2001). Hence, alternative methods of the dissemination of specialist archaeological data were pursued in the later twentieth century.
From the 1980s, archaeobotanical data tables were often consigned to microfiche following a Council for British Archaeology and Department of Environment report (Moore & Richards 2015: 31), with the example of the excavation of Roman Colchester where the contents of all archaeobotanical samples were available on microfiche (Murphy 1992). An alternative in the 2000s was providing data tables on CD Rom as seen, for instance, in the CD accompanying the study of a Roman farmstead in the Upper Thames Valley (Robinson 2007) or the One Poultry excavations in London (Hill and Rowsome 2011). Meanwhile, the inception of the Archaeology Data Service, a digital repository for heritage data, in 1996 meant archaeological datasets were increasingly digitally archived, for instance the data from the Channel Tunnel Rail Link Project (Foreman 2018) or a recent large-scale research excavation at Silchester (University of Reading 2018). In these cases, archaeobotanical data is available to download as a .csv file.
Whilst the data publication strategy of large excavations was shifting, the availability of data from post-excavation assessment reports has remained challenging. So-called ‘grey literature’ results from the initial evaluation stage of developer-funded investigations and accompanying post-excavation assessment often contain a semi-quantitative evaluation of archaeobotanical samples on a scale of abundance. Whilst paper reports were initially deposited with county Historic Environment Records, a process of digitisation focussing on the Roman period has meant many pdfs are now available through the ADS (Allen et al. 2018), whilst born-digital reports are now deposited through OASIS (Online AccesS to the Index of archaeological investigationS), as part of the reporting process (Evans 2015), althought the extent to which specialist appendices are included is variable.
These varying ‘publication’ strategies means archaeobotanical data is often available somewhere for recent developer-funded excavations and large-scale developer-funded excavations, even if much of this data is as a printed table or .pdf file (Evans 2015; Evans and Moore 2014). However, academic journals are typically perceived as the most high-status publication venue for archaeobotanical data, and a crucial publication venue for academics in order to comply with institutional requirements and the norms of career progression. Aside from the problem of access to pay-walled journals by those without institutional subscriptions to all journals, the publication of primary data alongside research articles faces various problems, from the outright lack of inclusion of data, to problematic curation of supplementary data and a lack of peer review of data (Costello et al. 2013; Warinner and d’Alpoim Guedes 2014: 155; Whitlock, 2011). The extent of these problems for archaeobotany is currently unknown. Given the growth in archaeobotanical data production as methodologies are introduced into many new regions and periods over the last decade, it is vital that we know whether the mass of new data being produced is made available and is being reused.
Recent important advances within archaeobotanical data sharing have focussed on the construction of the ARBODAT database, developed by Angela Kreuz at the Kommission für Archäologische Landesforschung in Hessen. The database is used by a range of researchers in Germany, the Czech Republic, France and England (Kreuz & Schäfer 2002). Data sharing enabled by the use of this database has facilitated research on Neolithic agriculture in Austria, Bulgaria and Germany (Kreuz et al. 2005), and Bronze Age agriculture in Europe (Stika and Heiss 2012). The use of this database makes data integration between specialists easier due to the shared data structure and metadata description, but often the primary archaeobotanical data is not made publicly available.
2.2. Meta-analysis in archaeobotany
Beyond the need to preserve information, a key reason for the formal sharing of archaeobotanical data is in its reuse to facilitate subsequent research. There has been a long-standing concern within archaeobotany with the need to aggregate datasets and identify temporal and spatial patterns. The palaeobotanist Clement Reid maintained his own database of Quaternary plant records in the late nineteenth century (Reid 1899), which formed the foundation of Godwin’s Quaternary database (Godwin 1975). Mid-twentieth century studies of prehistoric plant use compiled lists of archaeobotanical materials incorporating full references and the location of the archive (Jessen & Helbaek 1944). The International Work Group for Palaeoethnobotany was itself founded in 1968 in part with the aim to compile archaeobotanical data, first realised through the publication of Progress in Old World Palaeoethnobotany (Van Zeist, Wasylikowa & Behre 1991), and subsequently through the publication of annual lists of new records of cultivated plants (Kroll 1997).
To take England as an example, regional reviews produced by state heritage authorities have provided catalogues of archaeobotanical datasets in particular time periods and regions (e.g. Murphy 1998). When one archaeobotanist has undertaken the majority of study within a region, pieces of synthesis within books have provided a relatively comprehensive review, for instance in the Thames Valley, UK (Lambrick & Robinson 2009). Over the last decade regional synthesis has occurred within several funded reviews which produced catalogues of sites with archaeobotanical data (Lodwick 2014; McKerracher 2018; Parks 2012) and a series of funded projects in France have enabled regional synthesis (Lepetz & Zech-Matterne 2017). However, many of these reviews are not accompanied by an available underlying database, and draw upon reports which are themselves hard to access.
Through the 1990s and 2000s, a series of databases were constructed in order to collate data from sites in a particular region and facilitate synthetic research. However, these databases have all placed the role of data archiving onto later projects specifically funded to collate data, rather than sourcing datasets at the time of publication. Such a model is unsustainable, and is unlikely to result in all available datasets being compiled. The Archaeobotanical Computer Database (ABCD), published in 1996 in the first issue of Internet Archaeology, contained much of the archaeobotanical data from Britain available at the time of publication, largely at the level of individual samples. The database was compiled between 1989 and 1994 and is still accessible through the accompanying online journal publication (Tomlinson & Hall 1996). The ABCD made major contributions to recent reviews of the Roman and Medieval periods (Van der Veen, Livarda & Hill 2008; Van der Veen, Hill & Livarda 2013). However, the database could only be centrally updated, with the online resource remaining a static version, lacking much of the new data produced subsequent to the implementation of PPG16 in 1990. The ADEMNES database, created through a research project undertaken at the Universities of Freiburg and Tübingen, contains data from 533 eastern Mediterranean and Near Eastern sites (Riehl & Kümmel 2005). Kroll has maintained the Archaeobotanical Literature Database to accompany the Vegetation History and Archaeobotany articles (Kroll 2005) now accessible as a database (Kirleis & Schmültz 2018). Numerous other databases have collated archaeobotanical studies, including the COMPAG project (Fuller et al. 2015), the Cultural Evolution of Neolithic Europe project (Colledge 2016), RADAR in the Netherlands (van Haaster and Brinkkemper 1995), BRAIN Botanical Records of Archaeobotany Italian Network (Mercuri et al. 2015) and CZAD – Archaeobotanical database of Czech Republic (CZAD 2019).
The majority of databases have a restricted regional coverage, whilst research-project driven period-specific databases provide overlapping content. Whilst there are a wide range of archaeobotanical databases available, few contain primary datasets (other than the ABCD) which can be downloaded as .csv files. Data which is most commonly available are bibliographic references per site, with some indications of mode of preservation, quantity of archaeobotanical data, and sometimes taxa present. The databases do not inter-relate to each other, and function primarily as bibliographic sources enabling researchers to find comparative sites or to identify published datasets which need to be re-tabulated prior to meta-analysis. The IWGP website curates a list of resources, but otherwise the resources are often disseminated through the archaeobotany jiscmail list.
Beyond the aim of cataloguing archaeobotanical data within a region and period, meta-analysis is often used in archaeobotany to identify spatial and chronological trends in a range of past human activities, for instance crop choice, crop husbandry practices, plant food consumption, the trade in luxury foods or the use of plants in ritual. Meta-analysis can be undertaken on the basis of simple presence/absence data per site, but in order for such analysis to be rigorous and comparable, sample-level data must be utilised. For instance, sample-level data is required for meta-studies, in order to identify high-quality samples of unmixed crops for weed ecology analysis (Bogaard 2004), to assess the importance of context in the evaluation of wild plant foods (Wallace et al. 2019), or to use volumetric measurements as a proxy for scale (Lodwick 2017b). The reuse of archaeobotanical data also extends to include datasets used as “controls” in commonly used forms of statistical analysis, for instance Jones’s weed data from Amorgos, Greece, which is utilised as a control group in discriminant analysis of crop-processing stage (Jones 1984), and ethnographic observations of crop items in different crop-processing stages (Jones 1990).
2.3. Open data principles and solutions
Debates over issues of data publication and meta-analysis have been on-going across scientific disciplines over the last decade (Editors 2009), and have been summarised within principles of open science, as recently set out in relation to archaeology (Marwick et al. 2017). Open Data is one of the three core principles for promoting transparency in social science (Miguel et al. 2014). The FAIR principles, developed by representatives from academia, industry, funding agencies, industry and publishers, provide four principles which data sharing should meet for use by both humans and machines – Findability, Accessibility, Interoperability, and Reusability (Wilkinson et al. 2016). A recent report assessing the adoption and impact of FAIR principles across academia in the UK included archaeology as a case study (Allen and Hartland 2018: 46). It reported how the ADS was often used to archive data, but that “The journal itself provides the “story” about the data, the layer that describes what the data is, how it was collected and what the author thinks it means.” The report also raises the problem that smaller projects may not have the funding to utilise the ADS, meaning that other repositories are utilised. Increasingly, archaeological data is made available through a wide range of data repositories (OSF, Mendeley Data, Zenodo, Open Context), university data repositories (e.g. ORA-Data), or social networking sites for academics (Academia.edu, ResearchGate). More widely in archaeology, some have observed that archaeological data is rarely published (Kintigh et al. 2014), and recent reviews have reported low levels of data sharing (Huggett 2018; Marwick & Pilaar Birch 2018). A closely related issue is that of data reuse. Responsible reuse of primary data encourages the sharing of primary data (Atici et al. 2013), but levels of data reuse in archaeology are thought to remain low (Huggett 2018). Principles for responsible data citation in archaeology have recently been developed summarising how datasets should be cited (Marwick & Pilaar Birch 2018).
In order to assess the current status of data sharing, citation and data re-use in archaeobotany, a review was undertaken of the publication of primary data and the publication of meta-analysis in major archaeological journals over the last ten years, building on recent pilot studies within archaeology (Marwick & Pilaar Birch 2018). The review of academic journals provided a contrast to recent assessments of archaeobotanical data deriving from developer-funded archaeology (Lodwick 2017c; Van der Veen, Hill & Livarda 2013). Journal articles have been selected as the focus of this study as the provision of online supplementary materials in the majority of journals and the ability to insert hyperlinks to persistent identifiers (eg a DOI) to link to datasets available elsewhere should not limit the publication of data and references. Much archaeobotanical data is also published elsewhere, especially from projects not based in the university sector, that is commercial or community archaeology in the UK. Archaeobotanical datasets emanating from this research are more commonly published through monographs, county journal articles, and unpublished (or grey literature) reports, but these are beyond the scope of the current review.
All journal articles were included which represent the principle reporting of a new archaeobotanical assemblage. The selected journals fall within three groups. First, what is considered the specialist archaeobotanical journal (Vegetation History and Archaeobotany (VHA)). Second, archaeological science journals (Archaeological and Anthropological Sciences, Environmental Archaeology, The Holocene, Journal of Archaeological Science (JAS), Journal of Archaeological Science: Reports (JASR), Journal of Ethnobiology, Quaternary International, Journal of Wetland Archaeology), which can be considered as specialist sub-disciplinary journals which should be maintaining data-quality. Third, general archaeology journals (Antiquity, Journal of Field Archaeology, Oxford Journal of Archaeology, Journal of Anthropological Archaeology, Journal of World Prehistory). Finally, the broader cross-disciplinary journals PLoS One and Proceedings of the National Academy of Sciences (PNAS) were included. Published articles from the past ten years (2009–2018) have been analysed in order to assess the availability of plant macrofossil data. This ten-year period brackets the period where most archaeological journals have moved online and adopted supplementary materials.
Data citation in synthetic studies has been assessed in the same range of publications. The extent of data reuse ranges from the analysis of whole sample data to the presence/absence of individual crops. The location of a data citation has been assessed in the same range of publications, with the addition of journals where occasional research incorporating archaeobotanical data is featured (Britannia, Journal of Archaeological Research, Ethnobiology Letters, Medieval Archaeology, Proceedings of the Prehistoric Society, World Archaeology). The underlying dataset for the analysis is available in Lodwick 2019.
4.1. Primary data sharing
Here, the location of primary archaeobotanical data, that is sample level counts of macroscopic plant remains, was assessed for 239 journal articles across 16 journals (Lodwick 2019 Table 1). Figure 1 shows the results grouped by journal. Overall, only 56% of articles shared their primary data. In, Antiquity, JAS, JASR, PLOS One, Quaternary International and VHA, the highest proportion of publications did not include their primary data, that is to say that the sample-by-sample counts of plant macrofossils was not available. This level of data is comparable to the findings of other pilot studies in archaeology. Marwick and Pilaar Birch found a data sharing rate of 53% from 48 articles published in Journal of Archaeological Science in Feb – May 2017 (Marwick & Pilaar Birch 2018: 7), and confirm previous assertions that data is often withheld in archaeology (Kansa 2012: 499). This is better than some disciplines, with a 9% data sharing rate on publication found across high impact journal science publications (n = 500) (Alsheikh-Ali et al. 2011) and 13% in biology, chemistry, mathematics and physics (n = 4370) (Womack 2015), yet still indicates that nearly half of articles did not include primary data. Primary archaeobotanical data is more likely to be shared in archaeobotanical and archaeological science journals than general archaeology journals. However, within the primary archaeobotanical journal, VHA, 51% of articles do not include their primary data (Figure 1).
Chart showing the location of primary archaeobotanical data by journal in primary archaeobotanical data publications.
Where primary data was not shared, the data which was available ranged from summary statistics, typically counts or frequencies, reported either by site, site phase, or feature group. Figure 2 summarises these results by year, showing that there is a gradient within articles not sharing their full ‘raw’ data, from those only provided sample counts on one aspect of the archaeobotanical assemblage, to those only presenting data graphically or within discussion. Beyond full data, the most common form of data shared is either summary counts per site or summary counts per feature or phase. Whilst this data does enable some level of reuse, the results of any sample-level data analysis presented within an article cannot be verified, and the data cannot be reused for crop-processing or weed ecology analysis which requires sample level data. Furthermore, such data would have been collected on a sample-by-sample basis, but this information is lost from the resulting publication.
Chart showing the form of archaeobotanical data shared by year in primary archaeobotanical data publications.
The forms in which data are made available vary across journals. The sharing of primary data within an article remains the most common data sharing form in archaeobotany (Figure 1). Data tables in text require manual handling to extract data, in journals such as VHA, whilst in other journals in-text tables can be downloaded as .csv files. These however would not be citable as a separate dataset. Supplementary datasets are the third most common form of data sharing. Indeed, the use of electronic supplementary material has been advocated recently for by some journals, such as the Journal of Archaeological Science (Torrence, Martinón-Torres & Rehren 2015). Microsoft Excel spreadsheets are the most common form of supplementary data, followed by .pdfs and then word documents (Figure 1). Both .xlsx and .docx are proprietary file formats, and not recommended for long term archiving or open science principles. There is no indication of improvement over the last decade in the form of data sharing. In 2018, 50% of articles did not share their primary data, and where the data was shared, it was in proprietary forms (.docx, .xlsx) or those that do not easily facilitate data reuse (.pdf) (Figure 3).
Chart showing the location of archaeobotanical data from 2009–2018 in primary archaeobotanical data publications.
Just one of the articles included in this review incorporated a dataset archived in a repository (Farahani 2018), in contrast to the substantial growth in data repositories across academic disciplines (Marcial & Hemminger 2010). Other examples provide the underlying data for monograph publications, such as that of the archaeobotanical data from Gordion, Turkey (Marston 2017a, 2017b), Silchester, UK (Lodwick 2018; University of Reading 2018) and Vaihingen, Germany (Bogaard 2011a; Bogaard, 2011b).
Several of the journals that have been assessed have research data policies. In the case of Vegetation History and Archaeobotany, sufficient papers have been surveyed to assess the impact of the research data policy on the availability of data. Figure 4 show the proportion of data sharing formats through time just for VHA (note the small sample size). The introduction of a research data policy in 2016 encouraging data sharing in repositories has not resulted in any datasets being shared in that format. Of the 10 articles published in PLOS One after the introduction of a clear research data policy in 2014, 4 did not contain primary data. However, elsewhere, journals with no research data policy, such as Antiquity, has one of the lower levels of data sharing (Figure 1).
Chart showing the location of primary archaeobotanical data in Vegetation History and Archaeobotany.
There are various reasons for why a primary dataset may be lacking. The option of providing supplementary datasets has been available in many of the journals here since before the start of the surveyed period (e.g. Vegetation History and Archaeobotany in 2004), and so cannot be a reason for the absence of data publication in this journal while it may be a reason in other journals. Reasons suggested for a lack of data sharing within archaeology include technological limitations, and resistance amongst some archaeologists to making their data available due to cautions of exposing data to scrutiny, lost opportunities of analysis before others use it and loss of ‘capital’ of data (Moore & Richards 2015: 34–35). Furthermore, control over how data tables is presented (taxa ordering, summary data presented) may also contribute to the preferential publishing of data within journal articles. Another factor to consider is the emphasis on the creation of new data through archaeological research (Huvila 2016). The creation of a new archaeobotanical dataset through primary analysis is a key form of training in archaeobotany, and the perception of the value of the reuse of other previously published archaeobotanical journals may be low, hence not encouraging the sharing of well-documented datasets. Excellent exams of data reuse have resulted in influential studies (Bogaard 2004; Riehl 2008; Wallace et al. 2019), and would hopefully encourage further data sharing in the future.
Given that there are numerous examples of meta-analysis which do take place in archaeobotany, it seems likely that the prevalent form of data sharing is through informal data sharing between individual specialists. However, this does not improve access to data in the long term, and is inefficient and time consuming, with large potential for data errors (Kansa & Kansa 2013), and relies on personal networks, which are likely to exclude some researchers. The absence of primary data in many archaeobotanical publications thus inhibits the verification of patterns observed within a dataset, and strongly limits the re-use potential of a dataset.
4.2. Data citation
One of the common arguments for increasing data sharing is an associated increase in the citation of the articles which have data available. Here, the data citation practices of meta-analyses of plant macrofossil data undertaken over the last decade have been reviewed. 20 journals were consulted, including a wider range of period-specific journals, and 107 articles were assessed (Lodwick 2019 Table 2). Data citation was assessed as ‘in text’ or ‘in table’ to refer to when the citation and the bibliographic reference were within the article, as ‘in supplementary data’ when the citation and reference were within the supplementary materials, and as ‘no citation’ when no citation and reference was provided.
21% of articles (n = 22) did not contain any citations to the underlying studies. 16% (n = 17) contained citations within supplementary data files. 50% of articles (n = 53) contained a citation within a table within the main article, and 14% (n = 15) contained citations within the main text. For the 21% of articles without data citations, the results of these studies could not be reproduced without consulting individual authors. The papers supplying the underlying data also received no credit for producing these datasets. Where articles contain citations within the main article (in text or table), full credit is provided to the underlying studies, a citation link is created through systems such as google scholar, and the study can be easily built upon in the future. Where the citation is provided within supplementary data, the original studies do receive attribution, but are not linked to so easily.
Through time, there is a steady decrease in the proportion of studies without citations to the underlying data, whereby of the 17 meta-analysis articles published in 2018, only one had no data citations. In comparison, in 2009, 3 out of 8 meta-analysis articles contained no data citation (Figure 6). Overall this is a more positive outlook on the reuse of published data, but the consistent presence of articles lacking data citation indicates that improvements are needed. Reasons for a lack of data citation may include restrictions on word counts imposed by journals, a lack of technical knowledge in making large databases available, or the wish to hold on to a dataset to optimise usage. Considering the type of journal (Figure 5), levels of data citation are worse in general archaeology journals, with sub-disciplinary journals showing slightly better levels of data citation. In particular VHA has a lack of consistency in where data citations are located.
Chart showing the location of data citations in meta-analysis journal articles by journal type.
Chart showing the location of data citations in meta-analysis journal articles from 2009–2018.
4.3. Reuse of archived archaeobotanical datasets
The majority of data citations assessed in the previous section are to articles or book chapters rather than data-sets. The ADS currently hosts 66 data archives which have been tagged as containing plant macro data, deriving mainly from developer-funded excavations but also some research excavations. However, in some of these the plant macro data is contained within a pdf. As, the archiving of archaeobotanical datasets in data repositories is still at an early stage, the reuse of these datasets is assessed here on a case-by-case basis. The archaeobotanical dataset from the Neolithic site of Vaihingen, Germany (Bogaard 2011b) has not been cited on google scholar. Metrics are provided through the ADS, showing this dataset has been downloaded 56 times with 477 individual visits (as of 25/2/19). The archaeobotanical dataset from Gordion by Marston has no citations on Google Scholar (Marston 2017b), neither does the Giza botanical database (Malleson & Miracle 2018), but these are both very recently archived datasets. In contrast, the Roman Rural Settlement Project dataset, which includes site-level archaeobotanical data, has received greater levels of use, with 12 citations in Google Scholar, over 40,000 file downloads, and over 35,000 visits (Allen et al. 2018) and the archaeobotanical computer database (Tomlinson & Hall 1996) has been cited 44 times, and is the major dataset underpinning other highly-cited studies (Van der Veen, Livarda & Hill 2008; Van der Veen, Hill & Livarda 2013). Whilst there is clearly precedence for the reuse of archaeobotanical databases, current data citation practices within archaeobotany do not yet appear to be formally citing individual datasets, meaning an assessment of the reuse of archived archaeobotanical datasets is challenging.
5. Steps Forward
This review of data sharing, citation, and reuse practices in archaeobotany has found medium levels of data sharing, good levels of data citation, but so far limited levels of reuse of archived data sets. This picture is similar across archaeology, in part attributed to the status of archaeology as a small-science, where data-sharing takes place ad-hoc (Marwick & Pilaar Birch 2018). Here, recommendations are discussed for improving these data practices within archaeobotany, of applicability more widely in archaeology.
Clearly an important step is improving the sharing of plant macrofossil data. Given the reasonable small size of most archaeobotanical datasets (a .csv file < 1mb), and a lack of ethical conflicts, there seems to be few reasons why the majority of archaeobotanical data couldn’t be shared. In the case of developer-funded derived data, issues of commercial confidentiality could limit the sharing of data. A key stage is establishing why levels of data sharing are not higher. Issues within archaeobotany may include the conflict between having to publish results within excavation monographs, which may take some time to be published, and have limited visibility due to high purchase costs and no digital access, and the need to publish journal articles for career progression within academia. The production of an archaeobotanical dataset is very time-consuming, and interim publication on notable aspects of an assemblage may be considered as a necessary publication strategy. More broadly, one important aspect is issues of equity in access to digital archiving resources (Wright & Richards 2018), such as differential access to funds, training and knowledge. A recent study in Sweden found that we need to know concerns, needs, and wishes of archaeologists in order to improve preservation of archaeological data (Huvila 2016), especially when control of ones data may be linked to perceptions of job security. In order to make improvements in data sharing and reuse across archaeology, we need improved training in data sharing and the reuse of data in higher education (Touchon & McCoy 2016; Cook et al. 2018), improved training in data management (Faniel et al. 2018), and crucially, the necessary software skills to make the reuse of archived datasets attainable (Kansa & Kansa 2014: 91). Examples of good practice in archaeobotany are the Vaihingen and Gordion datasets which demonstrate how datasets can be archived in data repositories to accompany a monograph (Bogaard 2011b; Marston 2017b), whilst Farahani (2018) provides an excellent example of a journal article, where the primary data is supplied as a .csv in a cited data repository along with the R script for the analysis.
In tandem with the need to encourage authors to share their data, is the need for journals to create and implement research data policies. Given the existence of research data policies in many of the journals included here, this reflects other findings of the poor enforcement of data policies by journals (Marwick & Pilaar Birch 2018), supporting arguments that journals should not be relied upon to make data accessible, and data should instead by deposited in digital repositries. In order to implement change in data sharing, there is a role to play for learned societies and academic organisation in lobbying funding bodies, prioritising data sharing in research projects. A key step is through journal editorial boards, and the enforcement of any pre-existing research data policies (Nosek et al. 2015). Revi","['Technological limitations, resistance to exposing data to scrutiny, and desire to hold onto data for personal use.']",6097,multifieldqa_en_e,en,,aeb6cb26b11fc386727a529761d9d233ec7ba8dea9800b0f,"Technological limitations, resistance to exposing data to scrutiny, and desire to hold onto data for personal use.",114
What is the potential of SNNs in modeling the visual system?,"Paper Info

Title: Deep Spiking Neural Networks with High Representation Similarity Model Visual Pathways of Macaque and Mouse
Publish Date: 22 May 2023
Author List: Zhengyu Ma (from Department of Networked Intelligence, Peng Cheng Laboratory), Yu Liutao (from Department of Networked Intelligence, Peng Cheng Laboratory), Huihui Zhou (from Department of Networked Intelligence, Peng Cheng Laboratory), Allen Brain
Author Affiliation: CORNet-S ConvNeXt-Tiny ConvNeXt-Small EfficientNet, AlexNet RegNetY, ResNet34 ConvNeXt-Base CORNetSEW, ResNet8 ResNet101 SEW-ResNet18 ViT-L, GoogLeNet SEW-ResNet34 SEW-ResNet8 Wide

Figure

Figure 1: To conduct neural representation similarity experiments, we apply three similarity metrics to a layer-by-layer comparison between the responses of models and the neural activities of visual cortex.
Figure 2: For three datasets and three similarity metrics, each point indicates the final representation similarity score of a model.Each pair of SEW ResNet and ResNet with the same depth are linked by a gray solid line.In almost all conditions, SEW ResNet outperforms ResNet by a large margin.
Figure3: For three datasets and three similarity metrics, we plot the trajectories of similarity score with model layer depth.The models are divided into two groups: ResNet and SEW ResNet.The normalized layer depth ranges from 0 (the first layer) to 1 (the last layer).Because the depths of models are not the same, we first discretize the normalized depth into 50 bins, and then apply the cubic spline interpolation to the scores of each model, yielding the smooth trajectories shown in the plot.The fine, semitransparent lines are the trajectories of each model.The thick lines are the average trajectories among each group.
Figure 5: For Macaque-Synthetic dataset, trajectories of similarity score with model layer depth are plotted.The models are divided into two groups: ViT and CNN&SNN.The normalized layer depth ranges from 0 (the first layer) to 1 (the last layer).The calculation and plotting of the trajectories are the same as Figure 3.
Figure6: The basic block of SpikingMobileNet.""PW CONV"" is the pointwise convolution and ""DW CONV"" is the depthwise convolution.""SN"" is the spiking neuron.
Figure 7: Overall model rankings of the similarity scores on Allen Brain mouse dataset.The similarity scores of CNNs, SNNs and vision transformers are shown by blue, green and orange bars, respectively.
Figure 9: Overall model rankings of the similarity scores on Macaque-Synthetic dataset.
Figure 10: The Spearman's rank correlation between the overall model rankings of different metrics.There is a strong correlation between SVCCA and TSVD-Reg, but RSA has weaker correlations with them.
The correlation between the similarity scores and the model depth.r is Spearman's rank correlation coefficient.""-"" indicates that there is no significant correlation.
Architectures of SNNs.""sn"" denotes the spiking neuron.""g = 32"" denotes the grouped convolutions with 32 groups.The hyper-parameters of the spike-element-wise block are shown in the brackets with the number of stacked blocks outside.

abstract

Deep artificial neural networks (ANNs) play a major role in modeling the visual pathways of primate and rodent. However, they highly simplify the computational properties of neurons compared to their biological counterparts. Instead, Spiking Neural Networks (SNNs) are more biologically plausible models since spiking neurons encode information with time sequences of spikes, just like biological neurons do.
However, there is a lack of studies on visual pathways with deep SNNs models. In this study, we model the visual cortex with deep SNNs for the first time, and also with a wide range of state-of-the-art deep CNNs and ViTs for comparison. Using three similarity metrics, we conduct neural representation similarity experiments on three neural datasets collected from two species under three types of stimuli.
Based on extensive similarity analyses, we further investigate the functional hierarchy and mechanisms across species. Almost all similarity scores of SNNs are higher than their counterparts of CNNs with an average of 6.6%. Depths of the layers with the highest similarity scores exhibit little differences across mouse cortical regions, but vary significantly across macaque regions, suggesting that the visual processing structure of mice is more regionally homogeneous than that of macaques.
Besides, the multi-branch structures observed in some top mouse brain-like neural networks provide computational evidence of parallel processing streams in mice, and the different performance in fitting macaque neural representations under different stimuli exhibits the functional specialization of information processing in macaques.
Taken together, our study demonstrates that SNNs could serve as promising candidates to better model and explain the functional hierarchy and mechanisms of the visual system. Originally, the prototype of deep neural networks is inspired by the biological vision system . To date, deep neural networks not only occupy an unassailable position in the field of computer vision , but also become better models of the biological visual cortex compared to traditional models in the neuroscience community (Khaligh-Razavi and Kriegeskorte 2014; .
They have been successful at predicting the neural responses in primate visual cortex, matching the hierarchy of ventral visual stream (Güc ¸lü and van Gerven 2015; , and even controlling neural activity . Moreover, as training paradigms of mice and techniques for collecting neural activity (de Vries et al. 2020) have been greatly improved, there is a strong interest in exploring mouse visual cortex.
Deep neural networks also play an important role in revealing the functional mechanisms and structures of mouse visual cortex . Compared to biological networks, Artificial Neural Networks discard the complexity of neurons . Spiking Neural Networks, incorporating the concept of time and spikes, are more biologically plausible models .
To be more specific, because of their capabilities of encoding information with spikes, capturing the dynamics of biological neurons, and extracting spatio-temporal features, deep SNNs are highly possible to yield brain-like representations ). However, deep SNNs have not been employed to model visual cortex due to the immaturity of training algorithms.
Recently, a state-ofthe-art directly trained deep SNN , makes it possible to use deep SNNs as visual cortex models. Contributions. In this work, we conduct large-scale neural representation similarity experiments on SNNs and other high-performing deep neural networks to study the brain's visual processing mechanisms, with three datasets and three similarity metrics (Figure ).
Specifically, to the best of our knowledge, we are the first to use deep SNNs to fit complex biological neural representations and explore the biological visual cortex. We summarize our main contributions in four points as follows. • We find that SNNs outperform their counterparts of CNNs with the same depth and almost the same architectures in almost all experiments.
In addition, even with very different depths and architectures, SNNs can achieve top performance in most conditions. • By making a more direct comparison between macaques and mice for the first time, we reveal the differences in the visual pathways across the two species in terms of the homogeneity of visual regions and the increases of receptive field sizes across cortical visual pathways, which is consistent with previous physiological work.
• The multi-branch structures in neural networks benefit neural representation similarity to mouse visual cortex, providing computational evidence that parallel information processing streams are widespread between cortical regions in the mouse visual system. • Comparing the results of two macaque neural datasets under different stimuli, we reveal that the macaque vision system may have functional specialization for processing human faces and other natural scenes.
Altogether, as the first work to apply deep SNNs to fit neural representations, we shed light on visual processing mechanisms in both macaques and mice, demonstrating the potential of SNNs as a novel and powerful tool for research on the visual system. Our codes and appendix are available at https://github.com/Grasshlw/SNN-Neural-Similarity.
There are plenty of computational models of macaque and mouse visual systems for exploring the visual processing mechanisms recently. We summarize some of the outstanding work in the following. The network models of macaque visual system. In the early days, studies basically used simple feedforward neural networks as the models of the macaque visual system (Khaligh-Razavi and Kriegeskorte 2014; .
Recently, some bio-inspired or more complex models achieved better performance in fitting the neural representations of macaque visual cortex . proposed a brainlike shallow CNN with recurrent connections to better match the macaque ventral visual stream. By mimicking the primary stage of the primate visual system, VOneNets ) performed more robustly in image recognition while better simulating macaque V1.
Moreover, the representations learned by unsupervised neural networks ) also effectively matched the neural activity of macaque ventral visual stream. Although the above work developed many bio-inspired structures, the networks are still traditional ANNs in nature. Our work introduces deep SNNs for the first time to explore the visual processing mechanisms of macaque visual system.
The network models of mouse visual system. Largescale mouse neural dataset provided an experimental basis for model studies of mouse visual system (de Vries et al. 2020; . conducted comparisons between the representations of mouse visual cortex and the VGG16 trained on the Im-ageNet dataset. In , they developed a single neural network to model both the dorsal and ventral pathways with showing the functional specializations.
What's more, a large survey of advanced deep networks ) revealed some hierarchy and functional properties of mice. Similar to the studies of macaque visual system, deep SNNs have never been used to model the mouse visual system. In this work, we not only use SNNs as one of the candidates to fit the representations of mouse visual cortex, but also conduct direct comparisons between macaques and mice to further investigate the functional hierarchy and mechanisms of the two species.
Our work is conducted with three neural datasets. These datasets are recorded from two species under three types of stimuli. More specifically, there are neural responses of mouse visual cortex to natural scene stimuli, and responses of macaque visual cortex to face image and synthetic image stimuli. Allen Brain mouse dataset.
It is part of the Allen Brain Observatory Visual Coding dataset ) col-lected using Neuropixel probes from 6 regions simultaneously in mouse visual cortex. Compared to two-photon calcium imaging, Neuropixel probes simultaneously record the spikes across many cortical regions with high temporal resolution.
In these experiments, mice are presented with 118 250-ms natural scene stimuli in random orders for 50 times. Hundreds to thousands of neurons are recorded for each brain region. To get the stable neurons, we first concatenate the neural responses (average number of spikes in 10-ms bins across time) under 118 images for each neuron, and then preserve the neurons whose split-half reliability across 50 trials reaches at least 0.8.
Macaque-Face dataset. This dataset ) is composed of neural responses of 159 neurons in the macaque anterior medial (AM) face patch under 2,100 real face stimuli, recorded with Tungsten electrodes. For this dataset, we compute the average number of spikes in a time window of 50-350ms after stimulus onset and exclude eleven neurons with noisy responses by assessing the neurons' noise ceiling.
The details of the preprocessing procedure are the same as . Macaque-Synthetic dataset. This dataset is also about macaque neural responses which are recorded by electrodes under 3,200 synthetic image stimuli, and used for neural prediction in the initial version of Brain-Score . The image stimuli are generated by adding a 2D projection of a 3D object model to a natural background.
The objects consist of eight categories, each with eight subclasses. The position, pose, and size of each object are randomly selected. 88 neurons of V4 and 168 neurons of IT are recorded. The neural responses are preprocessed to the form of average firing rate and can be downloaded from Brain-Score. Since the core visual function of macaque and mouse visual cortex is to recognize objects, the basic premise of model selection is that the model has good performance on object recognition tasks (e.g.
classification on ImageNet). Based on this premise, we employ 12 SNNs, 43 CNNs, and 26 vision transformers, all of which are pretrained on the Ima-geNet dataset and perform well in the classification task. As for SNNs, we use SEW ResNet as the base model, which is the deepest and SOTA directly trained SNN .
Furthermore, by combining the residual block used in SEW ResNet and the hierarchy of the visual cortex, we build several new SNNs and train them on the ImageNet using SpikingJelly ) (see Appendix A for model structures and the details of model training). As for CNNs and vision transformers, we use 44 models from the Torchvision model zoo , 22 models from the Timm model zoo ) and 3 models from the brain-like CNNs, CORnet family ).
In the feature extraction procedures of all models, we feed the same set of images used in biological experiments to the pretrained models and obtain features from all chosen layers. Different from CNNs and vision transformers, the features of SNNs are spikes in multiple time steps. To obtain the representation similarity between biological visual cortex and computational models, we apply three similarity metrics to computing similarity scores: representational similarity analysis (RSA) , regression-based encoding method and singular vector canonical correlation analysis (SVCCA) .
RSA has already been widely used to analyze neural representations of a model and a brain to different stimuli at the population level, while the regression-based encoding method directly fits the model features to neural activity data. SVCCA is originally proposed to compare features of deep neural networks, and then Buice 2019) used it to compare representation matrices from mouse visual cortex and DNNs, which demonstrated its effectiveness.
With the same model and same cortical region, we use these metrics for a layer-by-layer comparison to compute the similarity scores. The maximum similarity score across layers for a given cortical region is considered to be the level of representation similarity between the model and the cortical region.
Finally, in a given dataset, we take the average score of all cortical regions as the final similarity score for each model, which gives the overall model rankings. The implementation of each similarity metric is as follows. RSA. For two response matrices R ∈ R n×m from each layer of models and each cortical region, where n is the number of units/neurons and m is the number of stimuli, we calculate the representational similarity between the responses to each pair of image stimuli using the Pearson correlation coefficient r, yielding two representational dissimilarity matrices (RDM ∈ R m×m , where each element is the correlation distance 1 − r).
Then, the Spearman rank correlation coefficient between the flattened upper triangles of these two matrices is the metric score. Regression-Based Encoding Method. Firstly, we run truncated singular value decomposition (TSVD) to reduce the feature dimension of model layers to 40. Secondly, the features after dimensionality reduction are fitted to the representations of each neuron by ridge regression.
Finally, we compute the Pearson correlation coefficient between the predicted and ground-truth representations of each neuron and take the mean of all correlation coefficients as the metric score. More specifically, we apply leave-one-out crossvalidation to obtain predicted representations of each neuron.
For simplicity, we name this method 'TSVD-Reg'. SVCCA. For both the responses of model layers and cortical regions, we use TSVD to reduce the dimension of unit/neuron to 40, yielding two reduced representation matrices. Then we apply canonical correlation analysis (CCA) to these two matrices to obtain a vector of correlation coefficients (the length of the vector is 40).
The metric score is the mean of the vector. Because of the invariance of CCA to affine transformations , in this procedure, we only need to ensure that the stimulus dimension is consistent and aligned, even if the unit/neuron dimension is different. Dimensionality reduction plays an important role in this method to make the number of model features comparable to the number of neurons in cortical regions, since the former usually far exceeds the latter.
In addition, dimensionality reduction helps to determine which features are important to the original data, while CCA suffers in important feature detection. Using just CCA performs badly, which has been proven by . To check how similar the models are to the visual cortex's mechanisms in visual processing, we rank the final similarity scores of all models and conduct comparisons among three types of models (CNNs, SNNs, and vision transformers).
Specially, we focus on comparing SNN (SEW ResNet) and CNN (ResNet) with the same depth and almost the same architectures (Figure ). The final similarity score of a model is the average similarity score across all cortical regions. (The overall rankings can be found in Appendix B and the comparisons among three types of models are shown in Appendix C.)
Allen brain mouse dataset. No single model achieves the highest final similarity scores with all three metrics. For a fair comparison, we apply the paired t-test to SEW ResNet and ResNet with the same depth. For all three metrics, SEW ResNet performs better than ResNet by a large margin (t = 5.857, p = 0.004; t = 7.666, p = 0.002; t = 7.592, p = 0.002) 1 . 1 The results of the three similarity metrics are separated by semicolons, in the order of SVCCA, TSVD-Reg, and RSA.
Other Macaque-Face dataset. For both SVCCA and TSVD-Reg, Wide-SEW-ResNet14 and Wide-SEW-ResNet8 achieve the first and second highest final similarity scores respectively. But for RSA, TNT-S and Inception-ResNet-V2 take their place and outperform other models by a large margin. As for SEW ResNet and ResNet, the former performs significantly better than the latter for both SVCCA and TSVD-Reg (t = 8.195, p = 0.001; t = 7.528, p = 0.002).
However, the difference is not significant for RSA (t = 1.117, p = 0.327). Specifically, the similarity score of SEW ResNet152 is only slightly higher than that of ResNet152, and at the depth of 50 and 101, SEW ResNet's scores are lower than ResNet's. Macaque-Synthetic dataset. Similar to the results of Allen Brain dataset, no model performs best for all three metrics.
SEW ResNet performs moderately better than ResNet (t = 3.354, p = 0.028; t = 3.824, p = 0.019; t = 2.343, p = 0.079). The only contrary is that SEW ResNet18 performs worse than ResNet18 for RSA. Further, to check the details of comparison between the SNNs and their CNN counterparts, we analyze the trajectories of similarity score across model layers (Figure ).
As for ResNet and SEW ResNet with the same depth, the trends of their similarities across model layers are almost the same, but the former's trajectory is generally below the latter's. In other words, the similarity scores of SEW ResNet are higher than those of ResNet at almost all layers. Taken together, the results suggest that when the overall results that appear below also correspond to the three metrics in this order, unless the correspondence is stated in the text.
architectures and depth are the same, SNNs with spiking neurons perform consistently better than their counterparts of CNNs with an average increase of 6.6%. Besides, SEW ResNet14 also outperforms the brain-like recurrent CNN, CORnet-S, with the same number of layers (see more details in Appendix B). Two properties of SNNs might contribute to the higher similarity scores.
On the one hand, IF neurons are the basic neurons of spiking neural networks. The IF neuron uses several differential equations to roughly approximate the membrane potential dynamics of biological neurons, which provides a more biologically plausible spike mechanism for the network. On the other hand, the spiking neural network is able to capture the temporal features by incorporating both time and binary signals, just like the biological visual system during information processing.
To figure out the distinctions in the functional hierarchy between macaques and mice, for each cortical region, we obtain the normalized depth of the layer that achieves the highest similarity score in each model. Then, we divide models (excluding vision transformers) into two groups based on their depths and conduct investigations on these two groups separately.
A nonparametric ANOVA is applied to each group for testing whether layer depths change significantly across cortical regions. For mouse visual cortex (Figure (a)), taking the deep model group as an example, ANOVA shows overall significant changes in depth across cortical regions for TSVD-Reg and RSA (Friedman's χ 2 = 49.169,
p = 2.0 × 10 −9 ; χ 2 = 19.455, p = 0.002). But there is no significant change for SVCCA (χ 2 = 8.689, p = 0.122). According to these results, the differences in depth across regions are indeterminacy and irregular. Meanwhile, the trends of layer depth between some regions contradict the hierarchy observed in physiological experiments of mice (those between VISp and VISrl for TSVD-Reg and between VISal and VISpm for RSA).
However, for macaque visual cortex (Figure (b)), there are significant differences (t = −5.451, p = 6.5 × 10 −6 ; t = −8.312, p = 2.8 × 10 −9 ; t = −3.782, p = 6.9 × 10 −4 , also taking the deep model group as an example) between V4 and IT, and the trend is consistent with the information processing hierarchy in primate visual cortex.
The comparative analyses of the best layer depths of the shallow and deep model groups also exhibit the differences between macaques and mice. For mouse visual cortex, the best layer depths of shallow models are significantly higher than those of deep models. Compared to deep models, most shallow models achieve the top similarity scores in intermediate and even later layers.
Differently, for macaque visual cortex, the depth of models has little effect on the depth of the most similar layer. What's more, we find that the most similar layer of mouse visual cortex always occurs after the 28 × 28 feature map is downsampled to 14 × 14, which leads to the layer depths' difference between shallow and deep models.
Nevertheless, the best layer of macaque IT appears in the last part of networks, where the feature map has been downsampled more times. In summary, our results might reveal two distinctions in the functional hierarchy between macaques and mice. First, there is a distinct functional hierarchical structure of macaque ventral visual pathway, while there might be no clear sequential functional hierarchy in mouse visual cortex.
One explanation is that the mouse visual cortex is organized into a parallel structure and the function of mouse cortical regions are more generalized and homogeneous than those of macaques. Another possibility would be that even though the sequential relations exist among mouse cortical regions as proposed in anatomical and physiological work, they are too weak for the current deep neural networks to capture.
Additionally, mice perform more complex visual tasks than expected with a limited brain capacity . Consequently, the neural responses of mouse visual cortex may contain more information not related to object recognition that neural networks focus on. Secondly, it is well known that the units in the neural networks get larger receptive fields after downsampling, and through the analyses of differences between two groups of models based on depth, we find the feature map of the best layer for mouse is downsampled fewer times than that for macaque.
Based on these results, we provide computational evidence that the increased ratio of the receptive field size in cortical regions across the mouse visual pathway is smaller than those across the macaque visual pathways, which echoes some physio- Macaque-Face dataset --- Table : The correlation between the similarity scores and the number of parameters.
r is Spearman's rank correlation coefficient. ""-"" indicates that there is no significant correlation. To explore the processing mechanisms in the visual cortex of macaques and mice, we investigate the model properties from the whole to the details. As shown in Table and 2, we first measure the correlation between the similarity scores and the sizes (i.e. the number of trainable parameters and the depth) of network models.
For Allen Brain mouse dataset, there are significant negative correlations between the similarity scores and the number of parameters for three metrics while there is no correlation with the depth. Conversely, for the two macaque neural datasets, the similarity scores are highly correlated with the depth of networks, but not with the number of parameters.
Specifically, there is a positive correlation for Macaque-Face dataset while a negative correlation for Macaque-Synthetic dataset. (We also apply the linear regression to analyze the correlation between the similarity scores and the model size. The results are consistent with Spearman's rank correlation and are shown in Appendix E).
Based on these results, we further investigate more detailed properties of neural networks to explain the processing mechanisms in the visual cortex. For the mouse dataset, on the one hand, the best layer depths show non-significant changes across the mouse cortical regions as mentioned in the previous section.
On the other hand, the similarity scores of the mouse dataset are only correlated with the number of model parameters but not with the depth of models. It calls into the question whether any detailed structures in the neural networks help to reduce the number of parameters and improve its similarity to mouse visual cortex.
Therefore, we explore the commonalities between models that have the top 20% representation similarities (see Appendix D) for Allen Brain dataset. As expected, the top models contain similar structures, such as fire module, inception module, and depthwise separable convolution. All these structures essentially process information through multiple branches/channels and then integrate the features from each branch.
The models with this type of structure outperform other models (t = 2.411, p = 0.024; t = 3.030, p = 0.007; t = 1.174, p = 0.247). Moreover, we apply the depthwise separable convolution to SNNs, which yields a positive effect. The representation similarity of Spiking-MobileNet is higher than SEW-ResNet50 with a similar depth (+0.8%; +3.9%; +12.1%).
In fact, some studies using multiple pathways simulate the functions of mouse visual cortex to some extent . Our results further suggest that not only the mouse visual cortex might be an organization of parallel structures, but also there are extensive parallel information processing streams between each pair of cortical regions .
For the two macaque datasets with different stimuli, not only are the model rankings significantly different, but also the correlations between the similarity scores and the model depth are totally opposite. These results corroborate the following two processing mechanisms in macaques: the ventral visual stream of primate visual cortex possesses canonical coding principles at different stages; the brain exhibits a high degree of functional specialization, such as the visual recognition of faces and other objects, which is reflected in the different neural responses of the corresponding region (although the face patch AM is a sub-network of IT, they differ in the neural representations).
Besides, as shown in Figure , The calculation and plotting of the trajectories are the same as Figure . the similarity scores of vision transformers reach the maximum in the early layers and then decrease. Differently, the scores of CNNs and SNNs keep trending upwards, reaching the maximum in almost the last layer.
On the other hand, Appendix C shows that vision transformers perform well in Macaque-Face dataset but poorly in Macaque-Synthetic dataset. Considering the features extraction mechanism of vision transformers, it divides the image into several patches and encodes each patch as well as their internal relation by self-attention.
This mechanism is effective for face images that are full of useful information. However, the synthetic image consists of a central target object and a naturalistic background. When vision transformers are fed with this type of stimuli, premature integration of global information can lead to model representations containing noise from the unrelated background.
What's more, when we take all models with the top 20% representation similarities as a whole for analyses, as described in the above paragraph, the properties that enable networks to achieve higher neural similarity are not yet clear. Taken together, the computational mechanism of the better models may reveal core processing divergence to different types of stimuli in the visual cortex.
In this work, we take large-scale neural representation similarity experiments as a basis, aided by analyses of the similarities across models and the visual cortical regions. Compared to other work, we introduce SNNs in the similarity analyses with biological neural responses for the first time, showing that SNNs achieve higher similarity scores than CNNs that have the same depth and almost the same architectures.
As analyzed in Section 3.1, two properties of SNNs might serve as the explanations for their high similarity scores. The subsequent analyses of the models' simulation performance and structures indicate significant differences in functional hierarchies between macaque and mouse visual cortex. As for macaques, we observed a clear sequential hi-erarchy.
However, as for mouse visual cortex, some work ) exhibits that the trend of the model feature complexity roughly matches the processing hierarchy, but other work suggests that the cortex ) is organized into a parallel structure. Our results are more supportive of the latter. Furthermore, we provide computational evidence not only that the increased ratio of the receptive field size in cortical regions across the mouse visual pathway is smaller than those across the macaque visual pathway, but also that there may be multiple pathways with parallel processing streams between mouse cortical regions.
Our results also clearly reveal that the processing mechanisms of macaque visual cortex differ to various stimuli. These findings provide us with new insights into the visual processing mechanisms of macaque and mouse, which are the two species that dominate the research of biological vision systems and differ considerably from each other.
Compared to CNNs, the study of task-driven deep SNNs is just in its initial state. Although we demonstrate that SNNs outperform their counterparts of CNNs, SNNs exhibit similar properties as CNNs in the further analyses. In this work, we only build several new SNNs by taking the hints from the biological visual hierarchy, while many well-established structures and learning algorithms in CNNs have not been applied to SNNs yet.
In addition, the neural datasets used in our experiments are all collected under static image stimuli, lacking rich dynamic information to some certain, which may not fully exploit the properties of SNNs. Given that SNNs perform well in the current experiments, we hope to explore more potential of SNNs in future work.
In conclusion, as more biologically plausible neural networks, SNNs may serve as a shortcut to explore the biological visual cortex. With studies on various aspects of SNNs, such as model architectures, learning algorithms, processing mechanisms, and neural coding methods, it's highly promising to better explain the sophisticated, complex, and diverse vision systems in the future.

Implementation Details of SNNs Spiking Neuron Model

For all SNNs, we use the Integrate-and-Fire (IF) model as the spiking neuron model, which acts as the activation layer in neural networks. As mentioned in , V t , X t and S t denote the state (membrane voltage), input (current) and output (spike) of the spiking neuron model respectively at time-step t, and the dynamics of the IF model can be described as follows:
(1) (2) (3) While V t is the membrane voltage after the trigger of a spike, H t is also the membrane voltage, but after charging and before a spike firing. Θ(x) is the unit step function, so S t equals 1 when H t is greater than or equal to the threshold voltage V thresh and 0 otherwise. Meanwhile, when a spike fires, V t is reset to V reset .
Here, we set V thresh = 1 and V reset = 0. In addition, because Θ(x) is non-differentiable at 0, the surrogate gradient method is applied to approximate the derivative function during back-propagation. Here, we use the inverse tangent function as the surrogate gradient function and the derivative function is
(5) In our experiments on SNNs, we not only use SEW ResNet proposed by ), but also build several new SNNs. On the one hand, we improve the spike-elementwise block in SEW ResNet with new architectures referring to studies on ResNet , as shown in Table . On the other hand, as the multi-branch structures in CNNs increase neural representation similarity to mouse visual cortex, we use depthwise separable convolutions and follow the overall architecture of MobileNetV2 to build the SpikingMobileNet, the basic block of which is shown in Figure .
Our implementation is based on SpikingJelly , an open-source framework of deep SNN. We use the ImageNet dataset to pre-train the new SNNs. Following the settings for training SEW ResNet , we train the models for 320 epochs on 8 GPUs (NVIDIA V100), using SGD with a mini-batch size of 32. The momentum is 0.9 and the weight decay is 0. The initial learning rate is 0.1 and we decay it with a cosine annealing, where the maximum number of iterations is the same as the number of epochs.
For all SNNs, we set the simulation duration T = 4.

Overall model rankings

The results of model rankings are shown in Figure , 8 and 9. We also apply the Spearman's rank correlation to the overall model rankings of different metrics, which is shown in Figure .

Score Comparisons among Model Groups

We conduct comparisons of similarity scores among CNNs, SNNs, and vision transformers. The results are shown in Figure .

Overall CNN rankings

The results of CNN rankings are shown in Figure , 13 and 14.

Correlations between the Model Sizes and the Similarity Scores

The results of linear regression to model sizes and the similarity scores are shown in Figure , 16 and 17.

The ImageNet Accuracy and the Similarity Scores

The results are shown in Figure .",['SNNs have the potential to better model and explain the functional hierarchy and mechanisms of the visual system.'],5588,multifieldqa_en_e,en,,6b35731428ea6d9b480338b90572d21690c2fbb89ebba249,SNNs have the potential to better model and explain the functional hierarchy and mechanisms of the visual system.,113
Who is responsible for carrying out the functions assigned under the act?,"Bare Acts Live
Himachal Pradesh Town and Country Planning Act, 1977
Himachal Pradesh Town And Country Planning Rules, 1978
3. Form of Notice.
4. Manner of publication of notice.
5. Manner of publication of Regional Plan.
6. Notice of Modifications in Regional Plan.
7. Manner of publication of existing land-use map.
8. Manner of publication of approved Interim Development Plan.
9. Manner of publication of draft development plan.
10. Manner of publication of approved development plan.
11. Intention of development undertaken on behalf of Union or State Government.
12. Form of application for permission for development of land by others.
13. Form of permission.
14. Manner of communication of order under sub-section (4) of Section 31.
16. Notice by owner to purchase interest in land.
17. Manner of communication of revocation and modification permission to development.
20. Preparation of town development scheme.
21. Acquisition of land.
22. Mode of levy.
23. Power to borrow money.
24. Terms and conditions subject to which loans may be raised by the Special area Development Authority.
1. Short title, extent, commencement and application.
3. Director and other officers.
4. Establishment of regions.
5. Director to prepare regional plan.
6. Survey.
7. Contents of regional plan.
8. Preparation of regional plan.
9. Finalisation of regional plan.
10. Restriction on use of land or development thereof.
11. Exclusion from claims of amount in certain cases.
12. Review of regional plan.
13. Planning area.
14. Director to prepare development plans.
15. Existing land use maps.
16. Freezing of land use.
17. Interim development plans.
18. Development plan.
19. Publication of draft development plan.
20. Sanction of development plans.
21. Director to prepare sectoral plan.
22. Contents of sectoral plan.
23. Provisions of sections 19 and 20 to apply to sectoral plan.
24. Review of development plan and sectoral plan.
25. Director to control land use.
26. Conformity with development plan.
27. Prohibition of development without permission.
28. Development undertaken on behalf of Union or State Government.
29. Development by local authority or by any authority constituted under this Act.
30. Application for permission for development by others.
30A. Exemption from development permission in rural areas falling within Planning or Special Area.
30B. Exemption in respect of development of certain lands or buildings.
31. Grant or refusal of permission.
34. Lapse of permission.
35. Obligation to acquire land.
36. Deletion of reservation of designated land from draft or final development plan.
37. Power of revocation and modification or permission to development.
38. Penalty for unauthorised development or for use otherwise than in conformity with development plan.
39. Power to require removal of unauthorised development.
40. Establishment of Town and Country Development Authority.
41. Incorporation of Town and Country Development Authority.
42. Constitution of Town and Country Development Authority.
42A. Constitution of Town and Country Development Authority for the Capital Town of Himachal Pradesh.
43. Term of office of Chairman and other members.
44. Resignation of members and filling of casual vacancy.
45. Remuneration of Chairman.
46. Leave of absence and appointment etc. of acting Chairman.
47. Meeting of Town and Country Development Authority.
48. Chief Executive Officer.
49. Other officers and servants.
50. Conditions of service of Chief Executive Officer and other officers and servants.
51. Town development schemes.
53. Power to revise the development schemes.
54. Power of State Government to give Directions.
55. Restriction on land use and development.
56. Lapse of scheme.
57. Town development scheme public purpose.
58. Acquisition of land for Town and Country Development Authority.
59. Developments.
60. Disposal of land, buildings and other development works.
61. Development charges.
63. Fund of Town and Country Development Authority.
64. Annual budget.
66. Constitution of special areas.
67. Special area Development Authority.
68. Incorporation of Special Area Development Authority.
70. Functions.
71. Powers.
72. Fund of Special Area Development Authority.
73. Annual estimates.
74. Power of State Government of supervision and control.
76. Power of Government to review plans etc. for ensuring conformity.
78. Dissolution of authorities.
79. Right of entry.
80. Jurisdiction of Court.
82. Member and officers to be public servants.
83. Suit and other proceedings.
84. Vacancy not to invalidate proceedings.
85. Member to continue till successor enters upon office.
86. Interpretation of regional plan etc.
87. Powers to moke rules.
89. Power to lay the rules and regulations.
The Himachal Pradesh Town and Country Planning Act, 1977
(as amended by Amendment Act No. 22 of 1983)
Amended by Act No. 8 of 2009
Act published in the Rajpatra, Extraordinary, dated the 30th September, 1977 vide Law Department Notification No. LLR-D(6)5/77, dated the 22nd September, 1977.
An Act to make provision for planning and development and use of land; to make better provision for the preparation of development plans and sectoral plans with a view to ensuring that town planning schemes are made in a proper manner and their execution is made effective; to constitute the Town and Country and Development Authority for proper implementation of town and country development plan; to provide for the development and administration of special areas through the Special Area Development Authority; to make provision for the compulsory acquisition of land required for the purpose of the development plans and for purposes connected with the matters aforesaid.
Be it enacted by the Himachal Pradesh Legislative Assembly in the Twenty-eighth Year of the Republic of India as follows:-
1. Short title, extent, commencement and application. - (1) This Act may be called the Himachal Pradesh Town and Country Planning Act, 1977.
(3) It shall come into force on such date as the State Government may, by notification, appoint and different dates may be appointed for different areas and for different provisions of this Act.
(4) Nothing in this Act shall apply to-
(a) lands comprised within a cantonment under the Cantonments Act, 1924; (2 of 1924).
(b) lands owned, hired or requisitioned by the Central Government for the purpose of naval, military and air force works;
(c) lands under the control of railway administration for the purpose of construction and maintenance of works under Chapter III of the Indian Railways Act, 1890; (9 of 1890) and
(d) lands owned by any department of the Central Government where operational constructions are going on.
(a) ""agriculture"" includes horticulture, farming, raising of annual or periodical crops, fruits, vegetables, flowers, grass, fodder, trees or any kind of cultivation of soil, the reserving of land for fodder, grazing or thatching areas, breeding and keeping of livestock including cattle, horses, donkeys, mules, pigs, breeding of fish and keeping of bees, and the use of land ancillary to the farming of land, but does not include-
(i) keeping of cattle purely for the purpose of milking and selling the milk and milk products,
(ii) a garden which is an appendage of buildings, and the expression ""agricultural"" shall be construed accordingly;
(b) ""amenity"" includes roads and streets, water and electric supply, open spaces, parks, recreational area, natural feature, playgrounds, street lighting, drainage, sewerage and other utilities, services and conveniences;
(c) ""building"" includes any structure or erection, or part of a structure or erection, which is intended to be used for residential, industrial, commercial or other purposes, whether in actual use or not;
(d) ""building operation"" includes-
(i) erection or re-erection of a building or any part thereof,
(ii) roofing or re-roofing of any part of building or an open space,
(iii) any material alteration or enlargement of a building,
(iv) any such alteration of a building as is likely to alter its drainage or sanitary arrangements, or materially affect its security,
(v) the construction of a door opening on any street or land not belonging to the owner;
(e) ""commercial use"" means the use of any land or building or part thereof for the purpose of carrying on any trade, business or profession, or sale or exchange of goods of any type whatsoever and includes running of with a view to make profit hospitals, nursing homes, infirmaries, educational institutions, hostels, restaurants and boarding houses not being attached to any educational institutions, sarais and also includes the use of any land or building for storage of goods or as buildings for storage of goods or as an office whether attached to an industry or otherwise;
(f) ""court"" means the principal civil court of original jurisdiction in the district;
(g) ""development"" with its grammatical variations means the carrying out of a building, engineering, mining or other operations in, on, over or under land, or the making of any material change in any building or land or in the use of either, and includes sub-division of any land;
(h) ""development plan"" means interim development plan or development plan prepared under this Act;
(i) ""director"" means the Director of Town and Country Planning appointed under this Act;
(j) ""existing land use map� means a map indicating the use to which lands in any specified area are put at the time of preparing the map, and includes the register prepared, with the map giving details of land-use;
(k) ""land"" includes benefits to arise out of land and things attached to the earth or permanently fastened to anything attached to the earth;
(l) ""member"" means a member of a Town and Country Development Authority or a Special Area Development Authority, as the case may be, and includes a Chairman thereof;
(m) ""occupier"" includes-
(i) a tenant,
(ii) an owner in occupation of or otherwise using his land,
(iii) a rent free tenant,
(iv) a licensee, and
(v) any person liable to pay to the owner, damages for the use and occupation of the land;
(n) ""owner"" includes a mortgagee in possession, a person who for the time being is receiving or is entitled to receive, or has received, the rent or premium for any land whether on his own account or on behalf of or for the further benefit of any other person or as an agent, trustee, guardian or receiver for any other person or for religious or charitable institutions or who would receive the rent or be entitled to receive the rent or premium if the land were to be let and includes a head of a Government department, General Manager of a Railway and the Chief Executive Officer, by whatever name designated, or a local authority, statutory authority, company, corporation or undertaking in respect of properties under their control;
(o) ""planning area"" means any area declared to be planning area under this Act;
(p) ""region"" means any area established to be a region under this Act;
(q) ""regional plan"" means a plan for the region prepared under this Act and approved by the State Government;
(r) ""sector"" means any sector of a planning area for which, under the development plan, a detailed sectoral plan is prepared;
(s) ""slum area"" means any predominantly residential area, where the dwellings which by reason of dilapidation, over-crowding, faulty arrangement of design, lack of ventilation, light or sanitary facilities or any combination of these factors are detrimental to safety, health or moral and which is defined by a development plan as a slum area;
(t) ""special area"" means a special area designated as such under section 66;
(u) ""Special Area Development Authority� means an authority constituted under section 67;
(v) ""Town Development Scheme"" means a scheme prepared for the implementation of the provisions of a development plan by the Town and Country Development Authority; and
(w) ""Town and Country Development Authority"" means an authority established under section 40.
Director of Town and Country Planning
3. Director and other officers. - (1) After the commencement of this Act the State Government shall, by notification in the Official Gazette, appoint an officer for the purpose of carrying out functions assigned to him under this Act, as the Director of Town and Country Planning for the State and may appoint such other categories of officers as it may deem fit.
(2) The Director shall exercise such powers and perform such duties as are conferred or imposed upon him by or under this Act and the officers appointed to assist the Director shall, within such area as the State Government may specify, exercise such powers and perform such duties conferred and imposed on the Director by or under this Act as the State Government may, by special or general order, direct.
(3) The officers appointed to assist the Director shall be subordinate to him and shall work under his guidance, supervision and control.
4. Establishment of regions. - (1) The State Government may, by notification,-
(a) declare any area in the State to be a region for the purposes of this Act;
(b) define the limits of such area; and
(c) specify the name by which such region shall be known.
(2) The State Government may, by notification, alter the name of any such region and on such alteration, any reference in any law or instrument or other document to the region shall be deemed to be a reference to the region as re-named unless expressly otherwise provided or the context so requires.
(3) The State Government may, by notification,-
(a) alter the limits of a region so as to include therein or exclude therefrom Such area as may be specified in the notification;
(b) amalgamate two or more regions so as to form one region;
(c) divide any region into two or more region; or
(d) declare that the whole or part of the area comprising a region shall cease to be a region or part thereof.
5. Director to prepare regional plan. - Subject to the provisions of this Act and the rules made thereunder, it shall be the duty of the Director-
(i) to carry out a survey of the regions;
(ii) to prepare an existing land use map; and
(iii) to prepare a regional plan.
6. Survey. - (1) The Director shall, with a view to prepare the existing land use map, and other maps as are necessary for the purpose of regional plan,-
(a) carry out such surveys as may be necessary;
(b) obtain from any department of Government and any local authority such maps, survey reports and land records as may be necessary for the purpose.
(2) It shall be the duty of every Government department and local authority to furnish, as soon as may be possible, maps, reports and record, as may be required by the Director.
7. Contents of regional plan. - The regional plan shall indicate the manner in which land in the region should be used, the phasing of development, the net work of communications and transport, the proposals for conservation and development of natural resources, and in particular-
(a) allocation of land to such purposes as residential, industrial; agricultural or as forests or for mineral exploitation;
(b) reservation of open spaces for recreational purposes, gardens, tree belts, and animal sanctuaries;
(c) access or development of transport and communication facilities such as roads, railways, water ways, and the allocation and development of airports;
(d) requirements and suggestions for development of public utilities such as water supply, drainage and electricity;
(e) allocation of areas to be developed as ""Special Areas� wherein new towns, townships, large industrial estates or any other type of large development projects may be established;
(f) landscaping and the preservation of areas in their natural state,
(g) measures relating to the prevention of erosion, including rejuvenation of forest areas;
(h) proposals relating to irrigation, water supply or flood control works.
8. Preparation of regional plan. - (1) After preparation of the existing land use map, the Director shall cause to be prepared a draft regional plan and publish it by making a copy thereof available for inspection and publishing a notice in such form and manner as may be prescribed inviting objections and suggestions from any person with respect to the draft plan before such date as may be specified in the notice, such date not being earlier than sixty days from the publication of the notice. Such notice shall specify in regard to the draft plan the following particulars, namely-
(a) the existing land use map and the narrative report thereon;
(b) a narrative report supported by necessary map and charts explaining the provisions of the draft plan;
(c) a note indicating the priorities assigned to works included in the draft plan and the phasing of the programme of development as such;
(d) a notice on the role being assigned to different departments of Government, the Town and Country Development Authorities; the Special Area Development Authorities, and the Local Authorities in the enforcement and implementation of draft plan.
(2) The Director shall consider all the objections and suggestions received by him within the period specified in the notice under sub-section (1) and shall, after giving a reasonable opportunity to all persons affected thereby of being heard, prepare the regional plan containing such modifications, if any, as he considers necessary and submit it to the State Government for approval together with all connected documents, plans, maps and charts.
9. Finalisation of regional plan. - (1) The State Government may approve the draft regional plan submitted under section 8 with or without modification or reject or return the same to the Director for reconsideration.
(2) Immediately after the draft regional plan is approved under sub-section (1) the State Government shall publish in such manner, as may be prescribed, a notice stating that the regional plan has been approved and mentioning a place where a copy of the plan may be inspected at all reasonable hours and shall specify therein a date on which the regional plan shall come into operation:
Provided that where the State Government approves the draft regional plan with modifications, it shall not be published, unless the State Government having published such modifications in the Official Gazette along with a notice inviting objections and suggestions thereon, within a period of not less than thirty days from the date of publication of such notice have considered the objections and suggestions after giving a reasonable opportunity of being heard to persons affected thereby.
10. Restriction on use of land or development thereof. - (1) Notwithstanding anything contained in any other law for the time being in force, on or after the date of publication of the draft regional plan, no person, authority, department of Government or any other person shall change the use of land for any purpose other than agriculture, or carry out any development in respect of any land contrary to the provisions of the draft plan, without the prior approval of the Director or any officer next to him authorised by the Director, in this behalf.
(2) Notwithstanding anything contained in any law for the time being in force, the permission referred to in sub-section (1) shall not be granted otherwise than in conformity with the provision of the draft or final plan and no permission, if granted, shall be construed to confer any legal right whatsoever on the person seeking the permission.
(3) If any work is carried out in contravention of the provisions of this section, the Municipal Corporation or Municipal Committee within its local area, and the Collector in area outside such local areas, may cause such work to be removed or demolished at the cost of the defaulter, which shall be recovered from him in the same manner as an arrear of land revenue:
Provided that no action shall be taken under this sub-section unless the person concerned is given a reasonable opportunity of being heard and a notice calling upon him to remove or demolish the work within a time specified therein.
(4) Any person aggrieved by the order of the Municipal Corporation, Municipal Committee or Collector, as the case may be, calling upon to remove or demolish the work may prefer an appeal to the Director within fifteen days of the receipt of the notice under sub-section (3) and the order of the Director in such appeal shall be final.
11. Exclusion from claims of amount in certain cases. - Where the regional plan assigns a particular land use to a certain areas and any land situate therein is already put to such use, subject to substantially similar restrictions in force under any other law which was in force on the date on which restrictions were imposed by or under this Act and if amount in respect of such restrictions have already been paid under any such other law which was in force for the time being in respect of the property or any right or interest therein to the claimant, or any predecessor in interest of the claimant, the owner shall not be entitled to any further amount on account of injury or damage caused to his rights by reasons of the restrictions placed on the use of the land under the provisions of this Act.
12. Review of regional plan. - (1) The Director may, on his own motion or if so required by the State Government, at any time after a regional plan has come into operation, undertake the review and evaluation of the regional plan and make such modification in it as may be justified by the circumstances.
(2) The foregoing provisions of this Chapter shall, so far as they can be made applicable, apply to the modifications under sub-section (1) as these provisions apply in relation to the preparation, publication and approval of a regional plan.
Planning Area and Development Plans
13. Planning area. - (1) The State Government may, by notification, constitute planning areas for the purposes of this Act and define the limits thereof.
(2) The State Government may, by notification.-
(a) alter the limits of a planning area so as to include therein or excluse therefrom such area as may be specified in the notification;
(b) amalgamate two or more planning areas so as to constitute one planning area;
(c) divide any planning area into two or more planning areas;
(d) declare that the whole or part of the area constituting the planning area shall cease to be a planning area or part thereof.
14. Director to prepare development plans. - Subject to the provisions of this Act and the rules made thereunder, the Director shall-
(a) prepare an existing land use map;
(b) prepare an interim development plan;
(c) prepare a development plan;
(d) prepare a sectoral plan;
(e) carry such surveys and inspections and obtain such pertinent reports from government departments, local authorities and public institutions as may be necessary for the preparation of the plans;
(f) perform such duties and functions as are supplemental, incidental, and consequential to any of the foregoing functions or as may be assigned by the State Government for the purpose of carrying out the provisions of this Act.
15. Existing land use maps. - (1 )The Director shall carryout the survey and prepare an existing land use map and, forthwith publish the same in such manner as may be prescribed together with public notice of the preparation of the map and of the place or places where the copies may be inspected, inviting objections and suggestions in writing from any person with respect thereto within thirty days from the date of publication of such notice.
(2) After the expiry of the period specified in the notice published under sub-section (1), the Director may, after allowing a reasonable opportunity of being heard to all such persons who have filed the objections or suggestions, make such modifications therein as may be considered desirable.
(3) As soon as may be after the map is adopted with or without modifications the Director shall publish a public notice of the adoption of the map and the place or places where the copies of the same may be inspected.
(4) A copy of the notice shall also be published in the Official Gazette and it shall be conclusive evidence of the fact that the map has been duly prepared and adopted.
16. Freezing of land use. - On the publication of the existing land use map under section 15-
(a) no person shall institute or change the use of any land or carry out any development of land for any purpose other than that indicated in the existing land use map without the permission in writing of the Director;
Provided that the Director shall not refuse permission if the change is for the purpose of agriculture;
(b) no local authority or any officer or other authority shall, not withstanding anything contained in any other law for the time being in force, grant permission for the change in use of land otherwise than as indicated in the existing land use map without the permission in writing of the Director;
[(c) no Registrar or the Sub-Registrar, appointed under the Indian Registration Act, 1908, shall, in any planning area constituted under section 13, register any deed or document of transfer of any sub-division of land by way of sale, gift, exchange, lease or mortgage with possession, unless the sub-division of land is duly approved by the Director, subject to such rules as may be framed in this behalf by the State Government:]
Provided that the Registrar or the Sub-Registrar may register any transfer,-
(i) where the land is owned by a person and the transfer is made without involving any further divisions;
(ii) where the partition/sub-division of land is made in a Joint Hindu Family;
(iii) where the lease is made in relation to a part or whole of a building;
(iv) where the mortgage is made for procuring the loans for construction or improvements over the land either from the Government or from any other financial institution constituted or established under any law for the time being in force or recognised by the State Government.
17. Interim development plans. - As soon as may be, after the declaration of a planning area, the Director shall, within such time as may be necessary, prepare, after consultation with local authorities concerned, if any, and submit to the State Government an interim development plan for the planning area or any of its parts and such other area or areas contiguous or adjacent to the planning areas as the State Government may direct to be included in the interim development plan.
(2) The interim development plan shall-
(a) indicate broadly the land use proposed in the planning area;
(b) allocate broadly areas or sector of land for-
(i) residential, industrial, commercial or agricultural purposes,
(ii) open spaces, parks and gardens, green belts, zoological gardens and play-grounds,
(iii) public institutions and offices,
(iv) such special purposes as the Director may deem fit;
(c) lay down the pattern of National and State Highways connecting the planning area with the rest of the region, ring roads, arterial roads and the major roads within the planning areas;
(d) provide for the location of airports, railway stations, bus termini and indicate the proposed extension and development of railways and canals;
(e) make proposals for general land scaping and preservation of natural areas;
(f) project the requirement of the planning area of such amenities and utilities as water, drainage, electricity and suggest their fulfilment;
(g) propose broad based regulations for sectoral development, by way of guide-lines, within each sector of the location, height, size of buildings and structures, open spaces, court-yards and the use to which such buildings and structures and land may be put;
(h) lay down the board-based traffic circulation patterns in a city;
(i) suggest architectural control features, elevation and frontage of buildings and structures;
(j) indicate measures for flood control, prevention of air and water pollution, disposal of garbage and general environmental control.
(3) Subject to provisions of the rules made under this Act for regulating the form and contents of the interim development plan any such plan shall include such maps and such descriptive matter as may be necessary to explain and illustrate the proposals in the interim development plan.
(4) As soon as may be, after the submission of the interim development plan, under sub-section (1) the State Government may either approve the interim development plan or may approve it with such modification as it may consider necessary.
(5) The State Government shall publish the interim development plan as approved under sub-section (4) in the Official Gazette. The interim development plan shall come into operation from the date of its publication in the Official Gazette and shall be binding on all local authorities functioning within the planning areas.
18. Development plan. - A development plan shall -
(a) indicate broadly the land use proposed in the planning areas;
(b) allocate broadly areas or sector of land for,-
(c) lay down the pattern of National and State Highways connecting the planning area with the rest of the region, ring roads, arterial roads, and the major roads within the planning area;
(d) provide for the location of airports, railway stations, bus termini and indicate the proposed extension and development of railways;
(g) propose broad-based regulations for sectoral development, by way of guide-line, within each sector of the location, height, size of buildings and structures, open spaces, court-yards and the use to which such buildings and structures and land may be put;
(h) lay down the broad-based traffic circulation patterns in a city;
19. Publication of draft development plan. - (1) The Director shall forthwith publish the draft development plan prepared under section 18 in such manner as may be prescribed together with a notice of the preparation of the draft development plan and the place or places where the copies may be inspected, inviting objections and suggestions in writing from any person with respect thereto, within thirty days from the date of publication of such notice Such notice shall specify in regard to the draff development plan the following particulars, namely:-
(i) the existing land use maps;
(ii) a narrative report, supported by maps and charts, explaining the provisions of the draft development plan;
(iii) the phasing of implementation of the draft development plan as suggested by the Director,
(iv) the provisions for enforcing the draft development plan and stating the manner in which permission to development may be obtained;
(v) an approximate estimate of the cost of land acquisition for public purposes and the cost of works involved in the implementation of the plan.
(2) The Director shall, not later than ninety days after the date of expiry of the notice period under sub-section (1), consider all the objections and suggestions as may be received within the period specified in the notice under sub-section (1)and shall, after giving reasonable opportunity to all persons affected thereby of being heard, make such modifications in the draft development plans as he may consider necessary, and submit, not later than six months after the publication of the draft development plan, the plan so modified, to the State Government for approval together with all connected documents, plans, maps and charts.
20. Sanction of development plans. - (1) As soon as may be after the submission of the development plan under section 19 the State Government may either approve the development plan or may approve it with such modifications as it may consider necessary or may return it to the Director to modify the same or to prepare a fresh plan in accordance with such directions as it may issue in this behalf.
(2) Where the State Government approves the development plan with modifications, the State Government shall, by a notice, published in the Official Gazette, invite objections and suggestions in respect of such modifications within a period of not less than thirty days from the date of publication of the notice in the Official Gazette.
(3) After considering objections and suggestions and after giving a hearing to the persons desirous of being heard the State Government may confirm the modification in the development plan.
(4) The State Government shall publish the development plan as approved, under the foregoing provisions in the Official Gazette and shall along with the plan publish a public notice, in such manner as may be prescribed, of the approval of the development plan and the place or places where the copies of the approved development plan may be inspected.
(5) The development plan shall come into operation from the date of publication thereof in the Official Gazette and as from such date shall be binding on all Development Authorities constituted under this Act and all local authorities functioning within the planning area.
(6) After the coming into operation of the development plan, the interim development plan shall stand modified or altered to the extent the proposals in the development plan are at variance with the interim development plan.
Sectoral Plan
21. Director to prepare sectoral plan. - The Director may, on his own motion, at any time after the publication of the development plan, or thereafter if so required by the State Government shall, within six months of such requisition, prepare a sectoral plan.
22. Contents of sectoral plan. - (1) The sectoral plan shall enlarge the details of land use as indicated in the development plan and shall -
(a) indicate the land liable to acquisition for public purpose or the purposes of the Union Government, the State Government, the Town and Country Development Authority, the Special Area Development Authority, the local authority or any other authority established by or under any enactment for the time being in force.
Provided that no land shall be so designated unless the acquisition proceedings are likely to be completed within ten years of the preparation of the plan;
(b) define in detail and provide for areas reserved for agriculture, public and semi-public open spaces, parks, playgrounds, gardens, recreational areas, green belts and natural reserves;
(c) allocate in detail areas or sectors for residential, commercial, industrial, agricultural and other purposes;
(d) define and provide for the complete road and street pattern for the present and in the future and indicate the traffic circulation;
(e) lay down in detail the projected road and street improvement;
(f) indicate and provide for areas reserved for public buildings, institutions and civic developments;
(g) assess, make projections for and provide for the future requirements of amenities, services and utilities such as municipal, transport, electricity, water and drainage;
(h) prescribe in detail the sectoral regulations for each sector, with a view to facilitating on individual layout and regulating the location, height, number of storeys and the size of buildings and other structures, the size of the court-yards, courts and other open spaces and the use of the buildings, structures and land;
(i) define areas which have been badly laid out or areas which have developed so as to form slums, and provide for their proper development and/or relocation;
(j) designate areas for future development and expansion;
(k) indicate the phasing of the programme of development.
(2) The sectoral plan may and if possible shall, indicate -
(a) control over architectural features; elevation and frontage of buildings and structures; and
(b) the details of development of specific areas for housing, shopping centres, industrial areas, educational and cultural institutions and civic centres.
23. Provisions of sections 19 and 20 to apply to sectoral plan. - The provisions of sections 19 and 20 shall apply for the preparation, publication, approval and operation of sectoral plan as they apply in respect of the development plan.
24. Review of development plan and sectoral plan. - The Director may on his own motion or if so required by the State Government shall, at any time after the sectoral plan has come into operation, undertake a review and evaluation of the development plan and sectoral plan.
(2) The foregoing provisions of sections 19, 20 and 23 shall, so far as may be apply to the modification under sub-section (1) as those provisions apply in relation to the preparation, publication and approval of a development plan or a sectoral plan.
Control of Development And Use of Land
25. Director to control land use. - The overall control of development and the use of land in the planning area shall, as from the date of publication in the Official Gazette of a notification by the State Government, vest in the Director.
26. Conformity with development plan. - (1) After coming into force of the development plan, the use and development of land shall conform to the provisions of the development plan:
Provided that the Director may, at his discretion, permit the continued use of land for the purpose for which it was being used at the time of the coming into Operation of the development plan.
Provided further that such permission shall not be granted for a period exceeding seven years from the date of coming into operation of the development plan.
27. Prohibition of development without permission. - After coming into operation of the development plan, no person shall change the use of any land or carry out any development of land without the permission in writing of the Director.
Provided that no such permission shall be necessary -
(a) for carrying out works for the maintenance, repair or alteration of any building which does not materially alter the external appearance of the building;
(b) for carrying out work for the improvement or maintenance of a highway, road or public street by the Union or State Government or an authority established under this Act or by a local authority having jurisdiction, provided that such maintenance or improvement does not change the road alignment contrary to the provisions of the development plan;
(c) for the purpose of inspecting, repairing or renewing any drain, sewers, mains, pipes, cables, telephone or other apparatus including the breaking open of any street or other land for that purpose;
(d) for the excavation or soil shaping in the interest of agriculture;
(e) for restoration of land to its normal use where land has been used temporarily for any other purposes.
(f) for use for any purpose incidental to the use of building for human habitation, or any other building or land attached to such buildings;
(g) for the construction of a road intended to give access to land solely for agricultural purposes
28. Development undertaken on behalf of Union or State Government. - (1) When the Union Government or the State Government intends to carry out development of any land for the purpose of its departments or offices or authorities, the officer-in-charge thereof shall inform in writing to the Director the intention of the Government to do so, giving full particulars thereof, accompanied by such documents and plans as may be prescribed at least thirty days before undertaking such development.
(2) Where the Director raises any objection to the proposed development on the ground that the development is not in conformity with the provisions of the development plan, the officer shall,-
(i) make necessary modification in the proposals for development to meet the objections raised by the Director, or
(ii) submit the proposal for development together with the objections raised by the Director to the State Government for decision:
Provided that where no modification is proposed by the Director within thirty days of the receipt of the proposed plan by the Government, the plan will be presumed to have been approved.
(3) The State Government, on receipt of the proposals for development together with the objections of the Director shall, approve the proposals with or without modifications or direct the officer to make such modifications in the proposals as it considers necessary in the circumstances.
(4) The decision of the State Government under sub-section (3) shall be final and binding.
29. Development by local authority or by any authority constituted under this Act. - Where a local authority or any authority specially constituted under this Act intends to carry out development on any land for the purpose of that authority, the procedure applicable to the Union or State Government, under section 28 shall, mutatis' mutandis, apply in respect of such authority.
30. Application for permission for development by others. - (1) Any person, not being the Union Government, State Government, a local authority or a special authority constituted under this Act intending to carry out any development on any land, shall make an application in writing to the Director for permission, in such form and containing such particulars and accompanied by such documents as may be prescribed.
(2) Such application shall also be accompanied by such fee as may be prescribed.
[30A. Exemption from development permission in rural areas falling within Planning or Special Area. - (1) Any person who owns land in rural areas, falling within Planning or Special Areas wherein neither Interim Development Plan nor Development Plan has been notified, shall be exempted from permission under this Act for the following development activities up to the limits as may be prescribed: -
(i) Residential activities such as farm-houses and residential houses up to three storeys, cattle shed, toilet, septic tank, kitchen, store, parking shed or garage and rain shelter;
(ii) Commercial activities such as basic commercial activities like shops of general merchandise, cobbler, barber, tailoring, fruit, vegetable, tea or sweet, eating places and dhabas, chemist and farm produce sale depot;
(iii) Service Industries such as cottage or house-hold, service industries like carpentry, knitting, weaving, blacksmith, goldsmith, atta-chakki with capacity up to five horse-power, water mill, agriculture equipments or machinery repair, electrical, electronic and house-hold appliances;
(iv) Public amenities such as public amenities like panchayat offices, schools, mahila mandals, yuvak mandals, community halls, post offices, dispensaries and clinics (including health, veterinary and Indian System of Medicines) information technology kiosks, patwar khanas, guard huts, anganwaries, electricity and telephone installations and connections, roads and paths, ropeways, water tanks, rain harvesting tanks, overhead or underground water tan.",['The Director of Town and Country Planning is responsible for carrying out the functions assigned under the act.'],6958,multifieldqa_en_e,en,,a8da0d2585bb54d571a2ab4a539b61eef56b5e271692be42,The Director of Town and Country Planning is responsible for carrying out the functions assigned under the act.,111
When did the Tevatron Collider Run II start and when is it expected to end?,"\section{INTRODUCTION}
The Tevatron Collider Run II started in March 2002 and is expected
to continue until the end of this decade. The Tevatron and the 
two detectors, CDF and D\O, have been performing  well in 2004,
each experiment is collecting data at the rate 
of $\approx$10 pb$^{-1}$ per week.
The total  luminosity accumulated by August 2004 is $\approx$500 pb$^{-1}$
per detector.
The rich physics program includes the
production and precision measurement of properties of  standard model (SM)
objects, as well as searches for phenomena beyond standard model.
In this brief review we focus on areas of most interest 
to the lattice community. We present
new results on the top quark mass
and their implication for the mass of the SM Higgs boson, 
on searches for the SM Higgs boson, on evidence for the $X(3872)$ state, 
on searches for pentaquarks, and on $b$ hadron properties.
All Run II results presented here are preliminary. 

\section{TOP QUARK MASS}

The experiments CDF and D\O\ published several direct  measurements of
the top quark pole mass, $\ensuremath{M_{\mathrm{top}}}$, 
based on Run I data (1992-1996).
The ``lepton $+$ jets'' channel yields the most precise determination of
$\ensuremath{M_{\mathrm{top}}}$. Recently, the
D\O\ collaboration published a new measurement~\cite{Mtop1-D0-l+j-new},
based on a powerful analysis technique yielding  greatly improved precision.
The differential probability 
that the measured variables in any event correspond to the signal
is calculated as a function of $\ensuremath{M_{\mathrm{top}}}$. 
The maximum in the product of the individual event probabilities 
provides the best estimate of $\ensuremath{M_{\mathrm{top}}}$.
The critical differences from previous analyses 
in the lepton $+$ jets decay channel lie in 
the assignment of more 
weight to events that are well measured or more likely to correspond to  
$t \bar t$ signal, 
and  the handling of the combinations of final-state objects
(lepton, jets, and imbalance in transverse momentum) 
and their identification with
top-quark decay products in an event. 
The new combined value for the top-quark mass from Run I is 
$\ensuremath{M_{\mathrm{top}}}  =  178.0\pm4.3~\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$.

In Run II, both collaborations  have been exploring several different techniques 
for $\ensuremath{M_{\mathrm{top}}}$
measurements. The best single CDF result comes from a dynamic likelihood method
(DLM). The method is similar to
the technique used in Ref.~\cite{Mtop1-D0-l+j-new}.
The result is $\ensuremath{M_{\mathrm{top}}} = 177.8^{+4.5}_{-5.0} (stat) \pm  6.2 (syst) ~\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$.
The joint likelihood of the selected events is shown in Fig. ~\ref{fig:cdf_tml}. 
The Run II goal is a 1\% uncertainty on $\ensuremath{M_{\mathrm{top}}}$. 




\begin{figure}[htb]
\vspace*{-5mm}
\includegraphics[height=5.8cm,width=8.1cm]  {data_22ev_likelihood.eps}
\vspace*{-1.2cm}
\caption{The joint likelihood of top candidates(CDF).}
\label{fig:cdf_tml}
\end{figure}




\section{SEARCH FOR SM HIGGS BOSON}


The constraints on the SM Higgs ($H$)  boson  mass from
published  measurements, updated to include the new D\O\ top mass
measurement~\cite{Mtop1-D0-l+j-new}, are
$M_H = 117 ^{+67}_{-45}~\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$, $M_H < 251~\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$ at 95\% C.L.
The  new most likely  value of $M_H$
is above the experimentally excluded range,
and sufficiently low for $H$ to be observed at the Tevatron.


\begin{figure}[htb]
\vspace*{-5mm}
\includegraphics[height=7.5cm,width=7.8cm]  {d0_wbb_fig_3_err.eps}
\vspace*{-1.1cm}
\caption{Distribution of the dijet
invariant mass for $W+2 b$-tagged jets  events,
compared to the expectation (D\O). 
}
\label{fig:d0_wbb_2tag}
\end{figure}



D\O\  has conducted a search for $H$ at $M_H < 140~\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$ 
in the production channel  
$p \bar{p} \rightarrow WH \rightarrow  e \nu b \bar{b}$. 
The experimental signature of  $WH \rightarrow e \nu b \bar{b}$
is a final state with 
one high $p_T$ electron, two  $b$ jets, and
large missing transverse energy  resulting from
the undetected neutrino.
The dominant backgrounds to $WH$ production
are  $W b \bar{b}$, $t \bar{t}$ and single-top production.
The distribution 
of the dijet mass for events with two $b$-tagged jets is shown in
Fig.~\ref{fig:d0_wbb_2tag}. 
Also shown is the  expected contribution ($0.06$ events)  
from the $b \bar{b}$ decay of a
SM Higgs boson with $M_H =$ 115 $\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$.
No events are observed in the  dijet mass window of 85--135  $\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$.
D\O\ sets a limit on the cross section
for $\sigma( p\bar{p} \rightarrow WH) \times B(H \rightarrow b \bar{b}) $
of 9.0 pb at the 95\% C.L.,  for a 115  $\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$ Higgs boson.
The results for mass points 105, 125, and 135 $\ensuremath{\mathrm{ Ge\kern -0.1em V }\kern -0.2em /c^2 }$
 are 11.0, 9.1 and 12.2 pb,  respectively.



\begin{figure}[htb]
\vspace*{-1.2cm}
\includegraphics[height=0.33\textheight,width=8.0cm]{whww_aps04_bw.eps}

\vspace*{-1.2cm}
\caption{95\% limits on the $H$ production (CDF).}
\label{fig:cdf_whww}
\end{figure}


CDF  has done  a similar search, allowing either an  electron or a muon  
in the final state.  Both groups have also searched for $H$ produced in
gluon-gluon fusion, with subsequent decay to a pair of $W$ bosons.
The CDF results for both channels  are shown in Fig.~\ref{fig:cdf_whww}. 



\section{THE STATE X(3872)}


\begin{figure}[htb]

\includegraphics[height=8.0cm,width=7.5cm]  {X3872cdfPRL1FullM.eps}
\vspace*{-1cm}
\caption{The $X(3872)$ signal (CDF).}
\label{fig:cdf_x}
\end{figure}




 The existence of the $X(3872)$ state discovered by 
the Belle Collaboration~\cite{Belle-X}
 has been confirmed 
 in $p \bar{p}$ collisions by  CDF~\cite{cdf-X} (see Fig.~\ref{fig:cdf_x})
and D\O~\cite{d0-X}.
 It is still unclear whether this particle is a $c\bar{c}$ state,
 or a more complex object.  When the data are separated according to
production and decay variables, D\O\  finds no significant
differences between the $X(3872)$ and
the $c \bar{c}$ state $\psi(2S)$.
CDF has analysed the ``lifetime'' distribution of the $X(3872)$ events in order to
quantify what fraction of this state arises from decay of $B$ hadrons, as opposed to
those produced promptly. The authors find that for the selected samples
28.3$\pm$1.0$(stat)\pm$0.7$(syst)$\% of $\psi(2S)$ candidates are from $b$ decays,
whereas 16.1$\pm$4.9$(stat)\pm$2.0$(syst)$\% of $X$ mesons arise from such decays.





\section{SEARCH FOR PENTAQUARKS}



\begin{figure}[htb]

\includegraphics[height=0.27\textheight,width=7.6cm]  {mpks_1stminbias.eps}
\vspace*{-1.2cm}

\caption{Invariant mass distribution of an identified proton and a $K^0_s$ candidate. (CDF)
}
\label{fig:pqtheta}
\end{figure}



\begin{figure}[htb]

\vspace*{-0.9cm}
\includegraphics[height=0.25\textheight,width=8.0cm]  {CM_xicst_cc_1.eps}
\vspace*{-1.2cm}
\caption{Invariant mass distribution of the $(\Xi^-,\pi^+)$ system. (CDF) 
}
\label{fig:pqxi}
\end{figure}


\begin{figure}[htb]
\vspace*{-0.9cm}

\includegraphics[height=0.25\textheight,width=7.6cm]  {theta_note_dstp_dedx_pt.eps}
\vspace*{-1.2cm}
\caption{Mass of the ($D^{*+}\bar p$) system. The arrow indicates the position of 
the $\Theta_c$ state (CDF).}
\label{fig:pqthetac}
\end{figure}



Following reports of evidence for exotic
baryons containing five quarks (pentaquarks), CDF has analysed 
its data for evidence of the following pentaquarks:
$\Theta^+$ ($uud\bar d \bar s$), doubly strange states 
$\Xi_{3/2}$, charmed states $\Theta_c$, and, most recently, 
a state $(udus\bar b)$, dubbed $R^+_s$, through its weak decay to $(J/\psi, p)$. 
With its excellent particle indentification and mass resolution,
CDF has a unique capability to search for  pentaquark states.
The signals of known states: $\phi$, $\Lambda$,
$\Lambda(1520)$, $K^*$, $\Xi$, 
compare favorably with those provided
by the authors of  the pentaquark evidence.
The group finds no evidence for pentaquark states, see Figs 
~\ref{fig:pqtheta},{\ref{fig:pqxi},\ref{fig:pqthetac}.
This can be interpreted as an indication that the pentaquark production 
in $p \bar p$ collisions is heavily suppressed compared to the conventional
hadron production, or as an evidence against the existence of pentaquarks.

\clearpage

\section{RECENT B PHYSICS RESULTS}


\subsection{Spectroscopy}

CDF has measured the mass of $b$ hadrons in exclusive $J/\psi$ channels.
The measurements of the $B_s$ and $\Lambda_b$ (Fig. \ref{fig:masslb})
masses are the current world's best.\\

$m(B^+)$ = 5279.10$\pm$0.41$(stat)\pm$0.36$(syst)$,

$m(B^0)$ = 5279.63$\pm$0.53$(stat)\pm$0.33$(syst)$,

$m(B_s)$ = 5366.01$\pm$0.73$(stat)\pm$0.33$(syst)$,

$m(\Lambda_b)$ = 5619.7$\pm$1.2$(stat)\pm$1.2$(syst)$ MeV/$c^2$.\\


\begin{figure}[htb]
\vspace*{-1mm}
\includegraphics[height=0.30\textheight,width=7.5cm]  {lambdav1c.eps}
\vspace*{-1cm}

\caption{The mass spectrum of $\Lambda_b$ candidates (CDF).}
\label{fig:masslb}
\end{figure}


D\O\ reports the first observation of the excited $B$ mesons 
$B_1$ and $B^*_2$ as two separate states in fully reconstructed
decays to $B^{(*)}\pi$. The mass of $B_1$ is measured to be
5724$\pm$4$\pm$7 MeV/c$^2$, and the mass difference $\Delta M$ between
$B^*_2$ and $B_1$ is 23.6$\pm$7.7$\pm$3.9 MeV/c$^2$
(Fig.  \ref{fig:d0_bexc}).

D\O\ observes semileptonic $B$ decays to narrow $D^{**}$ states,
the orbitally excited states  of the $D$ meson
seen as resonances in the $D^{*+}\pi^-$ invariant mass spectrum.
The $D^*$ mesons are reconstructed through the decay sequence 
$D^{*+} \rightarrow D^0\pi^+$, $D^0\rightarrow K^-\pi^+$.
The invariant mass  of oppositely charged $(D^*,\pi)$ pairs
is shown in Fig.  \ref{fig:d0_dstst}.
The mass peak between 2.4 and 2.5 GeV/$c^2$ can be interpreted as two merged 
narrow $D^{**}$ states, $D^0_1(2420)$ and $D^0_2(2460)$.
The combined branching fraction is 
$ {\cal B}(B\rightarrow D^0_1,D^0_2)\cdot {\cal B}(D^0_1,D^0_2\rightarrow D^{*+}\pi^-)=(0.280\pm0.021(stat)\pm0.088(syst)$\%. The systematic error includes the unknown phase between the
two resonances. Work is in progress on extracting the two Breit-Wigner
amplitudes.


\begin{figure}[htb]
\vspace*{-2mm}
\hspace*{-3mm}
\includegraphics[height=0.28\textheight,width=8.3cm]  {B08F02.eps}

\vspace*{-1cm}
\caption{Mass difference $\Delta M = M(B\pi)-M(B)$ for exclusive $B$ decays.
The background-subtracted signal is a sum of 
$B^*_1 \rightarrow B^* \pi$, $B^* \rightarrow B \gamma $ (open area)
and $B^*_2 \rightarrow B^*\pi$ $B^*\rightarrow B \gamma$ (lower peak in the shaded area)
and $B^*_2 \rightarrow B \pi$ (upper peak in the shaded area)  
(D\O).}
\label{fig:d0_bexc}
\end{figure}


\begin{figure}[htb]
\includegraphics[height=0.25\textheight,width=7.5cm]  {B05F03.eps}

\vspace*{-1cm}
\caption{The invariant mass distribution of
$(D^*,\pi)$ pairs, opposite sign (points) and same-sign (solid histogram).}
\label{fig:d0_dstst}
\end{figure}






\subsection{Lifetimes}


CDF and D\O\ have measured  lifetimes of $b$ hadrons through the exclusively
reconstructed decays $B^+ \rightarrow J/\psi K^+$, $B^0 \rightarrow J/\psi K^{*0}$,
$B_s \rightarrow J/\psi \phi$, 
and $\Lambda_b \rightarrow J/\psi \Lambda$
(Fig. \ref{fig:d0_lbctau}).
The latest results are:  \\



 $\tau(B^+)$=1.65 $\pm$ 0.08 $^{+0.096}_{-0.123}$  ps ~(D\O\ 2003),

 $\tau(B^+)$=1.662 $\pm$ 0.033  $\pm$ 0.008  ps ~(CDF),

 $\tau(B^0_d)$=1.473  $^{+0.052}_{-0.050}$ $\pm$ 0.023    ps ~(D\O).

 $\tau(B^0_d)$=1.539 $\pm$ 0.051  $\pm$ 0.008  ps ~(CDF),

 $\tau(B^0_s)$=1.444   $^{+0.098}_{-0.090}$ $\pm$ 0.020   ps ~(D\O),

 $\tau(B^0_s)$=1.369 $\pm$ 0.100 $\pm$ $^{+0.008}_{0.010}$  ps ~(CDF),


 $\tau(\Lambda_b)$=1.221 $^{+0.217}_{-0.179}$ $\pm$ 0.043  ps ~(D\O),


 $\tau(\Lambda_b)$=1.25 $\pm$ 0.26 $\pm$ 0.10  ps ~(CDF 2003).\\



The measured lifetimes correspond to the following lifetime ratios:\\

$\tau(B^+)/\tau(B^0_d)$   =  1.080$\pm$0.042     ~(CDF),
 
$\tau(B^0_s)/\tau(B^0_d)$ =  0.890$\pm$0.072    ~(CDF),

$\tau(B^0_s)/\tau(B^0_d)$ = 0.980$ ^{+0.075}_{-0.070}   \pm$0.003    ~(D\O),

$\tau(\Lambda_b)/\tau(B^0_d)$ =  0.874$ ^{+0.169}_{-0.142}   \pm$0.028    ~(D\O).\\



\begin{figure}[htb]
\includegraphics[height=0.3\textheight,width=8.2cm]  {d0_lbctau_B11F02.eps}
\vspace*{-1cm}

\caption{ Fit projection on  $c\tau$ for the $\Lambda_b$ candidates.  (D\O)}
\label{fig:d0_lbctau}
\end{figure}


The $B_s$ lifetime measurements listed above are results of
a single-lifetime fit to data, integrated over the decay angles.
Because  of the presence of  final
states  common to ${B_s^0}$\ and its charge conjugate ${\overline{B}_s^0}$,
the two meson states   are expected
to mix in such a way that the two CP  eigenstates may have a relatively
large lifetime difference.
It is possible to
separate the two CP components of ${B_s^0 \rightarrow J/\psi \phi}$\ and thus to measure the
lifetime difference by studying the time evolution of the
polarization states of the vector mesons in the final state.
CDF has carried out a combined analysis of $B_s$ lifetimes 
and polarization amplitudes. The results for the lifetimes of the
low mass (CP even) and high mass (CP odd) eigenstates, and the relative 
width difference are:\\

 $\tau_L = 1.05 ^{+0.16}_{-0.13} \pm 0.02$ ~ps,
 
 $\tau_H = 2.07 ^{+0.58}_{-0.46} \pm 0.03$ ~ps,

 $\Delta \Gamma /\overline \Gamma   = 0.65 ^{+0.25}_{-0.33} \pm 0.01$.\\

Figure \ref{fig:cdf_dg} shows  the scan of the likelihood function 
for $\Delta \Gamma /\overline \Gamma$.
Pseudoexperiments tossed with $\Delta \Gamma /\overline \Gamma =0$
yield the betting odds for observing the above results at
1/315. For $\Delta \Gamma /\overline \Gamma = 0.12$ (SM prediction,
which has recently been updated to 0.14$\pm$0.05~\cite{dg_un}) the betting odds are
1/84.

\begin{figure}[htb]
\vspace*{-1mm}
\includegraphics[height=0.3\textheight,width=8.2cm]  {cdf_scan-dg-un.eps}

\vspace*{-1cm}
\caption{Scan of the likelihood function 
for $\Delta \Gamma /\overline \Gamma$ (CDF).
}
\label{fig:cdf_dg}
\end{figure}




D\O\ has used a novel technique to  measure the lifetime ratio
of the charged and neutral $B$ mesons, exploiting the large
semileptonic sample. $B$ hadrons were reconstructed in the channels
$B\rightarrow \mu^+ \nu D^*(2010)^-X$, which are dominated by $B^0$ decays, 
and  $B\rightarrow \mu^+ \nu D^0X$, which are dominated by $B^+$ decays.
The lifetime ratio was
obtained from the variation of the ratio of the number of events in these two
processes at different decay lengths.
The result is \\


$\tau(B^+)/\tau(B^0_d)$   =  1.093$\pm$0.021$\pm$0.022.      ~(D\O)




\subsection{Towards $B_s$ mixing}

Measurement of the $B_s$ oscillation frequency via ${B_s^0}$ -${\overline{B}_s^0}$ ~mixing
will provide an important constraint on the CKM matrix. The oscillation
frequency is proportional to the mass difference between the mass eigenstates,
$\Delta m_s$, and is related to the CKM matrix through 
$\Delta m_s \propto |V_{tb}V_{ts}|$. When combined with the
$B_d$ mass difference, $\Delta m_d$ it helps in extraction of $|V_{td}|$,
and thereby the CP violating phase. 

As a benchmark for future $B_s$ oscillation measurement, both groups
study  $B_d$ mixing, gaining an understanding of the different components
of a $B$ mixing analysis (sample composition, flavor tagging, vertexing,
asymmetry fitting). For a sample of partially reconstructed decays
$B\rightarrow D^*(2010)^+\mu^-X$, D\O\ obtains 
$\Delta m_d = 0.506 \pm 0.055 (stat) \pm  0.049 (syst))$ ps$^{-1}$ and
$\Delta m_d = 0.488 \pm 0.066 (stat) \pm  0.044 (syst))$ ps$^{-1}$
when employing  opposite side muon tagging and the same side tagging,
respectively.

The CDF result for semileptonic channels is
$\Delta m_d = 0.536 \pm 0.037 (stat) \pm  0.009 (s.c.) \pm 0.015 (syst)$ ps$^{-1}$.
CDF also reports a result on $B$ oscillations using fully reconstructed
decays:
$\Delta m_d = 0.526 \pm 0.056 (stat) \pm  0.005 (syst))$ ps$^{-1}$.

Reconstructing $B_s$ decays into different final states is another
important
 step in the ${B_s^0}$ -${\overline{B}_s^0}$ ~mixing analysis.
Thanks to the  large muon and tracking coverage,   D\O\ is accumulating
a  high statistics sample of semileptonic $B_s$ decays.
D\O\ reconstructs the $B_s \rightarrow D^+_s \mu^- X$ decays, with
$D^+_s \rightarrow \phi \pi^+ $ and
$D^+_s \rightarrow K^* K^+ $,
at a rate of $\approx$ 40(25) events per pb$^{-1}$,  respectively.
Figure \ref{fig:d0_bsdsphipi} shows the mass distribution of the
$D^+_s \rightarrow \phi \pi$ candidates.


\begin{figure}[htb]
\vspace*{-5mm}
\includegraphics[height=0.3\textheight,width=8.0cm]  {blds-250.eps}
\vspace*{-1.2cm}
\caption{  $D^+_s \rightarrow \phi \pi^+$  signal. (D\O)}
\label{fig:d0_bsdsphipi}
\end{figure}


\begin{figure}[htb]
\vspace*{-10mm}
\hspace*{-4mm}
\includegraphics[height=0.35\textheight,width=7.9cm]  {cdf_Bs-DsPi-PhiPi.eps}

\vspace*{-1.0cm}
\caption{ $B_s \rightarrow D_s \pi$, $D_s \rightarrow \phi \pi$  signal.  (CDF)}
\label{fig:cdf_bsdsphipi}
\end{figure}


CDF has clean signals for fully hadronic, flavor-specific  $B_s$ decays,
providing the best sensitivity to $B_s$ oscillations at high
$\Delta m_s$. Figure \ref{fig:cdf_bsdsphipi} shows the signal for
the best channel, $B_s \rightarrow D_s \pi$, $D_s \rightarrow \phi \pi$.

\clearpage


\subsection{Rare decays}

The purely leptonic decays $B_{d,s}^0 \rightarrow \mu^+
\mu^-$ are flavor-changing neutral current (FCNC) processes.
In the standard model, these decays are forbidden at the tree level and
proceed at a very low rate through higher-order diagrams.
The latest SM prediction~\cite{sm_ref3}
is ${\cal B}(B^0_s \rightarrow \mu^+ \mu^-)=(3.42\pm 0.54)\times
10^{-9}$, where the error is dominated by non-perturbative uncertainties. The
leptonic branching fraction of the $B_d^0$ decay is suppressed by CKM matrix elements $|V_{td}/V_{ts}|^2$
leading to a predicted SM branching fraction of $(1.00\pm0.14)\times 10^{-10}$.
The best published experimental bound (Fig.~\ref{fig:cdf_bsmumu})
 for the branching fraction
of $B^0_s$ $(B^0_d)$ is presently
${\cal B}(B^0_s \, (B^0_d) \rightarrow \mu^+\mu^-)<7.5\times 10^{-7}\, 
(1.9\times 10^{-7})$ at the 95\% C.L.~\cite{cdfII}.
The decay amplitude of $B^0_{d,s} \rightarrow \mu^+ \mu^-$ can be
significantly enhanced in some extensions of the SM. 

\begin{figure}[htb]
\includegraphics[height=8.3cm,width=7.9cm]  {cdfbsmumu_results_prl.eps}

\vspace*{-1cm}
\caption{Invariant mass for the events passing all requirements. (CDF)}
\label{fig:cdf_bsmumu}
\end{figure}


Assuming no contributions 
from the decay $B^0_d\rightarrow \mu^+\mu^-$ in the signal region,
D\O\  finds the conservative upper limit on the branching fraction 
to be ${\cal B}(B^0_s \rightarrow \mu^+ \mu^-) \leq 4.6\times 10^{-7}$ 
at the 95\% C.L. (Fig.~\ref{fig:d0_bsmumu}).






\begin{figure}[htb]
\includegraphics[height=5.0cm,width=8.0cm]  {B06F03.eps}
\vspace*{-1cm}
\caption{Invariant mass for the events  passing all requirements. (D\O)}
\label{fig:d0_bsmumu}
\end{figure}

",['The Tevatron Collider Run II started in March 2002 and is expected to continue until the end of this decade.'],2431,multifieldqa_en_e,en,,d70ca68d05af951cd8ff2052095597d1b4dca557bfb0b40b,The Tevatron Collider Run II started in March 2002 and is expected to continue until the end of this decade.,108
What happens to Ngotho after he attacks Jacobo at a workers' strike?,"Weep Not, Child is a 1964 novel by Kenyan author Ngũgĩ wa Thiong'o. It was his first novel, published in 1964 under the name James Ngugi. It was among the African Writers Series. It was the first English language|English novel to be published by an East African. Thiong'o's works deal with the relationship between Africans and white settlers in colonial Kenya, and are heavily critical of colonial rule. Specifically, Weep Not, Child deals with the Mau Mau Uprising, and ""the bewildering dispossession of an entire people from their ancestral land."" Ngũgĩ wrote the novel while he was a student at Makerere University.

The book is divided into two parts and eighteen chapters. Part one deals mostly with the education of Njoroge, while part two deals with the rising Mau Mau movement.

Plot summary

Njoroge, a little boy, is urged to attend school by his mother. He is the first one of his family able to go to school. His family lives on the land of Jacobo, an African made rich by his dealings with white settlers, namely Mr. Howlands, the most powerful land owner in the area. Njoroge's brother Kamau works as an apprentice to a carpenter, while Boro, the eldest living son, is troubled by his experiences while in forced service during World War II, including witnessing the death of his elder brother. Ngotho, Njoroge's father and a respected man in the surrounding area, tends Mr. Howlands' crops, but is motivated by his passion to preserve his ancestral land, rather than for any compensation or loyalty.

One day, black workers call for a strike to obtain higher wages. Ngotho is ambivalent about participating in the strike because he fears he will lose his job. However, he decides to go to the gathering, even though his two wives do not agree. At the demonstration, there are calls for higher wages. Suddenly, the white police inspector brings Jacobo to the gathering to pacify the native people. Jacobo tries to put an end to the strike. Ngotho attacks Jacobo, and the result is a riot where two people are killed. Jacobo survives and swears revenge. Ngotho loses his job and Njoroge’s family is forced to move. Njoroge’s brothers fund his education and seem to lose respect for their father.

Mwihaki, Jacobo's daughter and Njoroge's best friend, enters a girls' only boarding school, leaving Njoroge relatively alone. He reflects upon her leaving, and realizes that he was embarrassed by his father's actions towards Jacobo. For this reason, Njoroge is not upset by her exit and their separation. Njoroge switches to another school.

For a time, everyone's attention is focused on the upcoming trial of Jomo Kenyatta – a revered leader of the movement. Many blacks think that he is going to bring forth Kenya’s independence. But Jomo loses the trial and is imprisoned. This results in further protests and greater suppression of the black population.

Jacobo and a white landowner, Mr. Howlands, fight against the rising activities of the Mau Mau, an organization striving for Kenyan economic, political, and cultural independence. Jacobo accuses Ngotho of being the leader of the Mau Mau and tries to imprison the whole family. Meanwhile, the situation in the country is deteriorating. Six black men are taken out of their houses and executed in the woods.

One day Njoroge meets Mwihaki again, who has returned from boarding school. Although Njoroge had planned to avoid her due to the conflict between their fathers, their friendship is unaffected. Njoroge passes an important exam that allows him to advance to High School. His village is proud of him, and collects money to pay Njoroge's High School tuition.

Several months later, Jacobo is murdered in his office by a member of the Mau Mau. Mr. Howlands has Njoroge removed from school for questioning. Both father and son are brutally beaten before release and Ngotho is left barely alive. Although there doesn't seem to be a connection between Njoroge's family and the murder, it is eventually revealed that Njoroge's brothers are behind the assassination, and that Boro is the real leader of the Mau Mau. Ngotho soon dies from his injuries and Njoroge finds out that his father was protecting his brothers. Kamau has been imprisoned for life. Only Njoroge and his two mothers remain free, and Njoroge is left as the sole provider of his two mothers. Njoroge fears that he cannot make ends meet; he gives up hope of continuing in school and loses faith in God.

Njoroge asks Mwihaki's for support, but she is angry because of her father’s death. When he finally pledges his love to her, she refuses to leave with him, realizing her obligation to Kenya and her mother. Njoroge decides to leave town and makes an attempt at suicide; however, he fails when his mothers find him before he is able to hang himself. The novel closes with Njoroge feeling hopeless, and ashamed of cowardice.

Characters in Weep Not, Child
 Njoroge: the main character of the book whose main goal throughout the book is to become as educated as possible.
 Ngotho: Njoroge's father. He works for Mr.Howlands and is respected by him until he attacks Jacobo at a workers strike. He is fired and the family is forced to move to another section of the country. Over the course of the book his position as the central power of the family weakened, to the point where his self-realization that he has spent his whole life waiting for the prophecy (that proclaims the blacks will be returned their land) to come true rather than fighting for Kenyan independence, leads to his depression.
 Nyokabi and Njeri: the two wives of Ngotho. Njeri is Ngotho's first wife, and mother of Boro, Kamau, and Kori. Nyokabi is his second wife, and the mother of Njoroge and Mwangi.
 Njoroge has four brothers: Boro, Kamau, Kori and Mwangi (who is Njoroge's only full brother, who died in World War II).
 Boro: Son of Njeri who fights for the Allies in World War II. Upon returning his anger against the colonial government is compounded by their confiscation of the his land. Boro's anger and position as eldest son leads him to question and ridicule Ngotho, which eventually defeats their father's will (upon realizing his life was wasted waiting and not acting). It is eventually revealed that Boro is the leader of the Mau Mau (earlier alluded to as ""entering politics"") and murders Mr.Howlands. He is caught by police immediately after and is scheduled to be executed by the book's end. It is highly likely that it is also Boro who kills Jacobo.
 Mwihaki: Njoroge's best friend (and later develops into his love interest). Daughter of Jacobo. When it is revealed that his family killed Jacobo (most likely Boro), Mwihaki distances herself from Njoroge, asking for time to mourn her father and care for her mother.
 Jacobo: Mwihaki's father and an important landowner. Chief of the village.
 Mr. Howlands: A white settler who emigrated to colonial Kenya and now owns a farm made up of land that originally belonged to Ngotho's ancestors. Has three children: Peter who died in World War II before the book's beginning, a daughter who becomes a missionary, and Stephen who met Njoroge while the two were in high school.

Themes and motifs
Weep Not, Child integrates Gikuyu mythology and the ideology of nationalism that serves as catalyst for much of the novel's action. The novel explores the negative aspects of colonial rule over Kenya. Njoroge's aspiration to attend university is frustrated by both the violence of the Mau Mau rebels and the violent response of the colonial government. This disappointment leads to his alienation from his family and ultimately his suicide attempt.

The novel also ponders the role of saviours and salvation. The author notes in his The River Between: ""Salvation shall come from the hills. From the blood that flows in me, I say from the same tree, a son shall rise. And his duty shall be to lead and save the people."" Jomo Kenyatta, the first prime minister of Kenya, is immortalised in Weep Not, Child. The author says, ""Jomo had been his (Ngotho's) hope. Ngotho had come to think that it was Jomo who would drive away the white man. To him, Jomo stood for custom and traditions purified by grace of learning and much travel."" Njoroge comes to view Jomo as a messiah who will win the struggle against the colonial government.

See also

Things Fall Apart
Death and the King's Horseman

References

External links
Official homepage of Ngũgĩ wa Thiong'o
BBC profile of Ngũgĩ wa Thiong'o
Weep Not, Child at Google Books

British Empire in fiction
Novels set in colonial Africa
Historical novels
Kenyan English-language novels
Novels by Ngũgĩ wa Thiong'o
Novels set in Kenya
1964 novels
Heinemann (publisher) books
Postcolonial novels
African Writers Series
1964 debut novels","[""After attacking Jacobo at a workers' strike, Ngotho loses his job and Njoroge's family is forced to move.""]",1504,multifieldqa_en_e,en,,0ecdb8439f1360140995c6f5f6cc99c38cebb9216d1395e4,"After attacking Jacobo at a workers' strike, Ngotho loses his job and Njoroge's family is forced to move.",105
Which orders did Mufti-e-Azam-e-Hind receive Khilafat from?,"Ghousul Waqt, Mufti-e-Azam-e-Hind (radi Allahu anhu) was born on Monday, 22nd of Zil Hijjah 1310 AH (18 July 1892) in the most beautiful city of Bareilly Shareef, India. It was in this very city that his illustrious father, the Mujaddid (Reviver) of Islam, Imam-e-Ahle Sunnat, A'la Hazrat, Ash Shah Imam Ahmed Raza Khan Al Qaderi (radi Allahu anhu) was born (1856 - 1921).
At the time of the birth of Ghousul Waqt, Mufti-e-Azam-e-Hind (radi Allahu anhu), his distinguished father, was in Mahrerah Shareef, one of the great spiritual centers of the Sunni World. On that very night, Sayyiduna A'la Hazrat (radi Allahu anhu) dreamt that he had been blessed with a son and in his dream he named his son ""Aale Rahmaan"". Hazrat Makhdoom Shah Abul Hussain Ahmadi Noori (radi Allahu anhu), one of the great personalities of Mahrerah Shareef, named the child ""Abul Barkaat Muhiy'yuddeen Jilani"".
Mufti-e-Azam-e-Hind (radi Allahu anhu) was later named ""Mustapha Raza Khan"". His Aqiqa was done on the name of ""Muhammad"", which was the tradition of the family.
Upon the birth of Ghousul Waqt, Mufti-e-Azam-e-Hind (radi Allahu anhu) Sayyiduna Shah Abul Hussain Ahmadi Noori (radi Allahu anhu) told A'la Hazrat (radi Allahu anhu), ""Maulana! When I come to Bareilly Shareef, then I will definitely see this child. He is a very blessed child.""
As promised, when Sayyiduna Abul Hussain Ahmadi Noori (radi Allahu anhu) went to Bareilly Shareef, he immediately summoned to see Mufti-e-Azam-e-Hind (radi Allahu anhu) who was only six (6) months old. Sayyiduna Noori Mia (radi Allahu anhu), as he was also famously known, congratulated A'la Hazrat (radi Allahu anhu) and said, ""This child will be of great assistance to the Deen and through him the servants of Almighty Allah will gain great benefit. This child is a Wali. From his blessed sight thousands of stray Muslims will become firm on the Deen. He is a sea of blessings.""
On saying this, Sayyiduna Noori Mia (radi Allahu anhu) placed his blessed finger into the mouth of Mufti-e-Azam-e-Hind (radi Allahu anhu) and made him a Mureed. He also blessed him with I'jaazat and Khilafat at the same time. (Mufti Azam Hind Number, pg. 341). Not only did he receive Khilafat in the Qaderi Silsila (Order), but also in the Chishti, Nakshbandi, Suharwardi, and Madaari Orders. Mufti-e-Azam-e-Hind (radi Allahu anhu) also received Khilafat from his blessed father, A'la Hazrat, Ash Shah Imam Ahmed Raza Khan Al Qaderi (radi Allahu anhu).
Ghousul Waqt, Mufti-e-Azam-e-Hind (radi Allahu anhu) attained most of his early education from his illustrious family - from his father, A'la Hazrat, Ash Shah Imam Ahmed Raza Khan Al Qaderi (radi Allahu anhu) the Mujaddid of Islam, whose status and position even at that time cannot be explained in these few lines. He also studied Kitaabs under the guidance of Hazrat Moulana Haamid Raza Khan (his elder brother), Maulana Shah Rahm Ilahi Maglori and Maulana Sayed Basheer Ahmad Aligarhi and Maulana Zahurul Hussain Rampuri (radi Allahu anhum). He studied various branches of knowledge under the guidance of his most learned and blessed father, A'la Hazrat (radi Allahu anhu). He gained proficiency in the many branches of Islamic knowledge from among which are: Tafseer; Hadith; Fiqh; Laws of Jurisprudence; Sarf; Nahw; Tajweed; Conduct of Language; Philosophy; Logistics; Mathematics; History etc.; Arithmetic; Aqaid (Belief); Taasawwaf; Poetry; Debating; Sciences; etc.
Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu's) brilliance as an Islamic Scholar manifested itself when he was a still a youth, but overflowing with knowledge and wisdom. He wrote his first historical Fatawa (Islamic Ruling) when he was only 13 years old. It dealt with the topic of ""Raza'at"" - affinity between persons breast fed by the same woman. The following has been recorded with regards to this occasion.
Hazrat Maulana Zafrud'deen and Hazrat Maulana Sayed Abdur Rasheed (radi Allahu anhum) were at the Darul Ifta (Fatawa Department) at this stage. One day, Mufti-e-Azam-e-Hind (radi Allahu anhu) walked into the Darul Ifta and noticed that Hazrat Maulana Zafrud'deen (radi Allahu anhu) was writing a certain Fatawa. He was taking ""Fatawa Razvia"" from the shelf as his reference. On seeing this, Mufti-e-Azam-e-Hind (radi Allahu anhu) said, ""Are you relying on Fatawa Razvia to write an answer?"" Maulana Zafrud'deen (radi Allahu anhu) replied, ""Alright then, why don't you write the answer without looking."" Mufti-e-Azam-e-Hind (radi Allahu anhu) then wrote a powerful answer without any problem. This was the Fatawa concerning ""Raza'at"" - the very first Fatawa which he had written.
Sayyiduna A'la Hazrat (radi Allahu anhu) then signed the Fatawa. He also commanded Hafiz Yaqeenudeen (radi Allahu anhu) to make a stamp for Mufti-e-Azam-e-Hind (radi Allahu anhu) as a gift and said that it should read as follows: ""Abul Barkaat Muhiy'yuddeen Jilani Aale Rahmaan urf Mustapha Raza Khan.""
This incident took place in 1328 AH. After this incident Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) spent another 12 years writing Fatawas at the feet of A'la Hazrat (radi Allahu anhu). He was given this immense responsibility of issuing Fatawas even while A'la Hazrat (radi Allahu anhu) was in this physical world. He continued this trend until his last breath. The stamp which was given to him was mislaid during his second Hajj when his bags were lost.
Mufti-e-Azam-e-Hind (radi Allahu anhu) married the blessed daughter of his paternal uncle, Hazrat Muhammad Raza Khan (radi Allahu anhu). He had 6 daughters and one son, Hazrat Anwaar Raza (radi Allahu anhu), who passed away during childhood.
""Khuda Kheyr se Laaye Wo Din Bhi Noori, Madine ki Galiya Buhara Karoo me""
Tajedaare Ahle Sunnah, Taaje Wilayat Wa Karaamat, Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) went twice for Hajj - in 1905 and 1945. He performed his third Hajj in 1971.
Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) was the first person to go for Hajj without a photograph in his passport. He refused to take a photograph. Mufti-e-Azam-e-Hind (radi Allahu anhu) was allowed to go for Hajj without a photograph in his passport and without taking any vaccinations.
During his trip to Makkatul Mukarramah, Mufti-e-Azam-e-Hind (radi Allahu anhu), also had the opportunity of meeting those Ulema whom his father, Sayidduna A'la Hazrat (radi Allahu anhu), met during his visit to Haramain Sharifain. These great Ulema were from amongst the students of Sayed Yahya Almaan (radi Allahu anhu). A few of the Ulema that he met were Allamah Sayed Ameen Qutbi; Allamah Sayed Abbas Alawi and Allamah Sayed Noor Muhammad (radi Allahu anhum) - to mention just a few. They narrated many incidents which had taken place during Sayyiduna A'la Hazrat (radi Allahu anhu's) visit to Haramain Sharifain. They then requested Khilafat from Mufti-e-Azam-e-Hind, (radi Allahu anhu) which he bestowed upon them.
Tajedaare Ahle Sunnah, Taaje Wilayat Wa Karaamat, Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) was aware of the actual time of his Wisaal.
On the 6th of Muharram (1981) he said, ""All those who intended to become my Mureed but for some reason or the other could not come to me, I have made all of them Mureed and I have given their hands into the hand of Sayidduna Ghousul Azam (radi Allahu anhu).""
On the 12th of Muharram (1981) Hazrat said, ""All those who asked me to make Dua for them, I have made Dua for their Jaiz (permissible) intentions to be fulfilled. May Allah accept this Dua."" On this day he asked those that were present concerning date. They told him that it was the 12th of Muharram. On hearing this he became silent.
On the 13th of Muharram, he again asked concerning the date and the Mureedeen present said that it was Wednesday, the 13th of Muharram. On hearing this Mufti-e-Azam-e-Hind (radi Allahu anhu) said, ""Namaaz will be held at Nau Mahla Musjid"". Those present did not understand what he meant, but remained silent out of respect. After some time again Mufti-e-Azam-e-Hind (radi Allahu anhu) said, ""Did anybody tell you about the Namaaz. I will read Jumma Namaaz in Nau Mahla Masjid."" After some time Hazrat said, ""Did anybody say anything about the Fatiha."" Those present just gazed at each others faces and remained silent. Only later did they realise what Mufti-e-Azam-e-Hind (radi Allahu anhu) was implying. Hazrat was spiritally present for Jummah at the Nau Mahla Masjid! Mufti-e-Azam-e-Hind (radi Allahu anhu) was not only giving hope to the Mureedeen but also informing them of his Wisaal.
The shining star of A'la Hazrat, Ash Shah Imam Ahmed Raza Khan (radi Allahu anhu), the glitter and the hope for the hearts of millions throughout the world, the Mujaddid of the 15th Century, the Imam of his time, Huzoor Sayyidi Sarkaar Mufti-e- Azam-e-Hind (radi Allahu anhu) left the Aalame Duniya to Journey towards the Aalame Aakhira. It was 1.40 p.m. on the eve of the 14th of Muharram 1402 AH (1981).
""Chal diye tum Aankho me ashko ka darya chor kar, har jigar me dard apna meetha meetha chor kar""
Rawa Aankho se he Ashko ke Dhaare Mufti-e-Azam, Kaha Ho Be Saharo Ka Sahara Mufti-e-Azam""
On Friday, the 15th of Muharram, at 8. 00 a.m. the Ghusl of Mufti-e-Azam-e-Hind (radi Allahu anhu) took place. His nephew, Hazrat Maulana Rehan Raza Khan (radi Allahu anhu) performed the Wudhu. Hazrat Allamah Mufti Mohammed Akhtar Raza Khan Azhari performed the Ghusl. Sultan Ashraf Sahib used the jug to pour water. The following persons were present during the Ghusl : Hazrat Maulana Rehan Raza Khan (radi Allahu anhu), Hazrat Allamah Mufti Mohammed Akhtar Raza Khan, Sayed Mustaaq Ali, Maulana Sayed Muhammad Husain, Sayed Chaif Sahib, Maulana Naeemullah Khan Sahib Qibla, Maulana Abdul Hamid Palmer Razvi, Muhammad Esa of Mauritius, Ali Husain Sahib, Hajji Abdul Ghaffar, Qari Amaanat Rasool Sahib and a few other Mureeds and family members.
Hazrat Allamah Mufti Mohammed Akhtar Raza Khan Azhari and Hazrat Maulana Rehan Raza Khan (radi Allahu anhu) have stated that at the time of the Ghusl Shareef of Mufti-e-Azam-e-Hind (radi Allahu anhu) the Chaadar mistakenly moved a little. Immediately, Mufti-e-Azam-e-Hind (radi Allahu anhu) held the Chaadar between his two fingers and covered the area that the Chaadar exposed. Those present thought that the Chaadar had just got caught between Mufti-e-Azam-e-Hind (radi Allahu anhu's) fingers. They tried to remove the Chaadar from between his fingers but it would not move. The first person to notice this Karaamat was Hazrat Allamah Mohammed Akhtar Raza Khan Azhari. He showed this to everyone. Mufti-e-Azam-e-Hind (radi Allahu anhu's) fingers did not move until the area was properly covered.
""Zinda hojate he jo marte he haq ke Naam par, Allah, Allah Maut ko kis ne Masiha Kardiya""
""Janaaze se utha kar haath Pakri Chaadare Aqdas, He too Zinda He ye Zinda Karaamat Mufti e Azam""
As he had wished, the Janaza Salaah of Mufti-e-Azam-e-Hind (radi Allahu anhu) was performed by Maulana Sayed Mukhtar Ashraf Jilani at the Islamia Inter College grounds in Bareilly Shareef. Two and a half million (2 500 000) Muslims attended his Janazah Salaah. Mufti-e-Azam-e-Hind (radi Allahu anhu) is buried on the left-hand-side of Sayyiduna A'la Hazrat (radi Allahu anhu). Those who lowered Mufti-e-Azam-e-Hind (radi Allahu anhu) in his Qabr Shareef have stated that they were continously wiping out perspiration from the forehead of Mufti-e-Azam-e-Hind (radi Allahu anhu) right up to the last minute.
""Maangne Waala sub kuch paaye rota aaye hasta Jaaye"", ""Ye He Unki Adna Karamat Mufti Azam Zinda Baad""
Wealth, presidency, minister ship, worldly satisfaction and happiness can be given to a person by anyone, but such people do not have the spiritual insight to give tranquility to a disturbed heart and they cannot put a smile onto the face of a depressed person. But Tajedaare Ahle Sunnah, Taaje Wilayat Wa Karaamat, Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) gave both the treasures of the physical world and the spiritual worlds to those in need. To be his servant was not less than kingship. Every day hundreds and thousands of people in need of spiritual, physical and academic needs would come to him and each one of them returned with complete satisfaction.
""Jhuki Hai Gardane Dar Par Tumhare, Taaj Waalo Ki, Mere Aqa Mere Maula Wo Taajul Auliyah Tum Ho""
Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) is that light of such an illustrious family whose radiance reflected itself in his character and manners that he displayed - such qualities that very few would be able to reach perfection. His character was the true embodiment of the Sunnah of Sayyiduna Rasulullah (sallal laahu alaihi wasallam). He shone like a star in the darkness of the night.
Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) possessed great heights of good character, moral standards, kindness, sincerity, love and humbleness. He never refused the invitation of any poor Muslim. He always stayed away from those who were very wealthy and lavish. He was the possessor of great moral and ethical values.
It is stated that once Akbar Ali Khan, a Governor of U.P., came to visit Mufti-e-Azam-e-Hind (radi Allahu anhu). Mufti-e-Azam-e-Hind (radi Allahu anhu) did not meet him but left to a place called Puraana Shahar (Old City) to visit a poor Sunni Muslim who was very ill and at the doorstep of death.
In another occasion, Fakhruddeen Ali Ahmad, the President of a Political Party, came to visit Mufti-e-Azam-e-Hind (radi Allahu anhu) but was refused this opportunity. Many other proud ministers had also come to meet Mufti-e-Azam-e-Hind (radi Allahu anhu) but met with the same fate. This was due to his extreme dislike for politics and involvement in worldly affairs.
Mufti-e-Azam-e-Hind (radi Allahu anhu) never fell short in entertaining those who came to visit him. When he was physically fit he used go into the Visitors Section and ask each person whether they had eaten or not. He used to ask them if they partook in tea or not. He used to continuously enquire as to whether they were experiencing any difficulties or not. It was often seen that he would personally carry the dishes into the house for the visitors! He was definitely blessed with the characters of the ""Salfe Saliheen"" or The Pious Servants of Allah.
Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) was a pillar of hospitality and humbleness. If he reprimanded a certain person for doing something un-Islamic or if he became displeased with anyone for some reason or the other, he used to also explain to the person in a very nice way and also try to cheer that person. He would then make Dua in abundance for such a person. His Mureeds (Disciples), on many ocassions, used to recite Manqabats (Poetry) in his praise. On hearing such Manqabats he would say, ""I am not worthy of such praise. May Allah make me worthy.""
Many people came to him for his blessings. Others would come for Ta'weez. He never refused anyone. It is also not known how many homes were being supported through the kindness and hospitality of Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu). He always entertained those who came from far and near to the best of his means. He used to even give most of his visitors train and bus fares to travel. In winter, he would give warm clothes, warm sheets and blankets to the poor and the needy.
Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) gave Khilafat to many Ulema-e-Ikraam and personally tied the Amaama (Turban) on their heads. He gave cloaks, turbans and hats to many people. Once, during winter, a few of the Khaadims were present with Mufti-e-Azam-e-Hind (radi Allahu anhu). He was lying on his bed and covered with a shawl. A certain Maulana Abu Sufyaan touched Mufti-e-Azam-e-Hind (radi Allahu anhu's) shawl and commented as to how beautiful it was. Mufti-e-Azam-e-Hind (radi Allahu anhu) immediately removed the shawl and presented it to him. Although the Moulana refused to accept it Mufti-e-Azam-e-Hind (radi Allahu anhu) gave it to him forcefully.
All of his Mehfils were full of knowledge and Barkah. Many questions on Tassawuf were easily answered by him. It seemed as if the rains of mercy and rays of Noor were spread all over his Mehfils.
Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) always wanted to see a Muslim's inner and outer personality. He always advised them to mould their lives according to the principles and the commands of Islam. He always showed discomfort to those who did not have beards, those who wore hats and to those who wore ultra-western clothes. He used to warn such Muslims. Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) used to show his displeasure towards those who wore ties. He used to tug at their ties and commanded them to abstain from wearing a tie. He also asked them to make Tauba from such acts.
Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) always commanded Muslims to give or take anything with their right hand. He stopped the Muslims from calling the governments as their ""Sarkaar"" or leaders. He never kept any ordinary Kitaab on the books of Tafseer or Hadith. Whenever he sat in a Meelad-un-Nabi (sallal laahu alaihi wasallam) or Mehfil-e-Zikr, he always sat with utmost respect until the very end.
Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) never spat towards the Qibla. He never stretched his legs in the direction of the Qibla. Whenever he entered the cemetery, he never used his entire feet to walk on the ground. He always walked on his toes. At times, he would stand on his toes for about half an hour in the graveyard making Dua-e- Maghfirat!
He always stopped Muslims from doing any false fortune telling. If any death or loss took place in the house of a Muslim, Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) would go to comfort the people of that house but he would never eat there. He always advised those in sorrow to make Sabr and remember Almighty Allah. He always respected Ulema-e-Ikraam. He respected the Sayeds in such a manner as a slave will respect his King. He prohibited Muslims from keeping un-Islamic names. He preferred such names as Abdullah, Abdur Rahmaan, Muhammad and Ahmad.
Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) always performed his Salaah in Jamaah whether he was on journey or not. The moment he put his foot out of his house to go towards the Masjid, he used to be surrounded by his Mureeds (disciples) and well-wishers who would follow him till the Masjid door which was just a few feet away from his house. While some would be kissing his blessed hands, others tried to talk with him. He would reply to all those who made Salaam to him. On entering the Masjid, he would immediately recite the dua prescribed.
Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) would then remove his Amaama and then sit down to perform Wudhu. He would wash all the parts thoroughly so that the Sunnahs were accomplished. He would perform his Salaah with great sincerity and used to be lost in the worship of his Creator. The person who looked at him from a distance would have instantly understood that Mufti-e-Azam-e-Hind (radi Allahu anhu) had left all the worldly desires and was intent upon pleasing his Creator.
Once, while Mufti-e-Azam-e-Hind (radi Allahu anhu) was traveling from Nagpur, it was time for Maghrib Salaah. He immediately disembarked from the train. The people told Mufti-e-Azam-e-Hind (radi Allahu anhu) that the train was about to leave, but he was intent on performing his Salaah. His companions also disembarked with him. They had just performed their Wudhu and were making Niyyah for Salaah when the train left the station. All of Mufti-e-Azam-e-Hind (radi Allahu anhu's) and his companions luggages' were left on the train. A few un-Islamic people who were there said ""the Mias train had left him"". Mufti-e-Azam-e-Hind (radi Allahu anhu) was still in Salaah.
When they all had completed their Salaah, they noticed that the station platform was empty. They became a little worried since all their luggage had gone with the train, but still Mufti-e-Azam-e-Hind (radi Allahu anhu) looked undisturbed. His companions were busy talking about the luggage when they noticed the station guard, followed by a group of travellers, running towards them. The guard came up to Mufti-e-Azam-e-Hind (radi Allahu anhu) and said, ""Huzoor! The train is stuck!"" Mufti-e-Azam-e-Hind (radi Allahu anhu) said, ""The engine is damaged."" The train was brought back and Mufti-e-Azam-e-Hind (radi Allahu anhu) and his companions sat in the train. After some repairs the train left with him and his companions seated in it!
Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) was drowned in the love for the Holy Prophet, Sayyiduna Rasulullah (sallal laahu alaihi wasallam). Everything he did was for the pleasure of Almighty Allah and Sayyiduna Rasulullah (sallal laahu alaihi wasallam). All that he had gained was due to the intense love which he possessed for the Holy Prophet (sallal laahu alaihi wasallam).
His extreme and intense love for the Holy Prophet (sallal laahu alaihi wasallam) can be understood by the fact that during the latter stages of his life, even though he was very ill, he would sit for hours with great respect in the Naath Mehfils and would shed tears in his love for Sayyiduna Rasulullah (sallal laahu alaihi wasallam). He used to celebrate the Meelad-un-Nabi (sallal laahu alaihi wasallam) each year with great splendour. The programme used to begin on the eve of the 12th of Rabi-ul-Awwal and used to continue till the next day just before lunch. The invitation was open to all Muslims and they all used to be fed.
Even after examining the Naath Shareefs written by Mufti-e-Azam-e-Hind (radi Allahu anhu) one would see that every word written dislayed his measureless love for the Holy Prophet (sallal laahu alaihi wasallam).
In the world of poetry, Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) was a Giant of his time. Most of his poems were in the form of Humd (Praise of Allah), Naath Shareef, Qasidas and Manqabats compiled in the Arabic, Urdu, Persian and Hindi languages. All these poems were compiled into a book which is famously known as ""Samaane Bakhshish"" which is still available toady. Samaane Bakhshsish is a treasure chest which flows with pearls of love for Sayyiduna Rasoolullah (sallal laahu alaihi wasallam). The compilation of Samaane Bakhshish is through the blessings of Sayyiduna Rasoolullah (sallal laahu alaihi wasallam).
""Ye Dil Ye Jigr Hai Ye Aankhe Ye Sar Hai, Jaha Chaaho Rakho Qadam Ghause Azam""
""Once a very young descendant of Sayyiduna Sheikh Abdul Qaadir Jilani (radi Allahu anhu), Hazrat Peer Taahir Ala'uddeen (radi Allahu anhu), visited Bareilly Shareef. The respect and honour that Mufti-e-Azam-e-Hind (radi Allahu anhu) showed towards him was out of this world. Mufti-e-Azam-e-Hind (radi Allahu anhu) used to walk bare feet behind him with great respect.""
The great Ulema of the time have stated that Mufti-e-Azam-e-Hind (radi Allahu anhu) was lost to such an extent in the love for Sayyiduna Ghousul Azam, Sheikh Abdul Qaadir Jilani (radi Allahu anhu) that even physically he began to resemble Sheikh Abdul Qaadir Jilani (radi Allahu anhu).
""Dekh Kar Shakle Mufti Azam, Ghause Azam ki Yaad Aayi he""
Ghousul Waqt, Mufti-e-Azam-e-Hind (radi Allahu anhu) had great respect and love for the Ulema and for Sayeds (Descendants of Sayyiduna Rasulullah sallal laahu alaihi wasallam). The respect which he showed towards them is beyond explanation.
One day, in 1979, a lady came with her little child to ask for Ta'weez. It was a very hot day and she was informed that Mufti-e-Azam-e-Hind (radi Allahu anhu) was resting. The lady, however, was in great need for the particular Ta'weez. She asked someone to see if Mufti-e-Azam-e-Hind (radi Allahu anhu) was awake but nobody had the nerve of going near him while he was resting as they considered this to be disrespectful. Taking her child she commented, ""What did we know that the words of Sayeds will not be heard in this place"".
It is not known how Mufti-e-Azam-e-Hind (radi Allahu anhu) heard this, but he immediately summoned one of the Mureeds. He instructed him to call the lady and not give her grief. The woman then sent her child to Mufti-e-Azam-e-Hind (radi Allahu anhu). He asked the child's name and showed great love and respect towards this young child. With great affection, he placed his hand on the child's head. He even asked someone to bring an apple for the child. From behind the curtain, he spoke to the lady concerning her problem and immediately wrote a Ta'weez for her.
Mufti-e-Azam-e-Hind (radi Allahu anhu) then sent a message to his family requesting that the mother and child should only be allowed to leave after the heat became less intense; that they should be well entertained and that no shortage should be spared in entertaining these Sayeds.
When Allamah Sadru Shariah Maulana Amjad Ali Al Qadri (radi Allahu anhu), the author of the famous ""Bahare Shariah,"" used to come to Bareilly Shareef for the Urs Shareef of Sayyiduna A'la Hazrat (radi Allahu anhu), Mufti-e-Azam-e-Hind (radi Allahu anhu) used to go to the railway station to welcome him and showed great respect towards this Scholar of Islam. He also showed great respect towards Sayyidi Hafiz-e-Millat and Hazrat Maulana Hasmat Ali Khan Sahib (radi Allahu anhum). He also showed respect towards his own Mureeds and Khalifas who were Alims.
""Hawa he Gotand wa Tez lekin Chiraagh Apna Jala Raha he, Wo Marde Durwesh jis ko Haq ne diye the Andaze Khusrawana""
The sign of a true Mo'min is that he never submits himself before an enemy. In the worst of circumstances a Mo'min announces that which is the truth. Sayyiduna Rasulullah (sallal laahu alaihi wasallam) said, ""To speak the truth before a tyrant King is a great Jihad."" So imagine the excellence of a person who always spoke the truth at all times, a person who always raised the flag of truth and honesty, and a person who never left the path of truth in his entire life!
Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) was one such person. He is one of the greatest leaders of the Sunnis. His boldness and fearlessness is difficult to explain. His entire life was spent speaking against Deobandis, Wahabis and all the other misleading sects, whether is was against the West, Qadianism, or Najdism he always challenged them right till the very end. He always propagated the true Deen and the Path of the Ahle Sunnah Wa Jamaah. With his Fatawas, he helped protect the Imaan of not only the Muslims in India and Pakistan, but of Muslims throughout the world.
He attacked the enemies of Islam through his writings, sayings, actions, etc. He did everything in his capacity to challenge the enemies of Islam. No person in his presence could say or do anything against Shariah. No person could speak against that which was the truth. It is stated by one of Mufti-e-Azam-e-Hind (radi Allahu anhu's) Khaadim's, who accompanied him on a journey by train, that there were some people in the train who were consuming alcohol. When Mufti-e-Azam-e-Hind (radi Allahu anhu) saw them, he reprimanded them and told them to desist from such a Haraam act. They did not listen to his advise so he scolded the leader of the group who was a young and well-built person. He gave the young person a hard slap which caused the bottle of alcohol to fall far from his hand. The Khaadim expected the person to retaliate but, who had the nerve to retaliate against this Lion of Islam! They became afraid and sat down quietly. Later some of them came up to Mufti-e-Azam-e-Hind (radi Allahu anhu) and begged for forgiveness for their shameful behavior.
""Tassawuf, Philsafa, Tafseer ki fiqhi Masa'il, Subhi kahte hai ke Aqida Kusha he Mufti Azam""
Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu), who after writing his first Fatawa while still a student at ""Darul Uloom Manzare Islam"", was given the status of Mufti due to his immense knowledge. When the Muslim World began to see his knowledge and Fatawas brightenening the world, they began calling him ""Mufti-e-Azam"" or The Most Exalted Mufti of the Time. This title alone became the name he was recognised by. Whenever the name ""Mufti Azam Hind"" was mentioned, it referred to none other than his exalted personality.
Remember that he or she only is exalted who has been blessed with this excellence by Almighty Allah and His Beloved Rasool (sallal laahu alaihi wasallam). Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) was a personality free from pride, lavishness and self- fame. His status was bestowed upon him by Almighty Allah and His Beloved Rasool (sallal laahu alaihi wasallam). That person to whom Almighty Allah and His Rasool (sallal laahu alaihi wasallam) grants such excellence, then such excellence cannot be understood by ordinary mortals. This is one of the reasons why the entire world was brightened and received the benefits of his knowledge of Fiqh.
There came a stage when Mufti-e-Azam-e-Hind (radi Allahu anhu) was not only known as ""Mufti-e-Azam-e-Hind"" but he was also known as ""Mufti-e-Azam-e-Alam"" or The Grand Mufti of the World.
It is recorded that on his trip to the Haramain Sharifain the Ulema of the Hejaz (Arabia), Syria, Egypt, Iraq, and from many other countries came to him to solve Fiqh Mas'alas. Many became his Mureeds. This is how his Faiz of Shariah and Tariqah spread its rays throughout the world. While in the Hejaz Shareef, he also had to deal with many Fatawas that poured in from various countries, such as, Africa, Mauritius, United Kingdom, America, Sri Lanka, Pakistan, Malaysia, Bangladesh, and many other places. He answered every single one of them in a very dedicated and professional manner.
During the reign of General Ayub Khan a ""Rooyat Hilal Committee"" was formed in Pakistan for the purpose of sighting the moon for every Islamic Month, and more importantly, for Eid-ul-Fitr and Eid-ul-Adha. An aeroplane was flown up to a certain height and the moon would be sighted from there. This form of Shahaadah (Confirmation) of the sighting of the moon via an aeroplane was readily accepted by the Pakistani Government. In this manner, Eid was celebrated.
On a specific occasion, on the 29th of Ramadaan, an aero plane was flown from the East to the West of Pakistan and the moon was reported to be sighted. This sighting was announced by the Hilaal Committee, but the Sunni Ulema of Pakistan did not accept this confirmation. The Ulema of Pakistan sent questionnaires to the Ulema throughout the world for clarification and one such questionnaire was sent to Mufti-e-Azam-e-Hind (radi Allahu anhu). Many Ulema replied that the confirmation had to be accepted and that it was permissible, but Mufti-e-Azam-e-Hind (radi Allahu anhu) clearly replied that this was not permissible. His Fatawa read as follows:"" The Command of Shariah is to sight the Moon and fast or celebrate Eid. Where the Moon is not sighted the Qazi should give an Islamic decision in connection with a confirmation. The moon must be sighted from the ground level or any place attached to the ground. With regards to the matter of using the plane - to sight the moon via a plane is wrong because the moon sets and does not perish. This is why it is sometimes sighted on the 29th and sometimes on the 30th. If to fly in a plane to sight the moon is a condition, then by increasing altitude the moon will be sighted even on the 27th and 28th. In this case, will the sighting be confirmed for the 27th or 28th? No person in his right sense will accept this. Thus under these circumstances, how would it be proper to sight the moon on the 29th?""
This Fatawa of Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) appeared in every newspaper in Pakistan as ""Headline News"".
The following month, on the 27th and the 28th, the Pakistan Government sent an aeroplane at a higher altitude and found that the moon was visible on these days. The Government of Pakistan then accepted the Fatawa of Mufti-e-Azam-e-Hind (radi Allahu anhu) and the Hilaal Committee of Pakistan was disbanded.
Mufti-e-Azam-e-Hind (radi Allahu anhu) wrote more or less 50 000 Fatawas in his lifetime. His word was accepted by great Ulema. Shamsul Ulema, Hazrat Maulana Shamsud'deen Ja'fari (radi Allahu anhu) stated: ""In this era, there is no greater expert in Fiqha than Huzoor Mufti-e-Azam-e-Hind (radi Allahu anhu). Whenever I present myself in his high court I always sit with my head bowed and I listen to his words in silence. I do not have the audacity to talk in abundance to him.""
""Amaanat Hind-o-Paak he is baat ke Shaahid, Ke badal deti he minto me Huqumat Mufti-e-Azam""
The year 1976 was a very difficult period for the Muslims in India. Certain Ulema, bought of by the Saudi Riyals and American Dollars, passed the Fatawa making Vasectomy (male sterilization to prevent birth of children) permissible. The Indian Government made Vasectomy necessary for every male in India at that time.
Muslims of India were in search of a Saviour to prevent such a law from being passed as this would mean them not having any more children. They were looking for someone who would stand and fight for their religious rights. All the Muslims looked towards the city of Bareilly Shareef, the city of light and truth, for an answer to this controversy. All of a sudden that Mujahhid of Islam rose with the torch of knowledge and light against the winds of enmity and destruction - Mufti-e-Azam-e-Hind (radi Allahu anhu). He immediately issued the true Fatawa on vasectomy and said, ""Vasectomy is Haraam, Haraam, Haraam."" This news spread throughout India. Through the Dua and firmness of Mufti-e-Azam-e-Hind (radi Allahu anhu) on this issue, the Government that wished to pass this law had lost power, and a new government came into power. The law on Vasectomy was abolished!
Once, Maulana Abdul Hadi Al Qaderi and Soofi Iqbal Sahib asked Ghousul Waqt, Mufti-e-Azam-e-Hind (radi Allahu anhu) the following question: ""Huzoor! Can one remember his Sheikh in Namaaz?"" Mufti-e-Azam-e-Hind (radi Allahu anhu) answered by saying, ""If you need to remember anyone in Namaaz then you should remember Tajedare Do Aalam, Habbibe Khuda (sallal laahu alaihi wasallam). Yes, just as people tend to gaze here and there in Namaaz - if, in this way, the thought of one's Peer comes into the mind, then there is no hindrance"". Subhan-Allah! Such caution is in this answer! This answer has also contradicted the Deobandi belief. By looking at the life of Mufti-e-Azam-e-Hind (radi Allahu anhu) and reading his Fatawas, one would see his status and excellence in the spiritual domain. His spiritual life was according to that of his renowned and distinguished father, Sayyiduna A'la Hazrat (radi Allahu anhu).
When the Americans were announcing there journey to the moon, a few Ulema were present with Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu). Amongst these Ulema were Shamsul Ulema Hazrat Maulana Shamsud'deen and Allamah Ghulam Jilani Mirati (radi Allahu anhum). They were discussing the concepts concerning the sun and the moon. Mufti-e-Azam-e- Hind (radi Allahu anhu) said that the sky and the earth are both stationary and that the moon and the sun are in motion. On hearing this Allama Ghulam Jilani Mirati (radi Allahu anhu) said, ""In the Holy Quran it is said, 'Wash Shamsu Tajri Li Mustaqaril'laha'. In other words, the sun is in motion in its fixed abode. From the word 'Tajri', it is obvious that the sun is in motion and from the word 'Mustaqaril'laha' it is obvious that it is stationary in one place. How can both these concepts be right?""
In answer to this, Mufti-e-Azam-e-Hind, Moulana Mustapha Raza Khan (radi Allahu anhu) immediately said, ""It was commanded to Hazrat Adam (alaihis salaam) and Hazrat Hawa (radi Allahu anha) (as follows): 'Walakum fil Ardi Mustaqar'. Does this mean that they were stationary in only one portion of the earth? Did they not walk around (on the earth)? To be Mustaqar means to be stationary in your surrounding, not to come out of your boundaries. To move but to move within your boundaries of movement."" On hearing this Allama Mirati Sahib (radi Allahu anhu) became silent.
Hazrat Muhaddith-e-Azam-e-Hind (radi Allahu anhu) said: ""IN THIS TIME, THAT PERSONALITY WHOSE TAQWA (PIETY) IS MORE THAN HIS FATAWA, IS NONE OTHER THAN THE SON OF SAYYIDI A'LA HAZRAT (RADI ALLAHU ANHU) WHOSE BEAUTIFUL NAME IS MUSTAPHA RAZA AND THIS NAME COMES ON MY TONGUE WITHOUT PROBLEM AND IT ALLOWS ME TO GAIN GREAT BLESSINGS."" Once Hazrat Muhaddith-e-Azam (radi Allahu anhu) wrote the following words on the Fatawa of Mufti-e-Azam-e-Hind (radi Allahu anhu): ""THIS IS THE SAYING OF SUCH AN AALIM WHOM TO FOLLOW IS COMPULSORY ""
Huzoor Sayyidi Hafiz-e-Millat (radi Allahu anhu) stated, ""A PERSON DOES NOT GET PROPER RESPECT AND ACCEPTANCE IN HIS OWN TOWN, BUT THE ACCEPTANCE AND RESPECT THAT HUZOOR MUFTI AZAM HAS GAINED IN HIS TOWN CANNOT BE FOUND ANYWHERE ELSE. THIS IS OPEN PROOF OF HIS KARAMAAT AND WILAYAT"". He then said, ""MUFTI AZAM IS A KING, HE IS A KING"". (Which means that he should be respected and treated as a King).
Huzoor Mujjahid-e-Millat (radi Allahu anhu) said, ""IN THIS TIME, THE PERSONALITY OF HUZOOR MUFTI AZM HIND (RADI ALLAHU ANHU) IS A UNIQUE ONE, ESPECIALLY IN THE FIELD OF IFTA, BUT ALSO IN HIS DAILY CONVERSATIONS - THE MANNER IN WHICH HE SPOKE AND EXPLAINED CAN BE UNDERSTOOD BY ONLY THE PEOPLE OF KNOWLEDGE.""
The ""Imam Ghazzali"" of his time, Allama Saeed Ahmad Kazmi Shah Sahib (radi Allahu anhu) says, ""THE STATUS OF SAYYIDI MUFTI AZAM HIND (RADI ALLAHU ANHU) CAN BE UNDERSTOOD FROM THIS THAT HE IS THE SON AND THE BELOVED OF MUJJADIDE DEEN-O-MILLAT, IMAM AHLE SUNNAT, ASH SHAH IMAM AHMAD RAZA KHAN (RADI ALLAHU ANHU).""
Hazrat Qari Maslihud'deen (radi Allahu anhu) says, ""AFTER THE WISAAL OF MY MURSHAD, THE CENTRAL POINT OF MY FOCUS WAS THE PERSONALITY OF HUZOOR MUFTI AZAM HIND (RADI ALLAHU ANHU) AND NOT ONLYWAS HE THE POINT OF MY FOCUS, BUT ALSO THAT OF THE ENTIRE SUNNI POPULATION.""
One of the greatest Karamats of a Mo'min is for him to be always steadfast on Shariat-e-Mustapha and Sunnat-e-Mustapha (sallal laahu alaihi wasallam). A Mo'min must be prepared to accept all the difficulties and calamities of life. When faced by any calamity he should always make Shukr to Allah Almighty.
These outstanding qualities can be found in the life of Mufti-e-Azam-e-Hind (radi Allahu anhu). He was always steadfast and firm on Shariat-e-Mustapha (sallal laahu alaihi wasallam). It is said that it is impossible to move a mountain from its place but it was not possible to move Mufti-e-Azam-e-Hind (radi Allahu anhu) from the Shariat-e-Mustapha (sallal laahu alaihi wasallam). Every second in the life of Mufti-e-Azam-e-Hind (radi Allahu anhu) was a Karaamat. Volumes can be written about the Karaamats of Mufti-e-Azam-e-Hind (radi Allahu anhu). He himself is a living Karaamat!
""Kaha tak Raaz likhoge karaamat Mufti-e-Azam, Sarapa hi Sarapa he karaamat Mufti-e-Azam""
For the purpose of Fuyooz-o-barkaat we will quote one such Karaamat.
Once Hazrat went for the Urs of Hazrat Mahboob-e-Ilahi, Kwaja Nizaamud'deen Awliyah (radi Allahu anhu) to Delhi. He stayed at a place called 'Koocha Jilan' with Ashfaaq Ahmad Sahib. At this place, a certain Wahabi Maulvi began arguing with Hazrat concerning the Ilme Ghaib (Knowledge of the Unseen) of Huzoor Anwar (sallal laahu alaihi wasallam). Ashfaaq Ahmad Sahib asked Hazrat not to argue with this person as it would not make any difference to him. Hazrat said, ""Let him speak. I will listen to him and all those who are present should also listen attentively. The reason why nothing makes a difference to Maulvi Sahib is because nobody listens to him properly. So let him say that which he wishes."" Maulvi Saeedud'deen then spoke for approximately 15 minutes explaining how Rasoolullah (sallal laahu alaihi wasallam) did not possess Ilme Ghaib. He spoke for some time and then became silent.
Hazrat then said, ""If you have forgotten anything concerning your argument then please try to remember."" The Maulvi Sahib spent another half an hour trying to prove that Huzoor (sallal laahu alaihi wasallam) did not possess Ilme Ghaib.
After listening to his arguments Hazrat said, ""You should immediately repent from your false belief. Allah has definitely blessed Huzoor (sallal laahu alaihi wasallam) with Ilme Ghaib and you have tried to contradict it in every way you could. If you do not mind, then also listen to my argument"".
Then very sarcastically Hazrat said, ""What is the responsibility of a son towards his widowed mother?"" Maulvi Sahib in answer said, ""I will not answer this as it is not relevant to the topic of discussion"".
Hazrat then said, ""I did not mind when you questioned me, but in any case just listen to my questions. There is no need to answer them"".
The second question Hazrat asked was, ""How is it to take a loan from someone and then hide from him? Can you become weary of your crippled son and leave him to beg? To make Hajj Badal from... ""
This question was not yet completed when the Wahabi Maulvi fell at the feet of Mufti-e-Azam-e-Hind (radi Allahu anhu) and said, ""Hazrat! It is enough. The problem has been solved. Today I have realised that Huzoor (sallal laahu alaihi wasallam) has Ilme Ghaib. If not by now the Munaafiqeen would have destroyed the Islamic Missions. If Almighty Allah has shown you those things about me which nobody else here knows about, then I cannot imagine all that which He has informed Rasoolullah (sallal laahu alaihi wasallam) of"".
The Wahabi Maulvi immediately repented and became Mureed of Mufti-e-Azam-e-Hind (radi Allahu anhu).
Each year, Mufti-e-Azam-e-Hind (radi Allahu anhu) used to go to Calcutta for missionary work. The Pope used to also visit Calcutta and although he received good coverage in the media, very few Christians turned up to meet the Pope. The Christians of Calcutta became very jealous whenever Mufti-e-Azam-e-Hind (radi Allahu anhu) visited that city as, without any news coverage, he attracted thousands of people who came to see him.
The Christians decided to insult Huzoor Mufti-e-Azam-e-Hind (radi Allahu anhu) and lower his personality in the eyes of the people. They trained three Christians to approach Huzoor Mufti-e-Azam-e-Hind (radi Allahu anhu) with the pretence that they were going to become his Mureeds. This was their plan: Whenever Hazrat was going to make any person his Mureed, he would ask the person to say, ""Say that you have given your hand into the hands of Ghous-e-Azam (radi Allahu anhu)."" The Christians where then going to say that Hazrat is a liar (Allah forbid) since that was not the hand of Ghous-e-Azam (radi Allahu anhu)!
The three Christians, now disguised as Muslims went to Huzoor Mufti-e-Azam (radi Allahu anhu) with the pretence of becoming his Mureed. When two of the Christians saw Hazrat's noorani face they became afraid of carrying out their plans, but the third Christian, who was very stubborn, decided to carry out the plan.
He sat in front of Huzoor Mufti-e-Azam-e-Hind (radi Allahu anhu) and Hazrat proceeded with making him a Mureed. When Hazrat said, ""Say that you have given your hand into the hands of Ghous-e-Azam (radi Allahu anhu),"" he said, ""I am giving my hand in the hand of Mufti-e-Azam."" He was implying that Hazrat was asking him to lie when he was made to say a moment ago that he is not going to lie.
Huzoor Mufti-e-Azam-e-Hind (radi Allahu anhu) again commanded him to say, ""Say that you have given your hand into the hands of Ghous-e-Azam (radi Allahu anhu)."" He again said, ""I am giving my hand in the hand of Mufti-e-Azam.""
Huzoor Mufti-e-Azam-e-Hind (radi Allahu anhu) came into a Jalaal (Spiritual Anger) state and said, ""Say that you are giving your hands into the hands of Ghous-e-Azam (radi Allahu anhu)."" To the surprise of many, the Christian began continuously saying, ""I have given my hands into the hands of Ghous-e-Azam, I have my given hands into the hands of Ghous-e-Azam (radi Allahu anhu) . . ..""
When asked about his behavior, the Christian said that as Huzoor Mufti-Azam-e-Hind (radi Allahu anhu) commanded him for the final time to say that he has given his hands into the hands of Ghous-e-Azam (radi Allahu anhu), he actually saw two bright hands emerging from Hazrat's hands and the Christian says that he is sure that these hands were none other the mubarak hands of Ghous-e-Azam (radi Allahu anhu).
That Christian then asked Huzoor Mufti-e-Azam-e-Hind (radi Allahu anhu) for forgiveness and explained to him what his true intentions were. He immediately accepted Islam and became a Mureed. The news of this Karaamat spread far and wide and thousands of Christians accepted Islam at Hazrat's hands. Subhan-Allah! This incident was narrated by Hazrat Moulana Abdul Hamid Palmer Noori Razvi, a close Khalifa of Huzoor Mufti-e-Azam-e-Hind (radi Allahu anhu).
Huzoor Sayyidi Sarkaar Mufti-e-Azam-e-Hind (radi Allahu anhu's) Mazaar Shareef is situated in Mohalla Saudagran, Bareilly Shareef. Every year thousands of Mureeds and lovers of Huzoor Mufti-e-Azam-e-Hind (radi Allahu anhu) present themselves at Bareilly Shareef for his Urs Mubaarak.
Mufti-e-Azam-e-Hind (radi Allahu anhu's) Mureedeen were not only ordinary people but his Mureeds also consisted of great Ulema, Muftis, Mufassirs, Poets, Philosophers, Professors, Doctors, etc. It is said that he has millions of Mureedeen.
In India - Mufas'sire Azam Hind Hazrat Ibrahim Raza (radi Allahu anhu); Hazrat Maulana Tahseen Raza Khan; Hazrat Maulana Rehan Raza Khan (radi Allahu anhu); Hazrat Allamah Mufti Mohammed Akhtar Raza Khan Azhari; Muhadithe Kabeer Hazrat Maulana Mufti Zia Ul Mustapaha Sahib; Hazrat Maulana Arshadul Qaadri Sahib.
His Eminence, Shaikh Mufti Mohammad Akhtar Raza Khan Azhari Al-Qaderi, was born on the 25th of Safar in the year 1942 in Bareilly, the citadel of spirituality and learning. He is the great grandson of A'la Hazrat, Shaikh Imam Ahmed Raza Fazil-e Barelvi (rahmatullahi alaih), the Mujaddid (Reviver) of Islam in the 14th Century Hijri.
Under the tutorship of renowned Ulama, he attained the degree of Fazile Deeniyat (Graduation in Islamic Theology) from Darul Uloom Manzare Islam, Bareilly. After spending three years (1963 - 1966) at the Al Azhar University in Cairo, Egypt, his Eminence post-graduated in Arabic Literature and Deeniyat with specialization in Ahadith (Prophetic Tradition) and Tafseer (Quranic Exegesis) with high distinctions.
On his return home, he joined Darul Uloom Manzare Islam, Bareilly Shareef. Thereafter, he left the Darul Uloom and established his own Darul-Ifta with the permission of his maternal grandfather, Huzoor Mufti-e-Azam Hind, Shaikh Mufti Muhammad Mustapha Raza Khan (rahmatullahi alaih). His Eminence, Mufti-e-Azam Hind (rahmatullahi alaih) declared him his Ja'Nashin (Successor) while the great Shaikh was present in this world.
His Eminence inherited the skill in the issuing of Fatawa (Legal Islamic Rulings) and in tackling the complex issues relating to Fiqh (Islamic Jurisprudence) directly from Mufti-e-Azam (radi Allahu anhu) who inherited it directly from Mujaddid-e-Deen-o-Millat, Ash Shah Imam Ahmed Raza Bareilvi (rahmatullahi alaih).
He is not only the Successor and a trustworthy custodian of Fatawa writing of Shaikh Mufti-e-Azam Hind (rahmatullahi alaih), but also the custodian of learning, knowledge, sanctity and saintliness, of his grandfather, Hujjatul Islam, Moulana Muhammad Haamid Raza Khan (rahmatullahi alaihi).
His father, Moulana Muhammad Ibrahim Raza Khan Jilaani Mia (rahmatullahi alaih), was a great Aalim and Saint. He was well-versed in the commentary of the Holy Quran and so was given the title of Mufassir-e-Azam-e-Hind or Great Commentator of the Holy Quran in India.
His Eminence, Mufti Akhtar Raza Khan Azhari, travels extensively propagating the Deen and is a world-renowned preacher and a spiritual guide. Thousands of Muslims in India and abroad are attached with his Silsila. His Eminence has many Khulafa. He was also given the title of Taajush Shari'ah.
Besides being a great Mufti and Aalim, he is also a poet and an academic writer. His Diwan (Collection of Poems) was published for the first time entitled Naghmat-e-Akhtar. Later, it was published entitled Safina-e-Bakhshish in 1986, a chrono-grammical name, derived by Dr. Abdun Naim Azizi. Safina-e-Bakhshish includes Mufti Akhtar Raza Khan's Urdu and Arabic poems and was compiled and published by Dr. Abdun Naim Azizi. Many of Allama Mohammad Akhtar Raza's Naaths and Manqabats have not been published as yet.
Amongst his academic works, a few are as follows: (1) Taswiron Ka Hukm, (2) T.V. aur Video ka Operation, (3) Difae Kanzul Imaan, (4) Sharhe-Hadise Niyat, (5) Al-Haqqul Mobeen (Arabic), (6) Difa Kanzul Imaan Part I & II (7) Mer-atun-Najdi'ah (Arabic) (8) Hazrat Ibrahim ke Waalid Tariq ya Azar, etc.
His Darul-Ifta is now the central Darul Ifta of not only Bareilly Shareef, but of the Sunni world and he has continued the prestige of Fatawa writing of his grand-father and great grand-father. To date, he has written more than 5 000 Fatawa.Besides being well-versed in Arabic, Persian, and Urdu he has also a good knowledge of English. He has written many Fatawa in the English Language. The original book, Few English Fatawa, was first published by Edara Sunni Duniya, 82 Saudagran, Bareilly Shareef by his Eminence. Allama Mufti Naseem Ashraf Habibi, who is the Head Advisor and Mufti of the Imam Ahmed Raza Academy and of Sunni Ulama Council included a few more unpublished Fatawas, which was also written or orally dictated in English by Hazrat Azhari Sahib.
May Almighty Allah keep Hazrat Allama Mufti Mohammad Akhtar Raza Khan Azhari firm on Maslak-e-A'la Hazrat and serve as a beacon of guidance. May He grant his Eminence good health and long life. Aameen.","['Mufti-e-Azam-e-Hind received Khilafat in the Qaderi, Chishti, Nakshbandi, Suharwardi, and Madaari Orders.']",8281,multifieldqa_en_e,en,,bda4a6b251a601b7f7f956f298b59d8e99993845c1eaf7d6,"Mufti-e-Azam-e-Hind received Khilafat in the Qaderi, Chishti, Nakshbandi, Suharwardi, and Madaari Orders.",105
What factors control the reliance of artificial organisms on plasticity?,"Paper Info

Title: Environmental variability and network structure determine the optimal plasticity mechanisms in embodied agents
Publish Date: Unkown
Author List: Sina Khajehabdollahi (from Department of Computer Science, University of Tübingen)

Figure

Figure2: An outline of the network controlling the foraging agent.The sensor layer receives inputs at each time step (the ingredients of the nearest food), which are processed by the plastic layer in the same way as the static sensory network, Fig.1.The output of that network is given as input to the motor network, along with the distance d and angle α to the nearest food, the current velocity v, and energy E of the agent.These signals are processed through two hidden layers to the final output of motor commands as the linear and angular acceleration of the agent
Figure4: The evolved parameters θ = (θ 1 , . . ., θ 8 ) of the plasticity rule for the reward prediction (a.) and the decision (b.) tasks, for a variety of parameters (p tr = 0.01, d e ∈ 0, 0.1, . . ., 1, and σ ∈ 0, 0.1, . . ., 1 in all 100 combinations).Despite the relatively small difference between the tasks, the evolved learning rules differ considerably.For visual guidance, the lines connect θs from the same run.
Figure5: a.The trajectory of an agent (blue line) in the 2D environment.A well-trained agent will approach and consume food with positive values (green dots) and avoid negative food (red dots).b.The learning rate of the plastic sensory network eta p grows with the distance between environments d e c. and decreases with the frequency of environmental change.d.The fitness of an agent (measured as the total food consumed over its lifetime) increases over generations of the EA for both the scalar and binary readouts in the sensory network.e.The Pearson correlation coefficient of an evolved agent's weights with the ingredient value vector of the current environment (E 1 -blue, E 2 -red).In this example, the agent's weights are anti-correlated with its environment, which is not an issue for performance since the motor network can interpret the inverted signs of food.

abstract

The evolutionary balance between innate and learned behaviors is highly intricate, and different organisms have found different solutions to this problem. We hypothesize that the emergence and exact form of learning behaviors is naturally connected with the statistics of environmental fluctuations and tasks an organism needs to solve.
Here, we study how different aspects of simulated environments shape an evolved synaptic plasticity rule in static and moving artificial agents. We demonstrate that environmental fluctuation and uncertainty control the reliance of artificial organisms on plasticity. Interestingly, the form of the emerging plasticity rule is additionally determined by the details of the task the artificial organisms are aiming to solve.
Moreover, we show that coevolution between static connectivity and interacting plasticity mechanisms in distinct sub-networks changes the function and form of the emerging plasticity rules in embodied agents performing a foraging task. One of the defining features of living organisms is their ability to adapt to their environment and incorporate new information to modify their behavior.
It is unclear how the ability to learn first evolved , but its utility appears evident. Natural environments are too complex for all the necessary information to be hardcoded genetically and more importantly, they keep changing during an organism's lifetime in ways that cannot be anticipated ; . The link between learning and environmental uncertainty and fluctuation has been extensively demonstrated in both natural ; , and artificial environments .
Nevertheless, the ability to learn does not come without costs. For the capacity to learn to be beneficial in evolutionary terms, a costly nurturing period is often required, a phenomenon observed in both biological , and artificial organisms . Additionally, it has been shown that in some complex environments, hardcoded behaviors may be superior to learned ones given limits in the agent's lifetime and envi-ronmental uncertainty ; ; .
The theoretical investigation of the optimal balance between learned and innate behaviors in natural and artificial systems goes back several decades. However, it has recently found also a wide range of applications in applied AI systems ; . Most AI systems are trained for specific tasks, and have no need for modification after their training has been completed.
Still, technological advances and the necessity to solve broad families of tasks make discussions about life-like AI systems relevant to a wide range of potential application areas. Thus the idea of open-ended AI agents that can continually interact with and adapt to changing environments has become particularly appealing.
Many different approaches for introducing lifelong learning in artificial agents have been proposed. Some of them draw direct inspiration from actual biological systems ; . Among them, the most biologically plausible solution is to equip artificial neural networks with some local neural plasticity , similar to the large variety of synaptic plasticity mechanisms ; ; that performs the bulk of the learning in the brains of living organisms .
The artificial plasticity mechanisms can be optimized to modify the connectivity of the artificial neural networks toward solving a particular task. The optimization can use a variety of approaches, most commonly evolutionary computation. The idea of meta-learning or optimizing synaptic plasticity rules to perform specific functions has been recently established as an engineering tool that can compete with stateof-the-art machine learning algorithms on various complex tasks ; ; Pedersen and Risi (2021); .
Additionally, it can be used to reverse engineer actual plasticity mechanisms found in biological neural networks and uncover their functions ; . Here, we study the effect that different factors (environ-arXiv:2303.06734v1 [q-bio.NC] 12 Mar 2023 mental fluctuation and reliability, task complexity) have on the form of evolved functional reward-modulated plasticity rules.
We investigate the evolution of plasticity rules in static, single-layer simple networks. Then we increase the complexity by switching to moving agents performing a complex foraging task. In both cases, we study the impact of different environmental parameters on the form of the evolved plasticity mechanisms and the interaction of learned and static network connectivity.
Interestingly, we find that different environmental conditions and different combinations of static and plastic connectivity have a very large impact on the resulting plasticity rules. We imagine an agent who must forage to survive in an environment presenting various types of complex food particles. Each food particle is composed of various amounts and combinations of N ingredients that can have positive (food) or negative (poison) values.
The value of a food particle is a weighted sum of its ingredients. To predict the reward value of a given resource, the agent must learn the values of these ingredients by interacting with the environment. The priors could be generated by genetic memory, but the exact values are subject to change. To introduce environmental variability, we stochastically change the values of the ingredients.
More precisely, we define two ingredient-value distributions E 1 and E 2 and switch between them, with probability p tr for every time step. We control how (dis)similar the environments are by parametrically setting E 2 = (1 − 2d e )E 1 , with d e ∈ [0, 1] serving as a distance proxy for the environments; when d e = 0, the environment remains unchanged, and when d e = 1 the value of each ingredient fully reverses when the environmental transition happens.
For simplicity, we take values of the ingredients in E 1 equally spaced between -1 and 1 (for the visualization, see Fig. ). The static agent receives passively presented food as a vector of ingredients and can assess its compound value using the linear summation of its sensors with the (learned or evolved) weights, see Fig. .
The network consists of N sensory neurons that are projecting to a single post-synaptic neuron. At each time step, an input X t = (x 1 , . . . , x N ) is presented, were the value x i , i ∈ {1, . . . , N } represents the quantity of the ingredient i. We draw x i independently form a uniform distribution on the [0, 1] interval (x i ∼ U (0, 1)).
The value of each ingredient w c i is determined by the environment (E 1 or E 2 ). The postsynaptic neuron outputs a prediction of the food X t value as y t = g(W X T t ). Throughout the paper, g will be either the identity function, in which case the prediction neuron is linear, or a step-function; however, it could be any other nonlinearity, such as a sigmoid or ReLU.
After outputting the prediction, the neuron receives feedback in the form of the real value of the input R t . The real value is computed as R t = W c X T t + ξ, where W c = (w c 1 , . . . , w c N ) is the actual value of the ingredients, and ξ is a term summarizing the noise of reward and sensing system ξ ∼ N (0, σ).
Figure : An outline of the static agent's network. The sensor layer receives inputs representing the quantity of each ingredient of a given food at each time step. The agent computes the prediction of the food's value y t and is then given the true value R t ; it finally uses this information in the plasticity rule to update the weight matrix.
For the evolutionary adjustment of the agent's parameters, the loss of the static agent is the sum of the mean squared errors (MSE) between its prediction y t and the reward R t over the lifetime of the agent. The agent's initial weights are set to the average of the two ingredient value distributions, which is the optimal initial value for the case of symmetric switching of environments that we consider here.
As a next step, we incorporate the sensory network of static agents into embodied agents that can move around in an environment scattered with food. To this end, we merge the static agent's network with a second, non-plastic motor network that is responsible for controlling the motion of the agent in the environment.
Specifically, the original plastic network now provides the agent with information about the value of the nearest food. The embodied agent has additional sensors for the distance from the nearest food, the angle between the current velocity and the nearest food direction, its own velocity, and its own energy level (sum of consumed food values).
These inputs are processed by two hidden layers (of 30 and 15 neurons) with tanh activation. The network's outputs are angular and linear acceleration, Fig. . The embodied agents spawn in a 2D space with periodic boundary conditions along with a number of food particles that are selected such that the mean of the food value distribution is ∼ 0. An agent can eat food by approaching it sufficiently closely, and each time a food particle is eaten, it is The sensor layer receives inputs at each time step (the ingredients of the nearest food), which are processed by the plastic layer in the same way as the static sensory network, Fig. .
The output of that network is given as input to the motor network, along with the distance d and angle α to the nearest food, the current velocity v, and energy E of the agent. These signals are processed through two hidden layers to the final output of motor commands as the linear and angular acceleration of the agent re-spawned with the same value somewhere randomly on the grid (following the setup of ).
After 5000 time steps, the cumulative reward of the agent (the sum of the values of all the food it consumed) is taken as its fitness. During the evolutionary optimization, the parameters for both the motor network (connections) and plastic network (learning rule parameters) are co-evolved, and so agents must simultaneously learn to move and discriminate good/bad food.
Reward-modulated plasticity is one of the most promising explanations for biological credit assignment . In our network, the plasticity rule that updates the weights of the linear sensor network is a rewardmodulated rule which is parameterized as a linear combination of the input, the output, and the reward at each time step:
Additionally, after each plasticity step, the weights are normalized by mean subtraction, an important step for the stabilization of Hebbian-like plasticity rules . We use a genetic algorithm to optimize the learning rate η p and amplitudes of different terms θ = (θ 1 , . . . , θ 8 ). The successful plasticity rule after many food presentations must converge to a weight vector that predicts the correct food values (or allows the agent to correctly decide whether to eat a food or avoid it).
To have comparable results, we divide θ = (θ 1 , . . . , θ 8 ) by We then multiply the learning rate η p with θ max to maintain the rule's evolved form unchanged, η norm p = η p • θ max . In the following, we always use normalized η p and θ, omitting norm . To evolve the plasticity rule and the moving agents' motor networks, we use a simple genetic algorithm with elitism .
The agents' parameters are initialized at random (drawn from a Gaussian distribution), then the sensory network is trained by the plasticity rule and finally, the agents are evaluated. After each generation, the bestperforming agents (top 10 % of the population size) are selected and copied into the next generation.
The remaining 90 % of the generation is repopulated with mutated copies of the best-performing agents. We mutate agents by adding independent Gaussian noise (σ = 0.1) to its parameters. To start with, we consider a static agent whose goal is to identify the value of presented food correctly. The static reward-prediction network quickly evolves the parameters of the learning rule, successfully solving the prediction task.
We first look at the evolved learning rate η p , which determines how fast (if at all) the network's weight vector is updated during the lifetime of the agents. We identify three factors that control the learning rate parameter the EA converges to: the distance between the environments, the noisiness of the reward, and the rate of environmental transition.
The first natural factor is the distance d e between the two environments, with a larger distance requiring a higher learning rate, Fig. . This is an expected result since the convergence time to the ""correct"" weights is highly dependent on the initial conditions. If an agent is born at a point very close to optimality, which naturally happens if the environments are similar, the distance it needs to traverse on the fitness landscape is small.
Therefore it can afford to have a small learning rate, which leads to a more stable convergence and is not affected by noise. A second parameter that impacts the learning rate is the variance of the rewards. The reward an agent receives for the plasticity step contains a noise term ξ that is drawn from a zero mean Gaussian distribution with standard deviation σ.
This parameter controls the unreliability of the agent's sensory system, i.e., higher σ means that the information the agent gets about the value of the foods it consumes cannot be fully trusted to reflect the actual value of the foods. As σ increases, the learning rate η p decreases, which means that the more unreliable an environment becomes, the less an agent relies on plasticity to update its weights, Fig. .
Indeed for some combinations of relatively small distance d e and high reward variance σ, the EA converges to a learning rate of η p ≈ 0. This means that the agent opts to have no adaptation during its lifetime and remain at the mean of the two environments. It is an optimal solution when the expected loss due to ignoring the environmental transitions is, on average, lower than the loss the plastic network will incur by learning via the (often misleading because of the high σ) environmental cues.
A final factor that affects the learning rate the EA will converge to is the frequency of environmental change during an agent's lifetime. Since the environmental change is modeled as a simple, two-state Markov process (Fig. ), the control parameter is the transition probability p tr . When keeping everything else the same, the learning rate rapidly rises as we increase the transition probability from 0, and after reaching a peak, it begins to decline slowly, eventually reaching zero (Fig. ).
This means that when environmental transition is very rare, agents opt for a very low learning rate, allowing a slow and stable convergence to an environment-appropriate weight vector that leads to very low losses while the agent remains in that environment. As the rate of environmental transition increases, faster learning is required to speed up convergence in order to exploit the (comparatively shorter) stays in each environment.
Finally, as the environmental transition becomes too fast, the agents opt for slower or even no learning, which keeps them ) and the decision (b.) tasks, for a variety of parameters (p tr = 0.01, d e ∈ 0, 0.1, . . . , 1, and σ ∈ 0, 0.1, . . . , 1 in all 100 combinations). Despite the relatively small difference between the tasks, the evolved learning rules differ considerably.
For visual guidance, the lines connect θs from the same run. near the middle of the two environments, ensuring that the average loss of the two environments is minimal (Fig. ). The form of the evolved learning rule depends on the task: Decision vs. Prediction The plasticity parameters θ = (θ 1 , . . . , θ 8 ) for the rewardprediction task converge on approximately the same point, regardless of the environmental parameters (Fig. ).
In particular, θ 3 → 1, θ 5 → −1, θ i → 0 for all other i, and thus the learning rule converges to: Since by definition y t = g(W t X T t ) = W t X T t (g(x) = x in this experiment) and R t = W c X T t + ξ we get: Thus the distribution of ∆W t converges to a distribution with mean 0 and variance depending on η p and σ and W converges to W c .
So this learning rule will match the agent's weight vector with the vector of ingredient values in the environment. We examine the robustness of the learning rule the EA discovers by considering a slight modification of our task. Instead of predicting the expected food value, the agent now needs to decide whether to eat the presented food or not.
This is done by introducing a step-function nonlinearity (g(x) = 1 if x ≥ 1 and 0 otherwise). Then the output y(t) is computed as: Instead of the MSE loss between prediction and actual value, the fitness of the agent is now defined as the sum of the food values it chose to consume (by giving y t = 1). Besides these two changes, the setup of the experiments remains exactly the same.
The qualitative relation between η p and parameters of environment d e , σ and p tr is preserved in the changed experiment. However, the resulting learning rule is significantly different (Fig. ). The evolution converges to the following learning rule: In both cases, the rule has the form ∆W t = η p X t [α y R t + β y ].
Thus, the ∆W t is positive or negative depending on whether the reward R t is above or below a threshold (γ = −β y /α y ) that depends on the output decision of the network (y t = 0 or 1). Both learning rules (for the reward-prediction and decision tasks) have a clear Hebbian form (coordination of preand post-synaptic activity) and use the incoming reward signal as a threshold.
These similarities indicate some common organizing principles of reward-modulated learning rules, but their significant differences highlight the sensitivity of the optimization process to task details. We now turn to the moving embodied agents in the 2D environment. To optimize these agents, both the motor network's connections and the sensory network's plasticity parameters evolve simultaneously.
Since the motor network is initially random and the agent has to move to find food, the number of interactions an agent experiences in its lifetime can be small, slowing down the learning. However, having the larger motor network also has benefits for evolution because it allows the output of the plastic network to be read out and transformed in different ways, resulting in a broad set of solutions.
The fitness of an agent (measured as the total food consumed over its lifetime) increases over generations of the EA for both the scalar and binary readouts in the sensory network. e. The Pearson correlation coefficient of an evolved agent's weights with the ingredient value vector of the current environment (E 1 -blue, E 2 -red).
In this example, the agent's weights are anti-correlated with its environment, which is not an issue for performance since the motor network can interpret the inverted signs of food. The agents can solve the task effectively by evolving a functional motor network and a plasticity rule that converges to interpretable weights (Fig. ).
After ∼ 100 evolutionary steps (Fig. ), the agents can learn the ingredient value distribution using the plastic network and reliably move towards foods with positive values while avoiding the ones with negative values. We compare the dependence of the moving and the static agents on the parameters of the environment: d e and the state transition probability p tr .
At first, in order to simplify the experiment, we set the transition probability to 0, but fixed the initial weights to be the average of E 1 and E 2 , while the real state is E 2 . In this experiment, the distance between states d e indicates twice the distance between the agent's initial weights and the optimal weights (the environment's ingredient values) since the agent is initialized at the mean of the two environment distributions.
Same as for the static agent, the learning rate increases with the distance d e (Fig. ). Then, we examine the effect of the environmental transition probability p tr on the evolved learning rate η p . In order for an agent to get sufficient exposure to each environment, we scale down the probability p tr from the equivalent experiment for the static agents.
We find that as the probability of transition increases, the evolved learning rate η p decreases (Fig. ). This fits with the larger trend for the static agent, although there is a clear difference when it comes to the increase for very small transition probabil-ities that were clearly identifiable in the static but not the moving agents.
This could be due to much sparser data and possibly the insufficiently long lifetime of the moving agent (the necessity of scaling makes direct comparisons difficult). Nevertheless, overall we see that the associations observed in the static agents between environmental distance d e and transition probability p tr and the evolved learning rate η p are largely maintained in the moving agents.
Still, more data would be needed to make any conclusive assertions about the exact effect of these environmental parameters on the emerging plasticity mechanisms. A crucial difference between the static and the moving agents is the function the plasticity has to perform. While in the static agents, the plasticity has to effectively identify the exact value distribution of the environment in order to produce accurate predictions, in the embodied agents, the plasticity has to merely produce a representation of the environment that the motor network can evolve to interpret adequately enough to make decisions about which food to consume.
To illustrate the difference, we plot the Pearson correlation coefficient between an agent's weights and the ingredient values of the environment it is moving in (Fig. ). We use the correlation instead of the MSE loss (which we used for the static agents in Fig. ) because the amplitude of the vector varies a lot for different agents and meaningful The evolved parameters of moving agents' plasticity rule for the g(s) = x, identity (a.) and the step function (Eq.
4) (b.) sensory networks (the environmental parameters here are d e ∈ [0, 1], σ = 0 and p tr = 0.001). The step function (binary output) network evolved a more structured plasticity rule (e.g., θ 3 > 0 for all realizations) than the linear network. Moreover, the learned weights for the identity network (c.) have higher variance and correlate significantly less with the environment's ingredient distribution compared to the learned weights for the thresholded network (d.)
conclusions cannot be drawn from the MSE loss. For many agents, the learned weights are consistently anti-correlated with the actual ingredient values (an example of such an agent is shown in Fig. ). This means that the output of the sensory network will have the opposite sign from the actual food value.
While in the static network, this would lead to very bad predictions and high loss, in the foraging task, these agents perform exactly as well as the ones where the weights and ingredients values are positively correlated, since the motor network can simply learn to move towards food for which it gets a negative instead of a positive sensory input.
This additional step of the output of the plastic network going through the motor network before producing any behavior has a strong effect on the plasticity rules that the embodied agents evolve. Specifically, if we look at the emerging rules the top performing agents have evolved (Fig. ), it becomes clear that, unlike the very well-structured rules of the static agents (Fig. ), there is now virtually no discernible pattern or structure.
The difference becomes even clearer if we look at the learned weights (at the end of a simulation) of the best-performing agents (Fig. ). While there is some correlation with the environment's ingredient value distribution, the variance is very large, and they do not seem to converge on the ""correct"" values in any way.
This is to some extent expected since, unlike the static agents where the network's output has to be exactly correct, driving the evolution of rules that converge to the precise environmental distribution, in the embodied networks, the bulk of the processing is done by the motor network which can evolve to interpret the scalar value of the sensory network's output in a variety of ways.
Thus, as long as the sensory network's plasticity rule co-evolves with the motor network, any plasticity rule that learns to produce consistent information about the value of encountered food can potentially be selected. To further test this assumption, we introduce a bottleneck of information propagation between the sensory and motor networks by using a step-function nonlinearity on the output of the sensory network (Eq.
4). Similarly to the decision task of the static network, the output of the sensory network now becomes binary. This effectively reduces the flow of information from the sensory to the motor network, forcing the sensory network to consistently decide whether food should be consumed (with the caveat that the motor network can still interpret the binary sign in either of two ways, either consuming food marked with 1 or the ones marked with 0 by the sensory network).
The agents perform equally well in this variation of the task as before (Fig. ), but now, the evolved plasticity rules seem to be more structured (Fig. ). Moreover, the variance of the learned weights in the bestperforming agents is significantly reduced (Fig. ), which indicates that the bottleneck in the sensory network is in-creasing selection pressure for rules that learn the environment's food distribution accurately.
We find that different sources of variability have a strong impact on the extent to which evolving agents will develop neuronal plasticity mechanisms for adapting to their environment. A diverse environment, a reliable sensory system, and a rate of environmental change that is neither too large nor too small are necessary conditions for an agent to be able to effectively adapt via synaptic plasticity.
Additionally, we find that minor variations of the task an agent has to solve or the parametrization of the network can give rise to significantly different plasticity rules. Our results partially extend to embodied artificial agents performing a foraging task. We show that environmental variability also pushes the development of plasticity in such agents.
Still, in contrast to the static agents, we find that the interaction of a static motor network with a plastic sensory network gives rise to a much greater variety of wellfunctioning learning rules. We propose a potential cause of this degeneracy; as the relatively complex motor network is allowed to read out and process the outputs from the plastic network, any consistent information coming out of these outputs can be potentially interpreted in a behaviorally useful way.
Reducing the information the motor network can extract from the sensory system significantly limits learning rule variability. Our findings on the effect of environmental variability concur with the findings of previous studies that have identified the constraints that environmental variability places on the evolutionary viability of learning behaviors.
We extend these findings in a mechanistic model which uses a biologically plausible learning mechanism (synaptic plasticity). We show how a simple evolutionary algorithm can optimize the different parameters of a simple reward-modulated plasticity rule for solving simple prediction and decision tasks.
Reward-modulated plasticity has been extensively studied as a plausible mechanism for credit assignment in the brain ; ; and has found several applications in artificial intelligence and robotics tasks ; . Here, we demonstrate how such rules can be very well-tuned to take into account different environmental parameters and produce optimal behavior in simple systems.
Additionally, we demonstrate how the co-evolution of plasticity and static functional connectivity in different subnetworks fundamentally changes the evolutionary pressures on the resulting plasticity rules, allowing for greater diversity in the form of the learning rule and the resulting learned connectivity.
Several studies have demonstrated how, in biological networks, synaptic plasticity heavily interacts with and is driven by network topology . Moreover, it has been recently demonstrated that biological plasticity mechanisms are highly redundant in the sense that any observed neural connectivity or recorded activity can be achieved with a variety of distinct, unrelated learning rules .
This observed redundancy of learning rules in biological settings complements our results and suggests that the function of plasticity rules cannot be studied independently of the connectivity and topology of the networks they are acting on. The optimization of functional plasticity in neural networks is a promising research direction both as a means to understand biological learning processes and as a tool for building more autonomous artificial systems.
Our results suggest that reward-modulated plasticity is highly adaptable to different environments and can be incorporated into larger systems that solve complex tasks. This work studies a simplified toy model of neural network learning in stochastic environments. Future work could be built on this basic framework to examine more complex reward distributions and sources of environmental variability.
Moreover, a greater degree of biological realism could be added by studying more plausible network architectures (multiple plastic layers, recurrent and feedback connections) and more sophisticated plasticity rule parametrizations. Additionally, our foraging simulations were constrained by limited computational resources and were far from exhaustive.
Further experiments can investigate environments with different constraints, food distributions, multiple seasons, more complex motor control systems and interactions of those systems with different sensory networks as well as the inclusion of plasticity on the motor parts of the artificial organisms.",['Environmental fluctuation and uncertainty control the reliance of artificial organisms on plasticity.'],5339,multifieldqa_en_e,en,,79ffdceb9859803e365c3de5d24c187ed06b15f04d04ae6a,Environmental fluctuation and uncertainty control the reliance of artificial organisms on plasticity.,101
What was the name of the first white settlement in McPherson County?,"McPherson County (standard abbreviation: MP) is a county located in the U.S. state of Kansas.  As of the 2020 census, the county population was 30,223. The largest city and county seat is McPherson. The county is named for Civil War General James B. McPherson.

History

Early history

For many millennia, the Great Plains of North America was inhabited by nomadic Native Americans. From the 16th century to 18th century, the Kingdom of France claimed ownership of large parts of North America. In 1762, after the French and Indian War, France secretly ceded New France to Spain, per the Treaty of Fontainebleau. In 1802, Spain returned most of the land to France, but keeping title to about 7,500 square miles.

In 1803, most of the land for modern day Kansas was acquired by the United States from France as part of the 828,000 square mile Louisiana Purchase for 2.83 cents per acre. In 1848, after the Mexican–American War, the Treaty of Guadalupe Hidalgo with Spain brought into the United States all or part of land for ten future states, including southwest Kansas. In 1854, the Kansas Territory was organized, then in 1861 Kansas became the 34th U.S. state.

19th century

From the 1820s to 1870s, the Santa Fe Trail passed through, what is now McPherson County. The trail entered the county, east of Canton, then south of Galva, then north of Inman, and west towards Lyons. In 1855, Charles O. Fuller established a ranch adjacent to the Running Turkey Creek Crossing about two miles south and one mile east of Galva. Fuller's Ranch provided accommodations for travelers on the Santa Fe Trail and was probably the first white settlement in McPherson County.

Peketon County was established in 1860, by the passage of a bill by S. N. Wood:  An act to establish Peketon County. Section 1. - That all that territory west of the sixth principal meridian and south of Township 16, in Kansas Territory, be and the same is hereby erected into a county, to be known by the name of Peketon County. On February 17, 1865, Peketon County was abolished, and McPherson County was made a part of Marion County, which extended from the west line of Chase County to the present western boundary of Kansas.

In 1868, Solomon Stephens and L. N. Holmberg were appointed Justices of the Peace—the first officers in what is now McPherson County. The next year (1869) occurred the first election for the township, now the county of McPherson. McPherson was regularly organized as a county in the spring of 1870, a mass meeting being held at Sweadal. Sweadal, the county seat thus selected, was located about one mile and a half southwest of the present site of Lindsborg. In September, however, the County Commissioners resolved to meet at the latter place, McPherson which had already been located some two years.

In April, 1873, a petition was filed for the county seat re-location. It was signed by 483 voters, and a special election was accordingly ordered for June 10. Upon that day, McPherson received 605 votes, New Gottland 325, King City 3 and Lindsborg 1; McPherson's majority over all, 276. In May the McPherson Town Company had offered, as an inducement for the location of the county seat at this point, the free use of rooms for ten years, and the donation of two squares of land on the town site. The offer was accepted the next month, the County Commissioners selecting blocks 56 and 65. Thus the county seat was established at McPherson and has remained since.

As early as 1875, city leaders of Marion held a meeting to consider a branch railroad from Florence. In 1878, Atchison, Topeka and Santa Fe Railway and parties from Marion County and McPherson County chartered the Marion and McPherson Railway Company.  In 1879, a branch line was built from Florence to McPherson, in 1880 it was extended to Lyons, in 1881 it was extended to Ellinwood. The line was leased and operated by the Atchison, Topeka and Santa Fe Railway. The line from Florence to Marion, was abandoned in 1968. In 1992, the line from Marion to McPherson was sold to Central Kansas Railway. In 1993, after heavy flood damage, the line from Marion to McPherson was abandoned. The original branch line connected Florence, Marion, Canada, Hillsboro, Lehigh, Canton, Galva, McPherson, Conway, Windom, Little River, Mitchell, Lyons, Chase, then connected with the original AT&SF main line at Ellinwood.

In 1887, the Chicago, Kansas and Nebraska Railway extended its main line from Herington to Pratt.  This main line connected Herington, Ramona, Tampa, Durham, Waldeck, Canton, Galva, McPherson, Groveland, Inman, Medora, Hutchinson, Whiteside, Partridge, Arlington,  Langdon, Turon, Preston, Natrona, Pratt.  In 1888, this main line was extended to Liberal.  Later, this line was extended to Tucumcari, New Mexico and Santa Rosa, New Mexico, where it made a connection with the Southern Pacific from El Paso, Texas.  The Chicago, Kansas and Nebraska Railway was absorbed by the Chicago, Rock Island and Pacific Railway. This line is also called the ""Golden State Route"".

20th century
The National Old Trails Road, also known as the Ocean-to-Ocean Highway, was established in 1912, and was routed through Windom, Conway, McPherson.

Geography

According to the U.S. Census Bureau, the county has a total area of , of which  is land and  (0.3%) is water.

Adjacent counties
 Saline County (north)
 Dickinson County (northeast)
 Marion County (east)
 Harvey County (southeast)
 Reno County (southwest)
 Rice County (west)
 Ellsworth County (northwest)

Major highways
  Interstate 135
  U.S. Route 56
  U.S. Route 81
  K-4
  K-61
  K-153

Demographics

The McPherson Micropolitan Statistical Area includes all of McPherson County.

2000 census
As of the census of 2000, there were 29,554 people, 11,205 households, and 7,966 families residing in the county.  The population density was 33 people per square mile (13/km2).  There were 11,830 housing units at an average density of 13 per square mile (5/km2).  The racial makeup of the county was 96.53% White, 0.81% Black or African American, 0.34% Native American, 0.32% Asian, 0.06% Pacific Islander, 0.79% from other races, and 1.16% from two or more races.  1.94% of the population were Hispanic or Latino of any race. 37.1% were of German, 12.9% Swedish, 12.1% American, 6.7% English and 6.3% Irish ancestry according to Census 2000.

There were 11,205 households, out of which 33.00% had children under the age of 18 living with them, 62.50% were married couples living together, 6.00% had a female householder with no husband present, and 28.90% were non-families. 25.50% of all households were made up of individuals, and 11.80% had someone living alone who was 65 years of age or older.  The average household size was 2.49 and the average family size was 2.99.

In the county, the population was spread out, with 25.40% under the age of 18, 10.30% from 18 to 24, 25.20% from 25 to 44, 21.80% from 45 to 64, and 17.30% who were 65 years of age or older.  The median age was 38 years. For every 100 females there were 95.90 males.  For every 100 females age 18 and over, there were 92.90 males.

The median income for a household in the county was $41,138, and the median income for a family was $48,243. Males had a median income of $33,530 versus $21,175 for females. The per capita income for the county was $18,921.  About 4.20% of families and 6.60% of the population were below the poverty line, including 5.20% of those under age 18 and 8.10% of those age 65 or over.

Government

Presidential elections
McPherson county is often carried by Republican candidates. The last time a Democratic candidate has carried this county was in 1964 by Lyndon B. Johnson.

Laws
Following amendment to the Kansas Constitution in 1986, the county remained a prohibition, or ""dry"", county until 1996, when voters approved the sale of alcoholic liquor by the individual drink with a 30 percent food sales requirement.

Education

Colleges
 McPherson College in McPherson
 Bethany College in Lindsborg
 Central Christian College in McPherson

Unified school districts
 Smoky Valley USD 400
 McPherson USD 418
 Canton-Galva USD 419
 Moundridge USD 423
 Inman USD 448

School district office in neighboring county
 Goessel USD 411
 Little River-Windom USD 444

Museums
 Birger Sandzén Memorial Gallery in Lindsborg
 McCormick-Deering Days Museum in Inman
 McPherson Museum in McPherson
 Lindsborg Old Mill & Swedish Heritage Museum in Lindsborg
 Kansas Motorcycle Museum in Marquette

Communities

Cities

 Canton
 Galva
 Inman
 Lindsborg
 Marquette
 McPherson (county seat) 
 Moundridge
 Windom

Unincorporated communities
† means a Census-Designated Place (CDP) by the United States Census Bureau.
 Conway
 Elyria†
 Groveland
 Johnstown
 New Gottland
 Roxbury†

Ghost towns
 Alta Mills
 Battle Hill
 Christian
 Doles Park
 Elivon
 King City
 Sweadal

Townships
McPherson County is divided into twenty-five townships.  The cities of Lindsborg and McPherson are considered governmentally independent and are excluded from the census figures for the townships.  In the following table, the population center is the largest city (or cities) included in that township's population total, if it is of a significant size.

See also
 List of people from McPherson County, Kansas
 National Register of Historic Places listings in McPherson County, Kansas
 McPherson Valley Wetlands
 Maxwell Wildlife Refuge

References

Notes

Further reading

 Wheeler, Wayne Leland. ""An Analysis of Social Change in a Swedish-Immigrant Community: The Case of Lindsborg, Kansas."" (PhD dissertation, University of Missouri-Columbia; ProQuest Dissertations Publishing, 1959. 5905657).

County
 Through the Years: A Pictorial History of McPherson County; McPherson Sentinel' Heritage House Publishing Co; 1992.
 McPherson County First Courthouse Built About 1869 or 1870; Lindsborg News-Record; March 30, 1959.
 Pioneer Life and Lore of McPherson County, Kansas; Edna Nyquist; Democratic-Opinion Press; 1932.
 A History of the Church of the Brethren in Kansas (includes McPherson College history); Elmer LeRoy Craik; McPherson Daily; Republican Press; 397 pages; 1922.
 Portrait and Biographical Record of Dickinson, Saline, McPherson, and Marion Counties, Kansas; Chapman Bros; 614 pages; 1893.
 Standard Atlas of McPherson County, Kansas; Geo. A. Ogle & Co; 82 pages; 1921.
 Plat Book of McPherson County, Kansas; North West Publishing Co; 50 pages; 1903.
 Edwards' Atlas of McPherson County, Kansas; John P. Edwards; 51 pages; 1884.

Trails
 The Story of the Marking of the Santa Fe Trail by the Daughters of the American Revolution in Kansas and the State of Kansas; Almira Cordry; Crane Co; 164 pages; 1915. (Download 4MB PDF eBook)
 The National Old Trails Road To Southern California, Part 1 (LA to KC); Automobile Club Of Southern California; 64 pages; 1916. (Download 6.8MB PDF eBook)

Mennonite Settlements
 Impact of Mennonite settlement on the cultural landscape of Kansas; Brenda Martin; Kansas State University; 1985/1988. 
 Mennonite settlement : the relationship between the physical and cultural environment; Susan Movle; University of Utah; 1975/1886.
 Status of Mennonite women in Kansas in their church and home relationships; Eva Harshbarger; Bluffton College; 1925/1945.

External links

County
 
 McPherson County - Directory of Public Officials
Historical
 , from Hatteberg's People'' on KAKE TV news
Maps
 McPherson County Maps: Current, Historic, KDOT
 Kansas Highway Maps: Current, Historic, KDOT
 Kansas Railroad Maps: Current, 1996, 1915, KDOT and Kansas Historical Society

 
Kansas counties
1867 establishments in Kansas
","[""The first white settlement in McPherson County was Fuller's Ranch, established by Charles O. Fuller.""]",1865,multifieldqa_en_e,en,,92bf77fcae3c753b768f1289eeb1e899fc8e75a14509a5f9,"The first white settlement in McPherson County was Fuller's Ranch, established by Charles O. Fuller.",100
What is the purpose of the baseline in the layout procedure?,"Probably one of the most frustrating things about building experimental aircraft, especially when starting with a minimum of pre-fabricated parts, is to start building and ending up with an unexpected result. Every builder starts a new project by wanting it to go ""perfectly."" So when things aren't going well, especially at the beginning, the frustration can lead to an unfinished airplane.
This is the first article in a series dedicated to helping builders of the Rand Robinson KR series planes build a straight and true fuselage -- the first part of the construction process. Borrowing from modern boatbuliding techniques, focus will be on the KR-2S, but the principles apply to the entire lineup of KR-1 & KR-2 series planes.
While building the KR-2(s) a common surprise is encountered by builders when the completed fuselage sides are laid into position to form the fuselage box section. With many hours spent building the sides flat, finding the once straight longerons that now bow up from the building surface, form a most dissatisfying ""banana"" shape. Especially when using the preformed fiberglass parts, this curve in the top longeron is not acceptable. The builder is left wondering what went wrong and no amount of clamping or brute force forming will solve the problem to any degree of satisfaction. The problem is not the builder's fault. The solution starts by understanding the three dimensional relationship of the assembled parts being built.
First understand that the plans show the finished form of the plane. They show the ""projected"" form as you would expect to see it if viewing an actual plane from the top, ends and from the side. Since the sides are sloped (flared) outward, looking from the side, the distances given by measuring the profile drawing are ""foreshortened"" and don't give the proper shape for building the fuselage with a flat top longeron. What needs to be done is to ""develop"" the ""true"" distances and shape of the flat panel so that when it is curved into position, the longerons lay flat.
Second, understand that the dimensions called for in the plans put a twist in the sides that tends to work the panel in two directions of curvature. This twist makes the panel ""undevelopable"" meaning that that shape cannot be unrolled into an equivalent flat shape. This is important when laying out the side and bottom panels onto flat plywood. To illustrate this, try forming a piece of paper around a soda can. The paper can be formed flat around the can either straight or at a diagonal to it's length. It has only one direction of curvature and is by definition ""developable"". Now try to form the same piece of paper around a baseball. It won't lie flat on the surface without some deformation (folding, wrinkling or tearing) of the paper. The ball has curvature in more that one direction and is a ""compounded"" shape. Paper (or plywood) can only be readily formed in developable shapes as opposed to aluminum or other metal which can accept in plane deformation. A developable surface is needed to lay out a curved surface when the materials used can't be deformed with any degree of in-plane strain.
Initially, the fuselage sides are laid out flat with reference to the top longeron measured to a straight chalk line. The bowing problem starts when the side panels are bent and sloped to form the fuselage box section. If the sides were not sloped (tumbled home), the section formed would be cylindrical and the longerons would lie flat. Since the sides are tumbled home, the section formed is now conical. When a conical shape is cut with a plane (building surface) not perpendicular to it's axis, the shape formed is elliptical -- exactly what happens with the top longeron. When it's built flat, bent to form a cylindrical section, and sloped to form a conical section, it takes on an elliptical shape firewall to tailstock.
This method borrows heavily from proven techniques used in the marine trades. It should be stressed at this point that although the layout procedure is not complicated, it is important to take your time. If the layout is not going well initially, start over! Better to erase layout errors now than to have them built it and cause surprises later.
Layout to ensure a fair and true fuselage starts by drawing a reference line (baseline) on the building surface. Refer to figures 2 & 3 and use a wire guide to draw a very straight baseline. About 500 lbs. Of tension should be adequate. One could use a chalk line, but we're talking airplanes here, not house framing.
The main layout difference is that the baseline isn't used as a reference for the top longeron. The baseline references the mid point of the firewall for the developed (and true dimensioned) side panel. Although the baseline will still be the reference, the top and bottom longerons will be laid separately.
Layout differences don't end there. Each of the stations (vertical members) will be laid out with a calculated separation so that when the panels are formed into position, they land on the spacing called for in the plans. Another major difference is that the bottom & side panels are applied after forming the fuselage box section. This is mainly to obtain the ability to ""fair"" the side and bottom surfaces and insure a straight and true shape.
Refer to figure 1 for the layout of the new developed side panel. The firewall (station a) is layed out perpendicular to the baseline. Longitudinal (station) measurements are given along the length of the baseline from the firewall. Vertical dimensions are given to reference the angle and breadths of the station at the baseline.
Notice that the top longeron is bowed outward and that the stations are spaced slightly greater than called out in the plans. When the panels are formed into the box frame section ,they will work into the dimensions specified in the plans.
Strike a centerline, longer than is needed on the building surface using a wire guide. Draw off the firewall line perpendicular to the centerline at one end.
Using the distances listed in the balloons, mark them off on the centerline. Distances are measured to the nearest sixteenth of an inch. Take time to mark them off carefully. Don't mark off the distances in a cumulative fashion. Use the firewall as a common reference.
Using the angles listed at each station, mark off a station line longer than is needed. The angles are measured to the nearest hundredth of a degree. Take time to mark them off carefully.
At each station, start by marking off each short (bottom longeron) line distance from the centerline. Use your set of trammels or beam compass for doing this. Mark the intersection of the short line with the station line.
At each station, mark off each long (top longeron) line distance from the intersection of the short line distance and the station line. Again the trammels or beam compass is best for completing this step. Mark the intersection of the long line distance with the station line.
Using the longeron as a batten, trace out the inside and outside curves of the longeron. After the batten is secure, in between each station, fasten a keeper block inside and outside to preserve the shape of the longeron taking care to avoid potential future interference with the diagonal members to be installed later. The fairing blocks can be removed or left in place if they won't interfere with building. The vertical station members and their diagonals can now be measured and positioned. Remember to refer to the plans for the material thickness direction.
After vertical and diagonal members are cut and fitted, take time to draw their outlines on the building surface to cut down on time and confusion when laying out the opposite side.
Finishing the side panel is accomplished in a manner similar to that called for in the handbook with the exception that the side and bottom skin panels will be attached later.
The next article in the series will discuss jigging and building techniques to ensure alignment and straightness of the flat built side panels. Also covered will be building a ""strongback"" jig to assure alignment of the side panels when they are formed into their final shape.
Part 3 in the series will cover assembly of the side panels using the jigs. Some joint details will be discussed that will ensure a stronger and more fair fuselage assembly. Also covered will be the layout & attachment of the side and bottom ply skins.
U.S. Mail: Densmore Associates, inc.
ANSI ""D"" size, computer generated plots of all the layout drawings in this series are available from the author for $30 plus postage & handling. Full (true size) scale plots may be made available depending on demand.
""Scarfing"" is the practice of splicing plywood so that short pieces of plywood can be used to span long distances. On the KR, it is required on both the fuselage skins and spar webs. The angle of the splice should be 10 to 12 degrees to maintain strength across the joint. Also, joints should coincide with structural members, such as spar webs or fuselage truss members.
This scarfer is made by mating a regular plunge router (this one costs about $50) to a table saw. Obviously, you really only need a table saw to cut the chamfer, but it does make a nice heavy table for scarfing. You could just as easily use a large work table as the base.First, set the table saw for a 5.5 degree cut (for a 1:12 joint, or 6.5 degree cut for a 10:1 joint), and run a 1 x 6 through on edge to chamfer a corner on the board. Then drill the board for three router mounting holes (two are countersunk) and connect the assembly to the table saw with two 1/4 inch bolts. Use a long (2-3 inch) straight cutting bit to do the cutting. Adjust the bit so it doesn't interfere with your table top, and go to town. Keep pressure on the plywood to ensure contact with the table while you're scarfing. Make sure you feed your material from the same end as you would if you were sawing, or the router will take your plywood away from you and put a big dent in your garage door.
In the late 60's Ken Rand and Stuart Robinson were working as flight system engineers for Douglas Avionics. Ken was working as an electrical engineer, having previously worked for Sperry as an autopilots project engineer, while Stu's degree was in aeronautical engineering from Northrop University. They were two of the guys at the end of the DC-8,9, and 10 assembly lines responsible for correcting some of the nits and picks in various systems before delivery to the customer.
They both wanted to build a fast, inexpensive airplane which was also economical to maintain. Several designs were considered, and plans were bought first for the Jeanie's Teenie and then the Taylor Monoplane. The Monoplane was more to their liking, but would require some modification to fit their needs. A cooperative redesign effort ensued, with virtually no dimensions left untouched. Only the basic fuselage structure, airfoil, and powerplant were retained. The tail shape was Stu's, and came directly from the big DC-8s parked on the ramp outside his office window. The landing gear was designed by Ken, after seeing the gear on a Dewey Bird at Santa Paula airport.
Ken was killed in his KR2 a short time later while flying over Cajon Pass in what was apparently a bad weather / low fuel accident. Ken's wife Jeanette became owner of RR overnight, and stepped up to keep the plans and parts coming. Much of the engineering needs are handled by Bill Marcy of Denver, who's been helping out since early '79.
To date, almost 6000 KR1, 9200 KR2, and 760 KR2S plan sets have been sold. 1200 KR2s are estimated to be flying, with 5 KR2Ss now in the air. Much of the development work done on KR's is now done by the builders themselves. KR builders tend to be innovative, which leads to some interesting modifications. Some of the mods that work eventually creep into the plans. The KR2S is a case in point. Many builders who'd heard of the pitch sensitivity and tight cabin of the KR2 began to build an enlarged version, with the length determined by the most commonly available longeron material. The result is a KR2 that is stretched 2"" between firewall and main spar, and 14"" behind the main spar. Higher gross weights dictated more wing area, with the new standard becoming the Diehl wing skin. Those who plan to carry passengers commonly stretch the cabin width a few inches, although 1.5 inches is the limit if you still want to use RR's premolded parts.
Mike Stearns addresses the KR Forum crowd.
This year's KR Forum featured guest speakers Mike Stearns, Steve Trentman, and Bill Marcey. Mike Stearns spoke on several topics, including the many sources for KR and homebuilding information available on the Internet. He also mentioned KRNet, the list server devoted entirely to KR aircraft, as well as several notable World Wide Web home pages. He also brought a sample of the new Rand Robinson wing skins with him, and discussed their high temperature core prepreg construction. His KR2S will receive the first set, which is currently being installed at Hinson Composites.
Steve Trentman spoke on his turbine installation. It uses a turbine engine which saw duty as an A7 attack jet starter engine. Total weight is about 85 pounds, while putting out around 90 horsepower. There is a small stockpile of these engines available from government surplus. sources. This engine can only be throttled back to 52% power, which leads to some pretty interesting landings. One inflight failure has been logged so far, with very little damage to the aircraft. More on this exciting development in next month's issue of KROnline.
Les Palmer's KR2 N202LP won Best KR2, Best Engine Installation, and People's Choice awards at the 1995 KR Gathering at Columbia, TN. After researching the KR series, and reading Neil Bingham's ""A Critical Analysis of the KR2"" (Jan 88 Sport Aviation), Les decided to build his as a single seater, stretched 24"" in the tail, while maintaining a stock width firewall. His fuselage is made from Douglas fir, which weighs in at 4 lbs heavier than if constructed from spruce. It is skinned with 1/8"" birch plywood. Spars are covered with plywoood on both fore and aft sides, ala KR2S. Diehl wing skins provide the lift. Horizontal stabilizer and elevator were stretched 7"" longer on each side, while the vertical stabilizer and rudder were stretched 8"" taller. . The fuselage to cowling junction was made more graceful by adding 1.5 inches to the height of the firewall end of the fuselage sides.
Les's canopy is a Dragonfly, using a four linkage system to swing forward when opening. The canopy frame fits snugly into a recess in the foward deck, providing an excellent wind and water seal. The fiberglass work is exemplary.
Seating is luxurious for one.
The cowling is also a work of art, and uses NACA ducts for efficiency. Female molds were made for all the fiberglass parts on Les's plane, so he could proabably be persuaded to make more, if demand dictates. Les also machines a multitude of KR aluminum and steel parts which he now offers for sale.
The firewall was reinforced with aluminum brackets and angles bolted between the longerons in anticipation of the 200 lb Subaru EA-81 engine installation. His 100 HP Asian version is outfitted with an American Holley 5200 caburetor and manifold. It uses a PSRU of Les's own design, featuring two spur gears with a 1.69:1 reduction ratio and a toothed belt. Other than tapping the crank for larger bolts to mount the redrive, no other engine modifications were required. Also, this is probably the only air conditioned KR2 on the planet. The prop is a 60/63 Hegy.
Originally built as a taildragger, the fixed gear is made from 4130 steel tubing. Custom cast 6.00x6 aluminum wheels and steel rotors are mated with 6"" Cleveland calipers for braking. An early taxi test accident damaged the main gear, and prompted Les to change to tricycle gear. Again, he designed his own fiberglass main gear, and uses a Diehl nose wheel fork with a 4130 strut and 6"" wheel up front.
Early tests revealed cooling problems, which prompted a radiator move from the firewall to a lower cowling location.
The first flight was almost a disaster, as test pilot Randy Smith lost power right after takeoff. He managed a 180 with a safe downwind landing with only minor nosewheel pant damage. The culprit proved to be a spark plug with too much reach, which was quickly remedied. Subsequent flights have shown water temp to be about 210 degrees, oil temp is 220-230, and airspeed is about 180 mph.
Shopping for the Partially Built KR.
This story starts about twenty years ago when I first started looking at the KR-2 as the plane I'd like to build. The only problem at that time was a lack of money, lack of knowledge, and a lack of job stability. I liked the design, except for the low ground clearance of the retractable gear and that a KR was going to be a tight fit for me to fly.
Over the past twenty years I've owned a number of planes, but still always wanted to build my own. I needed one that would fit me, my budget requirements, and have the speed and performance that I wanted. When ""KITPLANES"" published the article featuring Roy Marsh's new KR-2S, it was the first I had heard of any major modifications or improvements to the same old KR design. I believe that article and Roy Marsh's workmanship have probably been the greatest boon to Rand Robinson (RR) in the last twenty years. It certainly caught my eye! Here was the same design I had decided I wanted to build twenty years ago, with all of the improvements I wanted. It was sitting on fixed gear with some reasonable ground clearance. It had the capability to be built large enough to accommodate me. It has enough prefab parts available that it didn't have to be 100% scratch built if I decided to hurry the project along. And it had the speed I wanted. I knew that Roy's published speeds were probably not realistic expectations for the average KR, but after knocking around for the last three years in my Champ, anything over 90 mph seems pretty fast to me.
After purchasing the info kit and the sales video from Rand Robinson, the next step after deciding for sure to build this plane was to order the KR-2 plans and the KR-2S addendum. I finally got my plans and was putting together my first order to start the plane, when my partner in the Champ pointed out that there was a partially completed KR-2S for sale in Trade-a-plane. My initial answer was ""No, I don't even want to look at it. I want to build my own from scratch."" My partner insisted that for the advertised price and the fact that it wasn't too far away, I ought to at least give the guy a call and investigate it. ""No, I don't think I want to buy someone else's problems,"" I persisted. That night I went home and crunched up some numbers on the calculator and finally came to the conclusion that for the sake of my budget for the next several years, I really should give this guy a call.
Three days later, I flew to his place about 400 miles away to take a look at his project. At this point I should probably mention that I consider myself to be fairly knowledgeable about airplane construction, although the vast majority of my experience is with tube and fabric. The rest of this article deals with what I looked for and more importantly what I missed and have had to repair in the last year since I purchased the project.
When we went to the seller's house, I found that the left wing was built using the Dan Diehl wing skins and the right wing skins were leaning against the wall inside the house. Also the canopy was in the house with the canopy covered with paper and tape. I wanted to inspect the fuselage first, so off we went to the shop.
There I found a fuselage sitting on it's gear painted in primer gray. The first step was to inspect the quality of workmanship of what could be seen as it sat. The interior of the fuselage looked as if it had been built with a great deal of care. The fit and finish of all of the interior wood was very nice. Even the gussets looked like they had been painstakingly perfectly fitted. The glass work on the turtle back also looked very precise and clean. It was evenly faired into the vertical and horizontal stabs. The tail also appeared to be well built with the exception of a depression directly over the front and rear spars in the horizontal stabs. He explained that when he moved recently, that he had shot the plane with gray primer to protect it from the weather since he wouldn't have ready access to a shop to put it in right away. It ended up sitting out in the hot south Texas summer sun for a few weeks before he got a shop rented to work in. That caused the glass (or possibly the foam inside the horizontal stab) to swell, except that it held onto the spar, so it was slightly ballooned in front of and behind the spars. His recommendation was to fill it back smooth with micro.
I also found a small linear crack in the lower left wing spar cap on the left wing stub. It appeared to be from over tightening the rear spar wing attach fitting bolts. His explanation was that the crack wasn't important because the rear spars only job is to keep the wings from folding back. I also noticed that the holes for attaching the outer wing to the wing stub were badly rounded out on the rear spar. He explained that the Diehl wing skins require the rear spar to be swept slightly more forward than the stock wings. This won't allow you to use the rear spar attach fittings from RR and that I would need to fabricate a new set of rear spar attach fittings.
I also found that the aileron bellcranks were not built or installed as per plans, but found that they looked professional. I couldn't check for function since the right bellcrank and sheeve wasn't installed, the left wing also wasn't installed, and the right wing didn't exist yet.
Next we pulled the inspection panels off of the fuselage and tail and looked at everything I could see with a good flashlight. I didn't find anything else that might be questionable about the fuselage except for a cracked elevator trim tab that was damaged when it fell off it's hanging place on the wall.
Next we spent some time going over his builders log and builders photo album. I still hadn't seen anything that would dissuade me from buying this project.
At this point it was starting to get late and my ride down needed to get airborne for the flight home. I needed to make a decision about whether I wanted this project or not, but I hadn't inspected the wings and canopy yet. I took a cursory look at the left wing and saw lots on micro built up on it and some bubbles in the leading edge, but nothing that looked seriously wrong to my amateur eye. The right wing was only a set of spars in the shop and the Diehl wing skins in the house, so there wasn't much to look at there. The canopy was wrapped in paper and tape, so there wasn't much to look at there either. I decided that even if there were serious problems in the wing that was built, I would be money ahead to go ahead and buy the project. For the advertised price, I could build a new set of wings and still be way ahead financially. We negotiated a final price, shook hands, took my ride to the airport, and started off in search of a U-haul to haul the project home.
Now, at this point, some of you are thinking about what I surely must have forgotten to inspect and why didn't I take a local A & P or EAA member along for the ride. First of all, I don't know any mechanics locally that have any experience with glass and our EAA chapter of which I am VP is woefully lacking in fiberglass knowledge. Secondly, as you will see, I missed plenty. Some by ignorance, some by just not looking close enough.
Now for a list of the problems that I found over the last year and a few of the fixes that I came up with.
I found that the lower set of rear spar attach fittings on the left rear spar were installed backwards with the longer spaced hole towards the fuselage. Since this is the same place that also had the cracked spar cap, it required a major change. Also in the same area he had drilled through the rear spar with a hole saw to create a place for the aileron cable to pass through and managed to cut out the second from the outside vertical brace in the spar. Then he chose to install the aileron bellcranks in front of the rear spar, and cut another hole through the rear spar for the aileron push rod. He also managed to cut out the outside vertical brace in the spar. Since the holes were already drilled through the spar, the choices were to either cut out that section of spar cap and scarf a new piece in, cut the whole rear spar carrythrough out of the fuselage including ruining the left lower wing skin, or do something else creative to reinforce the spar cap and install a custom built set of attach fittings.
I also found that after I built and installed the right side wing stub ribs and skin that the aileron bellcrank setup would not work as installed. The cable that crosses between the two bellcranks had a sharp uphill from the sheeve to the bellcrank in the last 12 inches on either side. This combined with the radius that the bellcranks turn caused the cross cable to pull up tight when the ailerons were pushed to either end of their travel, but allowed the cables to go very slack when the ailerons were centered. Also the Aileron pushrods needed to pass directly through the lower set of rear wing attach fittings to attach to the aileron. This whole rear spar and aileron bellcrank setup was going to either have to be redesigned or cut out and built to plans. The bottom line is that the problems I observed when I inspected this part were much more serious than expected when I had to fix it.
I decided that I had to remove the rear fittings from the left wing to be replaced with the new set that my neighborhood machinist was cutting out for me. When I put the wing on the work bench to start removing the rear fittings, I thought I had better take a closer look at the bubbles in the leading edge. I found that as I pushed on the leading edge, it delaminated between the glass lay-up on top and the upper and lower wing skin edges that were floxed together underneath. I concluded that that area had to come apart and took a belt sander to the leading edge. What I found was that the leading edge had been floxed together and glassed over, but the mold release had never been scrubbed off the leading edge of the wing. It peeled apart for rebuild quite easily.
When I got back to removing the rear spar attach fittings, I noticed that the woodwork inside the wing looked awfully dull. The reason was that the wing had been closed up without varnishing any of the woodwork. This was rectified with a small hole saw, a number of extensions and a modified undercoating sprayer.
I also found that the aluminum drain fitting in the bottom of the left wing tank had been glassed into place upside down. The tapered pipe threads were tapered the wrong way to install the draincock into the tank. Retapping the fitting the right direction seemed to be a good fix for that problem.
When I finally got around to attaching the wing to the fuselage, I found that the front spar attach fittings were badly misaligned. Although they could be forced into alignment, I didn't think I needed that kind of preload on the main spar fittings. This problem was fixed by calling on my local neighborhood machinist to build me an aligning fixture and reaming the attach holes to the next larger size and ordering the new sized bolts.
On the fuselage I found that although it had new Cleveland wheels and brakes on it, one of the brakes had a severe wobble to it. I must complement the manufacturers for taking care of that problem. One call to the Cleveland factory and they shipped me a new set of wheels and brakes even though the receipt for this set was over four years old and in the original builders name. Their only concern was that this set had never been placed in service yet.
I chose to sand the load of micro off the left wing to see what it was covering. When I got down to the glass, I found that there was no glass for the aft inch and a half of the underside of the wing in front of the aileron hinge. With the Diehl wing skins, you build the wings, then cut the ailerons out of trailing edge of the wing. He had mismeasured and cut too much material off the bottom side of the trailing edge in front of the aileron. It was filled by floxing a piece of spruce into the gap to fill the space between the back edge of the fiberglass and the aileron mount. I chose to wrap the trailing edge of that wing, and the other wing to match with a couple of lay-ups of glass.
When I sanded the primer off the aforementioned damaged trim tab, I found that the hinge was floxed to the leading edge of the foam insides of the tab, but not the glass. I also chose to wrap the front of the trim tab with a lay-up of glass.
I decided to pull the paper off the canopy and take a look at it before I'm ready to bolt it on and fly. The original builder had blown his own canopy and after some of the previous problems, I was beginning to have some concerns about not having looked it over closely enough. The canopy turned out to have been blow a little too large. It ended up with a little larger bubble for headroom, which I didn't object to. However, it had more headroom on the right side than the left. Yes, it was just a little bit lopsided. The main problem was that the canopy is stretched thin enough that it can be easily pushed in with one hand when the weather is warm.. My fear was that this is just thin enough that it may decide to lay on my head or in my lap when flying on a warm day. It will have to be replaced.
I'm sure that many that are reading this could see several of the potential problems before I mentioned them, but some others may not have and I'm sure that there could have been many other problems that didn't but could have existed on this project. This is also not intended to be critical of the gentleman that started this project as many parts of it, especially the wood work are better than I could have done and much of his work is outstanding. I prefer to think that I'll end up with a better plane with his woodwork combined with my glasswork. This article is intended to feature some of the problems that you may run into in buying someone else's project.
The final question is, knowing what I have found over the past year, would I have still purchased this project. The answer is yes, but primarily because the price was right in that I am still money and work ahead of where I would be if I had started the project from scratch. There are a few things that I would have done differently, but nothing that I can't live with. Although I won't be able to say that I built it all from scratch, I have built and rebuild enough of the plane that I should have no problem qualifying under the 51% rule.
You can send comments directly to the author via e-mail at ""jscott@LANL.GOV"".
Here is an brief explanation of how I built my turtledecks. The jig was constructed from scrap plywood and a few 1x4s that I ripped into stringers. I made two temporary bulkheads from the plywood, one for each end. Remember the forward bulkhead needs to be shaped in a way that will closely match the aft end of your canopy frame. Make an aft bulkhead by placing a straight edge at the top of your forward bulkhead and the trailing edge of your horizontal stabilizer. This will give you an idea of how tall your aft bulkhead needs to be. As far as location, I placed my aft bulkhead just forward of the lower/front of my vertical fin. I constructed the jig on the fuselage, it is glued together with automotive bondo.
After the bulkheads were bondoed to the fuselage I used the stringers that I ripped from the 1x4s and bondoed them to the bulkheads. This gave me a male form to cover with thin plastic or posterboard. I stapled two layers of posterboard to the jig(thin plastic would work better). The posterboard wraps down two inches onto the fuselage. After I was satisfied with the way it looked, I then covered the entire thing with duct tape (fiberglass will not stick to duct tape) On top of this I wetout one layer of tri-ply cloth (22oz) that I had left over from an earlier project, and one layer of 8oz. bid. Remember to mask off your fuselage so you don't get epoxy on it. If you are not familiar with composite lay-ups, you should plan on razor cutting your lay-ups 4 to 6 hours after wetout while the lay-up is still soft enough to cut with a razorblade.
After the lay-up cured (2 or 3 days) it was removed from the jig, and the jig was removed from the fuselage and discarded. (be careful, the bondo sticks very well to the spruce, you could splinter your wood during removal) I now have a fiberglass skin that tends to hold the shape of the jig but is still flexible enough to work with. I made two bulkheads out of 1/4 last-a-foam (AS&S) using the plywood formers from the jig as a guide. I covered these foam bulkheads with one 8oz layer of glass on each side, with a glass to glass edge on the bottom. After cure these bulkheads were bondoed into place (to the fuselage)and the fiberglass skin was pulled down tight and floxed to the bulkheads. When the flox cured the bondo joints were broken, again being careful not to harm the wood. The turtledeck was removed from the fuselage and 2 inch tapes added to the bulkheads inside and out.
At this point the turtledeck looked great and only weighed about 5lbs. but I noticed you could deform the skin by pushing hard on the outside. So I flipped the turtledeck over and from 1/4 inch last-a-foam, I cut two inch wide strips that would run the entire length, forward and aft inside the turtledeck. In effect these would act as composite stringers, I made enough of these two inch wide strips to make up three stringers. One down the center (sort of a backbone) and one on each side of the ""backbone"" half the distance to the edge of the turtledeck. I sanded the edge of the foam so that when covered with a layer of bid @ 45degrees there would be a nice transition from the turtledeck skin up onto the foam and then back onto the turtledeck I scuff sanded and glued the foam stringers in with micro. I covered the foam stringers with one layer of 8oz bid @ 45degrees.
You can also send me email at: mikemims@pacbell.net if you have any questions or want to share your ideas.
KROnline is an online KR Newsletter devoted to sharing KR information with other builders and pilots in a timely manner. The first issue (September 96) is now available as a zipped MicroSoft Word file at http://members.aol.com/bshadr or as an html document at kronline9.html. If you'd like to submit articles or photos, email Randy Stein at BSHADR@aol.com ------------------------------------------------------------ Don't bother to email Randy though. KROnline has been retired since the KR Newsletter has improved.",['The baseline is used as a reference for the mid point of the firewall for the developed side panel.'],6340,multifieldqa_en_e,en,,33bfe67a3d40e71a5e5351ea5db4ea55df61a018d071074b,The baseline is used as a reference for the mid point of the firewall for the developed side panel.,99
What is the advantage of decorrelating the data before running the PLS algorithm?,"Paper Info

Title: Efficient nonparametric estimation of Toeplitz covariance matrices
Publish Date: March 20, 2023
Author List: Karolina Klockmann (from Department of Statistics and Operations Research, Universität Wien), Tatyana Krivobokova (from Department of Statistics and Operations Research, Universität Wien)

Figure

Figure 1: Spectral density functions (first row) and autocovariance functions (second row) for examples 1, 2, 3.
Figure 2: Distance between the first atom and the first center of mass of aquaporin (left) and the opening diameter y t over time t (right).
black line in the left plot) confirms that the covariance matrix estimated with our VST-DCT method almost completely decorrelates the channel diameter Y on the training data set.Next, we estimated the regression coefficients β with the usual PLS algorithm, ignoring the dependence in the data.Finally, we estimated β with PLS that takes into account dependence using our covariance estimator Σ.Based on these regression coefficient estimators, the prediction on the test set was calculated.The plot on the right side of Figure 2 shows the Pearson correlation between the true channel diameter on the test set and the prediction on the same test set based on raw (grey) and decorrelated data (black).
Figure 3: On the left, the auto-correlation function of Y (grey) and of Σ−1/2 Y (black), where Σ is estimated with the VST-DCT method; On the right, correlation between the true values on the test data set and prediction based on partial least squares (in grey) and corrected partial least squares (black).
Uniform distributionThe observations follow a uniform distribution with covariance matrices Σ 1 , Σ 2 , Σ 3 of examples 1, 2, 3, i.e., Y i = Σ 1/2 j X i , j = 1, 3, with X 1 , ...the parameter innov of the R function arima.sim is used to pass the innovations X 1 , ..., X n i.i.d.Table4, 5 and 6 show respectively the results for (A) p = 5000, n = 1, (B) p = 1000, n = 50 and (C) p = 5000, n = 10.
(A) p = 5000, n = 1: Errors of the Toeplitz covariance matrix and the spectral density estimators with respect to the spectral and L 2 norm, respectively, as well as the average computation time of the covariance estimators in seconds for one Monte Carlo sample (last column).
(C) p = 5000, n = 10: Errors of the Toeplitz covariance matrix and the spectral density estimators with respect to the spectral and L 2 norm, respectively, as well as the average computation time of the covariance estimators in seconds for one Monte Carlo sample (last column).
(A) p = 5000, n = 1: Errors of the Toeplitz covariance matrix and the spectral density estimators with respect to the spectral norm and the L 2 norm, respectively.Average computation time of the covariance estimators in seconds for one Monte Carlo sample (last column).
(B) p = 1000, n = 50: Errors of the Toeplitz covariance matrix and the spectral density estimators with respect to the spectral norm and the L 2 norm, respectively.Average computation time of the covariance estimators in seconds for one Monte Carlo sample (last column).
(C) p = 5000, n = 10: Errors of the Toeplitz covariance matrix and the spectral density estimators with respect to the spectral norm and the L 2 norm, respectively.Average computation time of the covariance estimators in seconds for one Monte Carlo sample (last column).

abstract

A new nonparametric estimator for Toeplitz covariance matrices is proposed. This estimator is based on a data transformation that translates the problem of Toeplitz covariance matrix estimation to the problem of mean estimation in an approximate Gaussian regression. The resulting Toeplitz covariance matrix estimator is positive definite by construction, fully data-driven and computationally very fast.
Moreover, this estimator is shown to be minimax optimal under the spectral norm for a large class of Toeplitz matrices. These results are readily extended to estimation of inverses of Toeplitz covariance matrices. Also, an alternative version of the Whittle likelihood for the spectral density based on the Discrete Cosine Transform (DCT) is proposed.
The method is implemented in the R package vstdct that accompanies the paper.

Introduction

Estimation of covariance and precision matrices is a fundamental problem in statistical data analysis with countless applications in the natural and social sciences. Covariance matrices with a Toeplitz structure arise in the study of stationary stochastic n = 1, to the best of our knowledge, there is no fully data-driven approach for selecting the banding/tapering/thresholding parameter.
suggested first to split the time series into non-overlapping subseries and then apply the cross-validation criterion of . However, it turns out that the right choice of the subseries length is crucial for this approach, but there is no data-based method available for this. In this work, an alternative way to estimate a Toeplitz covariance matrix and its inverse is chosen.
Our approach exploits the one-to-one correspondence between Toeplitz covariance matrices and their spectral densities. First, the given data are transformed into approximate Gaussian random variables whose mean equals to the logarithm of the spectral density. Then, the log-spectral density is estimated by a periodic smoothing spline with a data-driven smoothing parameter.
Finally, the resulting spectral density estimator is transformed into an estimator for Σ or its inverse. It is shown that this procedure leads to an estimator that is fully data-driven, automatically positive definite and achieves the minimax optimal convergence rate under the spectral norm over a large class of Toeplitz covariance matrices.
In particular, this class includes Toeplitz covariance matrices that correspond to long-memory processes with bounded spectral densities. Moreover, the computation is very efficient, does not require iterative or resampling schemes and allows to apply any inference and adaptive estimation procedures developed in the context of nonparametric Gaussian regression.
Estimation of the spectral density from a stationary time series is a research topic with a long history. Earlier nonparametric methods are based on smoothing of the (log-)periodogram, which itself is not a consistent estimator . Another line of nonparametric methods for estimating the spectral density is based on the Whittle likelihood, which is an ap-proximation to the exact likelihood of the time series in the frequency domain.
For example, estimated the spectral density from a penalized Whittle likelihood, while used polynomial splines to estimate the log-spectral density function maximizing the Whittle likelihood. Recently, Bayesian methods for spectral density estimation have been proposed (see , but these may become very computationally intensive in large samples due to posterior sampling.
The minimax optimal convergence rate for nonparametric estimators of Hölder continuous spectral densities from Gaussian stationary time series was obtained by under the L p , 1 ≤ p ≤ ∞, norm. Only few works on spectral density estimation show the optimality of the corresponding estimators. In particular, and derived convergence rates of their estimators for the log-spectral density under the L 2 norm, while neglecting the Whittle likelihood approximation error.
In general, most works on spectral density estimation do not exploit further the close connection to the corresponding Toeplitz covariance matrix estimation. In particular, an upper bound for the L ∞ risk of a spectral density estimator automatically provides an upper bound for the risk of the corresponding Toeplitz covariance matrix estimator under the spectral norm.
This fact is used to establish the minimax optimality of our nonparametric estimator for the Toeplitz covariance matrices. The main contribution of this work is to show that our proposed spectral density estimator is not only numerically very efficient, but also achieves the minimax optimal rate in the L ∞ norm, which in turn ensures the minimax optimality of the corresponding Toeplitz covariance matrix estimator.
The paper is structured as follows. In Section 2, the model is introduced and ap-proximate diagonalization of Toeplitz covariance matrices with the discrete cosine transform is discussed. Moreover, an alternative version of the Whittle's likelihood is proposed. In Section 3, new estimators for the Toeplitz covariance matrix and the precision matrix are derived, while in Section 4 their theoretical properties are presented.
Section 5 contains simulation results, Section 6 presents a real data example, and Section 7 closes the paper with a discussion. The proofs are given in the appendix to the paper.

Set up and diagonalization of Toeplitz matrices

Let Y 1 , . . . , Y n i.i.d. ∼ N p (0 p , Σ), where Σ is a (p × p)-dimensional positive definite covariance matrix with a Toeplitz structure, that is, Σ = {σ |i−j| } p i,j=1 0. The sample size n may tend to infinity or to be a constant. The case n = 1 corresponds to a single observation of a stationary time series, and in this case the data are simply denoted by Y ∼N p (0 p , Σ).
The dimension p is assumed to grow. The spectral density function f , corresponding to a Toeplitz covariance matrix Σ, is given by so that for f ∈ L 2 (−π, π) the inverse Fourier transform implies Hence, Σ is completely characterized by f , and the non-negativity of the spectral density function implies the positive definiteness of the covariance matrix.
Moreover, the decay of the autocovariance σ k is directly connected to the smoothness of f . Finally, the convergence rate of a Toeplitz covariance estimator and that of the corresponding spectral density estimator are directly related via Σ ≤ f ∞ := sup x∈ |f (x)|, where • denotes the spectral norm (see .
As in , we introduce a class of positive definite Toeplitz covariance matrices with Hölder continuous spectral densities. For β = γ + α > 0, where The optimal convergence rate for estimating Toeplitz covariance matrices over P β (M 0 , M 1 ) depends crucially on β. It is well known that the k-th Fourier coefficient of a function whose γ-th derivative is α-Hölder continuous decays at least with order O(k −β ) (see .
Hence, β determines the decay rate of the autocovariances σ k , which are the Fourier coefficients of the spectral density f , as k → ∞. In particular, this implies that for β ∈ (0, 1], the class P β (M 0 , M 1 ) includes Toeplitz covariance matrices corresponding to long-memory processes with bounded spectral densities, since the sequence of corresponding autocovariances is not summable.
A connection between Toeplitz covariance matrices and their spectral densities is further exploited in the following lemma. Lemma 1. Let Σ ∈ P β (M 0 , M 1 ) and let x j = (j − 1)/(p − 1), j = 1, ..., p, then where δ i,j is the Kroneker delta, O(•) terms are uniform over i, j = 1, . . . , p and divided by √ 2 when i, j ∈ {1, p} is the Discrete Cosine Transform I (DCT-I) matrix.
The proof can be found in Appendix A.1. This result shows that the DCT-I matrix approximately diagonalizes Toeplitz covariance matrices and that the diagonalization error depends to some extent on the smoothness of the corresponding spectral density. In the spectral density literature the discrete Fourier transform (DFT) matrix
, where i is the imaginary unit, is typically employed to approximately diagonalize Toeplitz covariance matrices. Using the fact that introduced an approximation for the likelihood of a single Gaussian stationary time series (case n = 1), the so-called Whittle likelihood (1) The quantity , where F j denotes the j-th column of F , is known as the periodogram at the j-th Fourier frequency.
Note that due to periodogram symmetry, only p/2 data points I 1 , ..., I p/2 are available for estimating the mean f (2πj/p), j = 1, . . . , p/2 , where x denotes the largest integer strictly smaller than x. The Whittle likelihood has become a popular tool for parameter estimation of stationary time series, e.g., for nonparametric and parametric spectral density estimation or for estimation of the Hurst exponent, see e.g., ; .
Lemma 1 yields the following alternative version of the Whittle likelihood where W j = (D t j Y ) 2 . Note that this likelihood approximation is based on twice as many data points W j as the standard Whittle likelihood. Thus, it allows for a more efficient use of the data Y to estimate the parameter of interest, such as the spectral density or the Hurst parameter.
Equations ( ) or (2) invite for the estimation of f by maximizing the (penalized) likelihood over certain linear spaces (e.g., spline spaces), as suggested e.g., in or . However, such an approach requires well-designed numerical methods to solve the corresponding optimization problem, since the spectral density in the second term of (1) or ( ) is in the denominator, which does not allow to obtain a closed-form expression for the estimator and often leads to numerical instabilities.
Also, the choice of the smoothing parameter becomes challenging. Therefore, we suggest an alternative approach that allows the spectral density to be estimated as a mean in an approximate Gaussian regression. Such estimators have a closed-form expression, do not require an iterative optimization algorithm and a smoothing parameter can be easily obtained with any conventional criterion.
First Hence, for W j = (D t j Y ) 2 , j = 1, . . . , p it follows with Lemma 1 that where Γ(a, b) denotes a gamma distribution with a shape parameter a and a scale parameter b. Note that the random variables W 1 , . . . , W p are only asymptotically independent. Obviously, E(W j ) = f (πx j ) + O(1), j = 1, . . .
, p. To estimate f from W 1 , . . . , W p , one could use a generalized nonparametric regression framework with a gamma distributed response, see e.g., the classical monograph by . However, this approach requires an iterative procedure for estimation, e.g., a Newton-Raphson algorithm, with a suitable choice for the smoothing parameter at each iteration step.
Deriving the L ∞ rate for the resulting estimator is also not a trivial task. Instead, we suggest to employ a variance stabilizing transform of that converts the gamma regression into an approximate Gaussian regression. In the next section we present the methodology in more detail for a general setting with n ≥ 1.

Methodology

For Y i ∼ N p (0 p , Σ), i = 1, . . . , n, it was shown in the previous section that with Lemma 1 the data can be transformed into gamma distributed random variables . . , n, j = 1, . . . , p, where for each fixed i the random variable W i,j has the same distribution as W j given in (3). Now the approach of Cai et al. ( ) is adapted to the setting n ≥ 1.
First, the transformed data points W i,j are binned, that is, fewer new variables . . , T . Note that the number of observations in a bin is m = np/T . In Theorem 1 in Section 4, we show that setting T = p υ for any υ ∈ ((4 − 2 min{β, 1})/3, 1) leads to the minimax optimal rate for the spectral density estimator.
To simplify the notation, m is handled as an integer (otherwise, one can discard several observations in the last bin). Next, applying the variance stabilizing transform (VST) ∼ where H(y) = {φ(m/2) + log (2y/m)} / √ 2 and φ is the digamma function (see . Now, the scaled and shifted log-spectral density H(f ) can be estimated with a periodic smoothing spline
where h > 0 denotes a smoothing parameter, q ∈ N is the penalty order and S per (2q − 1) a space of periodic splines of degree 2q − 1. The smoothing parameter h can be chosen either with generalized cross-validation (GCV) as derived in or with the restricted maximum likelihood, see . Once an estimator H(f ) is obtained, application of the inverse transform function H −1 (y) = m exp √ 2y − φ (m/2) /2 yields the spectral density estimator f = H −1 H(f ) .
Finally, using the inverse Fourier transform leads to the fol- The precision matrix Ω is estimated by the inverse Fourier transform of the reciprocal of the spectral density estimator, i.e., Ω = (ω |i−j| ) p i,j=1 with ωk = The estimation procedure for Σ and Ω can be summarised as follows. 1. Data Transformation:
where D is the (p × p)-dimensional DCT-I matrix as given in Lemma 1 and D j is its j-th column. 2. Binning: Set T = p υ for any υ ∈ ((4 − 2 min{β, 1})/3, 1) and calculate W i,j , k = 1, . . . , T.

VST:

where k are asymptotically i.i.d. Gaussian variables. Inverse VST: Estimate the spectral density f with f = H −1 H(f ) , where Note that Σ and Ω are positive definite matrices by construction, since their spectral density functions f and f −1 are non-negative, respectively. Unlike the banding and tapering estimators, the autocovariance estimators σk are controlled by a single smoothing parameter h, which can be estimated fully data-driven with several available automatic methods, which are numerically efficient and well-studied.
In addition, one can also use methods for adaptive mean estimation, see e.g., , which in turn leads to adaptive Toeplitz covariance matrix estimation. All inferential procedures developed in the Gaussian regression context can also be adopted accordingly.

Theoretical Properties

In this section, we study the asymptotic properties of the estimators f , Σ and Ω. The results are established under the asymptotic scenario where p → ∞ and p/n → c ∈ (0, ∞], that is, the dimension p grows, while the sample size n either remains fixed or also grows but not faster than p. This corresponds to the asymptotic scenario when the sample covariance matrix is inconsistent.
Let f be the spectral density estimator defined in Section 3, i.e., f = m exp{ √ 2 H(f ) − φ(m/2)}/2, where H(f ) is given in (4), m = np/T and φ is the digamma function. Furthermore, let Σ be the Toeplitz covariance matrix estimator and Ω the corresponding precision matrix defined in equations ( ) and (6), respectively.
The following theorem shows that both Σ and Ω attain the minimax optimal rate of convergence over the class and hT → ∞, then with T = p υ for any υ ∈ ((4 − 2 min{β, 1})/3, 1) and q = max{1, γ}, the spectral density estimator f , the corresponding covariance matrix estimator Σ and the precision matrix estimator Ω satisfy sup
For h {log(np)/(np)} . The proof of Theorem 1 can be found in the Appendix A.3 and is the main result of our work. The most important part of this proof is the derivation of the convergence rate for the spectral density estimator f under the L ∞ norm. In the original work, established an L 2 rate for a wavelet nonparametric mean estimator in a gamma regression where the data are assumed to be independent.
In our work, the spectral density estimator f is based on the gamma distributed data W i,1 , . . . , W i,p , which are only asymptotically independent. Moreover, the mean of these data is not exactly f (πx 1 ), . . . , f (πx p ), but is corrupted by the diagonalization error given in Lemma 1. This error adds to the error that arises via binning and VST and that describes the deviation from a Gaussian distribution, as derived in .
Finally, we need to obtain an L ∞ rather than an L 2 rate for our spectral density estimator. Overall, the proof requires different tools than those used in . To get the L ∞ rate for f , we first derive that for the periodic smoothing spline estimator H(f ) of the log-spectral density. To do so, we use a closed-form expression of its effective kernel obtained in , thereby carefully treating various (dependent) errors that describe deviations from a Gaussian nonparametric regression with independent errors and mean f (πx i ).
Note also that although the periodic smoothing spline estimator is obtained on T binned points, the rate is given in terms of the vector dimension p. Then, using the Cauchy-Schwarz inequality and a mean value argument, this rate is translated into the L ∞ rate for the spectral density estimator f . To obtain the rate for the Toeplitz covariance matrix estimator is enough to note that

Simulation Study

In this section, we compare the performance of the proposed Toeplitz covariance estimator, denoted as VST-DCT, with the tapering estimator of and with the sample covariance matrix. We consider Gaussian vectors Y 1 , ..., (3) such that the corresponding spectral density is Lipschitz continuous but not differentiable: f (x) = 1.44{| sin(x + 0.5π)| 1.7 + 0.45}.
In particular, var(Y i ) = 1.44 in all three examples. Figure shows the spectral densities and the corresponding autocorrelation functions for the three examples. A Monte Carlo simulation with 100 iterations is performed using R (version 4.1.2, seed 42). For our VST-DCT estimator, we use a cubic periodic spline, i.e., q = 2 is set in (4).
The binning parameters are set to T = 500 bins with m = 10 points for (A) and T = 500 bins with m = 100 points for both (B) and (C). To select the regularisation parameter for our estimator, we implemented the restricted maximum likelihood (ML) method, generalized cross validation (GCV) and the corresponding oracle versions, i.e., as if Σ were known.The tapering parameter
where T ap k (Σ ν 2 ) is the tapering estimator of with parameter k. If n = 1, that is, under scenario (A), suggest to split the time series Y into l non-overlapping subseries of length p/l and then proceed as before to select the tuning parameter k. To the best of our knowledge, there is no data-driven method for selecting this parameter l.
Using the true covariance matrix Σ, we selected l = 30 subseries for the example 1 and l = 15 subseries for the exam-ples 2 and 3. The parameter k can then be chosen by cross-validation as above. We employ this approach under scenario (A) instead of an unavailable fully data-driven criterion and name it semi-oracle.
Finally, for all three scenarios (A), (B) and (C), the oracle tapering parameter is computed using grid search for each Monte Carlo sample as kor = arg min k=2,3,...,p/2 T ap k ( Σ) − Σ , where Σ is the sample covariance matrix. To speed up the computation, one can replace the spectral norm by the 1 norm, as suggested by .
In Tables , the errors of the Toeplitz covariance estimators with respect to the spectral norm and the computation time for one Monte Carlo iteration are given for scenarios (A), (B) and (C), respectively. To illustrate the goodness-of-fit of the spectral density, the L 2 norm f − f 2 is also computed.
The results show that the tapering and VST-DCT estimator perform overall similar in terms of the spectral norm risk. This is not surprising as both estimators are proved to be rate-optimal. Moreover, both the tapering and VST-DCT estimators are clearly superior to the inconsistent sample Toeplitz covariance matrix.
A closer look at the numbers shows that the VST-DCT method has better constants, i.e., VST-DCT estimators have somewhat smaller errors in the spectral norm than the tapering estimators across all examples, but especially under scenario (C). The oracle estimators show similar behaviour, but are slightly less variable compared to the data-driven estimators.
In general, both the tapering and VST-DCT estimators perform best for example 1, second best for example 3 and worst for example 2, which traces back to functions complexity. In terms of computational time, both methods are similarly fast for scenarios (A) and (B). For scenario (C), the tapering method is much slower due to the multiple high-dimensional matrix multiplications in the cross-validation method.
It is expected that for larger p the tapering estimator is much more computationally intensive than the corresponding VST-DCT estimator. (1) polynomial σ ( (  To test how robust our approach is to deviations from the Gaussian assumption, we simulated the data from gamma and uniform distributions and conducted a simulation study for the same scenarios and examples.
The results are very similar to those of the Gaussian distribution, see supplementary materials for the details.

Application to Protein Dynamics

We revisit the data analysis of protein dynamics performed in Krivobokova et al. (2012) and . We consider data generated by the molecular dynamics (MD) simulations for the yeast aquaporin (Aqy1) -the gated water channel of the yeast Pichi pastoris. MD simulations are an established tool for studying biological systems at the atomic level on timescales of nano-to microseconds.
The data are given as Euclidean coordinates of all 783 atoms of Aqy1 observed in a 100 nanosecond time frame, split into 20 000 equidistant observations. Additionally, the diameter of the channel y t at time t is given, measured by the distance between two centers of mass of certain residues of the protein.
The aim of the analysis is to identify the collective motions of the atoms responsible for the channel opening. In order to model the response variable y t , which is a distance, based on the motions of the protein atoms, we chose to represent the protein structure by distances between atoms and certain fixed base points instead of Euclidean coordinates.
That is, we calculated where A t,i ∈ R 3 , i = 1, . . . , 783 denotes the i-th atom of the protein at time t, B j ∈ R 3 , j = 1, 2, 3, 4, is the j-th base point and d(•, •) is the Euclidean distance. Figure shows the diameter y t and the distance between the first atom and the first center of mass. It can therefore be concluded that a linear model Y = Xβ + holds, where
. This linear model has two specific features which are intrinsic to the problem: first, the observations are not independent over time and second, X t is high-dimensional at each t and only few columns of X are relevant for Y . have shown that the partial least squares (PLS) algorithm performs exceptionally well on this type of data, leading to a small-dimensional and robust representation of proteins, which is able to identify the atomic dynamics relevant for Y .
Singer et al. ( ) studied the convergence rates of the PLS algorithm for dependent observations and showed that decorrelating the data before running the PLS algorithm improves its performance. Since Y is a linear combination of columns of X, it can be assumed that Y and all columns of X have the same correlation structure.
Hence, it is sufficient to estimate Σ = cov(Y ) to decorrelate the data for the PLS algorithm, i.e., Σ −1/2 Y = Σ −1/2 Xβ + Σ −1/2 results in a standard linear regression with independent errors. Our goal now is to estimate Σ and compare the performance of the PLS algorithm on original and decorrelated data.
For this purpose, we divided the data set into a training and a test set (each with p = 10 000 observations). First, we tested whether the data are stationary. The augmented Dickey-Fuller test confirmed stationarity for Y with a p-value< 0.01. The Hurst exponent of Y is 0.85, indicating moderate long-range dependence supported by a rather slow decay of the sample autocovariances (see grey line in the left plot of Figure ).
Therefore, we set q = 1 for the VST-DCT estimator to match the low smoothness of the corresponding spectral density. Moreover, the smoothing parameter is selected with the restricted maximum likelihood method and T = 550 bins are used. Obviously, the performance of the PLS algorithm on the decorrelated data is significantly better for the small number of components.
In particular, with just one PLS component, the correlation between the true opening diameter on the test set and its prediction that takes into account the dependence in the data is already 0.54, while it is close to zero for PLS that ignores the dependence in the data. showed that the estimator of β based on one PLS component is exactly the ensemble-weighted maximally correlated mode (ewMCM), which is defined as the collective mode of atoms that has the highest probability to achieve a specific alteration of the response Y .
Therefore, an accurate estimator of this quantity is crucial for the interpretation of the results and can only be achieved if the dependence in the data is taken into account. Estimating Σ with a tapered covariance estimator has two practical problems. First, since we only have a single realization of a time series Y (n = 1), there is no datadriven method for selecting the tapering parameter.
Second, the tapering estimator turned out not to be positive definite for the data at hand. To solve the second problem, we truncated the corresponding spectral density estimator ftap to a small positive value, i.e., f + tap = max{ ftap , 1/ log(p)} (see . To select the tapering parameter with cross-validation, we experimented with different subseries lengths and found that the tapering estimator is very sensitive to this choice.
For example, estimating the tapered covariance matrix based on subseries of length 8/15/30 yields a correlation of 0.42/0.53/0.34 between the true diameter and the first PLS component, respectively. Altogether, our proposed estimator is fully data-driven, fast even for large sample sizes, automatically positive definite and can handle certain long-memory processes.
In contrast, the tapering estimator is not data-driven and must be manipulated to become positive definite. Our method is implemented in the R package vstdct.

Discussion

In this paper, we proposed a simple, fast, fully data-driven, automatically positive definite and minimax optimal estimator of Toeplitz covariance matrices from a large class that also includes covariance matrices of certain long-memory processes. Our estimator is derived under the assumption that the data are Gaussian.
However, simulations show that the suggested approach yields robust estimators even when the data are not normally distributed. In the context of spectral density estima- , for mixing processes (see Theorem 5.3 of Rosenblatt, 2012), as well as for non-linear processes (see . Since DFT and DCT matrices are closely related, we expect that equation (3) also holds asymptotically for these non-Gaussian time series, but consider a rigorous analysis to be beyond the scope of this paper.
In fact, our numerical experiments have even shown that if the spectral density is estimated from W j = f (πx j ) + j , that is, as if W j were Gaussian instead of gamma distributed, then the resulting spectral density estimator has almost the same L ∞ risk (and hence the corresponding covariance matrix has almost the same spectral norm).
Of course, such an estimator would lead to a wrong inference about f (πx j ), since the growing variance of W j would be ignored. Since our approach translates Toeplitz covariance matrix estimation into a mean estimation in an approximate Gaussian nonparametric regression, all approaches developed in the context of Gaussian nonparametric regression, such as (locally)
adaptive estimation, as well as the corresponding (simultaneous) inference, can be directly applied. Bayesian tools for adaptive estimation and inference in Gaussian nonparametric regression as proposed in can also be employed.

Appendix

Throughout the appendix, we denote by c, c 1 , C, C 1 , . . . etc. generic constants, that are independent of n and p. To simplify the notation, the constants are sometimes skipped and we write for less than or equal to up to constants. We embed the p-dimensional Toeplitz matrix Σ = toep(σ 0 , . . . , σ p−1 ) in a (2p − 2)dimensional circulant matrix Σ = toep(σ 0 , . . .
, σ p−1 , σ p−2 , . . . , σ 1 ). Then, Σ = with the conjugate transpose U * , and Λ is a diagonal matrix with the k-th diagonal value for k = 1, ..., p given by Furthermore, Σ = V * ΛV , where V ∈ C (2p−2)×p contains the first p columns of U . In particular, b(j, r) = b(j, 2p−r) and c(j, r) = −c(j, 2p−r) for r = p+1, . . .
, 2p−2. Together, we have (A.1) Some calculations show that for r = 1, . . . p Using the Taylor expansion of cot(x) for 0 < |x| < π one obtains for r = 1, . . . p where the O term does not depend on j and the hidden constant does not depend on r, p. If i = j, equations (A.1) -(A.3) imply where the O terms do not depend on j.
Since the complex exponential function is Lipschitz continuous with constant L = 1, it holds λ r = λ j + L r,j |r − j|p −1 where −1 ≤ L r,j ≤ 1 is a constant depending on r, j. Then, , it is sufficient to consider j = 1, ..., p − 1. We begin with first sum. For a shorter notation, we use k := r − 1 and l := j − 1 in the following.
Then, summing the squares of the first term in (A.4) for l = 0, ..., p−2 on sums of reciprocal powers. If p is even, then the residual terms are given by where φ and φ (1) denote the digamma function and its derivative. If p is odd, similar remainder terms can be derived. To see that R i (l, p) = O(p −1 ) for i = 1, 2, 3 and uniformly in l we use that asymptotically φ(x)∼ log(x)−1/(2x) and
The mixed term are both of the order p −1 . Furthermore, since the harmonic sum diverges at a rate of log(p). Finally, λ j = f (x j )+O{log(p)p −β } by the uniform approximation properties of the discrete Fourier series for Hölder continuous functions (see . All together, we have shown that (DΣD) j,j = where the O terms are uniform over j = 1, ..., p.
Case i = j and |i − j| is even In this case, (DΣD) i,j = a i a j uniformly in i, j. To show that a i a j 2p−2 r=1 λ r c(i, r)c(j, r) = O(p −1 ), we proceed similarly as before. Setting k=r−1, l=j−1, m=i−1 and using that l =m and |l−m| is even, one obtains where for even p the residual terms are given by If p is odd, analogous residual terms can be derived.
Using similar techniques as before, one can show that the two residual terms and the remaining mixed and square terms vanish at a rate of the order O(p −1 ) and uniformly in i, j. Case i = j and |i − j| is odd |r − i| and |r − j| are either odd and even, or even and odd. Without loss of generality, assume that |r − i| is even.
Then, (DΣD) i,j = a i a j 2p−2 r=1 λ r b(i, r)c(j, r). Since b(i, •) is an even function, c(j, •) is an odd function and λ r = λ 2p−r , it follows (DΣD) i,j = 0. The structure of the proof is as follows. First, we derive the L ∞ rate of the periodic smoothing spline estimator H(f ). Then, using the Cauchy-Schwarz inequality and a mean value argument, the convergence rate of the spectral density estimator f is
∞ the first claim of the theorem follows. Finally, we prove the second statement on the precision matrices. For the sake of clarity, some technical lemmas used in the proof are listed separately in A.4. hT → ∞, then with T = p υ for any υ ∈ ((4 − 2 min{1, β})/3, 1), the estimator H(f ) described in Section 3 with q = max{1, γ} satisfies
Proof : Application of the triangle inequality yields a bias-variance decomposition Set T = 2T − 2 and x k = (k − 1)/ T for k = 1, ..., T . Using Lemma 4, we can write where Mirroring and renumerating ζ k , η k , k is similar as for Y * k , k = 1, ..., T . Using the above representation, one can write First we reduce the supremum to a maximum over a finite number of points.
If q > 1, then W (•, x k ) is Lipschitz continuous with constant L > 0. In this case, it holds almost surely that sup ) is a piecewise linear function with knots at x j = j/ T . The factor (ζ k + ξ k ) can be considered as stochastic weights that do not affect the piecewise linear property. Thus, the supremum is attained at one of the knots x j = j/ T , j = 1, ..., T , and (A.7) is also valid for q = 1.
Again with (a + b) 2 ≤ 2a 2 + 2b 2 we obtain We start with bounding . This requires a bound on 1 • ψ 2 denotes the sub-Gaussian norm. In case of a Gaussian random variable the norm equals to the variance. Thus with Lemma 2 and Lemma 4, we obtain Lemma 1.6 of ) then yields Recall that T = p υ for some fixed υ ∈ ((4 − 2 min{1, β})/3, 1).
Using the inequality log(x) ≤ x a /a one can find constants x υ , C υ > 0 depending on υ but not on n, p such that log(2 T ) log(p) Next, we derive a bound for the second term The exponential decay property of the kernel K stated in Lemma 2 yields The first term in (A.9) can be bounded again with Lemma 1.6 of .
We use the fact that for not necessarily independent random variables X 1 , ..., X N R and R > 0 are constants. This is a consequence of Lemma 1 of which yields , it follows that N i=1 a i X i has a subGaussain distribution and the subGaussian norm is bounded by 2R( N i=1 a 2 i ) 1/2 . See for further details on the subgaussian distribution.
T h . For the second inequality Lemma 2(ii) is used. Applying Lemma 1.6 of then yields To bound the second term in (A.9), we use the moment bounds for ξ k derived in Lemma 4. Then, for all integers > 1 Combining the error bounds (A.10) and (A.11) and choosing R=m −1/2 gives By assumption T = p υ and m = np (1−υ) for some fixed υ ∈ ((4 − 2 min{1, β})/3, 1).
If is an integer such that ≥ 1/(1 − υ), then where we used log(x) ≤ x a /a with a = 1/(4 ). Consider 1/2 < β ≤ 1 and let 0 < χ < 1 be a constant. Applying log(x) ≤ x a /a twice with a = χ/(2 ) yields For any fixed υ∈((4 − 2 min{1, β})/3, 1) one can find an integer which is independent of n, p such that the right side of (A.12) holds.
Since p/n → c ∈ (0, ∞] and thus n/p = O(1) and p −1 = O(n −1 ), it follows for satisfying (A.12) that In total, choosing an integer Using the representation in Lemma 4 once more gives for each x ∈ [0, 1] The bounds on k in Lemma 4 imply Consider the case that β ≥ 1. In particular, q = γ and f (q) is α-Hölder continuous.
Since f is a periodic function with f (x) ∈ [δ, M 0 ] and H(y) ∝ φ(m/2)+ log (2y/m), it follows that {H(f )} (q) is also α-Hölder continuous. Extending g := H(f ) to the entire real line, we get Expanding g(t) in a Taylor series around x and using that h −1 K h is a kernel of order 2q, see Lemma 2(iii), it follows that for any x ∈ [0, 1]
where ξ x,t is a point between x and t. Using the fact that the kernel K h decays exponentially and that g (q) is α-Hölder continuous on [δ, M 0 ] with some constant L, the logarithm is Lipschitz continuous on a compact interval, it follows g = H(f ) is β-Hölder continuous. Expanding g to the entire line and using Lemma 2(iii) with
In a similar way as before, one obtains Note that T −β =o(h β ) as β > 1/2, T h → ∞ and h → 0 by assumption. Since the derived bounds are uniform for x ∈ [0, 1] it holds Putting the bounds A.13 and A.14 together gives If h > 0 such that h → 0 and hT → ∞, then with T = p υ for any υ ∈ ((4 − 2 min{1, β})/3, 1), the estimator f described in Section 3 with q = max{1, γ} satisfies
Proof : By the mean value theorem, it holds for some function g between H(f ) and To show that the second term on the right hand side of (A.15) is negligible we use the moment generating function of H(f ) ∞ . In the next paragraph, we derive the asymptotic order of E[exp{λ H(f ) ∞ }] for n, p → ∞, where λ > 0 may depend on n, p or not.
By the exponential decay property of the kernel K stated in Lemma 2 holds First, H(f ) ∞ is bounded with the maximum over a finite number of points. Calculating the derivative of s : Since δ δx s(x) > 0 almost surely for x = x k , the extrema occur at x k , k = 1, ..., T . Thus, for λ > 0 the moment generating function of H(f ) ∞ is bounded by
Let M j = ( T h) −1 T k=1 γ h (x j , x k ), which by Lemma 2 is bounded uniformly in j by some global constant M > 0. By the convexity of the exponential function we obtain √ 2 and by assumption 0 ≤ δ ≤ f ≤ M 0 . Using Lemma 3, Q k can be written as a sum of m = np/T independent gamma random variables, i.e.
The moment generating function of | log(X)| when X follows a Γ(a, b)-distribution is given by where Γ(a) is the gamma function and γ(a, b) is the lower incomplete gamma function. In particular, To derive the asymptotic order of E[exp{λ H(f ) ∞ }] for n, p → ∞ we first establish the asymptotic order of the ratio Γ(a + t)/Γ(a) for a → ∞.
We distinguish the two cases where t is independent of a and where t linearly depends on a. Thus, for 0 < t < a and t independent of a, equation (A.17) implies for a → ∞ that Γ(a + t)/Γ(a) = O(a t ). Similarly, it can be seen that Γ(a − t)/Γ(a) = O(a −t ). If 0 < t < a and t linearly depends on a, i.e. t = ca for some constant c ∈ (0, 1), then we get Γ(a ± t)/Γ(a) = O(a ±t exp{a}) for a → ∞.
Hence, for a fixed λ not depending on n, p and such that 0 < λ < m/( √ 2M j ) we get for sufficiently large n, p If λ = cm such that 0 < λ < m/( √ 2M j ), then for sufficiently large n, p b∈{cδ/m,cM 0 /m} (bm/2) for some constant L > 1. Set K = min j=1,..., T 1/( √ 2M j ) which is a constant independent of n, p. Altogether, we showed that for 0 < λ < Km and n, p → ∞
Bounding the right hand side of (A.15) for some constants c 0 , c 1 > 0 and n, p → ∞ Since g lies between H(f ) and H(f ), and f almost surely pointwise. Thus, for C > f ∞ = M 0 it holds where c 1 := H(C − M 0 ). Applying Markov inequality for t = cm with c ∈ (0, K) and C = 2L 4/c + M 0 where c, K, L are the constants in gives
Together with Proposition 1 follows Using the fact that the spectral norm of a Toeplitz matrix is upper bounded by the sup norm of its spectral density we get sup According to the mean value theorem, for a function g between H(f ) and H(f ), it holds that some constant c 1 > 0 not depending on n, p. Chosing the same constant C as in section A.3.2 it follows
Noting that 1/f ∞ ≤ 1/δ and 2/m exp {φ(m/2)} ∈ [0.25, 1] for m ≥ 1, (A.18) implies for some constants c 2 , c 3 > 0 and n, p → ∞ Since the derived bounds hold for each Σ(f ) ∈ F β , we get all together sup This section states some technical lemmata needed for the proof of Theorem 1. The proofs can be found in the supplementary material.
The first lemma lists some properties of the kernel K h and its extension K h on the real line. The proof is based on . Lemma 2. Let h > 0 be the bandwith parameter depending on N . (i) There are constants 0 < C < ∞ and 0 < γ < 1 such that for all for x, t ∈ [0, 1] Lemma 3 states that the sum of the correlated gamma random variables in each bin can be rewritten as a sum of independent gamma random variables.
for i = 1, ..., n and j = (k − 1)m + 1, ..., km, and x j = (j − 1)/(2p − 2). Finally, Lemma 4 gives explicit bounds for the stochastic and deterministic errors of the variance stabilizing transform. Thus, it quantifies the difference to an exact Gaussian regression setting. This result is a generalization of Theorem 1 of Cai et al.
(2010) adapted to our setting with n ≥ 1 observations and correlated observations. √ 2 can be written as where for the proof of the first statement. Furthermore, for x, t ∈ [0, 1] holds In particular, for some constants C 1 , C 2 > 0 depending on γ ∈ (0, 1) but not on h and x, it holds h (iii) See Lemma 15 of with p = 2q − 1.
It is sufficient to show the statement for n = 1 by independence of the Y i . Then, the number of points per bin is m = p/T . For simplicity, the index i is skipped in the following. First, we write Q k as a matrix-vector product and refactor it so that it corresponds to a sum of independent scaled χ 2 random variables.
In the second step, we calculate the scaling factors. Let E (km) be a diagonal matrix with ones on the (k − 1)m + 1, ..., km-th entries and otherwise zero diagonal elements. Then, By Theorem 1 of for the gamma distribution it follows where Wi,j iid. ∼ Γ(1/2, 2 f (x * k )) and such that Cov( Wi,j , Wi,h ) = Cov(W i,j , W i,h ) for j = (k − 1)p/T + 1, ..., kp/T and h ∈ {1, ..., p} \ {(k − 1)p/T + 1, ..., kp/T }.
Let θ be the maximum difference of the observations' means in each bin. Then, θ = max are defined via quantile coupling, it holds Z k = Φ −1 {F Q( Qk )} (see . Furthermore, define the uniform random variables Let ρ = Cov(Z k , Z l ). Then, the identity implies F Z,Z (x, y) − Φ(x)Φ(y) ≥ 0 for all x, y ∈ R ⇐⇒ ρ ≥ 0, (see .
Since Cov( Qk , Ql ) ≥ 0 and the ratio of two densities is non-negative, x = − 2/m, it follows that f Q(x) is monotone decreasing for x ≥ − 2/m. Furthermore, F Q(− m/2) ≤ 0.5 for all m ∈ N as f Q(x) is right-skewed. In particular, − m/2 ≤ F −1 Q (1/2) for all m ∈ N. Finally, since f Q(− 2/m) → φ(0) for m → ∞ there is a constant c > 0 not depending on m such that
The simulation study in Section 5 is performed in the same way, but with the uniform and the gamma distribution instead of the Gaussian distribution.",['Decorrelating the data before running the PLS algorithm improves the performance of the algorithm.'],8232,multifieldqa_en_e,en,,425c85c840db6f65192aa477d41c9c18a60a4ccaf0e1e6df,Decorrelating the data before running the PLS algorithm improves the performance of the algorithm.,98
Where is the club's headquarters located?,"Football Club Urartu (, translated Futbolayin Akumb Urartu), commonly known as Urartu, is an Armenian professional football team based in the capital Yerevan that currently plays in the Armenian Premier League. The club won the Armenian Cup three times, in 1992, 2007 and 2016. In 2013–2014, they won the Armenian Premier League for the first time in their history.

In early 2016, the Russia-based Armenian businessman Dzhevan Cheloyants became a co-owner of the club after purchasing the major part of the club shares. The club was known as FC Banants until 1 August 2019, when it was officially renamed FC Urartu.

History

Kotayk
Urartu FC were founded as FC Banants by Sarkis Israelyan on 21 January 1992 in the village of Kotayk, representing the Kotayk Province. He named the club after his native village of Banants (currently known as Bayan). Between 1992 and 1995, the club was commonly referred to as Banants Kotayk. During the 1992 season, the club won the first Armenian Cup. At the end of the 1995 transitional season, Banants suffered a financial crisis. The club owners decided that it was better to merge the club with FC Kotayk of Abovyan, rather than disband it. In 2001, Banants demerged from FC Kotayk, and was moved from Abovyan to the capital Yerevan.

Yerevan

FC Banants was relocated to Yerevan in 2001. At the beginning of 2003, Banants merged with FC Spartak Yerevan, but was able to limit the name of the new merger to FC Banants. Spartak became Banants's youth academy and later changed the name to Banants-2. Because of the merger, Banants acquired many players from Spartak Yerevan, including Samvel Melkonyan. After the merger, Banants took a more serious approach and have finished highly in the league table ever since. The club managed to lift the Armenian Cup in 2007.
Experience is making way for youth for the 2008 and 2009 seasons. The departures of most of the experienced players have left the club's future to the youth. Along with two Ukrainian players, Ugandan international, Noah Kasule, has been signed.

The club headquarters are located on Jivani Street 2 of the Malatia-Sebastia District, Yerevan.

Domestic

European

Stadium

The construction of the Banants Stadium was launched in 2006 in the Malatia-Sebastia District of Yerevan, with the assistance of the FIFA goal programme. It was officially opened in 2008 with a capacity of 3,600 seats. Further developments were implemented later in 2011, when the playing pitch was modernized and the capacity of the stadium was increased up to 4,860 seats (2,760 at the northern stand, 1,500 at the southern stand and 600 at the western stand).

Training centre/academy
Banants Training Centre is the club's academy base located in the Malatia-Sebastia District of Yerevan. In addition to the main stadium, the centre houses 3 full-size training pitches, mini football pitches as well as an indoor facility. The current technical director of the academy is the former Russian footballer Ilshat Faizulin.

Fans
The most active group of fans is the South West Ultras fan club, mainly composed of residents from several neighbourhoods within the Malatia-Sebastia District of Yerevan, since the club is a de facto representer of the district. Members of the fan club benefit from events organized by the club and many facilities of the Banants training centre, such as the mini football pitch, the club store and other entertainments.

Achievements
 Armenian Premier League
 Winner (1): 2013–14.
 Runner-up (5): 2003, 2006, 2007, 2010, 2018.

 Armenian Cup
 Winner (3): 1992, 2007, 2016.
 Runner-up (6): 2003, 2004, 2008, 2009, 2010, 2021–22

 Armenian Supercup
 Winner (1): 2014.
 Runner-up (5): 2004, 2007, 2009, 2010, 2016.

Current squad

Out on loan

Personnel

Technical staff

Management

Urartu-2

FC Banants' reserve squad play as FC Banants-2 in the Armenian First League. They play their home games at the training field with artificial turf of the Urartu Training Centre.

Managerial history
 Varuzhan Sukiasyan (1992–94)
 Poghos Galstyan (July 1, 1996 – June 30, 1998)
 Oganes Zanazanyan (2001–05)
 Ashot Barseghyan (2005–06)
 Nikolay Kiselyov (2006–07)
 Jan Poštulka (2007)
 Nikolay Kostov (July 1, 2007 – April 8, 2008)
 Nedelcho Matushev (April 8, 2008 – June 30, 2008)
 Kim Splidsboel (2008)
 Armen Gyulbudaghyants (Jan 1, 2009 – Dec 1, 2009)
 Ashot Barseghyan (interim) (2009)
 Stevica Kuzmanovski (Jan 1, 2010 – Dec 31, 2010)
 Rafael Nazaryan (Jan 1, 2011 – Jan 15, 2012)
 Volodymyr Pyatenko (Jan 17, 2013 – June 30, 2013)
 Zsolt Hornyák (July 1, 2013 – May 30, 2015)
 Aram Voskanyan (July 1, 2015 – Oct 11, 2015)
 Tito Ramallo (Oct 12, 2015 – Oct 3, 2016)
 Artur Voskanyan (Oct 3, 2016 – Aug 11, 2018)
 Ilshat Faizulin (Aug 12, 2018 –Nov 24, 2019)
 Aleksandr Grigoryan (Nov 25, 2019 –Mar 10, 2021)
 Robert Arzumanyan (10 March 2021–24 June 2022)
 Dmitri Gunko (27 June 2022–)","[""The club's headquarters are located on Jivani Street 2 of the Malatia-Sebastia District, Yerevan.""]",812,multifieldqa_en_e,en,,d5422b0fdbed42c0f1d2213edb6c7637802452e4ca7287eb,"The club's headquarters are located on Jivani Street 2 of the Malatia-Sebastia District, Yerevan.",97
What is the main advantage of the proposed method in terms of computation time?,"Paper Info

Title: Incorporating Human Path Preferences in Robot Navigation with Minimal Interventions
Publish Date: 16 Mar 2023
Author List: Oriana Peltzer, Dylan Asmar, Mac Schwager, Mykel Kochenderfer

Figure

Hyperplane arrangement of a twodimensional space containing two obstacles (colored in gray).The robot is located inside the pink polytope, surrounded by three adjacent obstacle-free polytopes.Each hyperplane on the boundary of the robot's polytope corresponds to one of the nonredundant constraints in eq.(4).(b)Graph derived from the hyperplane arrangement.The nodes on the graph designate polytopes, and edges designate transitions to adjacent polytopes.To estimate the human's preference, the robot updates a posterior over the goal and over which of the graph transitions φ 1 , φ 2 and φ 3 is preferred by the human.(c)Example preference defined over the graph.The location of the goal is indicated in yellow in the lower right polytope.For each node, the outgoing pink arrow designates the edge on the graph corresponding to the preferred transition between polytopes.
Simple, 10 × 10, 8 polytopes.(b) Map 2: Office, 10 × 10, 56 polytopes.(c) Map 3: Classroom, 20 × 20, 73 polytopes.(d) Sampled observations and robot's executed trajectories.
Fig.5: Maps used for simulating the robot navigation problem with path preferences.In (d), the heading angles observed are indicated with arrows.The goal is indicated with a pink circle, and the orange robot corresponds to the starting location.The blue robot follows a policy that accounts for path preference, while the green robot does not.The opacity of the robots increases with time.
Map 1 problem setup and example realizations for goal-only (green) and path preference (blue) solution methods.The robot starts at the lower left corner of the environment, and the goal of the task (pink circle) is in the upper left area.The robot does not know which goal, among 10 options (shown in light blue squares), is the correct goal.The human provides noisy observations, indicated by arrows, at each iteration.The green robot selects actions according to the goal-only baseline, and the blue robot uses our proposed method to infer path preferences.The polytopes composing G are drawn in blue.Probability of correct goal.WLPHVWHS +J (c) Entropy of goal distribution g.
Fig. 6: Probability of the correct goal, fig.6b, and entropy of the goal belief distribution P (g), fig.6c, for the same problem setup, fig.6a.In this problem instance, the human's preference is to go to the goal by passing on the right side of the obstacle.Results are averaged over 50 runs and the area filled represents one standard deviation above and below the mean value.The goal-only baseline shows an over-confident prediction (shown by the strong reduction in belief entropy) that the correct goal is less likely, making it more difficult to reach the correct goal compared to a method that accounts for path preference.
Success rates in the simple environment (Map 1).The results are averaged over 6 randomly sampled problem instances (start location, goal location, and goal possibilities), and over 50 runs per problem instance.∆T is the number of time steps separating two consecutive human inputs.The robot's mission time is Tmax = 30 time steps.We selected γ h = 1.5, corresponding to relatively noisy human inputs and making the problem more difficult to solve for the robot.
Computation times for Goal Only and Path Preference methods on Map 1 (fig.5a),Map 2 (fig.5b), and Map 3 (fig.5c),averaged over 100 runs with randomly sampled problem instances.The 95 % confidence interval is provided with the mean.We evaluate computation time at the first iteration of each run (where the search depth takes on its highest value Tmax).

abstract

Robots that can effectively understand human intentions from actions are crucial for successful human-robot collaboration. In this work, we address the challenge of a robot navigating towards an unknown goal while also accounting for a human's preference for a particular path in the presence of obstacles.
This problem is particularly challenging when both the goal and path preference are unknown a priori. To overcome this challenge, we propose a method for encoding and inferring path preference online using a partitioning of the space into polytopes. Our approach enables joint inference over the goal and path preference using a stochastic observation model for the human.
We evaluate our method on an unknown-goal navigation problem with sparse human interventions, and find that it outperforms baseline approaches as the human's inputs become increasingly sparse. We find that the time required to update the robot's belief does not increase with the complexity of the environment, which makes our method suitable for online applications.

INTRODUCTION

Collaboration between humans and robots has become increasingly important and one key aspect of this collaboration is the ability for robots to adapt to human decisions. In many scenarios, such as a robot navigating through a busy room to deliver an item, it is important for the robot to take into account human preferences.
For instance, humans may prefer a specific path that would allow their colleagues to notice the item being delivered, but this preference may change dynamically based on various factors such as changes in the environment or unforeseen circumstances. While some preferences can be incorporated into the path-planning process, accommodating dynamic user preferences in real-time remains challenging.
In this paper, we propose a way to enable robots to adapt to human preferences dynamically by leveraging real-time feedback to inform decision-making. In this work, we tackle the problem of robot navigation in which the robot cannot observe the goal or the preferred path to the goal, but must make navigation decisions that are influenced by humans through recommended actions.
Prior work has explored how to adapt to a human's preference through feedback, but such approaches often require a high level of intervention, which can be time-consuming and impractical in real-world scenarios. To optimize the use of human input and quickly infer the human's preference, Fig. : An autonomous robot navigates in a simulated classroom towards a goal location (pink circle).
At the start of its mission, it receives direction indications (arrows) from a human that indicate which path it should take to get to the goal. In this scenario, the human wants the robot to go around the desks on the right side of the classroom. A robot that does not reason over path preferences (green) will take the shortest path to the goal regardless of the human's input.
Our method (blue) infers the human's path preference from these indications and adapts to their recommendations. we propose an approach that leverages probabilistic representations of human preference and incorporates real-time feedback. Previous research by Bajcsy et al. considered an online adaptation problem in a manipulation task, where the person can apply forces to the robot to indicate their preferences.
By allowing the robot to continue its task while taking into account a probabilistic representation of human preference, their approach does not require frequent inputs. Building on this idea, we adopt a similar approach to adapt to a human's preference in the context of a robot autonomously navigating through a known environment, such as a cluttered office space.
Specifically, we focus on allowing the human to influence the robot's trajectory with respect to obstacles, by providing guidance on preferred routes or paths, while the robot continues to execute its task. Paths can be represented using homotopy classes . However, homotopies can pose computational challenges when used to encode and infer human preferences.
When the robot maintains a belief over homotopy classes, the inference problem can become exponentially complex with the number of obstacles in the space. Additionally, when the goal is unknown, the number of variables increases with the number of candidate destinations. This complexity can render the decision-making problem intractable.
Our solution is to encode path preference based on a partitioning of the environment into polytopes . This representation allows path preferences to be expressed as sets of preferred transitions between adjacent polytopes. Paths belonging to different homotopy classes correspond to different sequences of transitions.
By leveraging conditional independence assumptions, we can make the Bayesian inference problem tractable. These assumptions exploit the fact that human actions provide information about the path in a piece-wise manner. For example, indicating a preference for navigating around a particular obstacle only provides information about the local area and not the entire path.
Finally, after updating its belief representation over the human's preference, the robot can adapt to indications by replanning online. Our contributions are as follows. • We formulate the human-robot collaboration problem as a Partially Observable Markov Decision Process (POMDP) where both the goal of the task and the human's path preference are unknown random variables.
• We propose an encoding of a human's path preference using a partitioning of the environment into polytopes, along with conditional independence assumptions that make the Bayesian inference problem tractable to infer the task goal and path preference online. • Through simulations in two environments of different sizes and complexity, we show that our method is effective for solving problems where the robot must reach a goal that is unknown a-priori while simultaneously adapting to a human's indications.
Our method shows higher success rates compared to baseline approaches when the human inputs are sparse. Our approach enables a robot to make effective navigation decisions in collaboration with a human, even when the goal and path preference are not known in advance, and with minimal human input. In recent years, there has been a growing interest in shared autonomy and interactive systems, where humans and robots work together to accomplish tasks.
Several approaches have been proposed to address the challenge of enabling effective collaboration between human and robot agents while still achieving high task performance. Losey et al. and Jeon, Losey, and Sadigh propose a framework where a human operator is given control of a task-relevant latent action space while an autonomous system handles the rest.
Dragan and Srinivasa present a formalism for arbitrating between a user's input and a robot's policy when both human and robot share control of the same action space. Cognetti et al. [7] provide a method for real-time modifications of a path, . . . Fig. : We model the intent inference problem with the above diagram.
At each step in time, the robot receives an observation ot from the human conditioned on its current location st, the intended goal g, and the human's path preference θ. The robot updates its belief over g and θ and transitions to a next location st+1. while Hagenow et al. present a method that allows an outside agent to modify key robot state variables and blends the changes with the original control.
However, a common challenge of these approaches is the high level of intervention required from humans. Best and Fitch propose a method for predicting an agent's intended trajectory from observations. Rather than maintaining a belief over the agent's future path, they infer the agent's intended goal among a set of candidate locations at the boundary of the space.
This approach provides information on where the agent is heading and generates a distribution of candidate future trajectories for the agent. Inferring the goal of the task among a discrete set of candidates is also relevant to the area of shared autonomy. Javdani, Srinivasa, and Bagnell propose a formalism for shared control of a robotic arm, where the robot must assist the human in picking up an object but needs to infer which object the human has chosen from joystick inputs.
Planning with homotopy class constraints is useful in problems where the robot's requirements are given with respect to obstacles, and Yi, Goodrich, and Seppi consider topological constraints provided by human operators. Bhattacharya propose an efficient algorithm for solving pathplanning problems under homotopic constraints.
However, the number of homotopy classes for a given problem can be infinite, and as the robot changes location and updates its representation of the world, carrying out inference over homotopy classes in a dynamic environment requires recomputing the set of homotopies at every iteration, making the belief update challenging.
Prior work has addressed the challenge of shared autonomy by considering how robots can infer a human's intended goal, or how they can infer the preferred path to a goal. However, we argue that inferring the goal and the path as separate problems can lead to over-confidence in incorrect beliefs about the user's preferences.
To illustrate this point, consider the following scenario: a robot and a human are collaborating to move an object from one end of a room to Fig. : Using the hyperplanes composing the H-representation of each obstacle, we construct a hyperplane arrangement of the obstacle-free space (a). We define the human's preference for the robot's one step action choices as the posterior distribution (given all human input up to that point) over transitions from the current to the neighboring polytopes, i.e. edges on the graph.
Each time the robot transitions to a new polytope, the set of neighbor polytopes and the distribution over human preferences are updated. another, but there is an obstacle in the way. The human would like the robot to take a path around the obstacle on the left, even though the goal is on the right. If the robot only infers the goal from the human's inputs, it may incorrectly assume that the goal is on the right, and become over-confident in this belief.
On the other hand, if the robot only infers the preferred path, it may mistakenly assume that the goal is on the left, leading to a failure in completing the task. To overcome these challenges, our work proposes a joint inference approach that considers both the human's intended goal and their preferred path to that goal.
Specifically, we model the human's preference over different homotopy classes and leverage a conditional independence assumption to provide a tractable solution. In our approach, we assume that the human's inputs are noisily rational conditioned on both the goal and the preference. By jointly inferring the goal and path preference, we can avoid over-confidence in incorrect beliefs about the user's preferences, leading to improved system performance.
We consider the problem of robot navigation in a known environment to an unknown destination, where a human can intervene and provide a heading direction to the robot using a joystick or force cues. The human also has a preference on which path the robot should take with respect to obstacles, and our objective is for the robot to understand the human's intentions and execute the task with minimal interventions.
Let g be a discrete random variable denoting the goal of the task, belonging to a set of candidates Ω g , and let θ be a discrete-valued random variable representing the human's path preference, belonging to a set of possible preferences Θ. The physical location of the robot at time index t is denoted by s t ∈ R 2 , and the robot's action at time index t, belonging to some action space A, is denoted by a t .
The transition model T (s t+1 | s t , a t ) is deterministic, meaning the robot has full control over its future location. At any time step, the human may provide an observation to the robot. When the human intervenes, the robot receives a direction (heading angle) that can be mapped to a future location in space.
More specifically, we map the direction to an intended location, which is the resulting robot location after advancing in the indicated direction for one time step. For simplicity, we consider that the robot directly makes an observation o t of the location indicated by the human. We assume that the robot has a stochastic observation model for the human P (o t | s t , g, θ) that is conditioned on both the goal of the task g and the human's preferred path θ.
We further assume that having chosen a goal and path preference, the human takes actions to noisily minimize a cost function C g,θ that measures the cost of moving from the robot's current location to the goal along the preferred path. For example, C g,θ (s t , o t ) can be the length of the shortest path from location s t to the goal g after taking a first step to o t , and constrained by path preference θ.
We use C g,θ to induce a probability distribution over observations, given by: where γ h is a hyperparameter that designates the rationality coefficient. This model assumes the human will pick the lowest cost action with the highest probability and the likelihood of an action decreases exponentially with the increase in cost .
Our inclusion of the path preference θ sets our approach apart from . The model is shown in fig. represented as a Bayesian Network.

Inference

At each time step where the human provides an observation, the posterior P (g, θ) is given through the Bayesian update We note that the number of Bayesian updates required at each iteration to update the belief is equal to the cardinality of Ω g × Θ. In addition, each Bayesian update involves computing C g,θ ( .
, . ) in eq. ( ), which involves solving an optimization problem (such as a shortest path problem). In section IV, we propose a specific encoding of preference θ for resolving eq. ( ), while ensuring the number of computations of the cost C g,θ (., .) per update does not grow exponentially with the number of obstacles.

Decision Making

We consider a navigation problem where the robot receives reward according to the model R(s t , g, θ, a t ). We wish to find the optimal policy π that maximizes the expected discounted sum of future rewards, with discount factor γ. The above problem is a Partially Observable Markov Decision Process (POMDP) .
In this section, we propose an encoding of human's path preference θ for computing the posterior in eq. ( ). Devifrom the concept of homotopy classes, we define the preference according to a partitioning of the environment into polytopes, as shown in fig. , creating a hyperplane arrangement of the space.
Hyperplane arrangements have been used by Vincent and Schwager in the context of Neural Network verification. In our setting, we leverage this representation to define path preferences as preferred transitions between adjacent regions of the space.

Hyperplane Arrangement

We assume a two-dimensional environment composed of m polytopic obstacles, each defined by their half-space representation (H-representation) where A i ∈ R di×2 and b i ∈ R di , and where d i is the number of edges (hyperplanes) composing polytope i. Let n = i d i be the total number of hyperplanes. We leverage each obstacle's H-representation to construct a hyperplane arrangement of the environment as shown in fig.
.e. a partitioning of the space into polytopes. More specifically, each location in space belongs to a polytope j for which we can write an H-representation of the form where α j i ∈ {−1, 1} di is a vector specific to polytope j and obstacle i corresponding to the relative position of any point in the set with respect to each hyperplane in O i .
Fig. : Intent inference model in a hyperplane arrangement of the obstacle free space. We spatially decompose the preference θ into a set of preferred neighboring polytopes per region of the space. Within each polytope j, the human preference pj is a discrete distribution over the preferred neighbor in N (j).
We assume that for a location st belonging to polytope j, and given goal g and preference pj, the observation ot and any other preference p i,i =j are conditionally independent. Concatenating elements from each obstacle's Hrepresentation, we can write polytope j's H-representation as where Some of the constraints in eq. ( ) (corresponding to rows of A, b and α j ) are redundant, i.e. the set P j does not change upon their removal.
We can further reduce the Hrepresentation of a polytope to include only non-redundant constraints. By removing the rows corresponding to redundant constraints, we obtain new matrices A j e , b j e and α j e such that we can write the polytope's reduced H-representation as The non-redundant constraints correspond to edges of the polytope.
In other words, as the robot continually moves in space, the first hyperplane that it will cross upon exiting the polytope will correspond to one of the polytope's nonredundant constraints. Vincent and Schwager outline an iterative method for removing redundant constraints by solving n linear programs.
We use this method in practice for computing α j e for each polytope. We can now characterize each polytope by a vector α j e ∈ {−1, 1} n j e , where n j e ≤ n is the number of essential constraints of the polytope. The polytopes P j partition the environment into a hyperplane arrangement.

Path Preference

In this section, we provide a definition of preference θ according to a graphical representation of the environment based on the hyperplane arrangement. Under this representation, a path preference corresponds to a set of preferred transitions. In other words, for each polytope in the space, the human will have a preference to which neighboring polytope they wish to transition.
Let G := (V, E) be an undirected graph, where vertices are obstacle-free polytopes, and edges connect two adjacent polytopes. Each polytope is described by a unique vector α j as defined in eq. ( ). Two polytopes are adjacent if they share non-redundant constraints (rows in eq. ( )) corresponding to the same hyperplane (i.e. they are on opposite sides of the hyperplane).
Let N (v) be the set of neighbors of a vertex v. For each vertex, we denote p v the discrete-valued random variable describing which edge in N (v) the human intends to transition to. Using this formalism, we define a path preference as the set of preferred transitions over all nodes in the graph, Let m θ = v∈V |N (v)| be the cardinality of Θ, and m g = |Ω g | the number of possible goals.
A priori, the number of Bayesian updates required to update the belief at every iteration should be m θ × m g . Now, let us assume the conditional independence relationships described by the new problem diagram in fig. . More specifically, we introduce the assumption that conditioned on a robot location s t , the goal g, and the preference for the corresponding vertex p v in the graph, the observation o t and the preference for any other vertex are conditionally independent.
In other words, the observations the human provides can be defined conditioned only on the robot location, the goal, and the human's preference for its current vertex p v . By introducing this assumption, each update step only requires updating the joint (p v , g), reducing the number of cost computations to |N (v)| × m g .
We can notice that by introducing this assumption, we removed the direct relationship between the number of polytopes in the environment and the complexity of the Bayesian update in eq. ( ). In practice, components of θ are not mutually independent. For example, if the human preference at a vertex v 1 is
, it is unlikely that the human will also prefer p v2 = (v 2 , v 1 ) (turning back). We can improve our model by assuming a dependent relationship between preferences for adjacent edges, which does not significantly increase the complexity of the inference problem. An interesting property of our encoding is that any two paths that belong to different homotopy classes will cross different sequences of polytopes, i.e. they correspond to a different sequence of edges on G.
This can be proved by contradiction. Let us suppose that two continuous trajectories ξ 1 and ξ 2 , with the same start and end points and that do not intersect any obstacle, traverse the same regions in G in the same order. From the construction of the hyperplane arrangement, each polytope that the paths traverse through is obstacle-free.
Therefore, within each polytope, there is no obstacle in the area located in between the portions of ξ 1 and ξ 2 that belong to the region. A smooth transformation of ξ 1 into ξ 2 can be obtained by transforming each portion of ξ 1 belonging to the polytopes it intersects into the corresponding portion of ξ 2 for the same polytopes, where the extremities of the trajectory portions are connected to one another along the polytope's edges (where the same edge is crossed by both paths).
Along this transformation, the paths do not intersect any obstacle, and therefore ξ 1 and ξ 2 belong to the same homotopy class.

EXPERIMENTS

We evaluate our model on a simulated navigation task where the robot must reach a goal that is unknown a priori while respecting the path preferences indicated by a human. The robot navigates in a grid world containing obstacles. The transition model is deterministic: the robot selects an adjacent location on the grid to reach at the next time step.
The robot is also allowed to take diagonal actions. Each location s t in the map can be mapped to a vertex v t ∈ G. Therefore, the actions leading to locations mapped to different vertices correspond to edges on the graph. We note f (s t , a t ) the edge crossed by taking action a t from location s t .
The robot is given a mission time limit T max for reaching the goal. In this problem, we assume that the human selects actions to noisily minimize a cost function C g,θ , where θ is defined as per eq. ( ), corresponding to the length of the shortest path to the goal constrained by the preference (where the robot is only allowed to make transitions on G along preferred edges).
More specifically, where δ(s t , g | o t , p vt ) designates the length of the shortest path from s t to g passing by o t and constrained by preference p vt . This is a slight variant of the cost function proposed by Best and Fitch , where we add in a conditioning on the path preference. We compute costs by running the A path planning algorithm on the environment maps (grid worlds with diagonal actions) and impose preference constraints by pruning invalid transitions from the search tree.
Reward model. At each step in time, the robot receives a reward which is a sum of three components: a goal-specific reward a preference-specific reward or penalty We compute solutions to the POMDP defined in section III-B with the online solver POMCP , and with the particularity that within the rollouts, the robot does not expect to collect human inputs.
Each time a solution is computed, the robot takes an action and may receive an observation. If it does, it updates its belief distribution over the unknown problem variables and resolves the POMDP over a receding horizon.

Baselines

• Goal only. The robot solves the POMDP while ignoring the effects of path preference. Similarly to , we assume the human is taking action to minimize a goaldependent cost C g (s t , o t ) = δ(s t , g | o t ), where the conditioning on the preference is removed. We also omit the path preference's contribution to the reward R pref .
• Compliant. The robot complies with the human input, but does not take an initiative. If the user stops providing information, the robot continues in the last direction indicated for 5 time steps (conserving its momentum), then stops. • Blended. We designed an arbitration function to decide between our proposed policy (accounting for path preferences) and the user's recommendation when the robot receives inputs.
Our metric to evaluate confidence in the robot's prediction for the purpose of arbitration is the entropy of the intention distribution H(g, p i ), where p i denotes the preferred neighbor for the current region. Because our representation of the world is discrete, the arbitration is given by a step function.
Denoted by U , the action corresponding to the human's input, and P , the robot's prediction for the optimal action, we write the policy where we chose h = 1.6 as the confidence threshold.

Results

When evaluating the algorithm, we consider that a run is successful if the robot reached the goal within its allocated mission time T max and only made transitions between graph vertices corresponding to the human's preferences. We vary the time delay between human inputs, from constant guidance (∆ T = 1) to only a single observation (∆ T ≥ T max ).
Success rates. Table I reports the success rates for experiments conducted over six randomly sampled problem instances and 50 runs per instance in Map 1 (fig. ). When the human provides inputs at every iteration, the compliant policy shows the highest success rates. However, as ∆ T increases, the compliant robot is not able to accomplish the task within the allotted time as it does not receive sufficient inputs to do so, and performance decreases compared to the autonomous baselines.
We find that in these runs, accounting for path preference consistently improves performance compared with the goal-only baseline. Results also show that blending the user's input with the robot's policy (Path Preference + Blend) when the human provides information leads to improved performance. Belief entropy.
Figure shows a challenging problem instance where the directions the human provides do not align directly with the shortest path to the goal. By ignoring the effects of preferences in the problem model (goal only), the robot quickly infers from observations that the upper left goal is less likely than others (P (g) drops).
The strong decrease in entropy shows that the robot becomes overconfident in this prediction. Overconfidence in an incorrect goal will prevent the agent from finding the correct goal once the human's indications directly align with it, as it needs to correct for the wrong predictions, as shown in the path realization (fig.
). In this realization, the goal-only method (green robot) fails to search the upper left area within the allotted time. By accounting for path preferences in its model, the blue robot's entropy over the goal distribution decreases more steadily, allowing for it to leverage the human's latest observations and reach the goal successfully.
shows an over-confident prediction (shown by the strong reduction in belief entropy) that the correct goal is less likely, making it more difficult to reach the correct goal compared to a method that accounts for path preference. Computation time. In table II we provide the time required to solve the POMDP, and the time required to update the robot's belief as it receives new observations.
We compute solutions on three maps: a simple 10 × 10 grid world with 8 polytopes (fig. ), a 10 × 10 grid world with 56 polytopes (fig. ), and a 20×20 grid world with 73 polytopes (fig. ). The latter environment being larger, we increase the mission time and the depth of the search tree in POMCP from T max = 30 (Map 1 and Map 2) to T max = 60 (Map 3).
We do not notice an increase in the time required to update the robot's belief with an increase in problem complexity, which is consistent with our observation that the complexity of the Bayesian update should not increase with the number of obstacles or polytopes. On the contrary, the belief update time on Map 2 and Map 3, containing more obstacles, is reduced compared to the first map.
More obstacles result in fewer iterations when solving the constrained shortest path problem with A . Adding constraints due to the obstacles and polytopes reduces the size of the A search tree. C. Limitations Simulation environments. In our simulations, we hardcoded the preference policy over the maps (e.g. in Map 1, go around the table counter-clockwise).
We randomly sampled problem instances (start and goal locations, and goal options) to reduce the bias introduced by these preference choices. To best evaluate and compare the different approaches, it would be best to sample preferences among a distribution of preferences chosen by a human (for example, from benchmarks resulting from a collection of data).
Creating such a benchmark is an interesting direction for future work. Hyperplane arrangement construction. The main limitation of our approach is that the size and geometry of each polytope depends strongly on the geometry of the obstacles, as seen in fig. . Because of this, the robot can make predictions over preferences that are too refined compared with the topology of the environment.
A direct consequence is that when the size of the polytopes is small, the information provided by the human can be incorrectly interpreted as a preference on the robot's immediate action. Our method can be improved by changing the structure of the hyperplane arrangement so that it relies on the topology of the environment, but does not vary strongly with the geometry of the features in the environment.
For this purpose, topometric maps and region construction algorithms are promising directions. We presented an approach for encoding and inferring a human's path preference in an environment with obstacles. By leveraging a partitioning of the space into polytopes and a stochastic observation model, our method allows for joint inference over the goal and path preference even when both are unknown a-priori.
Our experiments on an unknown-goal navigation problem with sparse human interventions demonstrate the effectiveness of our approach and its suitability for online applications. The time required to update the robot's belief does not increase with the complexity of the environment, which further highlights the practicality of our method.",['The time required to update the belief does not increase with the complexity of the environment.'],5665,multifieldqa_en_e,en,,aeb34faf1b5507b32d6d5b49474ea5c71d4ec2aa84a82d93,The time required to update the belief does not increase with the complexity of the environment.,96
"What is the electron correlation parameter, $\Gamma_e$?","\section{Introduction}

Ultracold neutral plasmas studied in the laboratory offer access to a regime of plasma physics that scales to describe thermodynamic aspects of important high-energy-density systems, including strongly coupled astrophysical plasmas \cite{VanHorn,Burrows}, as well as terrestrial sources of neutrons \cite{Hinton,Ichimaru_fusion,Atzeni,Boozer} and x-ray radiation \cite{Rousse,Esarey}.  Yet, under certain conditions, low-temperature laboratory plasmas evolve with dynamics that are governed by the quantum mechanical properties of their constituent particles, and in some cases by coherence with an external electromagnetic field.   

The relevance of ultracold plasmas to such a broad scope of problems in classical and quantum many-body physics has given rise to a great deal of experimental and theoretical research on these systems since their discovery in the late 90s.  A series of reviews affords a good overview of progress in the last twenty years \cite{Gallagher,Killian_Science,PhysRept,Lyon}.  Here, we focus on the subset of ultracold neutral plasmas that form via kinetic rate processes from state-selected Rydberg gases, and emphasize in particular the distinctive dynamics found in the evolution of molecular ultracold plasmas.  

While molecular beam investigations of threshold photoionization spectroscopy had uncovered relevant effects a few years earlier \cite{Scherzer,Alt}, the field of ultracold plasma physics began in earnest with the 1999 experiment of Rolston and coworkers on metastable xenon atoms cooled in a magneto optical trap (MOT) \cite{Killian}.  

This work and many subsequent efforts tuned the photoionization energy as a means to form a plasma of very low electron temperature built on a strongly coupled cloud of ultracold ions.  Experiment and theory soon established that fast processes associated with disorder-induced heating and longer-time electron-ion collisional rate processes act to elevate the ion temperatures to around one degree Kelvin, and constrain the effective initial electron temperature to a range above 30 K \cite{Kuzmin,Hanson,Laha}.  

This apparent limit on the thermal energy of the electrons can be more universally expressed for an expanding plasma by saying that the electron correlation parameter, $\Gamma_e$, does not exceed 0.25, where, 
\begin{equation}
\Gamma_e = \frac{e^2}{4\pi \epsilon_0 a_{ws}}\frac{1}{k_B T_e}
\label{eqn:gamma_e}
\end{equation}
defines the ratio of the average unscreened electron-electron potential energy to the electron kinetic energy.  $a_{ws}$ is the Wigner-Seitz radius, related to the electron density by, $\rho_e = 1/(\frac{4}{3} \pi a_{ws}^3)$.  These plasmas of weakly coupled electrons and strongly coupled ions have provided an important testing ground for ion transport theory and the study of electron-ion collision physics \cite{Strickler}.

Soon after the initial reports of ultracold plasmas formed by direct photoionization, a parallel effort began with emphasis on the plasma that forms spontaneously by Penning ionization and electron-impact avalanche in a dense ultracold Rydberg gas \cite{Mourachko}.  This process affords less apparent control of the initial electron temperature.  But, pulsed field-ionization measurements soon established that the photoionized plasma and that formed by the avalanche of a Rydberg gas both evolve to quasi-equilibria of electrons, ions and high-Rydberg neutrals \cite{Rolston_expand,Gallagher}.  

Early efforts to understand plasmas formed by Rydberg gas avalanche paid particular attention to the process of initiation.  Evolution to plasma in effusive atomic beams was long known for high-Rydberg gases of caesium and well explained by coupled rate equations \cite{Vitrant}.  But, low densities and ultracold velocity distributions were thought to exclude Rydberg-Rydberg collisional mechanisms in a MOT.  

In work on ultracold Rydberg gases of Rb and Cs, Gallagher, Pillet and coworkers describe the initial growth of electron signal by a model that includes ionization by blackbody radiation and collisions with a background of uncooled Rydberg atoms \cite{Mourachko,Gallagher,Li,Comparat,Tanner}. This picture was subsequently refined to include many-body excitation and autoionization, as well as attractive dipole-dipole interactions \cite{Viteau,Pillet}, later confirmed by experiments at Rice \cite{Mcquillen}.  

The Orsay group also studied the effect of adding Rydberg atoms to an established ultracold plasma.  They found that electron collisions in this environment completely ionize added atoms, even when selected to have deep binding energies \cite{Vanhaecke}.  They concluded from estimates of electron trapping efficiency that the addition of Rydberg atoms does not significantly alter the electron temperature of the plasma.  

Tuning pair distributions by varying the wavelength of the excitation laser, Weidem\""uller and coworkers confirmed the mechanical effects of van der Waals interactions on the rates of Penning ionization in ultracold $^{87}$Rb Rydberg gases \cite{Amthor_mech}.  They recognized blackbody radiation as a possible means of final-state redistribution, and extended this mechanical picture to include long-range repulsive interactions \cite{Amthor_model}.  This group later studied the effects of spatial correlations in the spontaneous avalanche of Rydberg gases in a regime of strong blockade, suggesting a persistence of initial spatial correlations \cite{RobertdeSaintVincent}.  

Robicheaux and coworkers have recently investigated the question of prompt many-body ionization from the point of view of Monte Carlo classical trajectory calculations \cite{Goforth}.  For atoms on a regular or random grid driven classically by an electromagnetic field, they find that many-body excitation enhances prompt ionization by about twenty percent for densities greater than $5.6 \times 10^{-3}/(n_0^2 a_0)^3$, where $n_0$ is the principal quantum number of the Rydberg gas and $a_0$ is the Bohr radius.  They observed that density fluctuations (sampled from the distribution of nearest neighbour distances) have a greater effect, and point to the possible additional influence of secondary electron-Rydberg collisions and the Penning production of fast atoms not considered by the model, but already observed by Raithel and coworkers \cite{Knuffman}.  

The Raithel group also found direct evidence for electron collisional $\ell$-mixing in a Rb MOT \cite{Dutta}, and used selective field ionization to monitor evolution to plasma on a microsecond timescale in ultracold $^{85}$Rb $65d$ Rydberg gases with densities as low as $10^8$ cm$^{-3}$ \cite{WalzFlannigan}.  Research by our group at UBC has observed very much the same dynamics in the relaxation of Xe Rydberg gases of similar density prepared in a molecular beam \cite{Hung2014}.  In both cases, the time evolution to avalanche is well-described by coupled rate equations (see below), assuming an initializing density of Penning electrons determined by Robicheaux's criterion \cite{Robicheaux05}, applied to an Erlang distribution of Rydberg-Rydberg nearest neighbours.  

Theoretical investigations of ultracold plasma physics have focused for the most part on the long- and short-time dynamics of plasmas formed by direct photoionization \cite{PhysRept,Lyon}.  In addition to studies mentioned above, key insights on the evolution dynamics of Rydberg gases have been provided by studies of Pohl and coworkers exploring the effects of ion correlations and recombination-reionization on the hydrodynamics of plasma expansion \cite{Pohl:2003,PPR}.  Further research has drawn upon molecular dynamics (MD) simulations to reformulate rate coefficients for the transitions driven by electron impact between highly excited Rydberg states \cite{PVS}, and describe an effect of strong coupling as it suppresses three-body recombination \cite{Bannasch:2011}.  MD simulations confirm the accuracy of coupled rate equation descriptions for systems with $\Gamma$ as large as 0.3.  Newer calculations suggest a strong connection between the order created by dipole blockade in Rydberg gases and the most favourable correlated distribution of ions in a corresponding strongly coupled ultracold plasma \cite{Bannasch:2013}.  

Tate and coworkers have studied ultracold plasma avalanche and expansion theoretically as well as experimentally.  Modelling observed expansion rates, they recently found that $^{85}$Rb atoms in a MOT form plasmas with effective initial electron temperatures determined by initial Rydberg density and the selected initial binding energy, to the extent that these parameters determine the fraction of the excited atoms that ionize by electron impact in the avalanche to plasma \cite{Forest}.  This group also returned to the question of added Rydberg atoms, and managed to identify a crossover in $n_0$, depending on the initial electron temperature, that determines whether added Rydberg atoms of a particular initial binding energy act to heat or cool the electron temperature \cite{Crockett}.   

Our group has focused on the plasma that evolves from a Rydberg gas under the low-temperature conditions of a skimmed, seeded supersonic molecular beam.  In work on nitric oxide starting in 2008 \cite{Morrison2008,Plasma_expan,Morrison_shock,PCCP}, we established an initial kinetics of electron impact avalanche ionization that conforms with coupled rate equation models \cite{Saquet2011,Saquet2012,Scaling,haenelCP} and agrees at early times with the properties of ultracold plasmas that evolve from ultracold atoms in a MOT.  We have also observed unique properties of the NO ultracold plasma owing to the fact that its Rydberg states dissociate \cite{Haenel2017}, and identified relaxation pathways that may give rise to quantum effects \cite{SousMBL,SousNJP}.  The remainder of this review focuses on the nitric oxide ultracold plasma and the unique characteristics conferred by its evolution from a Rydberg gas in a laser-crossed molecular beam.  


\section{Avalanche to strong coupling in a molecular Rydberg gas}

\subsection{The molecular beam ultracold plasma compared with a MOT}

When formed with sufficient density, a Rydberg gas of principal quantum number $n_0>30$ undergoes a spontaneous avalanche to form an ultracold plasma \cite{Li,Morrison2008,RobertdeSaintVincent}.  Collisional rate processes combine with ambipolar hydrodynamics to govern the properties of the evolving plasma.  For a molecular Rydberg gas, neutral fragmentation, occurs in concert with electron-impact ionization, three-body recombination and electron-Rydberg inelastic scattering.  Neutral dissociation combined with radial expansion in a shaped distribution of charged particles, can give rise to striking effects of self-assembly and spatial correlation \cite{Schulz-Weiling2016,Haenel2017}.   

The formation of a molecular ultracold plasma requires the conditions of local temperature and density afforded by a high mach-number skimmed supersonic molecular beam.  Such a beam propagates at high velocity in the laboratory, with exceedingly well-defined hydrodynamic properties, including a propagation-distance-dependent density and sub-Kelvin temperature in the moving frame \cite{MSW_tutorial}.  The low-temperature gas in a supersonic molecular beam differs in three important ways from the atomic gas laser-cooled in a magneto-optical trap (MOT).

The milli-Kelvin temperature of the gas of ground-state NO molecules entrained in a beam substantially exceeds the sub-100 micro-Kelvin temperature of laser-cooled atoms in a MOT.  However, the evolution to plasma tends to erase this distinction, and the two further characteristics that distinguish a beam offer important advantages for ultracold plasma physics:  Charged-particle densities in a molecular beam can exceed those attainable in a MOT by orders of magnitude.  A great many different chemical substances can be seeded in a free-jet expansion, and the possibility this affords to form other molecular ultracold plasmas, introduces interesting and potentially important new degrees of freedom governing the dynamics of their evolution.


\subsection{Supersonic molecular beam temperature and particle density}

Seeded in a skimmed supersonic molecular beam, nitric oxide forms different phase-space distributions in the longitudinal (propagation) and transverse coordinate dimensions.  As it propagates in $z$, the NO molecules reach a terminal laboratory velocity, $u_{\parallel}$, of about 1400 ${\rm ms^{-1}}$, which varies with the precise seeding ratio.  

The distribution of $v_{\parallel}$, narrows to define a local temperature, $T_{\parallel}$, of approximately 0.5 K.  The beam forms a Gaussian spatial distribution in the transverse coordinates, $x$ and $y$.  In this plane, the local velocity, $v_{\perp}(r)$ is defined for any radial distance almost entirely by the divergence velocity of the beam, $u_{\perp}(r)$.  Phase-space sorting cools the temperature in the transverse coordinates, $T_{\perp}$ to a value as low as $\sim 5$ mK \cite{MSW_tutorial}.  

The stagnation pressure and seeding ratio determine the local density distribution as a function of $z$.  For example, expanding from a stagnation pressure of 500 kPa with a 1:10 seeding ratio, a molecular beam propagates 2.5 cm to a skimmer and then 7.5 cm to a point of laser interaction, where it contains NO at a peak density of $1.6 \times 10^{14}$ cm$^{-3}$.  

Here, crossing the molecular beam with a laser beam tuned to the transition sequence, ${\rm X} ~^2 \Pi_{1/2} ~N'' = 1 \xrightarrow{\omega_1} {\rm A} ~^2\Sigma^+ ~N'=0  \xrightarrow{\omega_2} n_0 f(2)$ forms a Gaussian ellipsoidal volume of Rydberg gas in a single selected principal quantum number, $n_0$, orbital angular momentum, $\ell = 3$, NO$^+$ core rotational quantum number, $N^+ = 2$ and total angular momentum neglecting spin, $N=1$.  

A typical $\omega_1$ pulse energy of 2 $\mu$J and a Gaussian width of 0.2 mm serves to drive the first step of this sequence in a regime of linear absorption.  Overlapping this volume by an $\omega_2$ pulse with sufficient fluence to saturate the second step forms a Rydberg gas ellipsoid with a nominal peak density of $5 \times 10^{12}$ cm$^{-3}$  \cite{Morrison2008,MSW_tutorial}.  Fluctuations in the pulse energy and longitudinal mode of $\omega_1$ cause the real density to vary.  For certain experiments, we find it convenient to saturate the $\omega_1$ transition, and vary the density of Rydberg gas by delaying $\omega_2$.  An $\omega_1$-$\omega_2$ delay, $\Delta t$, reduces the Rydberg gas density by a precise factor, $e^{-\Delta t/\tau}$, where $\tau$ is the 200 ns radiative lifetime of NO ${\rm A} ~^2\Sigma^+ ~N'=0$ \cite{Carter,Hancock}.


\subsection{Penning ionization}

The density distribution of a Rydberg gas defines a local mean nearest neighbour distance, or Wigner-Seitz radius of $ a_{ws} =  \left(3/4 \pi \rho \right)^{1/3} $, where $\rho$ refers to the local Rydberg gas density.  For example, a Rydberg gas with a density of $ \rho_0=0.5 \times 10^{12}$ cm$^{-3} $ forms an Erlang distribution \cite{Torquato.1990} of nearest neighbour separations with a mean value of $ 2 a_{ws}=1.6$  $\mu$m.  

A semi-classical model \cite{Robicheaux05} suggests that 90 percent of Rydberg molecule pairs separated by a critical distance, $ r_c = 1.8 \cdot 2 n_0^2 a_0 $ or less undergo Penning ionization within 800 Rydberg periods.  We can integrate the Erlang distribution from $ r=0 $ to the critical distance $r = r_c$ for a Rydberg gas of given $n_0$, to define the local density of Penning electrons ($ \rho_e$ at $t=0$) produced by this prompt interaction, for any given initial local density, $\rho_0$ by the expression:
\begin{equation}
\rho_e(\rho_0,n_0) = \frac{0.9}{2} \cdot 4 \pi \rho_0 ^2\int_0^{r_{c}} r^2 \mathrm{e}^{-\frac{4\pi}{3}\rho_0 r^3}\mathrm{d}r \quad.
\label{eqn:Erlang}
\end{equation}

Evaluating this definite integral yields an equation in closed form that predicts the Penning electron density for any particular initial Rydberg density and principal quantum number.
\begin{equation}
\rho_e(\rho_0,n_0) =\frac{0.9 \rho_0}{2}(1-\mathrm{e}^{-\frac{4\pi}{3}\rho_0 r_c^3}) \quad.
\label{Eq:PenDens}
\end{equation}
\begin{figure}[h!]
\centering
\includegraphics[scale=0.33]{Penning_Latice.pdf}
\caption{Distributions of ion-ion nearest neighbours following Penning ionization and electron-impact avalanche simulated for a predissociating molecular Rydberg gas of initial principal quantum number, $n_0$, from 30 to 80, and density of 10$^{12}$ cm$^{-3}$.  Dashed lines mark corresponding values of $a_{ws}$. Calculated by counting ion distances after relaxation to plasma in 10$^6$-particle stochastic simulations. Integrated areas proportional to populations surviving neutral dissociation.}
\label{fig:PL}
\end{figure}

Prompt Penning ionization acts on the portion of the initial nearest-neighbour distribution in the Rydberg gas that lies within $r_c$.  When a molecule ionizes, its collision partner relaxes to a lower principal quantum number, $n'<n_0/\sqrt{2}$.  This close-coupled interaction disrupts the separability of Rydberg orbital configurations in the Penning partner.  This causes mixing with core penetrating states that are strongly dissociative.  Penning partners are thus very likely to dissociate, leaving a spatially isolated distribution of ions.  We refer to the spatial correlation that results as a Penning lattice \cite{Sadeghi:2014}.  The extent of this effect varies depending on the local density and the selected initial principal quantum number.  Figure \ref{fig:PL} shows the degree to which Rydberg gases with initial principal quantum numbers from 30 to 80 form a Penning lattice for an initial density of $1 \times 10^{12} ~{\rm cm}^{-3}$.  

\subsection{Spontaneous electron-impact avalanche}

The electrons produced by prompt Penning ionization start an electron impact avalanche.  The kinetics of this process are well described by a set of coupled rate equations that account for state-to-state electron-Rydberg inelastic scattering, electron-impact ionization and three-body ion-electron recombination \cite{PPR,Saquet2011,Saquet2012,Scaling} using detailed rate coefficients,  $k_{ij}$, $k_{i,ion}$ and $k_{i,tbr}$ validated by MD simulations \cite{PVS}.  
\begin{eqnarray}
-\frac{d\rho_i}{dt}&=&\sum_{j}{k_{ij}\rho_e\rho_i}-\sum_{j}{k_{ji}\rho_e\rho_j} \nonumber\\
&& +k_{i,ion}\rho_e\rho_i-k_{i,tbr}\rho^3_e 
  \label{level_i}
\end{eqnarray}
\noindent and,
\begin{equation}
\frac{d\rho_e}{dt}=\sum_{i}{k_{i,ion}\rho_e^2}-\sum_{i}{k_{i,tbr}\rho^3_e}
  \label{electron}
\end{equation}

The relaxation of Rydberg molecules balances with collisional ionization to determine an evolving temperature of avalanche electrons to conserve total energy per unit volume. 
\begin{equation}
E_{tot}=\frac{3}{2}k_BT_e(t)\rho_e(t)-R\sum_i{\frac{\rho_i(t)}{n_i^2}},
  \label{energy}
\end{equation}
Here, for simplicity, we neglect the longer-time effects of Rydberg predissociation and electron-ion dissociative recombination \cite{Saquet2012}.

Such calculations show that the conversion from Rydberg gas to plasma occurs on a timescale determined largely by the local Penning electron density, or Penning fraction, $P_f = \rho_e/\rho_0$, which depends on the local density of Rydberg molecules and their initial principal quantum number.  

Avalanche times predicted by coupled rate equation calculations range widely.  For example, in a model developed for experiments on xenon, simulations predict that a Rydberg gas with $n_0 = 42$ at a density of $8.8 \times 10^8 ~{\rm cm}^{-3}$ ($P_f = 6 \times 10^{-5}$) avalanches with a half time of  40 $\mu$s \cite{Hung2014}.  At an opposite extreme, rate equations estimate that a Rydberg gas of NO with $n_0=60$ at a density of $1 \times 10^{12} ~{\rm cm}^{-3}$ ($P_f = 0.3$) rises to plasma in about 2 ns \cite{Saquet2012}.  

\begin{figure}[h!]
\centering
\includegraphics[width= .49 \textwidth]{SFI_n=49.pdf}
   \caption{Contour plots showing SFI signal as a function the applied field for an $nf(2)$ Rydberg gas with an initial principal quantum number, $n_0=49$.  Each frame represents 4,000 SFI traces, sorted by initial Rydberg gas density.  Ramp field beginning at 0 and 150 ns (top, left to right), and  300 and 450 ns (bottom) after the $\omega_2$ laser pulse.  The two bars of signal most evident at early ramp field delay times represent the field ionization of the $49f(2)$ Rydberg state respectively to NO$^+$ X $^1\Sigma^+$ cation rotational states, $N^+=0$ and 2.  The signal waveform extracted near zero applied field represents the growing population of plasma electrons.  
   }
\label{fig:SFI}
\end{figure}

Selective field ionization (SFI) probes the spectrum of binding energies in a Rydberg gas.  Applied as a function of time after photoexcitation, SFI maps the evolution from a state of selected initial principal quantum number, $n_0$, to plasma \cite{Haenel2017}.  Figure \ref{fig:SFI} shows SFI spectra taken at a sequence of delays after the formation of $49f(2)$ Rydberg gases of varying density.    

Here, we can see that a $49f(2)$ Rydberg gas with an estimated initial density $\rho_0 = 3 \times 10^{11} ~{\rm cm}^{-3}$ relaxes to plasma on a timescale of about 500 ns.  Observations such as these agree well with the predictions of coupled rate-equation calculations.  We can understand this variation in relaxation dynamics with $\rho_0$ and $n_0$ quite simply in terms of the corresponding density of prompt Penning electrons these conditions afford to initiate the avalanche to plasma.  

Figure \ref{fig:scaled_rise} illustrates this, showing how rise times predicted by coupled rate-equation simulations for a large range of initial densities and principal quantum number match when plotted as a function of time scaled by the ultimate plasma frequency and fraction of prompt Penning electrons.  The dashed line gives an approximate account of the scaled rate of avalanche under all conditions of Rydberg gas density and initial principal quantum number in terms of the simple sigmoidal function:

\begin{equation}
\frac{\rho_e}{\rho_0} = \frac{a}{b+e^{-c\tau}},
  \label{scaledEq1}
\end{equation}
where,
\begin{equation}
\tau = t \omega_e P_f^{3/4},
  \label{scaledEq2}
\end{equation}
in which $\omega_e$ is the plasma frequency after avalanche, $P_f$ is the fraction of prompt Penning electrons, and $a = 0.00062$,  $b =   0.00082$ and $c =     0.075$ are empirical coefficients.  

\begin{figure}[h!]
\centering
\includegraphics[width= .4 \textwidth]{sim_analytical_density.pdf}
   \caption{Rise in fractional electron density as a function of time scaled by the plasma frequency, $\omega_e$ and fraction, $\rho_e(t=0)/\rho_0 = P_f$, of prompt Penning electrons.  Simulation results shown for $n_0 = 30$, 50 and 70 with initial densities, $\rho_0 = 10^9,~10^{10},~10^{11},~{\rm and}~10^{12}~{\rm cm}^{-3}$.  
   }
\label{fig:scaled_rise}
\end{figure}


\subsection{Evolution to plasma in a Rydberg gas Gaussian ellipsoid}

As outlined above, the local density and principal quantum number together determine the rate at which a Rydberg gas avalanches to plasma.  Our experiment crosses a 2 mm wide cylindrically Gaussian molecular beam with a 1 mm diameter TEM$_{00}$ $\omega_1$ laser beam to produce a Gaussian ellipsoidal distribution of molecules excited to the A $^2\Sigma^+$ $v=0, ~N'=0$ intermediate state.  A larger diameter $\omega_2$ pulse then drives a second step that forms a Rydberg gas in a single $n_0f(2)$ state with the spatial distribution of the intermediate state.  

We model this shaped Rydberg gas as a system of 100 concentric ellipsoidal shells of varying density \cite{haenelCP}.  Coupled rate equations within each shell describe the avalanche to plasma.  This rate process proceeds from shell to shell with successively longer induction periods, determined by the local density as detailed above.  The rising conversion of Rydberg molecules to ions plus neutral dissociation products conserves the particle number in each shell.  We assume that local space charge confines electrons to shells, conserving quasi-neutrality.  Electrons exchange kinetic energy at the boundaries of each shell, which determines a single plasma electron temperature.  

\begin{figure}[h!]
\centering
\includegraphics[width= .5 \textwidth]{shell_model_100}
   \caption{(top frame) Cross-sectional contour diagram in the $x,y$ plane for $z=0$ describing the distribution of ion plus electron density over 100 shells of Gaussian ellipsoid with initial dimensions, $\sigma_x= 0.75$ mm and $\sigma_y= \sigma_z = 0.42$ mm and an initial $n_0 = 50$ Rydberg gas density, $\rho_0 = 2 \times 10^{11}$ cm$^{3}$ after an evolution time of 100 ns.  (bottom frame) Curves describing the (dashed) ascending ion and (solid) descending Rydberg gas densities of each shell as functions of evolution time, for $t=20$, 40, 60, 80 and 100 ns.  
   }
\label{fig:shell}
\end{figure}

The upper frame of Figure \ref{fig:shell} shows contours of NO$^+$ ion density after 100 ns obtained from a shell-model coupled rate-equation simulation of the avalanche of a Gaussian ellipsoidal Rydberg gas of nitric oxide with a selected initial state, $50f(2)$ and a density of $2 \times 10^{11}$ cm$^{-3}$.  Here, we simulate a relaxation that includes channels of predissociation at every Rydberg level and redistributes the energy released to electrons, which determines a uniform rising electron temperature for all shells.  

For comparison, the lower frame plots curves describing the ion density of each shell as a function of time from 20 to 100 ns, as determined by applying Eq \ref{scaledEq1} for the local conditions of initial Rydberg gas density.  This numerical approximation contains no provision for predissociation.  Coupled rate-equation simulations for uniform volumes show that predissociation depresses yield to some degree, but has less effect on the avalanche kinetics \cite{Saquet2012}.  Therefore, we can expect sets of numerically estimated shell densities, scaled to agree with the simulated ion density at the elapsed time of 100 ns to provide a reasonable account of the earlier NO$^+$ density profiles as a function of time.   


\begin{figure}[h!]
\centering
\includegraphics[width= .4 \textwidth]{Shell_pop}
   \caption{Global population fractions of particles as they evolve in the avalanche of a shell-model ellipsoidal Rydberg gas with the initial principal quantum number and density distribution of Figure \ref{fig:shell}
   }
\label{fig:shell_yields}
\end{figure}

For each time step, the difference, $\rho_0 - \rho_e$ defines the neutral population of each shell.  We assign a fraction of this population to surviving Rydberg molecules, such that the total population of NO$^*$ as a function of time agrees with the prediction of the shell-model simulation, as shown in Figure \ref{fig:shell_yields}.  We consider the balance of this neutral population to reflect NO$^*$ molecules that have dissociated to form N($^4$S) $+$ O($^3$P).  Figure \ref{fig:shell} plots these surviving Rydberg densities as a function of radial distance for each evolution time.  At the initial density of this simulation, note at each time step that a higher density of Rydberg molecules encloses the tail of the ion density distribution in $x$.   

\subsection{Plasma expansion and NO$^+$ - NO$^*$ charge exchange as an avenue of quench}

We regard the ions as initially stationary.  The release of electrons creates a radial electric potential gradient, which gives rise to a force, $-e\nabla \phi_{k,j}(t)$, that accelerates the ions in shell $j$ in direction $k$ according to \cite{Sadeghi.2012}:

\begin{align}
\frac{-e}{m'}\nabla \phi_{k,j}(t) = & \frac{\partial u_{k,j}(t)}{\partial t} \notag  \\
= & \frac{k_BT_e(t)}{m'\rho_j(t)} \frac{\rho_{j+1}(t) - \rho_j(t)}{r_{k,j+1}(t) - r_{k,j}(t)},
  \label{dr_dt}
\end{align}

\noindent where $\rho_j(t)$ represents the density of ions in shell $j$.  

The instantaneous velocity, $u_{k,j}(t)$ determines the change in the radial coordinates of each shell, $r_{k,j}(t)$, 
\begin{equation}
\frac{\partial r_{k,j}(t)}{\partial t}=u_{k,j}(t) = \gamma_{k,j}(t) r_{k,j}(t),
  \label{dr_dt}
\end{equation}
\noindent which in turn determines shell volume and thus its density, $ \rho_j(t)$.   
The electron temperature supplies the thermal energy that drives this ambipolar expansion.  Ions accelerate and $T_e$ falls according to: 

\begin{equation}
\frac{3k_B}{2}\frac{\partial T_e(t)}{\partial t}= -\frac{m'}{\sum_{j}{N_j}}\sum_{k,j}{N_j u_{k,j}(t)\frac{\partial u_{k,j}(t)}{\partial t}},
  \label{dr_dt}
\end{equation}
\noindent where we define an effective ion mass, $m'$, that recognizes the redistribution of the electron expansion force over all the NO$^+$ charge centres by resonant ion-Rydberg charge exchange, which occurs with a very large cross section \cite{PPR}.  
\begin{equation}
m' =\left (1+ \frac{\rho^*_{j}(t)}{ \rho_j(t)}\right) m ,
  \label{dr_dt}
\end{equation}
\noindent in which $\rho^*_{j}(t)$ represents the instantaneous Rydberg density in shell $j$.

The initial avalanche in the high-density core of the ellipsoid leaves few Rydberg molecules, so this term has little initial effect.  Rydberg molecules predominate in the lower-density wings.  There, momentum sharing by charge exchange assumes a greater importance.  

We see this most directly in the $\omega_2$ absorption spectrum of transitions to states in the $n_0 f(2)$ Rydberg series, detected as the long-lived signal that survives a flight time of 400 $\mu$s to reach the imaging detector.  The balance between the rising density of ions and the falling density of Rydberg molecules depends on the initial density of electrons produced by prompt Penning ionization.  As clear from Eq \ref{Eq:PenDens}, this Penning fraction depends sensitively on the principal quantum number, and for all principal quantum numbers, on the initial Rydberg gas density.  

\begin{figure}[h!]
\centering
\includegraphics[width= .4 \textwidth]{w2_spectra}
   \caption{Double-resonant spectra of nitric oxide Rydberg states in the $nf$ series converging to NO$^+$ $v=0$, $N^+=2$ (designated, $nf(2)$), derived from the late-peak signal obtained after a flight time of 400 $\mu$s by scanning $\omega_2$ for $\omega_1$ tuned to NO A $^2\Sigma^+$ $v=0$, $N'=0$ for initial $nf(2)$ densities from top to bottom of 0.07, 0.10, 0.13, 0.19, 0.27, 0.30, 0.32 and $3 \times 10^{12}$ cm$^{3}$.  
  }
\label{fig:w2_spectra}
\end{figure}

Figure \ref{fig:w2_spectra} shows a series of $\omega_2$ late-signal excitation spectra for a set of initial densities.  Here, we see a clear consequence of the higher-order dependence of Penning fraction - and thus the NO$^+$ ion - NO$^*$ Rydberg molecule balance - on $n_0$, the $\omega_2$-selected Rydberg gas initial principal quantum number.  This Penning-regulated NO$^+$ ion - NO$^*$ Rydberg molecule balance appears necessary as a critical factor in achieving the long ultracold plasma lifetime required to produce this signal.  We are progressing in theoretical work that explains the stability apparently conferred by this balance.  


\subsection{Bifurcation and arrested relaxation}

Ambipolar expansion quenches electron kinetic energy as the initially formed plasma expands.  Core ions follow electrons into the wings of the Rydberg gas.  There, recurring charge exchange between NO$^+$ ions and NO$^*$ Rydberg molecules redistributes the ambipolar force of the expanding electron gas, equalizing ion and Rydberg velocities.  This momentum matching effectively channels electron energy through ion motion into the overall $\pm x$ motion of gas volumes in the laboratory.  The internal kinetic energy of the plasma, which at this point is defined almost entirely by the ion-Rydberg relative motion, falls.  Spatial correlation develops, and over a period of 500 ns, the system forms the plasma/high-Rydberg quasi-equilibrium dramatically evidenced by the SFI results in Figure \ref{fig:SFI}.  

\begin{figure}[h!]
\centering
\includegraphics[width= .4 \textwidth]{Bifurcation.pdf}
   \caption{$x,y$ detector images of ultracold plasma volumes produced by 2:1 aspect ratio ellipsoidal Rydberg gases with selected initial state, $40f(2)$ after a flight time of 402 $\mu$s over a distance of 575 mm.  Lower frame displays the distribution in $x$ of the charge integrated in $y$ and $z$.  Both images represent the unadjusted raw signal acquired in each case after 250 shots.  
   }
\label{fig:bifurcation}
\end{figure}

In the wings, momentum redistribution owing to cycles of ion-Rydberg charge transfer retards radial expansion \cite{Pohl2003,PPR}.  By redirecting electron energy from ambipolar acceleration to $\pm x$ plasma motion, NO$^+$ to NO$^*$ charge exchange dissipates electron thermal energy.  This redistribution of energy released in the avalanche of the Rydberg gas to plasma, causes the ellipsoidal Rydberg gas to bifurcate \cite{Schulz-Weiling2016,Haenel2017}, forming very long-lived, separating charged-particle distributions.  We capture the electron signal from these recoiling volumes on an imaging detector as pictured in Figure \ref{fig:bifurcation}.  Here, momentum matching preserves density and enables ions and Rydberg molecules to relax to positions that minimize potential energy, building spatial correlation.  

The semi-classical description of avalanche and relaxation outlined above forms an important point of reference from which to interpret our experimental observations.  The laser crossed molecular beam illumination geometry creates a Rydberg gas with a distinctively shaped high-density spatial distribution.  This initial condition has an evident effect on the evolution dynamics.  We have developed semi-classical models that explicitly consider the coupled rate and hydrodynamic processes governing the evolution from Rydberg gas to plasma using a realistic, ellipsoidal representation of the ion/electron and Rydberg densities \cite{haenelCP}.  No combination of initial conditions can produce a simulation that conforms classically with the state of arrested relaxation we observe experimentally. 


\subsection{A molecular ultracold plasma state of arrested relaxation}

Thus, we find that spontaneous avalanche to plasma splits the core of an ellipsoidal Rydberg gas of nitric oxide. As ambipolar expansion quenches the electron temperature of this core plasma, long-range, resonant charge transfer from ballistic ions to frozen Rydberg molecules in the wings of the ellipsoid quenches the ion-Rydberg molecule relative velocity distribution. This sequence of steps gives rise to a remarkable mechanics of self-assembly, in which the kinetic energy of initially formed hot electrons and ions drives an observed separation of plasma volumes. These dynamics redistribute ion momentum, efficiently channeling electron energy into a reservoir of mass-transport. This starts a process that evidently anneals separating volumes to a state of cold, correlated ions, electrons and Rydberg molecules. 

We have devised a three-dimensional spin model to describe this arrested state of the ultracold plasma in terms of two, three and four-level dipole-dipole energy transfer interactions (spin flip-flops), together with Ising interactions that arise from the concerted pairwise coupling of resonant pairs of dipoles \cite{SousMBL,SousNJP}.  

The Hamiltonian includes the effects of onsite disorder owing to the broad spectrum of states populated in the ensemble and the unique electrostatic environment of every dipole.  Extending ideas developed for simpler systems \cite{Burin1,Sondhi}, one can make a case for slow dynamics, including an arrest in the relaxation of NO Rydberg molecules to predissociating states of lower principal quantum number.

Systems of higher dimension ought to thermalize by energy transfer that spreads from rare but inevitable ergodic volumes (Griffiths regions) \cite{Sarang2, Roeck_griffith, RareRegions_rev, Thermal_inclusions}.  However, a feature in the self-assembly of the molecular ultracold plasma may preclude destabilization by rare thermal domains:  Whenever the quenched plasma develops a delocalizing Griffiths region, the local predissociation of relaxing NO molecules promptly proceeds to deplete that region to a void of no consequence.

In summary, the classical dynamics of avalanche and bifurcation appear to create a quenched condition of low temperature and high disorder in which dipole-dipole interactions drive self-assembly to a localized state purified by the predissociation of thermal regions.  We suggest that this state of the quenched ultracold plasma offers an experimental platform for studying quantum many-body physics of disordered systems. \\

\section{Acknowledgments}
This work was supported by the US Air Force Office of Scientific Research (Grant No. FA9550-17-1-0343), together with the Natural Sciences and Engineering research Council of Canada (NSERC), the Canada Foundation for Innovation (CFI) and the British Columbia Knowledge Development Fund (BCKDF). 

",['It is the ratio of the average unscreened electron-electron potential energy to kinetic energy.'],5086,multifieldqa_en_e,en,,7f8fc6a04f8be9000b41098aa0f8d7fbfd42e2aa43bf7997,It is the ratio of the average unscreened electron-electron potential energy to kinetic energy.,95
What were the baselines?,"Introduction
The task of speculation detection and scope resolution is critical in distinguishing factual information from speculative information. This has multiple use-cases, like systems that determine the veracity of information, and those that involve requirement analysis. This task is particularly important to the biomedical domain, where patient reports and medical articles often use this feature of natural language. This task is commonly broken down into two subtasks: the first subtask, speculation cue detection, is to identify the uncertainty cue in a sentence, while the second subtask: scope resolution, is to identify the scope of that cue. For instance, consider the example:
It might rain tomorrow.
The speculation cue in the sentence above is ‘might’ and the scope of the cue ‘might’ is ‘rain tomorrow’. Thus, the speculation cue is the word that expresses the speculation, while the words affected by the speculation are in the scope of that cue.
This task was the CoNLL-2010 Shared Task (BIBREF0), which had 3 different subtasks. Task 1B was speculation cue detection on the BioScope Corpus, Task 1W was weasel identification from Wikipedia articles, and Task 2 was speculation scope resolution from the BioScope Corpus. For each task, the participants were provided the train and test set, which is henceforth referred to as Task 1B CoNLL and Task 2 CoNLL throughout this paper.
For our experimentation, we use the sub corpora of the BioScope Corpus (BIBREF1), namely the BioScope Abstracts sub corpora, which is referred to as BA, and the BioScope Full Papers sub corpora, which is referred to as BF. We also use the SFU Review Corpus (BIBREF2), which is referred to as SFU.
This subtask of natural language processing, along with another similar subtask, negation detection and scope resolution, have been the subject of a body of work over the years. The approaches used to solve them have evolved from simple rule-based systems (BIBREF3) based on linguistic information extracted from the sentences, to modern deep-learning based methods. The Machine Learning techniques used varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), while the deep learning approaches included Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12). Figures FIGREF1 and FIGREF1 contain a summary of the papers addressing speculation detection and scope resolution (BIBREF13, BIBREF5, BIBREF9, BIBREF3, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF6, BIBREF11, BIBREF18, BIBREF10, BIBREF19, BIBREF7, BIBREF4, BIBREF8).
Inspired by the most recent approach of applying BERT to negation detection and scope resolution (BIBREF12), we take this approach one step further by performing a comparative analysis of three popular transformer-based architectures: BERT (BIBREF20), XLNet (BIBREF21) and RoBERTa (BIBREF22), applied to speculation detection and scope resolution. We evaluate the performance of each model across all datasets via the single dataset training approach, and report all scores including inter-dataset scores (i.e. train on one dataset, evaluate on another) to test the generalizability of the models. This approach outperforms all existing systems on the task of speculation detection and scope resolution. Further, we jointly train on multiple datasets and obtain improvements over the single dataset training approach on most datasets.
Contrary to results observed on benchmark GLUE tasks, we observe XLNet consistently outperforming RoBERTa. To confirm this observation, we apply these models to the negation detection and scope resolution task, and observe a continuity in this trend, reporting state-of-the-art results on three of four datasets on the negation scope resolution task.
The rest of the paper is organized as follows: In Section 2, we provide a detailed description of our methodology and elaborate on the experimentation details. In Section 3, we present our results and analysis on the speculation detection and scope resolution task, using the single dataset and the multiple dataset training approach. In Section 4, we show the results of applying XLNet and RoBERTa on negation detection and scope resolution and propose a few reasons to explain why XLNet performs better than RoBERTa. Finally, the future scope and conclusion is mentioned in Section 5.
Methodology and Experimental Setup
We use the methodology by Khandelwal and Sawant (BIBREF12), and modify it to support experimentation with multiple models.
For Speculation Cue Detection:
Input Sentence: It might rain tomorrow.
True Labels: Not-A-Cue, Cue, Not-A-Cue, Not-A-Cue.
First, this sentence is preprocessed to get the target labels as per the following annotation schema:
1 – Normal Cue 2 – Multiword Cue 3 – Not a cue 4 – Pad token
Thus, the preprocessed sequence is as follows:
Input Sentence: [It, might, rain, tomorrow]
True Labels: [3,1,3,3]
Then, we preprocess the input using the tokenizer for the model being used (BERT, XLNet or RoBERTa): splitting each word into one or more tokens, and converting each token to it’s corresponding tokenID, and padding it to the maximum input length of the model. Thus,
Input Sentence: [wtt(It), wtt(might), wtt(rain), wtt(tom), wtt(## or), wtt(## row), wtt(〈pad 〉),wtt(〈pad 〉)...]
True Labels: [3,1,3,3,3,3,4,4,4,4,...]
The word ‘tomorrow' has been split into 3 tokens, ‘tom', ‘##or' and ‘##row'. The function to convert the word to tokenID is represented by wtt.
For Speculation Scope Resolution:
If a sentence has multiple cues, each cue's scope will be resolved individually.
Input Sentence: It might rain tomorrow.
True Labels: Out-Of-Scope, Out-Of-Scope, In-Scope, In-Scope.
First, this sentence is preprocessed to get the target labels as per the following annotation schema:
0 – Out-Of-Scope 1 – In-Scope
Thus, the preprocessed sequence is as follows:
True Scope Labels: [0,0,1,1]
As for cue detection, we preprocess the input using the tokenizer for the model being used. Additionally, we need to indicate which cue's scope we want to find in the input sentence. We do this by inserting a special token representing the token type (according to the cue detection annotation schema) before the cue word whose scope is being resolved. Here, we want to find the scope of the cue ‘might'. Thus,
Input Sentence: [wtt(It), wtt(〈token[1]〉), wtt(might), wtt(rain), wtt(tom), wtt(##or), wtt(##row), wtt(〈pad〉), wtt(〈pad〉)...]
True Scope Labels: [0,0,1,1,1,1,0,0,0,0,...]
Now, the preprocessed input for cue detection and similarly for scope detection is fed as input to our model as follows:
X = Model (Input)
Y = W*X + b
The W matrix is a matrix of size n_hidden x num_classes (n_hidden is the size of the representation of a token within the model). These logits are fed to the loss function.
We use the following variants of each model:
BERT: bert-base-uncaseds3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz (The model used by BIBREF12)
RoBERTa: roberta-bases3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin (RoBERTa-base does not have an uncased variant)
XLNet: xlnet-base-caseds3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin (XLNet-base does not have an uncased variant)
The output of the model is a vector of probabilities per token. The loss is calculated for each token, by using the output vector and the true label for that token. We use class weights for the loss function, setting the weight for label 4 to 0 and all other labels to 1 (for cue detection only) to avoid training on padding token’s output.
We calculate the scores (Precision, Recall, F1) for the model per word of the input sentence, not per token that was fed to the model, as the tokens could be different for different models leading to inaccurate scores. For the above example, we calculate the output label for the word ‘tomorrow', not for each token it was split into (‘tom', ‘##or' and ‘##row'). To find the label for each word from the tokens it was split into, we experiment with 2 methods:
Average: We average the output vectors (softmax probabilities) for each token that the word was split into by the model's tokenizer. In the example above, we average the output of ‘tom', ‘##or' and ‘##row' to get the output for ‘tomorrow'. Then, we take an argmax over the resultant vector. This is then compared with the true label for the original word.
First Token: Here, we only consider the first token's probability vector (among all tokens the word was split into) as the output for that word, and get the label by an argmax over this vector. In the example above, we would consider the output vector corresponding to the token ‘tom' as the output for the word ‘tomorrow'.
For cue detection, the results are reported for the Average method only, while we report the scores for both Average and First Token for Scope Resolution.
For fair comparison, we use the same hyperparameters for the entire architecture for all 3 models. Only the tokenizer and the model are changed for each model. All other hyperparameters are kept same. We finetune the models for 60 epochs, using early stopping with a patience of 6 on the F1 score (word level) on the validation dataset. We use an initial learning rate of 3e-5, with a batch size of 8. We use the Categorical Cross Entropy loss function.
We use the Huggingface’s Pytorch Transformer library (BIBREF23) for the models and train all our models on Google Colaboratory.
Results: Speculation Cue Detection and Scope Resolution
We use a default train-validation-test split of 70-15-15 for each dataset. For the speculation detection and scope resolution subtasks using single-dataset training, we report the results as an average of 5 runs of the model. For training the model on multiple datasets, we perform a 70-15-15 split of each training dataset, after which the train and validation part of the individual datasets are merged while the scores are reported for the test part of the individual datasets, which is not used for training or validation. We report the results as an average of 3 runs of the model. Figure FIGREF8 contains results for speculation cue detection and scope resolution when trained on a single dataset. All models perform the best when trained on the same dataset as they are evaluated on, except for BF, which gets the best results when trained on BA. This is because of the transfer learning capabilities of the models and the fact that BF is a smaller dataset than BA (BF: 2670 sentences, BA: 11871 sentences). For speculation cue detection, there is lesser generalizability for models trained on BF or BA, while there is more generalizability for models trained on SFU. This could be because of the different nature of the biomedical domain.
Figure FIGREF11 contains the results for speculation detection and scope resolution for models trained jointly on multiple datasets. We observe that training on multiple datasets helps the performance of all models on each dataset, as the quantity of data available to train the model increases. We also observe that XLNet consistently outperforms BERT and RoBERTa. To confirm this observation, we apply the 2 models to the related task of negation detection and scope resolution
Negation Cue Detection and Scope Resolution
We use a default train-validation-test split of 70-15-15 for each dataset, and use all 4 datasets (BF, BA, SFU and Sherlock). The results for BERT are taken from BIBREF12. The results for XLNet and RoBERTa are averaged across 5 runs for statistical significance. Figure FIGREF14 contains results for negation cue detection and scope resolution. We report state-of-the-art results on negation scope resolution on BF, BA and SFU datasets. Contrary to popular opinion, we observe that XLNet is better than RoBERTa for the cue detection and scope resolution tasks. A few possible reasons for this trend are:
Domain specificity, as both negation and speculation are closely related subtasks. Further experimentation on different tasks is needed to verify this.
Most benchmark tasks are sentence classification tasks, whereas the subtasks we experiment on are sequence labelling tasks. Given the pre-training objective of XLNet (training on permutations of the input), it may be able to capture long-term dependencies better, essential for sequence labelling tasks.
We work with the base variants of the models, while most results are reported with the large variants of the models.
Conclusion and Future Scope
In this paper, we expanded on the work of Khandelwal and Sawant (BIBREF12) by looking at alternative transfer-learning models and experimented with training on multiple datasets. On the speculation detection task, we obtained a gain of 0.42 F1 points on BF, 1.98 F1 points on BA and 0.29 F1 points on SFU, while on the scope resolution task, we obtained a gain of 8.06 F1 points on BF, 4.27 F1 points on BA and 11.87 F1 points on SFU, when trained on a single dataset. While training on multiple datasets, we observed a gain of 10.6 F1 points on BF and 1.94 F1 points on BA on the speculation detection task and 2.16 F1 points on BF and 0.25 F1 points on SFU on the scope resolution task over the single dataset training approach. We thus significantly advance the state-of-the-art for speculation detection and scope resolution. On the negation scope resolution task, we applied the XLNet and RoBERTa and obtained a gain of 3.16 F1 points on BF, 0.06 F1 points on BA and 0.3 F1 points on SFU. Thus, we demonstrated the usefulness of transformer-based architectures in the field of negation and speculation detection and scope resolution. We believe that a larger and more general dataset would go a long way in bolstering future research and would help create better systems that are not domain-specific.","['varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)']",2215,qasper,en,,c9d48df129798d2aed0161fa805a289e5f75306af3e22336,"varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)",332
How do they obtain the new context represetation?,"Introduction
Relation classification is the task of assigning sentences with two marked entities to a predefined set of relations. The sentence “We poured the <e1>milk</e1> into the <e2>pumpkin mixture</e2>.”, for example, expresses the relation Entity-Destination(e1,e2). While early research mostly focused on support vector machines or maximum entropy classifiers BIBREF0 , BIBREF1 , recent research showed performance improvements by applying neural networks (NNs) BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 on the benchmark data from SemEval 2010 shared task 8 BIBREF8 .
This study investigates two different types of NNs: recurrent neural networks (RNNs) and convolutional neural networks (CNNs) as well as their combination. We make the following contributions:
(1) We propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to the middle part.
(2) We present connectionist bi-directional RNN models which are especially suited for sentence classification tasks since they combine all intermediate hidden layers for their final decision. Furthermore, the ranking loss function is introduced for the RNN model optimization which has not been investigated in the literature for relation classification before.
(3) Finally, we combine CNNs and RNNs using a simple voting scheme and achieve new state-of-the-art results on the SemEval 2010 benchmark dataset.
Related Work
In 2010, manually annotated data for relation classification was released in the context of a SemEval shared task BIBREF8 . Shared task participants used, i.a., support vector machines or maximum entropy classifiers BIBREF0 , BIBREF1 . Recently, their results on this data set were outperformed by applying NNs BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 .
zeng2014 built a CNN based only on the context between the relation arguments and extended it with several lexical features. kim2014 and others used convolutional filters of different sizes for CNNs. nguyen applied this to relation classification and obtained improvements over single filter sizes. deSantos2015 replaced the softmax layer of the CNN with a ranking layer. They showed improvements and published the best result so far on the SemEval dataset, to our knowledge.
socher used another NN architecture for relation classification: recursive neural networks that built recursive sentence representations based on syntactic parsing. In contrast, zhang investigated a temporal structured RNN with only words as input. They used a bi-directional model with a pooling layer on top.
Convolutional Neural Networks (CNN)
CNNs perform a discrete convolution on an input matrix with a set of different filters. For NLP tasks, the input matrix represents a sentence: Each column of the matrix stores the word embedding of the corresponding word. By applying a filter with a width of, e.g., three columns, three neighboring words (trigram) are convolved. Afterwards, the results of the convolution are pooled. Following collobertWeston, we perform max-pooling which extracts the maximum value for each filter and, thus, the most informative n-gram for the following steps. Finally, the resulting values are concatenated and used for classifying the relation expressed in the sentence.
Input: Extended Middle Context
One of our contributions is a new input representation especially designed for relation classification. The contexts are split into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. Since in most cases the middle context contains the most relevant information for the relation, we want to focus on it but not ignore the other regions completely. Hence, we propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. Due to the repetition of the middle context, we force the network to pay special attention to it. The two contexts are processed by two independent convolutional and max-pooling layers. After pooling, the results are concatenated to form the sentence representation. Figure FIGREF3 depicts this procedure. It shows an examplary sentence: “He had chest pain and <e1>headaches</e1> from <e2>mold</e2> in the bedroom.” If we only considered the middle context “from”, the network might be tempted to predict a relation like Entity-Origin(e1,e2). However, by also taking the left and right context into account, the model can detect the relation Cause-Effect(e2,e1). While this could also be achieved by integrating the whole context into the model, using the whole context can have disadvantages for longer sentences: The max pooling step can easily choose a value from a part of the sentence which is far away from the mention of the relation. With splitting the context into two parts, we reduce this danger. Repeating the middle context increases the chance for the max pooling step to pick a value from the middle context.
Convolutional Layer
Following previous work (e.g., BIBREF5 , BIBREF6 ), we use 2D filters spanning all embedding dimensions. After convolution, a max pooling operation is applied that stores only the highest activation of each filter. We apply filters with different window sizes 2-5 (multi-windows) as in BIBREF5 , i.e. spanning a different number of input words.
Recurrent Neural Networks (RNN)
Traditional RNNs consist of an input vector, a history vector and an output vector. Based on the representation of the current input word and the previous history vector, a new history is computed. Then, an output is predicted (e.g., using a softmax layer). In contrast to most traditional RNN architectures, we use the RNN for sentence modeling, i.e., we predict an output vector only after processing the whole sentence and not after each word. Training is performed using backpropagation through time BIBREF9 which unfolds the recurrent computations of the history vector for a certain number of time steps. To avoid exploding gradients, we use gradient clipping with a threshold of 10 BIBREF10 .
Input of the RNNs
Initial experiments showed that using trigrams as input instead of single words led to superior results. Hence, at timestep INLINEFORM0 we do not only give word INLINEFORM1 to the model but the trigram INLINEFORM2 by concatenating the corresponding word embeddings.
Connectionist Bi-directional RNNs
Especially for relation classification, the processing of the relation arguments might be easier with knowledge of the succeeding words. Therefore in bi-directional RNNs, not only a history vector of word INLINEFORM0 is regarded but also a future vector. This leads to the following conditioned probability for the history INLINEFORM1 at time step INLINEFORM2 : DISPLAYFORM0
Thus, the network can be split into three parts: a forward pass which processes the original sentence word by word (Equation EQREF6 ); a backward pass which processes the reversed sentence word by word (Equation ); and a combination of both (Equation ). All three parts are trained jointly. This is also depicted in Figure FIGREF7 .
Combining forward and backward pass by adding their hidden layer is similar to BIBREF7 . We, however, also add a connection to the previous combined hidden layer with weight INLINEFORM0 to be able to include all intermediate hidden layers into the final decision of the network (see Equation ). We call this “connectionist bi-directional RNN”.
In our experiments, we compare this RNN with uni-directional RNNs and bi-directional RNNs without additional hidden layer connections.
Word Representations
Words are represented by concatenated vectors: a word embedding and a position feature vector.
Pretrained word embeddings. In this study, we used the word2vec toolkit BIBREF11 to train embeddings on an English Wikipedia from May 2014. We only considered words appearing more than 100 times and added a special PADDING token for convolution. This results in an embedding training text of about 485,000 terms and INLINEFORM0 tokens. During model training, the embeddings are updated.
Position features. We incorporate randomly initialized position embeddings similar to zeng2014, nguyen and deSantos2015. In our RNN experiments, we investigate different possibilities of integrating position information: position embeddings, position embeddings with entity presence flags (flags indicating whether the current word is one of the relation arguments), and position indicators BIBREF7 .
Objective Function: Ranking Loss
Ranking. We applied the ranking loss function proposed in deSantos2015 to train our models. It maximizes the distance between the true label INLINEFORM0 and the best competitive label INLINEFORM1 given a data point INLINEFORM2 . The objective function is DISPLAYFORM0
with INLINEFORM0 and INLINEFORM1 being the scores for the classes INLINEFORM2 and INLINEFORM3 respectively. The parameter INLINEFORM4 controls the penalization of the prediction errors and INLINEFORM5 and INLINEFORM6 are margins for the correct and incorrect classes. Following deSantos2015, we set INLINEFORM7 . We do not learn a pattern for the class Other but increase its difference to the best competitive label by using only the second summand in Equation EQREF10 during training.
Experiments and Results
We used the relation classification dataset of the SemEval 2010 task 8 BIBREF8 . It consists of sentences which have been manually labeled with 19 relations (9 directed relations and one artificial class Other). 8,000 sentences have been distributed as training set and 2,717 sentences served as test set. For evaluation, we applied the official scoring script and report the macro F1 score which also served as the official result of the shared task.
RNN and CNN models were implemented with theano BIBREF12 , BIBREF13 . For all our models, we use L2 regularization with a weight of 0.0001. For CNN training, we use mini batches of 25 training examples while we perform stochastic gradient descent for the RNN. The initial learning rates are 0.2 for the CNN and 0.01 for the RNN. We train the models for 10 (CNN) and 50 (RNN) epochs without early stopping. As activation function, we apply tanh for the CNN and capped ReLU for the RNN. For tuning the hyperparameters, we split the training data into two parts: 6.5k (training) and 1.5k (development) sentences. We also tuned the learning rate schedule on dev.
Beside of training single models, we also report ensemble results for which we combined the presented single models with a voting process.
Performance of CNNs
As a baseline system, we implemented a CNN similar to the one described by zeng2014. It consists of a standard convolutional layer with filters with only one window size, followed by a softmax layer. As input it uses the middle context. In contrast to zeng2014, our CNN does not have an additional fully connected hidden layer. Therefore, we increased the number of convolutional filters to 1200 to keep the number of parameters comparable. With this, we obtain a baseline result of 73.0. After including 5 dimensional position features, the performance was improved to 78.6 (comparable to 78.9 as reported by zeng2014 without linguistic features).
In the next step, we investigate how this result changes if we successively add further features to our CNN: multi-windows for convolution (window sizes: 2,3,4,5 and 300 feature maps each), ranking layer instead of softmax and our proposed extended middle context. Table TABREF12 shows the results. Note that all numbers are produced by CNNs with a comparable number of parameters. We also report F1 for increasing the word embedding dimensionality from 50 to 400. The position embedding dimensionality is 5 in combination with 50 dimensional word embeddings and 35 with 400 dimensional word embeddings. Our results show that especially the ranking layer and the embedding size have an important impact on the performance.
Performance of RNNs
As a baseline for the RNN models, we apply a uni-directional RNN which predicts the relation after processing the whole sentence. With this model, we achieve an F1 score of 61.2 on the SemEval test set.
Afterwards, we investigate the impact of different position features on the performance of uni-directional RNNs (position embeddings, position embeddings concatenated with a flag indicating whether the current word is an entity or not, and position indicators BIBREF7 ). The results indicate that position indicators (i.e. artificial words that indicate the entity presence) perform the best on the SemEval data. We achieve an F1 score of 73.4 with them. However, the difference to using position embeddings with entity flags is not statistically significant.
Similar to our CNN experiments, we successively vary the RNN models by using bi-directionality, by adding connections between the hidden layers (“connectionist”), by applying ranking instead of softmax to predict the relation and by increasing the word embedding dimension to 400.
The results in Table TABREF14 show that all of these variations lead to statistically significant improvements. Especially the additional hidden layer connections and the integration of the ranking layer have a large impact on the performance.
Combination of CNNs and RNNs
Finally, we combine our CNN and RNN models using a voting process. For each sentence in the test set, we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes. In case of a tie, we pick one of the most frequent classes randomly. The combination achieves an F1 score of 84.9 which is better than the performance of the two NN types alone. It, thus, confirms our assumption that the networks provide complementary information: while the RNN computes a weighted combination of all words in the sentence, the CNN extracts the most informative n-grams for the relation and only considers their resulting activations.
Comparison with State of the Art
Table TABREF16 shows the results of our models ER-CNN (extended ranking CNN) and R-RNN (ranking RNN) in the context of other state-of-the-art models. Our proposed models obtain state-of-the-art results on the SemEval 2010 task 8 data set without making use of any linguistic features.
Conclusion
In this paper, we investigated different features and architectural choices for convolutional and recurrent neural networks for relation classification without using any linguistic features. For convolutional neural networks, we presented a new context representation for relation classification. Furthermore, we introduced connectionist recurrent neural networks for sentence classification tasks and performed the first experiments with ranking recurrent neural networks. Finally, we showed that even a simple combination of convolutional and recurrent neural networks improved results. With our neural models, we achieved new state-of-the-art results on the SemEval 2010 task 8 benchmark data.
Acknowledgments
Heike Adel is a recipient of the Google European Doctoral Fellowship in Natural Language Processing and this research is supported by this fellowship.
This research was also supported by Deutsche Forschungsgemeinschaft: grant SCHU 2246/4-2.","['They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation.']",2435,qasper,en,,06baf6c4ae0f97a3131aa06bfa2b4878bd99ef96659cbf9c,"They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation.",315
What are two datasets model is applied to?,"Introduction
“Ché saetta previsa vien più lenta.”
– Dante Alighieri, Divina Commedia, Paradiso
Antisocial behavior is a persistent problem plaguing online conversation platforms; it is both widespread BIBREF0 and potentially damaging to mental and emotional health BIBREF1, BIBREF2. The strain this phenomenon puts on community maintainers has sparked recent interest in computational approaches for assisting human moderators.
Prior work in this direction has largely focused on post-hoc identification of various kinds of antisocial behavior, including hate speech BIBREF3, BIBREF4, harassment BIBREF5, personal attacks BIBREF6, and general toxicity BIBREF7. The fact that these approaches only identify antisocial content after the fact limits their practicality as tools for assisting pre-emptive moderation in conversational domains.
Addressing this limitation requires forecasting the future derailment of a conversation based on early warning signs, giving the moderators time to potentially intervene before any harm is done (BIBREF8 BIBREF8, BIBREF9 BIBREF9, see BIBREF10 BIBREF10 for a discussion). Such a goal recognizes derailment as emerging from the development of the conversation, and belongs to the broader area of conversational forecasting, which includes future-prediction tasks such as predicting the eventual length of a conversation BIBREF11, whether a persuasion attempt will eventually succeed BIBREF12, BIBREF13, BIBREF14, whether team discussions will eventually lead to an increase in performance BIBREF15, or whether ongoing counseling conversations will eventually be perceived as helpful BIBREF16.
Approaching such conversational forecasting problems, however, requires overcoming several inherent modeling challenges. First, conversations are dynamic and their outcome might depend on how subsequent comments interact with each other. Consider the example in Figure FIGREF2: while no individual comment is outright offensive, a human reader can sense a tension emerging from their succession (e.g., dismissive answers to repeated questioning). Thus a forecasting model needs to capture not only the content of each individual comment, but also the relations between comments. Previous work has largely relied on hand-crafted features to capture such relations—e.g., similarity between comments BIBREF16, BIBREF12 or conversation structure BIBREF17, BIBREF18—, though neural attention architectures have also recently shown promise BIBREF19.
The second modeling challenge stems from the fact that conversations have an unknown horizon: they can be of varying lengths, and the to-be-forecasted event can occur at any time. So when is it a good time to make a forecast? Prior work has largely proposed two solutions, both resulting in important practical limitations. One solution is to assume (unrealistic) prior knowledge of when the to-be-forecasted event takes place and extract features up to that point BIBREF20, BIBREF8. Another compromising solution is to extract features from a fixed-length window, often at the start of the conversation BIBREF21, BIBREF15, BIBREF16, BIBREF9. Choosing a catch-all window-size is however impractical: short windows will miss information in comments they do not encompass (e.g., a window of only two comments would miss the chain of repeated questioning in comments 3 through 6 of Figure FIGREF2), while longer windows risk missing the to-be-forecasted event altogether if it occurs before the end of the window, which would prevent early detection.
In this work we introduce a model for forecasting conversational events that overcomes both these inherent challenges by processing comments, and their relations, as they happen (i.e., in an online fashion). Our main insight is that models with these properties already exist, albeit geared toward generation rather than prediction: recent work in context-aware dialog generation (or “chatbots”) has proposed sequential neural models that make effective use of the intra-conversational dynamics BIBREF22, BIBREF23, BIBREF24, while concomitantly being able to process the conversation as it develops (see BIBREF25 for a survey).
In order for these systems to perform well in the generative domain they need to be trained on massive amounts of (unlabeled) conversational data. The main difficulty in directly adapting these models to the supervised domain of conversational forecasting is the relative scarcity of labeled data: for most forecasting tasks, at most a few thousands labeled examples are available, insufficient for the notoriously data-hungry sequential neural models.
To overcome this difficulty, we propose to decouple the objective of learning a neural representation of conversational dynamics from the objective of predicting future events. The former can be pre-trained on large amounts of unsupervised data, similarly to how chatbots are trained. The latter can piggy-back on the resulting representation after fine-tuning it for classification using relatively small labeled data. While similar pre-train-then-fine-tune approaches have recently achieved state-of-the-art performance in a number of NLP tasks—including natural language inference, question answering, and commonsense reasoning (discussed in Section SECREF2)—to the best of our knowledge this is the first attempt at applying this paradigm to conversational forecasting.
To test the effectiveness of this new architecture in forecasting derailment of online conversations, we develop and distribute two new datasets. The first triples in size the highly curated `Conversations Gone Awry' dataset BIBREF9, where civil-starting Wikipedia Talk Page conversations are crowd-labeled according to whether they eventually lead to personal attacks; the second relies on in-the-wild moderation of the popular subreddit ChangeMyView, where the aim is to forecast whether a discussion will later be subject to moderator action due to “rude or hostile” behavior. In both datasets, our model outperforms existing fixed-window approaches, as well as simpler sequential baselines that cannot account for inter-comment relations. Furthermore, by virtue of its online processing of the conversation, our system can provide substantial prior notice of upcoming derailment, triggering on average 3 comments (or 3 hours) before an overtly toxic comment is posted.
To summarize, in this work we:
introduce the first model for forecasting conversational events that can capture the dynamics of a conversation as it develops;
build two diverse datasets (one entirely new, one extending prior work) for the task of forecasting derailment of online conversations;
compare the performance of our model against the current state-of-the-art, and evaluate its ability to provide early warning signs.
Our work is motivated by the goal of assisting human moderators of online communities by preemptively signaling at-risk conversations that might deserve their attention. However, we caution that any automated systems might encode or even amplify the biases existing in the training data BIBREF26, BIBREF27, BIBREF28, so a public-facing implementation would need to be exhaustively scrutinized for such biases BIBREF29.
Further Related Work
Antisocial behavior. Antisocial behavior online comes in many forms, including harassment BIBREF30, cyberbullying BIBREF31, and general aggression BIBREF32. Prior work has sought to understand different aspects of such behavior, including its effect on the communities where it happens BIBREF33, BIBREF34, the actors involved BIBREF35, BIBREF36, BIBREF37, BIBREF38 and connections to the outside world BIBREF39.
Post-hoc classification of conversations. There is a rich body of prior work on classifying the outcome of a conversation after it has concluded, or classifying conversational events after they happened. Many examples exist, but some more closely related to our present work include identifying the winner of a debate BIBREF40, BIBREF41, BIBREF42, identifying successful negotiations BIBREF21, BIBREF43, as well as detecting whether deception BIBREF44, BIBREF45, BIBREF46 or disagreement BIBREF47, BIBREF48, BIBREF49, BIBREF50, BIBREF51 has occurred.
Our goal is different because we wish to forecast conversational events before they happen and while the conversation is still ongoing (potentially allowing for interventions). Note that some post-hoc tasks can also be re-framed as forecasting tasks (assuming the existence of necessary labels); for instance, predicting whether an ongoing conversation will eventually spark disagreement BIBREF18, rather than detecting already-existing disagreement.
Conversational forecasting. As described in Section SECREF1, prior work on forecasting conversational outcomes and events has largely relied on hand-crafted features to capture aspects of conversational dynamics. Example feature sets include statistical measures based on similarity between utterances BIBREF16, sentiment imbalance BIBREF20, flow of ideas BIBREF20, increase in hostility BIBREF8, reply rate BIBREF11 and graph representations of conversations BIBREF52, BIBREF17. By contrast, we aim to automatically learn neural representations of conversational dynamics through pre-training.
Such hand-crafted features are typically extracted from fixed-length windows of the conversation, leaving unaddressed the problem of unknown horizon. While some work has trained multiple models for different window-lengths BIBREF8, BIBREF18, they consider these models to be independent and, as such, do not address the issue of aggregating them into a single forecast (i.e., deciding at what point to make a prediction). We implement a simple sliding windows solution as a baseline (Section SECREF5).
Pre-training for NLP. The use of pre-training for natural language tasks has been growing in popularity after recent breakthroughs demonstrating improved performance on a wide array of benchmark tasks BIBREF53, BIBREF54. Existing work has generally used a language modeling objective as the pre-training objective; examples include next-word prediction BIBREF55, sentence autoencoding, BIBREF56, and machine translation BIBREF57. BERT BIBREF58 introduces a variation on this in which the goal is to predict the next sentence in a document given the current sentence. Our pre-training objective is similar in spirit, but operates at a conversation level, rather than a document level. We hence view our objective as conversational modeling rather than (only) language modeling. Furthermore, while BERT's sentence prediction objective is framed as a multiple-choice task, our objective is framed as a generative task.
Derailment Datasets
We consider two datasets, representing related but slightly different forecasting tasks. The first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9. This dataset uses carefully-controlled crowdsourced labels, strictly filtered to ensure the conversations are civil up to the moment of a personal attack. This is a useful property for the purposes of model analysis, and hence we focus on this as our primary dataset. However, we are conscious of the possibility that these strict labels may not fully capture the kind of behavior that moderators care about in practice. We therefore introduce a secondary dataset, constructed from the subreddit ChangeMyView (CMV) that does not use post-hoc annotations. Instead, the prediction task is to forecast whether the conversation will be subject to moderator action in the future.
Wikipedia data. BIBREF9's `Conversations Gone Awry' dataset consists of 1,270 conversations that took place between Wikipedia editors on publicly accessible talk pages. The conversations are sourced from the WikiConv dataset BIBREF59 and labeled by crowdworkers as either containing a personal attack from within (i.e., hostile behavior by one user in the conversation directed towards another) or remaining civil throughout.
A series of controls are implemented to prevent models from picking up on trivial correlations. To prevent models from capturing topic-specific information (e.g., political conversations are more likely to derail), each attack-containing conversation is paired with a clean conversation from the same talk page, where the talk page serves as a proxy for topic. To force models to actually capture conversational dynamics rather than detecting already-existing toxicity, human annotations are used to ensure that all comments preceding a personal attack are civil.
To the ends of more effective model training, we elected to expand the `Conversations Gone Awry' dataset, using the original annotation procedure. Since we found that the original data skewed towards shorter conversations, we focused this crowdsourcing run on longer conversations: ones with 4 or more comments preceding the attack. Through this additional crowdsourcing, we expand the dataset to 4,188 conversations, which we are publicly releasing as part of the Cornell Conversational Analysis Toolkit (ConvoKit).
We perform an 80-20-20 train/dev/test split, ensuring that paired conversations end up in the same split in order to preserve the topic control. Finally, we randomly sample another 1 million conversations from WikiConv to use for the unsupervised pre-training of the generative component.
Reddit CMV data. The CMV dataset is constructed from conversations collected via the Reddit API. In contrast to the Wikipedia-based dataset, we explicitly avoid the use of post-hoc annotation. Instead, we use as our label whether a conversation eventually had a comment removed by a moderator for violation of Rule 2: “Don't be rude or hostile to other users”.
Though the lack of post-hoc annotation limits the degree to which we can impose controls on the data (e.g., some conversations may contain toxic comments not flagged by the moderators) we do reproduce as many of the Wikipedia data's controls as we can. Namely, we replicate the topic control pairing by choosing pairs of positive and negative examples that belong to the same top-level post, following BIBREF12; and enforce that the removed comment was made by a user who was previously involved in the conversation. This process results in 6,842 conversations, to which we again apply a pair-preserving 80-20-20 split. Finally, we gather over 600,000 conversations that do not include any removed comment, for unsupervised pre-training.
Conversational Forecasting Model
We now describe our general model for forecasting future conversational events. Our model integrates two components: (a) a generative dialog model that learns to represent conversational dynamics in an unsupervised fashion; and (b) a supervised component that fine-tunes this representation to forecast future events. Figure FIGREF13 provides an overview of the proposed architecture, henceforth CRAFT (Conversational Recurrent Architecture for ForecasTing).
Terminology. For modeling purposes, we treat a conversation as a sequence of $N$ comments $C = \lbrace c_1,\dots ,c_N\rbrace $. Each comment, in turn, is a sequence of tokens, where the number of tokens may vary from comment to comment. For the $n$-th comment ($1 \le n \le N)$, we let $M_n$ denote the number of tokens. Then, a comment $c_n$ can be represented as a sequence of $M_n$ tokens: $c_n = \lbrace w_1,\dots ,w_{M_n}\rbrace $.
Generative component. For the generative component of our model, we use a hierarchical recurrent encoder-decoder (HRED) architecture BIBREF60, a modified version of the popular sequence-to-sequence (seq2seq) architecture BIBREF61 designed to account for dependencies between consecutive inputs. BIBREF23 showed that HRED can successfully model conversational context by encoding the temporal structure of previously seen comments, making it an ideal fit for our use case. Here, we provide a high-level summary of the HRED architecture, deferring deeper technical discussion to BIBREF60 and BIBREF23.
An HRED dialog model consists of three components: an utterance encoder, a context encoder, and a decoder. The utterance encoder is responsible for generating semantic vector representations of comments. It consists of a recurrent neural network (RNN) that reads a comment token-by-token, and on each token $w_m$ updates a hidden state $h^{\text{enc}}$ based on the current token and the previous hidden state:
where $f^{\text{RNN}}$ is a nonlinear gating function (our implementation uses GRU BIBREF62). The final hidden state $h^{\text{enc}}_M$ can be viewed as a vector encoding of the entire comment.
Running the encoder on each comment $c_n$ results in a sequence of $N$ vector encodings. A second encoder, the context encoder, is then run over this sequence:
Each hidden state $h^{\text{con}}_n$ can then be viewed as an encoding of the full conversational context up to and including the $n$-th comment. To generate a response to comment $n$, the context encoding $h^{\text{con}}_n$ is used to initialize the hidden state $h^{\text{dec}}_{0}$ of a decoder RNN. The decoder produces a response token by token using the following recurrence:
where $f^{\text{out}}$ is some function that outputs a probability distribution over words; we implement this using a simple feedforward layer. In our implementation, we further augment the decoder with attention BIBREF63, BIBREF64 over context encoder states to help capture long-term inter-comment dependencies. This generative component can be pre-trained using unlabeled conversational data.
Prediction component. Given a pre-trained HRED dialog model, we aim to extend the model to predict from the conversational context whether the to-be-forecasted event will occur. Our predictor consists of a multilayer perceptron (MLP) with 3 fully-connected layers, leaky ReLU activations between layers, and sigmoid activation for output. For each comment $c_n$, the predictor takes as input the context encoding $h^{\text{con}}_n$ and forwards it through the MLP layers, resulting in an output score that is interpreted as a probability $p_{\text{event}}(c_{n+1})$ that the to-be-forecasted event will happen (e.g., that the conversation will derail).
Training the predictive component starts by initializing the weights of the encoders to the values learned in pre-training. The main training loop then works as follows: for each positive sample—i.e., a conversation containing an instance of the to-be-forecasted event (e.g., derailment) at comment $c_e$—we feed the context $c_1,\dots ,c_{e-1}$ through the encoder and classifier, and compute cross-entropy loss between the classifier output and expected output of 1. Similarly, for each negative sample—i.e., a conversation where none of the comments exhibit the to-be-forecasted event and that ends with $c_N$—we feed the context $c_1,\dots ,c_{N-1}$ through the model and compute loss against an expected output of 0.
Note that the parameters of the generative component are not held fixed during this process; instead, backpropagation is allowed to go all the way through the encoder layers. This process, known as fine-tuning, reshapes the representation learned during pre-training to be more directly useful to prediction BIBREF55.
We implement the model and training code using PyTorch, and we are publicly releasing our implementation and the trained models together with the data as part of ConvoKit.
Forecasting Derailment
We evaluate the performance of CRAFT in the task of forecasting conversational derailment in both the Wikipedia and CMV scenarios. To this end, for each of these datasets we pre-train the generative component on the unlabeled portion of the data and fine-tune it on the labeled training split (data size detailed in Section SECREF3).
In order to evaluate our sequential system against conversational-level ground truth, we need to aggregate comment level predictions. If any comment in the conversation triggers a positive prediction—i.e., $p_{\text{event}}(c_{n+1})$ is greater than a threshold learned on the development split—then the respective conversation is predicted to derail. If this forecast is triggered in a conversation that actually derails, but before the derailment actually happens, then the conversation is counted as a true positive; otherwise it is a false positive. If no positive predictions are triggered for a conversation, but it actually derails then it counts as a false negative; if it does not derail then it is a true negative.
Fixed-length window baselines. We first seek to compare CRAFT to existing, fixed-length window approaches to forecasting. To this end, we implement two such baselines: Awry, which is the state-of-the-art method proposed in BIBREF9 based on pragmatic features in the first comment-reply pair, and BoW, a simple bag-of-words baseline that makes a prediction using TF-IDF weighted bag-of-words features extracted from the first comment-reply pair.
Online forecasting baselines. Next, we consider simpler approaches for making forecasts as the conversations happen (i.e., in an online fashion). First, we propose Cumulative BoW, a model that recomputes bag-of-words features on all comments seen thus far every time a new comment arrives. While this approach does exhibit the desired behavior of producing updated predictions for each new comment, it fails to account for relationships between comments.
This simple cumulative approach cannot be directly extended to models whose features are strictly based on a fixed number of comments, like Awry. An alternative is to use a sliding window: for a feature set based on a window of $W$ comments, upon each new comment we can extract features from a window containing that comment and the $W-1$ comments preceding it. We apply this to the Awry method and call this model Sliding Awry. For both these baselines, we aggregate comment-level predictions in the same way as in our main model.
CRAFT ablations. Finally, we consider two modified versions of the CRAFT model in order to evaluate the impact of two of its key components: (1) the pre-training step, and (2) its ability to capture inter-comment dependencies through its hierarchical memory.
To evaluate the impact of pre-training, we train the prediction component of CRAFT on only the labeled training data, without first pre-training the encoder layers with the unlabeled data. We find that given the relatively small size of labeled data, this baseline fails to successfully learn, and ends up performing at the level of random guessing. This result underscores the need for the pre-training step that can make use of unlabeled data.
To evaluate the impact of the hierarchical memory, we implement a simplified version of CRAFT where the memory size of the context encoder is zero (CRAFT $-$ CE), thus effectively acting as if the pre-training component is a vanilla seq2seq model. In other words, this model cannot capture inter-comment dependencies, and instead at each step makes a prediction based only on the utterance encoding of the latest comment.
Results. Table TABREF17 compares CRAFT to the baselines on the test splits (random baseline is 50%) and illustrates several key findings. First, we find that unsurprisingly, accounting for full conversational context is indeed helpful, with even the simple online baselines outperforming the fixed-window baselines. On both datasets, CRAFT outperforms all baselines (including the other online models) in terms of accuracy and F1. Furthermore, although it loses on precision (to CRAFT $-$ CE) and recall (to Cumulative BoW) individually on the Wikipedia data, CRAFT has the superior balance between the two, having both a visibly higher precision-recall curve and larger area under the curve (AUPR) than the baselines (Figure FIGREF20). This latter property is particularly useful in a practical setting, as it allows moderators to tune model performance to some desired precision without having to sacrifice as much in the way of recall (or vice versa) compared to the baselines and pre-existing solutions.
Analysis
We now examine the behavior of CRAFT in greater detail, to better understand its benefits and limitations. We specifically address the following questions: (1) How much early warning does the the model provide? (2) Does the model actually learn an order-sensitive representation of conversational context?
Early warning, but how early? The recent interest in forecasting antisocial behavior has been driven by a desire to provide pre-emptive, actionable warning to moderators. But does our model trigger early enough for any such practical goals?
For each personal attack correctly forecasted by our model, we count the number of comments elapsed between the time the model is first triggered and the attack. Figure FIGREF22 shows the distribution of these counts: on average, the model warns of an attack 3 comments before it actually happens (4 comments for CMV). To further evaluate how much time this early warning would give to the moderator, we also consider the difference in timestamps between the comment where the model first triggers and the comment containing the actual attack. Over 50% of conversations get at least 3 hours of advance warning (2 hours for CMV). Moreover, 39% of conversations get at least 12 hours of early warning before they derail.
Does order matter? One motivation behind the design of our model was the intuition that comments in a conversation are not independent events; rather, the order in which they appear matters (e.g., a blunt comment followed by a polite one feels intuitively different from a polite comment followed by a blunt one). By design, CRAFT has the capacity to learn an order-sensitive representation of conversational context, but how can we know that this capacity is actually used? It is conceivable that the model is simply computing an order-insensitive “bag-of-features”. Neural network models are notorious for their lack of transparency, precluding an analysis of how exactly CRAFT models conversational context. Nevertheless, through two simple exploratory experiments, we seek to show that it does not completely ignore comment order.
The first experiment for testing whether the model accounts for comment order is a prefix-shuffling experiment, visualized in Figure FIGREF23. For each conversation that the model predicts will derail, let $t$ denote the index of the triggering comment, i.e., the index where the model first made a derailment forecast. We then construct synthetic conversations by taking the first $t-1$ comments (henceforth referred to as the prefix) and randomizing their order. Finally, we count how often the model no longer predicts derailment at index $t$ in the synthetic conversations. If the model were ignoring comment order, its prediction should remain unchanged (as it remains for the Cumulative BoW baseline), since the actual content of the first $t$ comments has not changed (and CRAFT inference is deterministic). We instead find that in roughly one fifth of cases (12% for CMV) the model changes its prediction on the synthetic conversations. This suggests that CRAFT learns an order-sensitive representation of context, not a mere “bag-of-features”.
To more concretely quantify how much this order-sensitive context modeling helps with prediction, we can actively prevent the model from learning and exploiting any order-related dynamics. We achieve this through another type of shuffling experiment, where we go back even further and shuffle the comment order in the conversations used for pre-training, fine-tuning and testing. This procedure preserves the model's ability to capture signals present within the individual comments processed so far, as the utterance encoder is unaffected, but inhibits it from capturing any meaningful order-sensitive dynamics. We find that this hurts the model's performance (65% accuracy for Wikipedia, 59.5% for CMV), lowering it to a level similar to that of the version where we completely disable the context encoder.
Taken together, these experiments provide evidence that CRAFT uses its capacity to model conversational context in an order-sensitive fashion, and that it makes effective use of the dynamics within. An important avenue for future work would be developing more transparent models that can shed light on exactly what kinds of order-related features are being extracted and how they are used in prediction.
Conclusions and Future Work
In this work, we introduced a model for forecasting conversational events that processes comments as they happen and takes the full conversational context into account to make an updated prediction at each step. This model fills a void in the existing literature on conversational forecasting, simultaneously addressing the dual challenges of capturing inter-comment dynamics and dealing with an unknown horizon. We find that our model achieves state-of-the-art performance on the task of forecasting derailment in two different datasets that we release publicly. We further show that the resulting system can provide substantial prior notice of derailment, opening up the potential for preemptive interventions by human moderators BIBREF65.
While we have focused specifically on the task of forecasting derailment, we view this work as a step towards a more general model for real-time forecasting of other types of emergent properties of conversations. Follow-up work could adapt the CRAFT architecture to address other forecasting tasks mentioned in Section SECREF2—including those for which the outcome is extraneous to the conversation. We expect different tasks to be informed by different types of inter-comment dynamics, and further architecture extensions could add additional supervised fine-tuning in order to direct it to focus on specific dynamics that might be relevant to the task (e.g., exchange of ideas between interlocutors or stonewalling).
With respect to forecasting derailment, there remain open questions regarding what human moderators actually desire from an early-warning system, which would affect the design of a practical system based on this work. For instance, how early does a warning need to be in order for moderators to find it useful? What is the optimal balance between precision, recall, and false positive rate at which such a system is truly improving moderator productivity rather than wasting their time through false positives? What are the ethical implications of such a system? Follow-up work could run a user study of a prototype system with actual moderators to address these questions.
A practical limitation of the current analysis is that it relies on balanced datasets, while derailment is a relatively rare event for which a more restrictive trigger threshold would be appropriate. While our analysis of the precision-recall curve suggests the system is robust across multiple thresholds ($AUPR=0.7$), additional work is needed to establish whether the recall tradeoff would be acceptable in practice.
Finally, one major limitation of the present work is that it assigns a single label to each conversation: does it derail or not? In reality, derailment need not spell the end of a conversation; it is possible that a conversation could get back on track, suffer a repeat occurrence of antisocial behavior, or any number of other trajectories. It would be exciting to consider finer-grained forecasting of conversational trajectories, accounting for the natural—and sometimes chaotic—ebb-and-flow of human interactions.
Acknowledgements. We thank Caleb Chiam, Liye Fu, Lillian Lee, Alexandru Niculescu-Mizil, Andrew Wang and Justine Zhang for insightful conversations (with unknown horizon), Aditya Jha for his great help with implementing and running the crowd-sourcing tasks, Thomas Davidson and Claire Liang for exploratory data annotation, as well as the anonymous reviewers for their helpful comments. This work is supported in part by the NSF CAREER award IIS-1750615 and by the NSF Grant SES-1741441.","["" `Conversations Gone Awry' dataset, subreddit ChangeMyView"", ""An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. ""]",4718,qasper,en,,2589b46ee58a7600e17fa89a0d4fffd9a0faf1df49b3c035," `Conversations Gone Awry' dataset, subreddit ChangeMyView"", ""An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. ",266
What were their distribution results?,"10pt
1.10pt
[ Characterizing Political Fake News in Twitter by its Meta-DataJulio Amador Díaz LópezAxel Oehmichen Miguel Molina-Solana( j.amador, axelfrancois.oehmichen11, mmolinas@imperial.ac.uk ) Imperial College London This article presents a preliminary approach towards characterizing political fake news on Twitter through the analysis of their meta-data. In particular, we focus on more than 1.5M tweets collected on the day of the election of Donald Trump as 45th president of the United States of America. We use the meta-data embedded within those tweets in order to look for differences between tweets containing fake news and tweets not containing them. Specifically, we perform our analysis only on tweets that went viral, by studying proxies for users' exposure to the tweets, by characterizing accounts spreading fake news, and by looking at their polarization. We found significant differences on the distribution of followers, the number of URLs on tweets, and the verification of the users.
]
Introduction
While fake news, understood as deliberately misleading pieces of information, have existed since long ago (e.g. it is not unusual to receive news falsely claiming the death of a celebrity), the term reached the mainstream, particularly so in politics, during the 2016 presidential election in the United States BIBREF0 . Since then, governments and corporations alike (e.g. Google BIBREF1 and Facebook BIBREF2 ) have begun efforts to tackle fake news as they can affect political decisions BIBREF3 . Yet, the ability to define, identify and stop fake news from spreading is limited.
Since the Obama campaign in 2008, social media has been pervasive in the political arena in the United States. Studies report that up to 62% of American adults receive their news from social media BIBREF4 . The wide use of platforms such as Twitter and Facebook has facilitated the diffusion of fake news by simplifying the process of receiving content with no significant third party filtering, fact-checking or editorial judgement. Such characteristics make these platforms suitable means for sharing news that, disguised as legit ones, try to confuse readers.
Such use and their prominent rise has been confirmed by Craig Silverman, a Canadian journalist who is a prominent figure on fake news BIBREF5 : “In the final three months of the US presidential campaign, the top-performing fake election news stories on Facebook generated more engagement than the top stories from major news outlet”.
Our current research hence departs from the assumption that social media is a conduit for fake news and asks the question of whether fake news (as spam was some years ago) can be identified, modelled and eventually blocked. In order to do so, we use a sample of more that 1.5M tweets collected on November 8th 2016 —election day in the United States— with the goal of identifying features that tweets containing fake news are likely to have. As such, our paper aims to provide a preliminary characterization of fake news in Twitter by looking into meta-data embedded in tweets. Considering meta-data as a relevant factor of analysis is in line with findings reported by Morris et al. BIBREF6 . We argue that understanding differences between tweets containing fake news and regular tweets will allow researchers to design mechanisms to block fake news in Twitter.
Specifically, our goals are: 1) compare the characteristics of tweets labelled as containing fake news to tweets labelled as not containing them, 2) characterize, through their meta-data, viral tweets containing fake news and the accounts from which they originated, and 3) determine the extent to which tweets containing fake news expressed polarized political views.
For our study, we used the number of retweets to single-out those that went viral within our sample. Tweets within that subset (viral tweets hereafter) are varied and relate to different topics. We consider that a tweet contains fake news if its text falls within any of the following categories described by Rubin et al. BIBREF7 (see next section for the details of such categories): serious fabrication, large-scale hoaxes, jokes taken at face value, slanted reporting of real facts and stories where the truth is contentious. The dataset BIBREF8 , manually labelled by an expert, has been publicly released and is available to researchers and interested parties.
From our results, the following main observations can be made:
Our findings resonate with similar work done on fake news such as the one from Allcot and Gentzkow BIBREF9 . Therefore, even if our study is a preliminary attempt at characterizing fake news on Twitter using only their meta-data, our results provide external validity to previous research. Moreover, our work not only stresses the importance of using meta-data, but also underscores which parameters may be useful to identify fake news on Twitter.
The rest of the paper is organized as follows. The next section briefly discusses where this work is located within the literature on fake news and contextualizes the type of fake news we are studying. Then, we present our hypotheses, the data, and the methodology we follow. Finally, we present our findings, conclusions of this study, and future lines of work.
Defining Fake news
Our research is connected to different strands of academic knowledge related to the phenomenon of fake news. In relation to Computer Science, a recent survey by Conroy and colleagues BIBREF10 identifies two popular approaches to single-out fake news. On the one hand, the authors pointed to linguistic approaches consisting in using text, its linguistic characteristics and machine learning techniques to automatically flag fake news. On the other, these researchers underscored the use of network approaches, which make use of network characteristics and meta-data, to identify fake news.
With respect to social sciences, efforts from psychology, political science and sociology, have been dedicated to understand why people consume and/or believe misinformation BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 . Most of these studies consistently reported that psychological biases such as priming effects and confirmation bias play an important role in people ability to discern misinformation.
In relation to the production and distribution of fake news, a recent paper in the field of Economics BIBREF9 found that most fake news sites use names that resemble those of legitimate organizations, and that sites supplying fake news tend to be short-lived. These authors also noticed that fake news items are more likely shared than legitimate articles coming from trusted sources, and they tend to exhibit a larger level of polarization.
The conceptual issue of how to define fake news is a serious and unresolved issue. As the focus of our work is not attempting to offer light on this, we will rely on work by other authors to describe what we consider as fake news. In particular, we use the categorization provided by Rubin et al. BIBREF7 . The five categories they described, together with illustrative examples from our dataset, are as follows:
Research Hypotheses
Previous works on the area (presented in the section above) suggest that there may be important determinants for the adoption and diffusion of fake news. Our hypotheses builds on them and identifies three important dimensions that may help distinguishing fake news from legit information:
Taking those three dimensions into account, we propose the following hypotheses about the features that we believe can help to identify tweets containing fake news from those not containing them. They will be later tested over our collected dataset.
Exposure.
Characterization.
Polarization.
Data and Methodology
For this study, we collected publicly available tweets using Twitter's public API. Given the nature of the data, it is important to emphasize that such tweets are subject to Twitter's terms and conditions which indicate that users consent to the collection, transfer, manipulation, storage, and disclosure of data. Therefore, we do not expect ethical, legal, or social implications from the usage of the tweets. Our data was collected using search terms related to the presidential election held in the United States on November 8th 2016. Particularly, we queried Twitter's streaming API, more precisely the filter endpoint of the streaming API, using the following hashtags and user handles: #MyVote2016, #ElectionDay, #electionnight, @realDonaldTrump and @HillaryClinton. The data collection ran for just one day (Nov 8th 2016).
One straightforward way of sharing information on Twitter is by using the retweet functionality, which enables a user to share a exact copy of a tweet with his followers. Among the reasons for retweeting, Body et al. BIBREF15 reported the will to: 1) spread tweets to a new audience, 2) to show one’s role as a listener, and 3) to agree with someone or validate the thoughts of others. As indicated, our initial interest is to characterize tweets containing fake news that went viral (as they are the most harmful ones, as they reach a wider audience), and understand how it differs from other viral tweets (that do not contain fake news). For our study, we consider that a tweet went viral if it was retweeted more than 1000 times.
Once we have the dataset of viral tweets, we eliminated duplicates (some of the tweets were collected several times because they had several handles) and an expert manually inspected the text field within the tweets to label them as containing fake news, or not containing them (according to the characterization presented before). This annotated dataset BIBREF8 is publicly available and can be freely reused.
Finally, we use the following fields within tweets (from the ones returned by Twitter's API) to compare their distributions and look for differences between viral tweets containing fake news and viral tweets not containing fake news:
In the following section, we provide graphical descriptions of the distribution of each of the identified attributes for the two sets of tweets (those labelled as containing fake news and those labelled as not containing them). Where appropriate, we normalized and/or took logarithms of the data for better representation. To gain a better understanding of the significance of those differences, we use the Kolmogorov-Smirnov test with the null hypothesis that both distributions are equal.
Results
The sample collected consisted on 1 785 855 tweets published by 848 196 different users. Within our sample, we identified 1327 tweets that went viral (retweeted more than 1000 times by the 8th of November 2016) produced by 643 users. Such small subset of viral tweets were retweeted on 290 841 occasions in the observed time-window.
The 1327 `viral' tweets were manually annotated as containing fake news or not. The annotation was carried out by a single person in order to obtain a consistent annotation throughout the dataset. Out of those 1327 tweets, we identified 136 as potentially containing fake news (according to the categories previously described), and the rest were classified as `non containing fake news'. Note that the categorization is far from being perfect given the ambiguity of fake news themselves and human judgement involved in the process of categorization. Because of this, we do not claim that this dataset can be considered a ground truth.
The following results detail characteristics of these tweets along the previously mentioned dimensions. Table TABREF23 reports the actual differences (together with their associated p-values) of the distributions of viral tweets containing fake news and viral tweets not containing them for every variable considered.
Exposure
Figure FIGREF24 shows that, in contrast to other kinds of viral tweets, those containing fake news were created more recently. As such, Twitter users were exposed to fake news related to the election for a shorter period of time.
However, in terms of retweets, Figure FIGREF25 shows no apparent difference between containing fake news or not containing them. That is confirmed by the Kolmogorov-Smirnoff test, which does not discard the hypothesis that the associated distributions are equal.
In relation to the number of favourites, users that generated at least a viral tweet containing fake news appear to have, on average, less favourites than users that do not generate them. Figure FIGREF26 shows the distribution of favourites. Despite the apparent visual differences, the difference are not statistically significant.
Finally, the number of hashtags used in viral fake news appears to be larger than those in other viral tweets. Figure FIGREF27 shows the density distribution of the number of hashtags used. However, once again, we were not able to find any statistical difference between the average number of hashtags in a viral tweet and the average number of hashtags in viral fake news.
Characterization
We found that 82 users within our sample were spreading fake news (i.e. they produced at least one tweet which was labelled as fake news). Out of those, 34 had verified accounts, and the rest were unverified. From the 48 unverified accounts, 6 have been suspended by Twitter at the date of writing, 3 tried to imitate legitimate accounts of others, and 4 accounts have been already deleted. Figure FIGREF28 shows the proportion of verified accounts to unverified accounts for viral tweets (containing fake news vs. not containing fake news). From the chart, it is clear that there is a higher chance of fake news coming from unverified accounts.
Turning to friends, accounts distributing fake news appear to have, on average, the same number of friends than those distributing tweets with no fake news. However, the density distribution of friends from the accounts (Figure FIGREF29 ) shows that there is indeed a statistically significant difference in their distributions.
If we take into consideration the number of followers, accounts generating viral tweets with fake news do have a very different distribution on this dimension, compared to those accounts generating viral tweets with no fake news (see Figure FIGREF30 ). In fact, such differences are statistically significant.
A useful representation for friends and followers is the ratio between friends/followers. Figures FIGREF31 and FIGREF32 show this representation. Notice that accounts spreading viral tweets with fake news have, on average, a larger ratio of friends/followers. The distribution of those accounts not generating fake news is more evenly distributed.
With respect to the number of mentions, Figure FIGREF33 shows that viral tweets labelled as containing fake news appear to use mentions to other users less frequently than viral tweets not containing fake news. In other words, tweets containing fake news mostly contain 1 mention, whereas other tweets tend to have two). Such differences are statistically significant.
The analysis (Figure FIGREF34 ) of the presence of media in the tweets in our dataset shows that tweets labelled as not containing fake news appear to present more media elements than those labelled as fake news. However, the difference is not statistically significant.
On the other hand, Figure FIGREF35 shows that viral tweets containing fake news appear to include more URLs to other sites than viral tweets that do not contain fake news. In fact, the difference between the two distributions is statistically significant (assuming INLINEFORM0 ).
Polarization
Finally, manual inspection of the text field of those viral tweets labelled as containing fake news shows that 117 of such tweets expressed support for Donald Trump, while only 8 supported Hillary Clinton. The remaining tweets contained fake news related to other topics, not expressing support for any of the candidates.
Discussion
As a summary, and constrained by our existing dataset, we made the following observations regarding differences between viral tweets labelled as containing fake news and viral tweets labelled as not containing them:
These findings (related to our initial hypothesis in Table TABREF44 ) clearly suggest that there are specific pieces of meta-data about tweets that may allow the identification of fake news. One such parameter is the time of exposure. Viral tweets containing fake news are shorter-lived than those containing other type of content. This notion seems to resonate with our findings showing that a number of accounts spreading fake news have already been deleted or suspended by Twitter by the time of writing. If one considers that researchers using different data have found similar results BIBREF9 , it appears that the lifetime of accounts, together with the age of the questioned viral content could be useful to identify fake news. In the light of this finding, accounts newly created should probably put under higher scrutiny than older ones. This in fact, would be a nice a-priori bias for a Bayesian classifier.
Accounts spreading fake news appear to have a larger proportion of friends/followers (i.e. they have, on average, the same number of friends but a smaller number of followers) than those spreading viral content only. Together with the fact that, on average, tweets containing fake news have more URLs than those spreading viral content, it is possible to hypothesize that, both, the ratio of friends/followers of the account producing a viral tweet and number of URLs contained in such a tweet could be useful to single-out fake news in Twitter. Not only that, but our finding related to the number of URLs is in line with intuitions behind the incentives to create fake news commonly found in the literature BIBREF9 (in particular that of obtaining revenue through click-through advertising).
Finally, it is interesting to notice that the content of viral fake news was highly polarized. This finding is also in line with those of Alcott et al. BIBREF9 . This feature suggests that textual sentiment analysis of the content of tweets (as most researchers do), together with the above mentioned parameters from meta-data, may prove useful for identifying fake news.
Conclusions
With the election of Donald Trump as President of the United States, the concept of fake news has become a broadly-known phenomenon that is getting tremendous attention from governments and media companies. We have presented a preliminary study on the meta-data of a publicly available dataset of tweets that became viral during the day of the 2016 US presidential election. Our aim is to advance the understanding of which features might be characteristic of viral tweets containing fake news in comparison with viral tweets without fake news.
We believe that the only way to automatically identify those deceitful tweets (i.e. containing fake news) is by actually understanding and modelling them. Only then, the automation of the processes of tagging and blocking these tweets can be successfully performed. In the same way that spam was fought, we anticipate fake news will suffer a similar evolution, with social platforms implementing tools to deal with them. With most works so far focusing on the actual content of the tweets, ours is a novel attempt from a different, but also complementary, angle.
Within the used dataset, we found there are differences around exposure, characteristics of accounts spreading fake news and the tone of the content. Those findings suggest that it is indeed possible to model and automatically detect fake news. We plan to replicate and validate our experiments in an extended sample of tweets (until 4 months after the US election), and tests the predictive power of the features we found relevant within our sample.
Author Disclosure Statement
No competing financial interest exist.","['Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different']",3164,qasper,en,,5663b4bd81cbb6b052e49e8df64f91f4fa3146f4067b2f94,"Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different",246
"What is best performing model among author's submissions, what performance it had?","Introduction
In the age of information dissemination without quality control, it has enabled malicious users to spread misinformation via social media and aim individual users with propaganda campaigns to achieve political and financial gains as well as advance a specific agenda. Often disinformation is complied in the two major forms: fake news and propaganda, where they differ in the sense that the propaganda is possibly built upon true information (e.g., biased, loaded language, repetition, etc.).
Prior works BIBREF0, BIBREF1, BIBREF2 in detecting propaganda have focused primarily at document level, typically labeling all articles from a propagandistic news outlet as propaganda and thus, often non-propagandistic articles from the outlet are mislabeled. To this end, EMNLP19DaSanMartino focuses on analyzing the use of propaganda and detecting specific propagandistic techniques in news articles at sentence and fragment level, respectively and thus, promotes explainable AI. For instance, the following text is a propaganda of type `slogan'.
Trump tweeted: $\underbrace{\text{`}`{\texttt {BUILD THE WALL!}""}}_{\text{slogan}}$
Shared Task: This work addresses the two tasks in propaganda detection BIBREF3 of different granularities: (1) Sentence-level Classification (SLC), a binary classification that predicts whether a sentence contains at least one propaganda technique, and (2) Fragment-level Classification (FLC), a token-level (multi-label) classification that identifies both the spans and the type of propaganda technique(s).
Contributions: (1) To address SLC, we design an ensemble of different classifiers based on Logistic Regression, CNN and BERT, and leverage transfer learning benefits using the pre-trained embeddings/models from FastText and BERT. We also employed different features such as linguistic (sentiment, readability, emotion, part-of-speech and named entity tags, etc.), layout, topics, etc. (2) To address FLC, we design a multi-task neural sequence tagger based on LSTM-CRF and linguistic features to jointly detect propagandistic fragments and its type. Moreover, we investigate performing FLC and SLC jointly in a multi-granularity network based on LSTM-CRF and BERT. (3) Our system (MIC-CIS) is ranked 3rd (out of 12 participants) and 4th (out of 25 participants) in FLC and SLC tasks, respectively.
System Description ::: Linguistic, Layout and Topical Features
Some of the propaganda techniques BIBREF3 involve word and phrases that express strong emotional implications, exaggeration, minimization, doubt, national feeling, labeling , stereotyping, etc. This inspires us in extracting different features (Table TABREF1) including the complexity of text, sentiment, emotion, lexical (POS, NER, etc.), layout, etc. To further investigate, we use topical features (e.g., document-topic proportion) BIBREF4, BIBREF5, BIBREF6 at sentence and document levels in order to determine irrelevant themes, if introduced to the issue being discussed (e.g., Red Herring).
For word and sentence representations, we use pre-trained vectors from FastText BIBREF7 and BERT BIBREF8.
System Description ::: Sentence-level Propaganda Detection
Figure FIGREF2 (left) describes the three components of our system for SLC task: features, classifiers and ensemble. The arrows from features-to-classifier indicate that we investigate linguistic, layout and topical features in the two binary classifiers: LogisticRegression and CNN. For CNN, we follow the architecture of DBLP:conf/emnlp/Kim14 for sentence-level classification, initializing the word vectors by FastText or BERT. We concatenate features in the last hidden layer before classification.
One of our strong classifiers includes BERT that has achieved state-of-the-art performance on multiple NLP benchmarks. Following DBLP:conf/naacl/DevlinCLT19, we fine-tune BERT for binary classification, initializing with a pre-trained model (i.e., BERT-base, Cased). Additionally, we apply a decision function such that a sentence is tagged as propaganda if prediction probability of the classifier is greater than a threshold ($\tau $). We relax the binary decision boundary to boost recall, similar to pankajgupta:CrossRE2019.
Ensemble of Logistic Regression, CNN and BERT: In the final component, we collect predictions (i.e., propaganda label) for each sentence from the three ($\mathcal {M}=3$) classifiers and thus, obtain $\mathcal {M}$ number of predictions for each sentence. We explore two ensemble strategies (Table TABREF1): majority-voting and relax-voting to boost precision and recall, respectively.
System Description ::: Fragment-level Propaganda Detection
Figure FIGREF2 (right) describes our system for FLC task, where we design sequence taggers BIBREF9, BIBREF10 in three modes: (1) LSTM-CRF BIBREF11 with word embeddings ($w\_e$) and character embeddings $c\_e$, token-level features ($t\_f$) such as polarity, POS, NER, etc. (2) LSTM-CRF+Multi-grain that jointly performs FLC and SLC with FastTextWordEmb and BERTSentEmb, respectively. Here, we add binary sentence classification loss to sequence tagging weighted by a factor of $\alpha $. (3) LSTM-CRF+Multi-task that performs propagandistic span/fragment detection (PFD) and FLC (fragment detection + 19-way classification).
Ensemble of Multi-grain, Multi-task LSTM-CRF with BERT: Here, we build an ensemble by considering propagandistic fragments (and its type) from each of the sequence taggers. In doing so, we first perform majority voting at the fragment level for the fragment where their spans exactly overlap. In case of non-overlapping fragments, we consider all. However, when the spans overlap (though with the same label), we consider the fragment with the largest span.
Experiments and Evaluation
Data: While the SLC task is binary, the FLC consists of 18 propaganda techniques BIBREF3. We split (80-20%) the annotated corpus into 5-folds and 3-folds for SLC and FLC tasks, respectively. The development set of each the folds is represented by dev (internal); however, the un-annotated corpus used in leaderboard comparisons by dev (external). We remove empty and single token sentences after tokenization. Experimental Setup: We use PyTorch framework for the pre-trained BERT model (Bert-base-cased), fine-tuned for SLC task. In the multi-granularity loss, we set $\alpha = 0.1$ for sentence classification based on dev (internal, fold1) scores. We use BIO tagging scheme of NER in FLC task. For CNN, we follow DBLP:conf/emnlp/Kim14 with filter-sizes of [2, 3, 4, 5, 6], 128 filters and 16 batch-size. We compute binary-F1and macro-F1 BIBREF12 in SLC and FLC, respectively on dev (internal).
Experiments and Evaluation ::: Results: Sentence-Level Propaganda
Table TABREF10 shows the scores on dev (internal and external) for SLC task. Observe that the pre-trained embeddings (FastText or BERT) outperform TF-IDF vector representation. In row r2, we apply logistic regression classifier with BERTSentEmb that leads to improved scores over FastTextSentEmb. Subsequently, we augment the sentence vector with additional features that improves F1 on dev (external), however not dev (internal). Next, we initialize CNN by FastTextWordEmb or BERTWordEmb and augment the last hidden layer (before classification) with BERTSentEmb and feature vectors, leading to gains in F1 for both the dev sets. Further, we fine-tune BERT and apply different thresholds in relaxing the decision boundary, where $\tau \ge 0.35$ is found optimal.
We choose the three different models in the ensemble: Logistic Regression, CNN and BERT on fold1 and subsequently an ensemble+ of r3, r6 and r12 from each fold1-5 (i.e., 15 models) to obtain predictions for dev (external). We investigate different ensemble schemes (r17-r19), where we observe that the relax-voting improves recall and therefore, the higher F1 (i.e., 0.673). In postprocess step, we check for repetition propaganda technique by computing cosine similarity between the current sentence and its preceding $w=10$ sentence vectors (i.e., BERTSentEmb) in the document. If the cosine-similarity is greater than $\lambda \in \lbrace .99, .95\rbrace $, then the current sentence is labeled as propaganda due to repetition. Comparing r19 and r21, we observe a gain in recall, however an overall decrease in F1 applying postprocess.
Finally, we use the configuration of r19 on the test set. The ensemble+ of (r4, r7 r12) was analyzed after test submission. Table TABREF9 (SLC) shows that our submission is ranked at 4th position.
Experiments and Evaluation ::: Results: Fragment-Level Propaganda
Table TABREF11 shows the scores on dev (internal and external) for FLC task. Observe that the features (i.e., polarity, POS and NER in row II) when introduced in LSTM-CRF improves F1. We run multi-grained LSTM-CRF without BERTSentEmb (i.e., row III) and with it (i.e., row IV), where the latter improves scores on dev (internal), however not on dev (external). Finally, we perform multi-tasking with another auxiliary task of PFD. Given the scores on dev (internal and external) using different configurations (rows I-V), it is difficult to infer the optimal configuration. Thus, we choose the two best configurations (II and IV) on dev (internal) set and build an ensemble+ of predictions (discussed in section SECREF6), leading to a boost in recall and thus an improved F1 on dev (external).
Finally, we use the ensemble+ of (II and IV) from each of the folds 1-3, i.e., $|{\mathcal {M}}|=6$ models to obtain predictions on test. Table TABREF9 (FLC) shows that our submission is ranked at 3rd position.
Conclusion and Future Work
Our system (Team: MIC-CIS) explores different neural architectures (CNN, BERT and LSTM-CRF) with linguistic, layout and topical features to address the tasks of fine-grained propaganda detection. We have demonstrated gains in performance due to the features, ensemble schemes, multi-tasking and multi-granularity architectures. Compared to the other participating systems, our submissions are ranked 3rd and 4th in FLC and SLC tasks, respectively.
In future, we would like to enrich BERT models with linguistic, layout and topical features during their fine-tuning. Further, we would also be interested in understanding and analyzing the neural network learning, i.e., extracting salient fragments (or key-phrases) in the sentence that generate propaganda, similar to pankajgupta:2018LISA in order to promote explainable AI.","['For SLC task, the ""ltuorp"" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the ""newspeak"" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively).']",1541,qasper,en,,d25cf05e9fda9bdadb01c26a57122079766a565269fac749,"For SLC task, the ""ltuorp"" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the ""newspeak"" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively).",228
What baselines do they compare with?,"Introduction
There exists a class of language construction known as pun in natural language texts and utterances, where a certain word or other lexical items are used to exploit two or more separate meanings. It has been shown that understanding of puns is an important research question with various real-world applications, such as human-computer interaction BIBREF0 , BIBREF1 and machine translation BIBREF2 . Recently, many researchers show their interests in studying puns, like detecting pun sentences BIBREF3 , locating puns in the text BIBREF4 , interpreting pun sentences BIBREF5 and generating sentences containing puns BIBREF6 , BIBREF7 , BIBREF8 . A pun is a wordplay in which a certain word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect. Puns can be generally categorized into two groups, namely heterographic puns (where the pun and its latent target are phonologically similar) and homographic puns (where the two meanings of the pun reflect its two distinct senses) BIBREF9 . Consider the following two examples:
The first punning joke exploits the sound similarity between the word “propane"" and the latent target “profane"", which can be categorized into the group of heterographic puns. Another categorization of English puns is homographic pun, exemplified by the second instance leveraging distinct senses of the word “gut"".
Pun detection is the task of detecting whether there is a pun residing in the given text. The goal of pun location is to find the exact word appearing in the text that implies more than one meanings. Most previous work addresses such two tasks separately and develop separate systems BIBREF10 , BIBREF5 . Typically, a system for pun detection is built to make a binary prediction on whether a sentence contains a pun or not, where all instances (with or without puns) are taken into account during training. For the task of pun location, a separate system is used to make a single prediction as to which word in the given sentence in the text that trigger more than one semantic interpretations of the text, where the training data involves only sentences that contain a pun. Therefore, if one is interested in solving both problems at the same time, a pipeline approach that performs pun detection followed by pun location can be used.
Compared to the pipeline methods, joint learning has been shown effective BIBREF11 , BIBREF12 since it is able to reduce error propagation and allows information exchange between tasks which is potentially beneficial to all the tasks. In this work, we demonstrate that the detection and location of puns can be jointly addressed by a single model. The pun detection and location tasks can be combined as a sequence labeling problem, which allows us to jointly detect and locate a pun in a sentence by assigning each word a tag. Since each context contains a maximum of one pun BIBREF9 , we design a novel tagging scheme to capture this structural constraint. Statistics on the corpora also show that a pun tends to appear in the second half of a context. To capture such a structural property, we also incorporate word position knowledge into our structured prediction model. Experiments on the benchmark datasets show that detection and location tasks can reinforce each other, leading to new state-of-the-art performance on these two tasks. To the best of our knowledge, this is the first work that performs joint detection and location of English puns by using a sequence labeling approach.
Problem Definition
We first design a simple tagging scheme consisting of two tags { INLINEFORM0 }:
INLINEFORM0 tag means the current word is not a pun.
INLINEFORM0 tag means the current word is a pun.
If the tag sequence of a sentence contains a INLINEFORM0 tag, then the text contains a pun and the word corresponding to INLINEFORM1 is the pun.
The contexts have the characteristic that each context contains a maximum of one pun BIBREF9 . In other words, there exists only one pun if the given sentence is detected as the one containing a pun. Otherwise, there is no pun residing in the text. To capture this interesting property, we propose a new tagging scheme consisting of three tags, namely { INLINEFORM0 }.
INLINEFORM0 tag indicates that the current word appears before the pun in the given context.
INLINEFORM0 tag highlights the current word is a pun.
INLINEFORM0 tag indicates that the current word appears after the pun.
We empirically show that the INLINEFORM0 scheme can guarantee the context property that there exists a maximum of one pun residing in the text.
Given a context from the training set, we will be able to generate its corresponding gold tag sequence using a deterministic procedure. Under the two schemes, if a sentence does not contain any puns, all words will be tagged with INLINEFORM0 or INLINEFORM1 , respectively. Exemplified by the second sentence “Some diets cause a gut reaction,"" the pun is given as “gut."" Thus, under the INLINEFORM2 scheme, it should be tagged with INLINEFORM3 , while the words before it are assigned with the tag INLINEFORM4 and words after it are with INLINEFORM5 , as illustrated in Figure FIGREF8 . Likewise, the INLINEFORM6 scheme tags the word “gut"" with INLINEFORM7 , while other words are tagged with INLINEFORM8 . Therefore, we can combine the pun detection and location tasks into one problem which can be solved by the sequence labeling approach.
Model
Neural models have shown their effectiveness on sequence labeling tasks BIBREF13 , BIBREF14 , BIBREF15 . In this work, we adopt the bidirectional Long Short Term Memory (BiLSTM) BIBREF16 networks on top of the Conditional Random Fields BIBREF17 (CRF) architecture to make labeling decisions, which is one of the classical models for sequence labeling. Our model architecture is illustrated in Figure FIGREF8 with a running example. Given a context/sentence INLINEFORM0 where INLINEFORM1 is the length of the context, we generate the corresponding tag sequence INLINEFORM2 based on our designed tagging schemes and the original annotations for pun detection and location provided by the corpora. Our model is then trained on pairs of INLINEFORM3 .
Input. The contexts in the pun corpus hold the property that each pun contains exactly one content word, which can be either a noun, a verb, an adjective, or an adverb. To capture this characteristic, we consider lexical features at the character level. Similar to the work of BIBREF15 , the character embeddings are trained by the character-level LSTM networks on the unannotated input sequences. Nonlinear transformations are then applied to the character embeddings by highway networks BIBREF18 , which map the character-level features into different semantic spaces.
We also observe that a pun tends to appear at the end of a sentence. Specifically, based on the statistics, we found that sentences with a pun that locate at the second half of the text account for around 88% and 92% in homographic and heterographic datasets, respectively. We thus introduce a binary feature that indicates if a word is located at the first or the second half of an input sentence to capture such positional information. A binary indicator can be mapped to a vector representation using a randomly initialized embedding table BIBREF19 , BIBREF20 . In this work, we directly adopt the value of the binary indicator as part of the input.
The concatenation of the transformed character embeddings, the pre-trained word embeddings BIBREF21 , and the position indicators are taken as input of our model.
Tagging. The input is then fed into a BiLSTM network, which will be able to capture contextual information. For a training instance INLINEFORM0 , we suppose the output by the word-level BiLSTM is INLINEFORM1 . The CRF layer is adopted to capture label dependencies and make final tagging decisions at each position, which has been included in many state-of-the-art sequence labeling models BIBREF14 , BIBREF15 . The conditional probability is defined as:
where INLINEFORM0 is a set of all possible label sequences consisting of tags from INLINEFORM1 (or INLINEFORM2 ), INLINEFORM3 and INLINEFORM4 are weight and bias parameters corresponding to the label pair INLINEFORM5 . During training, we minimize the negative log-likelihood summed over all training instances:
where INLINEFORM0 refers to the INLINEFORM1 -th instance in the training set. During testing, we aim to find the optimal label sequence for a new input INLINEFORM2 :
This search process can be done efficiently using the Viterbi algorithm.
Datasets and Settings
We evaluate our model on two benchmark datasets BIBREF9 . The homographic dataset contains 2,250 contexts, 1,607 of which contain a pun. The heterographic dataset consists of 1,780 contexts with 1,271 containing a pun. We notice there is no standard splitting information provided for both datasets. Thus we apply 10-fold cross validation. To make direct comparisons with prior studies, following BIBREF4 , we accumulated the predictions for all ten folds and calculate the scores in the end.
For each fold, we randomly select 10% of the instances from the training set for development. Word embeddings are initialized with the 100-dimensional Glove BIBREF21 . The dimension of character embeddings is 30 and they are randomly initialized, which can be fine tuned during training. The pre-trained word embeddings are not updated during training. The dimensions of hidden vectors for both char-level and word-level LSTM units are set to 300. We adopt stochastic gradient descent (SGD) BIBREF26 with a learning rate of 0.015.
For the pun detection task, if the predicted tag sequence contains at least one INLINEFORM0 tag, we regard the output (i.e., the prediction of our pun detection model) for this task as true, otherwise false. For the pun location task, a predicted pun is regarded as correct if and only if it is labeled as the gold pun in the dataset. As to pun location, to make fair comparisons with prior studies, we only consider the instances that are labeled as the ones containing a pun. We report precision, recall and INLINEFORM1 score in Table TABREF11 . A list of prior works that did not employ joint learning are also shown in the first block of Table TABREF11 .
Results
We also implemented a baseline model based on conditional random fields (CRF), where features like POS tags produced by the Stanford POS tagger BIBREF27 , n-grams, label transitions, word suffixes and relative position to the end of the text are considered. We can see that our model with the INLINEFORM0 tagging scheme yields new state-of-the-art INLINEFORM1 scores on pun detection and competitive results on pun location, compared to baselines that do not adopt joint learning in the first block. For location on heterographic puns, our model's performance is slightly lower than the system of BIBREF25 , which is a rule-based locator. Compared to CRF, we can see that our model, either with the INLINEFORM2 or the INLINEFORM3 scheme, yields significantly higher recall on both detection and location tasks, while the precisions are relatively close. This demonstrates the effectiveness of BiLSTM, which learns the contextual features of given texts – such information appears to be helpful in recalling more puns.
Compared to the INLINEFORM0 scheme, the INLINEFORM1 tagging scheme is able to yield better performance on these two tasks. After studying outputs from these two approaches, we found that one leading source of error for the INLINEFORM2 approach is that there exist more than one words in a single instance that are assigned with the INLINEFORM3 tag. However, according to the description of pun in BIBREF9 , each context contains a maximum of one pun. Thus, such a useful structural constraint is not well captured by the simple approach based on the INLINEFORM4 tagging scheme. On the other hand, by applying the INLINEFORM5 tagging scheme, such a constraint is properly captured in the model. As a result, the results for such a approach are significantly better than the approach based on the INLINEFORM6 tagging scheme, as we can observe from the table. Under the same experimental setup, we also attempted to exclude word position features. Results are given by INLINEFORM7 - INLINEFORM8 . It is expected that the performance of pun location drops, since such position features are able to capture the interesting property that a pun tends to appear in the second half of a sentence. While such knowledge is helpful for the location task, interestingly, a model without position knowledge yields improved performance on the pun detection task. One possible reason is that detecting whether a sentence contains a pun is not concerned with such word position information.
Additionally, we conduct experiments over sentences containing a pun only, namely 1,607 and 1,271 instances from homographic and heterographic pun corpora separately. It can be regarded as a “pipeline” method where the classifier for pun detection is regarded as perfect. Following the prior work of BIBREF4 , we apply 10-fold cross validation. Since we are given that all input sentences contain a pun, we only report accumulated results on pun location, denoted as Pipeline in Table TABREF11 . Compared with our approaches, the performance of such an approach drops significantly. On the other hand, such a fact demonstrates that the two task, detection and location of puns, can reinforce each other. These figures demonstrate the effectiveness of our sequence labeling method to detect and locate English puns in a joint manner.
Error Analysis
We studied the outputs from our system and make some error analysis. We found the errors can be broadly categorized into several types, and we elaborate them here. 1) Low word coverage: since the corpora are relatively small, there exist many unseen words in the test set. Learning the representations of such unseen words is challenging, which affects the model's performance. Such errors contribute around 40% of the total errors made by our system. 2) Detection errors: we found many errors are due to the model's inability to make correct pun detection. Such inability harms both pun detection and pun location. Although our approach based on the INLINEFORM0 tagging scheme yields relatively higher scores on the detection task, we still found that 40% of the incorrectly predicted instances fall into this group. 3) Short sentences: we found it was challenging for our model to make correct predictions when the given text is short. Consider the example “Superglue! Tom rejoined,"" here the word rejoined is the corresponding pun. However, it would be challenging to figure out the pun with such limited contextual information.
Related Work
Most existing systems address pun detection and location separately. BIBREF22 applied word sense knowledge to conduct pun detection. BIBREF24 trained a bidirectional RNN classifier for detecting homographic puns. Next, a knowledge-based approach is adopted to find the exact pun. Such a system is not applicable to heterographic puns. BIBREF28 applied Google n-gram and word2vec to make decisions. The phonetic distance via the CMU Pronouncing Dictionary is computed to detect heterographic puns. BIBREF10 used the hidden Markov model and a cyclic dependency network with rich features to detect and locate puns. BIBREF23 used a supervised approach to pun detection and a weakly supervised approach to pun location based on the position within the context and part of speech features. BIBREF25 proposed a rule-based system for pun location that scores candidate words according to eleven simple heuristics. Two systems are developed to conduct detection and location separately in the system known as UWAV BIBREF3 . The pun detector combines predictions from three classifiers. The pun locator considers word2vec similarity between every pair of words in the context and position to pinpoint the pun. The state-of-the-art system for homographic pun location is a neural method BIBREF4 , where the word senses are incorporated into a bidirectional LSTM model. This method only supports the pun location task on homographic puns. Another line of research efforts related to this work is sequence labeling, such as POS tagging, chunking, word segmentation and NER. The neural methods have shown their effectiveness in this task, such as BiLSTM-CNN BIBREF13 , GRNN BIBREF29 , LSTM-CRF BIBREF30 , LSTM-CNN-CRF BIBREF14 , LM-LSTM-CRF BIBREF15 .
In this work, we combine pun detection and location tasks as a single sequence labeling problem. Inspired by the work of BIBREF15 , we also adopt a LSTM-CRF with character embeddings to make labeling decisions.
Conclusion
In this paper, we propose to perform pun detection and location tasks in a joint manner from a sequence labeling perspective. We observe that each text in our corpora contains a maximum of one pun. Hence, we design a novel tagging scheme to incorporate such a constraint. Such a scheme guarantees that there is a maximum of one word that will be tagged as a pun during the testing phase. We also found the interesting structural property such as the fact that most puns tend to appear at the second half of the sentences can be helpful for such a task, but was not explored in previous works. Furthermore, unlike many previous approaches, our approach, though simple, is generally applicable to both heterographic and homographic puns. Empirical results on the benchmark datasets prove the effectiveness of the proposed approach that the two tasks of pun detection and location can be addressed by a single model from a sequence labeling perspective.
Future research includes the investigations on how to make use of richer semantic and linguistic information for detection and location of puns. Research on puns for other languages such as Chinese is still under-explored, which could also be an interesting direction for our future studies.
Acknowledgments
We would like to thank the three anonymous reviewers for their thoughtful and constructive comments. This work is supported by Singapore Ministry of Education Academic Research Fund (AcRF) Tier 2 Project MOE2017-T2-1-156, and is partially supported by SUTD project PIE-SGP-AI-2018-01.","['They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF.']",2991,qasper,en,,2561836b391d69be52ba4d97583238407f2fa80afef9a15b,"They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF.",223
Why masking words in the decoder is helpful?,"Introduction
Text summarization generates summaries from input documents while keeping salient information. It is an important task and can be applied to several real-world applications. Many methods have been proposed to solve the text summarization problem BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . There are two main text summarization techniques: extractive and abstractive. Extractive summarization generates summary by selecting salient sentences or phrases from the source text, while abstractive methods paraphrase and restructure sentences to compose the summary. We focus on abstractive summarization in this work as it is more flexible and thus can generate more diverse summaries.
Recently, many abstractive approaches are introduced based on neural sequence-to-sequence framework BIBREF4 , BIBREF0 , BIBREF3 , BIBREF5 . Based on the sequence-to-sequence model with copy mechanism BIBREF6 , BIBREF0 incorporates a coverage vector to track and control attention scores on source text. BIBREF4 introduce intra-temporal attention processes in the encoder and decoder to address the repetition and incoherent problem.
There are two issues in previous abstractive methods: 1) these methods use left-context-only decoder, thus do not have complete context when predicting each word. 2) they do not utilize the pre-trained contextualized language models on the decoder side, so it is more difficult for the decoder to learn summary representations, context interactions and language modeling together.
Recently, BERT has been successfully used in various natural language processing tasks, such as textual entailment, name entity recognition and machine reading comprehensions. In this paper, we present a novel natural language generation model based on pre-trained language models (we use BERT in this work). As far as we know, this is the first work to extend BERT to the sequence generation task. To address the above issues of previous abstractive methods, in our model, we design a two-stage decoding process to make good use of BERT's context modeling ability. On the first stage, we generate the summary using a left-context-only-decoder. On the second stage, we mask each word of the summary and predict the refined word one-by-one using a refine decoder. To further improve the naturalness of the generated sequence, we cooperate reinforcement objective with the refine decoder.
The main contributions of this work are:
1. We propose a natural language generation model based on BERT, making good use of the pre-trained language model in the encoder and decoder process, and the model can be trained end-to-end without handcrafted features.
2. We design a two-stage decoder process. In this architecture, our model can generate each word of the summary considering both sides' context information.
3. We conduct experiments on the benchmark datasets CNN/Daily Mail and New York Times. Our model achieves a 33.33 average of ROUGE-1, ROUGE-2 and ROUGE-L on the CNN/Daily Mail, which is state-of-the-art. On the New York Times dataset, our model achieves about 5.6% relative improvement over ROUGE-1.
Text Summarization
In this paper, we focus on single-document multi-sentence summarization and propose a supervised abstractive model based on the neural attentive sequence-to-sequence framework which consists of two parts: a neural network for the encoder and another network for the decoder. The encoder encodes the input sequence to intermediate representation and the decoder predicts one word at a time step given the input sequence representation vector and previous decoded output. The goal of the model is to maximize the probability of generating the correct target sequences. In the encoding and generation process, the attention mechanism is used to concentrate on the most important positions of text. The learning objective of most sequence-to-sequence models is to minimize the negative log likelihood of the generated sequence as following equation shows, where $y^*_i$ is the i-th ground-truth summary token.
$$Loss = - \log \sum _{t=1}^N P(y_t^*|y_{<t}^*, X)$$   (Eq. 3)
However, with this objective, traditional sequence generation models consider only one direction context in the decoding process, which could cause performance degradation since complete context of one token contains preceding and following tokens, thus feeding only preceded decoded words to the decoder so that the model may generate unnatural sequences. For example, attentive sequence-to-sequence models often generate sequences with repeated phrases which harm the naturalness. Some previous works mitigate this problem by improving the attention calculation process, but in this paper we show that feeding bi-directional context instead of left-only-context can better alleviate this problem.
Text summarization models are usually classified to abstractive and extractive ones. Recently, extractive models like DeepChannel BIBREF8 , rnn-ext+RL BIBREF9 and NeuSUM BIBREF2 achieve higher performances using well-designed structures. For example, DeepChannel propose a salience estimation network and iteratively extract salient sentences. BIBREF16 train a sentence compression model to teach another latent variable extractive model.
Also, several recent works focus on improving abstractive methods. BIBREF3 design a content selector to over-determine phrases in a source document that should be part of the summary. BIBREF11 introduce inconsistency loss to force words in less attended sentences(which determined by extractive model) to have lower generation probabilities. BIBREF5 extend seq2seq model with an information selection network to generate more informative summaries.
Bi-Directional Pre-Trained Context Encoders
Recently, context encoders such as ELMo, GPT, and BERT have been widely used in many NLP tasks. These models are pre-trained on a huge unlabeled corpus and can generate better contextualized token embeddings, thus the approaches built on top of them can achieve better performance.
Since our method is based on BERT, we illustrate the process briefly here. BERT consists of several layers. In each layer there is first a multi-head self-attention sub-layer and then a linear affine sub-layer with the residual connection. In each self-attention sub-layer the attention scores $e_{ij}$ are first calculated as Eq. ( 5 ) () shows, in which $d_e$ is output dimension, and $W^Q, W^K, W^V$ are parameter matrices.
$$&a_{ij} = \cfrac{(h_iW_Q)(h_jW_K)^T}{\sqrt{d_e}}  \\ &e_{ij} = \cfrac{\exp {e_{ij}}}{\sum _{k=1}^N\exp {e_{ik}}} $$   (Eq. 5)
Then the output is calculated as Eq. ( 6 ) shows, which is the weighted sum of previous outputs $h$ added by previous output $h_i$ . The last layer outputs is context encoding of input sequence.
$$o_i = h_i + \sum _{j=1}^{N} e_{ij}(h_j W_V) $$   (Eq. 6)
Despite the wide usage and huge success, there is also a mismatch problem between these pre-trained context encoders and sequence-to-sequence models. The issue is that while using a pre-trained context encoder like GPT or BERT, they model token-level representations by conditioning on both direction context. During pre-training, they are fed with complete sequences. However, with a left-context-only decoder, these pre-trained language models will suffer from incomplete and inconsistent context and thus cannot generate good enough context-aware word representations, especially during the inference process.
Model
In this section, we describe the structure of our model, which learns to generate an abstractive multi-sentence summary from a given source document.
Based on the sequence-to-sequence framework built on top of BERT, we first design a refine decoder at word-level to tackle the two problems described in the above section. We also introduce a discrete objective for the refine decoders to reduce the exposure bias problem. The overall structure of our model is illustrated in Figure 1 .
Problem Formulation
We denote the input document as $X = \lbrace x_1, \ldots , x_m\rbrace $ where $x_i \in \mathcal {X}$ represents one source token. The corresponding summary is denoted as $Y = \lbrace y_1, \ldots , y_L\rbrace $ , $L$ represents the summary length.
Given input document $X$ , we first predict the summary draft by a left-context-only decoder, and then using the generated summary draft we can condition on both context sides and refine the content of the summary. The draft will guide and constrain the refine process of summary.
Summary Draft Generation
The summary draft is based on the sequence-to-sequence model. On the encoder side the input document $X$ is encoded into representation vectors $H = \lbrace h_1, \ldots , h_m\rbrace $ , and then fed to the decoder to generate the summary draft $A = \lbrace a_1, \ldots , a_{|a|}\rbrace $ .
We simply use BERT as the encoder. It first maps the input sequence to word embeddings and then computes document embeddings as the encoder's output, denoted by following equation.
$$H = BERT(x_1, \ldots , x_m)$$   (Eq. 10)
In the draft decoder, we first introduce BERT's word embedding matrix to map the previous summary draft outputs $\lbrace y_1, \ldots , y_{t-1}\rbrace $ into embeddings vectors $\lbrace q_1, \ldots , q_{t-1}\rbrace $ at t-th time step. Note that as the input sequence of the decoder is not complete, we do not use the BERT network to predict the context vectors here.
Then we introduce an $N$ layer Transformer decoder to learn the conditional probability $P(A|H)$ . Transformer's decoder-encoder multi-head attention helps the decoder learn soft alignments between summary and source document. At the t-th time step, the draft decoder predicts output probability conditioned on previous outputs and encoder hidden representations as Eq. ( 13 ) shows, in which $q_{<t} = \lbrace q_1, \ldots , q_{t-1}\rbrace $ . Each generated sequence will be truncated in the first position of a special token '[PAD]'.
$$&P^{vocab}_t(w) = f_{dec}(q_{<t}, H)  \\ &L_{dec} = \sum _{i=1}^{|a|} -\log P(a_i = y_i^*|a_{< i}, H) $$   (Eq. 13)
As Eq. () shows, the decoder's learning objective is to minimize negative likelihood of conditional probability, in which $y_i^*$ is the i-th ground truth word of summary.
However a decoder with this structure is not sufficient enough: if we use the BERT network in this decoder, then during training and inference, in-complete context(part of sentence) is fed into the BERT module, and although we can fine-tune BERT's parameters, the input distribution is quite different from the pre-train process, and thus harms the quality of generated context representations.
If we just use the embedding matrix here, it will be more difficult for the decoder with fresh parameters to learn to model representations as well as vocabulary probabilities, from a relative small corpus compared to BERT's huge pre-training corpus. In a word, the decoder cannot utilize BERT's ability to generate high quality context vectors, which will also harm performance.
This issue exists when using any other contextualized word representations, so we design a refine process to mitigate it in our approach which will be described in the next sub-section.
As some summary tokens are out-of-vocabulary words and occurs in input document, we incorporate copy mechanism BIBREF6 based on the Transformer decoder, we will describe it briefly.
At decoder time step $t$ , we first calculate the attention probability distribution over source document $X$ using the bi-linear dot product of the last layer decoder output of Transformer $o_t$ and the encoder output $h_j$ , as Eq. ( 15 ) () shows.
$$u_t^j =& o_t W_c h_j \\ \alpha _t^j =& \cfrac{\exp {u_t^j}}{\sum _{k=1}^N\exp {u_t^k}} $$   (Eq. 15)
We then calculate copying gate $g_t\in [0, 1]$ , which makes a soft choice between selecting from source and generating from vocabulary, $W_c, W_g, b_g$ are parameters:
$$g_t = sigmoid(W_g \cdot [o_t, h] + b_g) $$   (Eq. 16)
Using $g_t$ we calculate the weighted sum of copy probability and generation probability to get the final predicted probability of extended vocabulary $\mathcal {V} + \mathcal {X}$ , where $\mathcal {X}$ is the set of out of vocabulary words from the source document. The final probability is calculated as follow:
$$P_t(w) = (1-g_t)P_t^{vocab}(w) + g_t\sum _{i:w_i=w} \alpha _t^i$$   (Eq. 17)
Summary Refine Process
The main reason to introduce the refine process is to enhance the decoder using BERT's contextualized representations, so we do not modify the encoder and reuse it during this process.
On the decoder side, we propose a new word-level refine decoder. The refine decoder receives a generated summary draft as input, and outputs a refined summary. It first masks each word in the summary draft one by one, then feeds the draft to BERT to generate context vectors. Finally it predicts a refined summary word using an $N$ layer Transformer decoder which is the same as the draft decoder. At t-th time step the n-th word of input summary is masked, and the decoder predicts the n-th refined word given other words of the summary.
The learning objective of this process is shown in Eq. ( 19 ), $y_i$ is the i-th summary word and $y_{i}^*$ for the ground-truth summary word, and $a_{\ne i} = \lbrace a_1, \ldots , a_{i-1}, a_{i+1}, \ldots , a_{|y|}\rbrace $ .
$$L_{refine} = \sum _{i=1}^{|y|} -\log P(y_i = y_i^*|a_{\ne i}, H) $$   (Eq. 19)
From the view of BERT or other contextualized embeddings, the refine decoding process provides a more complete input sequence which is consistent with their pre-training processes. Intuitively, this process works as follows: first the draft decoder writes a summary draft based on a document, and then the refine decoder edits the draft. It concentrates on one word at a time, based on the source document as well as other words.
We design the word-level refine decoder because this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences.
The parameters are shared between the draft decoder and refine decoder, as we find that using individual parameters the model's performance degrades a lot. The reason may be that we use teach-forcing during training, and thus the word-level refine decoder learns to predict words given all the other ground-truth words of summary. This objective is similar to the language model's pre-train objective, and is probably not enough for the decoder to learn to generate refined summaries. So in our model all decoders share the same parameters.
Researchers usually use ROUGE as the evaluation metric for summarization, however during sequence-to-sequence model training, the objective is to maximize the log likelihood of generated sequences. This mis-match harms the model's performance, so we add a discrete objective to the model, and optimize it by introducing the policy gradient method. For example, the discrete objective for the summary draft process is as Eq. ( 21 ) shows, where $a^s$ is the draft summary sampled from predicted distribution, and $R(a^s)$ is the reward score compared with the ground-truth summary, we use ROUGE-L in our experiment. To balance between optimizing the discrete objective and generating readable sequences, we mix the discrete objective with maximum-likelihood objective. As Eq. () shows, minimizing $\hat{L}_{dec}$ is the final objective for the draft process, note here $L_{dec}$ is $-logP(a|x)$ . In the refine process we introduce similar objectives.
$$L^{rl}_{dec} = R(a^s)\cdot [-\log (P(a^s|x))]  \\ \hat{L}_{dec} = \gamma * L^{rl}_{dec} + (1 - \gamma ) * L_{dec} $$   (Eq. 21)
Learning and Inference
During model training, the objective of our model is sum of the two processes, jointly trained using ""teacher-forcing"" algorithm. During training we feed the ground-truth summary to each decoder and minimize the objective.
$$L_{model} = \hat{L}_{dec} + \hat{L}_{refine}$$   (Eq. 23)
At test time, each time step we choose the predicted word by $\hat{y} = argmax_{y^{\prime }} P(y^{\prime }|x)$ , use beam search to generate the draft summaries, and use greedy search to generate the refined summaries.
Settings
In this work, all of our models are built on $BERT_{BASE}$ , although another larger pre-trained model with better performance ( $BERT_{LARGE}$ ) has published but it costs too much time and GPU memory. We use WordPiece embeddings with a 30,000 vocabulary which is the same as BERT. We set the layer of transformer decoders to 12(8 on NYT50), and set the attention heads number to 12(8 on NYT50), set fully-connected sub-layer hidden size to 3072. We train the model using an Adam optimizer with learning rate of $3e-4$ , $\beta _1=0.9$ , $\beta _2=0.999$ and $\epsilon =10^{-9}$ and use a dynamic learning rate during the training process. For regularization, we use dropout BIBREF13 and label smoothing BIBREF14 in our models and set the dropout rate to 0.15, and the label smoothing value to 0.1. We set the RL objective factor $\gamma $ to 0.99.
During training, we set the batch size to 36, and train for 4 epochs(8 epochs for NYT50 since it has many fewer training samples), after training the best model are selected from last 10 models based on development set performance. Due to GPU memory limit, we use gradient accumulation, set accumulate step to 12 and feed 3 samples at each step. We use beam size 4 and length penalty of 1.0 to generate logical form sequences.
We filter repeated tri-grams in beam-search process by setting word probability to zero if it will generate an tri-gram which exists in the existing summary. It is a nice method to avoid phrase repetition since the two datasets seldom contains repeated tri-grams in one summary. We also fine tune the generated sequences using another two simple rules. When there are multi summary sentences with exactly the same content, we keep the first one and remove the other sentences; we also remove sentences with less than 3 words from the result.
To evaluate the performance of our model, we conduct experiments on CNN/Daily Mail dataset, which is a large collection of news articles and modified for summarization. Following BIBREF0 we choose the non-anonymized version of the dataset, which consists of more than 280,000 training samples and 11490 test set samples.
We also conduct experiments on the New York Times(NYT) dataset which also consists of many news articles. The original dataset can be applied here. In our experiment, we follow the dataset splits and other pre-process settings of BIBREF15 . We first filter all samples without a full article text or abstract and then remove all samples with summaries shorter than 50 words. Then we choose the test set based on the date of publication(all examples published after January 1, 2007). The final dataset contains 22,000 training samples and 3,452 test samples and is called NYT50 since all summaries are longer than 50 words.
We tokenize all sequences of the two datasets using the WordPiece tokenizer. After tokenizing, the average article length and summary length of CNN/Daily Mail are 691 and 51, and NYT50's average article length and summary length are 1152 and 75. We truncate the article length to 512, and the summary length to 100 in our experiment(max summary length is set to 150 on NYT50 as its average golden summary length is longer).
On CNN/Daily Mail dataset, we report the full-length F-1 score of the ROUGE-1, ROUGE-2 and ROUGE-L metrics, calculated using PyRouge package and the Porter stemmer option. On NYT50, following BIBREF4 we evaluate limited length ROUGE recall score(limit the generated summary length to the ground truth length). We split NYT50 summaries into sentences by semicolons to calculate the ROUGE scores.
Results and Analysis
Table 1 shows the results on CNN/Daily Mail dataset, we compare the performance of many recent approaches with our model. We classify them to two groups based on whether they are extractive or abstractive models. As the last line of the table shows, the ROUGE-1 and ROUGE-2 score of our full model is comparable with DCA, and outperforms on ROUGE-L. Also, compared to extractive models NeuSUM and MASK- $LM^{global}$ , we achieve slight higher ROUGE-1. Except the four scores, our model outperforms these models on all the other scores, and since we have 95% confidence interval of at most $\pm $ 0.20, these improvements are statistically significant.
As the last four lines of Table 1 show, we conduct an ablation study on our model variants to analyze the importance of each component. We use three ablation models for the experiments. One-Stage: A sequence-to-sequence model with copy mechanism based on BERT; Two-Stage: Adding the word-refine decoder to the One-Stage model; Two-Stage + RL: Full model with refine process cooperated with RL objective.
First, we compare the Two-Stage+RL model with Two-Stage ablation, we observe that the full model outperforms by 0.30 on average ROUGE, suggesting that the reinforcement objective helps the model effectively. Then we analyze the effect of refine process by removing word-level refine from the Two-Stage model, we observe that without the refine process the average ROUGE score drops by 1.69. The ablation study shows that each module is necessary for our full model, and the improvements are statistically significant on all metrics.
To evaluate the impact of summary length on model performance, we compare the average rouge score improvements of our model with different length of ground-truth summaries. As the above sub-figure of Figure 2 shows, compared to Pointer-Generator with Coverage, on length interval 40-80(occupies about 70% of test set) the improvements of our model are higher than shorter samples, confirms that with better context representations, in longer documents our model can achieve higher performance.
As the below sub-figure of Figure 2 shows, compared to extractive baseline: Lead-3 BIBREF0 , the advantage of our model will fall when golden summary length is greater than 80. This probably because that we truncate the long documents and golden summaries and cannot get full information, it could also because that the training data in these intervals is too few to train an abstractive model, so simple extractive method will not fall too far behind.
Additional Results on NYT50
Table 2 shows experiments on the NYT50 corpus. Since the short summary samples are filtered, NYT50 has average longer summaries than CNN/Daily Mail. So the model needs to catch long-term dependency of the sequences to generate good summaries.
The first two lines of Table 2 show results of the two baselines introduced by BIBREF15 : these baselines select first n sentences, or select the first k words from the original document. Also we compare performance of our model with two recent models, we see 2.39 ROUGE-1 improvements compared to the ML+RL with intra-attn approach(previous SOTA) carries over to this dataset, which is a large margin. On ROUGE-2, our model also get an improvement of 0.51. The experiment proves that our approach can outperform competitive methods on different data distributions.
Pre-trained language models
Pre-trained word vectors BIBREF17 , BIBREF18 , BIBREF19 have been widely used in many NLP tasks. More recently, pre-trained language models (ELMo, GPT and BERT), have also achieved great success on several NLP problems such as textual entailment, semantic similarity, reading comprehension, and question answering BIBREF20 , BIBREF21 , BIBREF22 .
Some recent works also focus on leveraging pre-trained language models in summarization. BIBREF23 pretrain a language model and use it as the sentiment analyser when generating reviews of goods. BIBREF24 train a language model on golden summaries, and then use it on the decoder side to incorporate prior knowledge.
In this work, we use BERT(which is a pre-trained language model using large scale unlabeled data) on the encoder and decoder of a seq2seq model, and by designing a two stage decoding structure we build a competitive model for abstractive text summarization.
Conclusion and Future Work
In this work, we propose a two-stage model based on sequence-to-sequence paradigm. Our model utilize BERT on both encoder and decoder sides, and introduce reinforce objective in learning process. We evaluate our model on two benchmark datasets CNN/Daily Mail and New York Times, the experimental results show that compared to previous systems our approach effectively improves performance.
Although our experiments are conducted on summarization task, our model can be used in most natural language generation tasks, such as machine translation, question generation and paraphrasing. The refine decoder and mixed objective can also be applied on other sequence generation tasks, and we will investigate on them in future work.","[""ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences.""]",3919,qasper,en,,ea4db579b5224fe0fa413ee436a5f4f56777dfa3e25d99d7,"ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences.",201
How big is the difference in performance between proposed model and baselines?,"Introduction
Attempts toward constructing human-like dialogue agents have met significant difficulties, such as maintaining conversation consistency BIBREF0. This is largely due to inabilities of dialogue agents to engage the user emotionally because of an inconsistent personality BIBREF1. Many agents use personality models that attempt to map personality attributes into lower dimensional spaces (e.g. the Big Five BIBREF2). However, these represent human personality at a very high-level and lack depth. They prohibit the ability to link specific and detailed personality traits to characters, and to construct large datasets where dialogue is traceable back to these traits.
For this reason, we propose Human Level Attributes (HLAs), which we define as characteristics of fictional characters representative of their profile and identity. We base HLAs on tropes collected from TV Tropes BIBREF3, which are determined by viewers' impressions of the characters. See Figure FIGREF1 for an example. Based on the hypothesis that profile and identity contribute effectively to language style BIBREF4, we propose that modeling conversation with HLAs is a means for constructing a dialogue agent with stable human-like characteristics. By collecting dialogues from a variety of characters along with this HLA information, we present a novel labelling of this dialogue data where it is traceable back to both its context and associated human-like qualities.
We also propose a system called ALOHA (Artificial Learning On Human Attributes) as a novel method of incorporating HLAs into dialogue agents. ALOHA maps characters to a latent space based on their HLAs, determines which are most similar in profile and identity, and recovers language styles of specific characters. We test the performance of ALOHA in character language style recovery against four baselines, demonstrating outperformance and system stability. We also run a human evaluation supporting our results. Our major contributions are: (1) We propose HLAs as personality aspects of fictional characters from the audience's perspective based on tropes; (2) We provide a large dialogue dataset traceable back to both its context and associated human-like attributes; (3) We propose a system called ALOHA that is able to recommend responses linked to specific characters. We demonstrate that ALOHA, combined with the proposed dataset, outperforms baselines. ALOHA also shows stable performance regardless of the character's identity, genre of the show, and context of the dialogue. We plan to release all of ALOHA's data and code.
Related Work
Task completion chatbots (TCC), or task-oriented chatbots, are dialogue agents used to fulfill specific purposes, such as helping customers book airline tickets, or a government inquiry system. Examples include the AIML based chatbot BIBREF5 and DIVA Framework BIBREF6. While TCC are low cost, easily configurable, and readily available, they are restricted to working well for particular domains and tasks.
Open-domain chatbots are more generic dialogue systems. An example is the Poly-encoder from BIBREF7 humeau2019real. It outperforms the Bi-encoder BIBREF8, BIBREF9 and matches the performance of the Cross-encoder BIBREF10, BIBREF11 while maintaining reasonable computation time. It performs strongly on downstream language understanding tasks involving pairwise comparisons, and demonstrates state-of-the-art results on the ConvAI2 challenge BIBREF12. Feed Yourself BIBREF13 is an open-domain dialogue agent with a self-feeding model. When the conversation goes well, the dialogue becomes part of the training data, and when the conversation does not, the agent asks for feedback. Lastly, Kvmemnn BIBREF14 is a key-value memory network with a knowledge base that uses a key-value retrieval mechanism to train over multiple domains simultaneously. We use all three of these models as baselines for comparison. While these can handle a greater variety of tasks, they do not respond with text that aligns with particular human-like characteristics.
BIBREF15 li2016persona defines persona (composite of elements of identity) as a possible solution at the word level, using backpropagation to align responses via word embeddings. BIBREF16 bartl2017retrieval uses sentence embeddings and a retrieval model to achieve higher accuracy on dialogue context. BIBREF17 liu2019emotion applies emotion states of sentences as encodings to select appropriate responses. BIBREF18 pichl2018alquist uses knowledge aggregation and hierarchy of sub-dialogues for high user engagement. However, these agents represent personality at a high-level and lack detailed human qualities. LIGHT BIBREF19 models adventure game characters' dialogues, actions, and emotions. It focuses on the agent identities (e.g. thief, king, servant) which includes limited information on realistic human behaviours. BIBREF20 pasunuru2018game models online soccer games as dynamic visual context. BIBREF21 wang2016learning models user dialogue to complete tasks involving certain configurations of blocks. BIBREF22 antol2015vqa models open-ended questions, but is limited to visual contexts. BIBREF23 bordes2016learning tracks user dialogues but is goal-oriented. BIBREF24 ilinykh2019meetup tracks players' dialogues and movements in a visual environment, and is grounded on navigation tasks. All of these perform well in their respective fictional environments, but are not a strong representation of human dialogue in reality.
Methodology ::: Human Level Attributes (HLA)
We collect HLA data from TV Tropes BIBREF3, a knowledge-based website dedicated to pop culture, containing information on a plethora of characters from a variety of sources. Similar to Wikipedia, its content is provided and edited collaboratively by a massive user-base. These attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics. We believe that TV Tropes is better for our purpose of fictional character modeling than data sources used in works such as BIBREF25 shuster2019engaging because TV Tropes' content providers are rewarded for correctly providing content through community acknowledgement.
TV Tropes defines tropes as attributes of storytelling that the audience recognizes and understands. We use tropes as HLAs to calculate correlations with specific target characters. We collect data from numerous characters from a variety of TV shows, movies, and anime. We filter and keep characters with at least five HLA, as those with fewer are not complex enough to be correctly modeled due to reasons such as lack of data. We end up eliminating 5.86% of total characters, and end up with 45,821 characters and 12,815 unique HLA, resulting in 945,519 total character-HLA pairs. Each collected character has 20.64 HLAs on average. See Figure FIGREF1 for an example character and their HLAs.
Methodology ::: Overall Task
Our task is the following, where $t$ denotes “target"":
Given a target character $c_t$ with HLA set $H_t$, recover the language style of $c_t$ without any dialogue of $c_t$ provided.
For example, if Sheldon Cooper from The Big Bang Theory is $c_t$, then $H_t$ is the set of HLA on the left side of Figure FIGREF1.
We define the language style of a character as its diction, tone, and speech patterns. It is a character specific language model refined from a general language model. We must learn to recover the language style of $c_t$ without its dialogue as our objective is to imitate human-like qualities, and hence the model must understand the language styles of characters based on their traits. If we feed $c_t$'s dialogue during training, the model will likely not effectively learn to imitate language styles based on HLAs, but based on the correlation between text in the training and testing dialogues BIBREF26.
We define character space as the character representations within the HLA latent space (see Figure FIGREF4), and the set $C = \lbrace c_1,c_2,...,c_n\rbrace $ as the set of all characters. We define Observation (OBS) as the input that is fed into any dialogue model. This can be a single or multiple lines of dialogue along with other information. The goal of the dialogue model is to find the best response to this OBS.
Methodology ::: ALOHA
We propose a three-component system called ALOHA to solve the task (see Figure FIGREF6). The first component, Character Space Module (CSM), generates the character space and calculates confidence levels using singular value decomposition BIBREF27 between characters $c_j$ (for $j = 1$ to $n$ where $j \ne t$) and $c_t$ in the HLA-oriented neighborhood.
The second component, Character Community Module (CCM), ranks the similarity between our target character $c_t$ with any other character $c_j$ by the relative distance between them in the character space.
The third component, Language Style Recovery Module (LSRM), recovers the language style of $c_t$ without its dialogue by training the BERT bi-ranker model BIBREF28 to rank responses from similar characters. Our results demonstrate higher accuracy at retrieving the ground truth response from $c_t$. Our system is also able to pick responses which are correct both in context as well as character space.
Hence, the overall process for ALOHA works as follows. First, given a set of characters, determine the character space using the CSM. Next, given a specific target character, determine the positive community and negative set of associated characters using the CCM. Lastly, using the positive community and negative set determined above along with a dialogue dataset, recover the language style of the target.
Methodology ::: Character Space Module (CSM)
CSM learns how to rank characters. We can measure the interdependencies between the HLA variables BIBREF29 and rank the similarity between the TV show characters. We use implicit feedback instead of neighborhood models (e.g. cosine similarity) because it can compute latent factors to transform both characters and HLAs into the same latent space, making them directly comparable.
We define a matrix $P$ that contains binary values, with $P_{u,i} = 1$ if character $u$ has HLA $i$ in our dataset, and $P_{u,i} = 0$ otherwise. We define a constant $\alpha $ that measures our confidence in observing various character-HLA pairs as positive. $\alpha $ controls how much the model penalizes the error if the ground truth is $P_{u,i} = 1$. If $P_{u,i} = 1$ and the model guesses incorrectly, we penalize by $\alpha $ times the loss. But if $P_{u,i} = 0$ and the model guesses a value greater than 0, we do not penalize as $\alpha $ has no impact. This is because $P_{u,i} = 0$ can either represent a true negative or be due to a lack of data, and hence is less reliable for penalization. See Equation DISPLAY_FORM8. We find that using $\alpha =20$ provides decent results.
We further define two dense vectors $X_u$ and $Y_i$. We call $X_u$ the “latent factors for character $u$"", and $Y_i$ the “latent factors for HLA $i$"". The dot product of these two vectors produces a value ($X_u^TY_i$) that approximates $P_{u,i}$ (see Figure FIGREF9). This is analogous to factoring the matrix $P$ into two separate matrices, where one contains the latent factors for characters, and the other contains the latent factors for HLAs. We find that $X_u$ and $Y_i$ being 36-dimensional produces decent results. To bring $X_u^TY_i$ as close as possible to $P_{u,i}$, we minimize the following loss function using the Conjugate Gradient Method BIBREF30:
The first term penalizes differences between the model's prediction ($X_u^TY_i$) and the actual value ($P_{u,i}$). The second term is an L2 regularizer to reduce overfitting. We find $\lambda = 100$ provides decent results for 500 iterations (see Section SECREF26).
Methodology ::: Character Community Module (CCM)
CCM aims to divide characters (other than $c_t$) into a positive community and a negative set. We define this positive community as characters that are densely connected internally to $c_t$ within the character space, and the negative set as the remaining characters. We can then sample dialogue from characters in the negative set to act as the distractors (essentially negative samples) during LSRM training.
As community finding is an ill-defined problem BIBREF31, we choose to treat CCM as a simple undirected, unweighted graph. We use the values learned in the CSM for $X_u$ and $Y_i$ for various values of $u$ and $i$, which approximate the matrix $P$. Similar to BIBREF29 hu2008collaborative, we can calculate the correlation between two rows (and hence two characters).
We then employ a two-level connection representation by ranking all characters against each other in terms of their correlation with $c_t$. For the first level, the set $S^{FL}$ is the top 10% (4582) most highly correlated characters with $c_t$ out of the 45,820 total other characters that we have HLA data for. For the second level, for each character $s_i$ in $S^{FL}$, we determine the 30 most heavily correlated characters with $s_i$ as set $S^{SL}_i$. The positive set $S^{pos}$ are the characters which are present in at least 10 $S^{SL}_i$ sets. We call this value 10 the minimum frequency. All other characters in our dialogue dataset make up the negative set $S^{neg}$. These act as our positive community and negative set, respectively. See Algorithm 1 in Appendix A for details, and Figure FIGREF11 for an example.
Methodology ::: Language Style Recovery Module (LSRM)
LSRM creates a dialogue agent that aligns with observed characteristics of human characters by using the positive character community and negative set determined in the CCM, along with a dialogue dataset, to recover the language style of $c_t$ without its dialogue. We use the BERT bi-ranker model from the Facebook ParlAI framework BIBREF32, where the model has the ability to retrieve the best response out of 20 candidate responses. BIBREF12, BIBREF19, BIBREF0 choose 20 candidate responses, and for comparison purposes, we do the same.
Methodology ::: Language Style Recovery Module (LSRM) ::: BERT
BIBREF28 is first trained on massive amounts of unlabeled text data. It jointly conditions on text on both the left and right, which provides a deep bi-directional representation of sentence inference. BERT is proven to perform well on a wide range of tasks by simply fine-tuning on one additional layer. We are interested in its ability to predict the next sentence, called Next Sentence Prediction. We perform further fine-tuning on BERT for our target character language style retrieval task to produce our LSRM model by optimizing both the encoding layers and the additional layer. We use BERT to create vector representations for the OBS and for each candidate response. By passing the first output of BERT's 12 layers through an additional linear layer, these representations can be obtained as 768-dimensional sentence-level embeddings. It uses the dot product between these embeddings to score candidate responses and is trained using the ranking loss.
Methodology ::: Language Style Recovery Module (LSRM) ::: Candidate response selection
is similar to the procedure from previous work done on grounded dialogue agents BIBREF0, BIBREF19. Along with the ground truth response, we randomly sample 19 distractor responses from other characters from a uniform distribution of characters, and call this process uniform character sampling. Based on our observations, this random sampling provides multiple context correct responses. Hence, the BERT bi-ranker model is trained by learning to choose context correct responses, and the model learns to recover a domain-general language model that includes training on every character. This results in a Uniform Model that can select context correct responses, but not responses corresponding to a target character with specific HLAs.
We then fine-tune on the above model to produce our LSRM model with a modification: we randomly sample the 19 distractor responses from only the negative character set instead. We choose the responses that have similar grammatical structures and semantics to the ground truth response, and call this process negative character sampling. This guides the model away from the language style of these negative characters to improve performance at retrieving responses for target characters with specific HLAs. Our results demonstrate higher accuracy at retrieving the correct response from character $c_t$, which is the ground truth.
Experiment ::: Dialogue Dataset
To train the Uniform Model and LSRM, we collect dialogues from 327 major characters (a subset of the 45,821 characters we have HLA data for) in 38 TV shows from various existing sources of clean data on the internet, resulting in a total of 1,042,647 dialogue lines. We use a setup similar to the Persona-Chat dataset BIBREF0 and Cornell Movie-Dialogs Corpus BIBREF33, as our collected dialogues are also paired in terms of valid conversations. See Figure FIGREF1 for an example of these dialogue lines.
Experiment ::: HLA Observation Guidance (HLA-OG)
We define HLA Observation Guidance (HLA-OG) as explicitly passing a small subset of the most important HLAs of a given character as part of the OBS rather than just an initial line of dialogue. This is adapted from the process used in BIBREF0 zhang2018personalizing and BIBREF10 wolf2019transfertransfo which we call Persona Profiling. Specifically, we pass four HLAs that are randomly drawn from the top 40 most important HLAs of the character. We use HLA-OG during training of the LSRM and testing of all models. This is because the baselines (see Section SECREF31) already follow a similar process (Persona Profiling) for training. For the Uniform Model, we train using Next Sentence Prediction (see Section SECREF12). For testing, HLA-OG is necessary as it provides information about which HLAs the models should attempt to imitate in their response selection. Just passing an initial line of dialogue replicates a typical dialogue response task without HLAs. See Table TABREF19. Further, we also test our LSRM by explicitly passing four HLAs of `none' along with the initial line of dialogue as the OBS (No HLA-OG in Table TABREF19).
Experiment ::: Training Details ::: BERT bi-ranker
is trained by us on the Persona-Chat dataset for the ConvAI2 challenge. Similar to BIBREF0 zhang2018personalizing, we cap the length of the OBS at 360 tokens and the length of each candidate response at 72 tokens. We use a batch size of 64, learning rate of 5e-5, and perform warm-up updates for 100 iterations. The learning rate scheduler uses SGD optimizer with Nesterov's accelerated gradient descent BIBREF34 and is set to have a decay of 0.4 and to reduce on plateau.
Experiment ::: Training Details ::: Uniform Model
is produced by finetuning the BERT bi-ranker on the dialogue data discussed in Section SECREF15 using uniform character sampling. We use the same hyperparameters as the BERT bi-ranker along with half-precision operations (i.e. float16 operations) to increase batch size as recommended BIBREF7.
Experiment ::: Training Details ::: LSRM
is produced by finetuning on the Uniform Model discussed above using negative character sampling. We use the same hyperparameters as the BERT bi-ranker along with half-precision operations (i.e. float16 operations) to increase batch size as recommended.
Evaluation ::: CSM Evaluation
We begin by evaluating the ability of the CSM component of our system to correctly generate the character space. To do so, during training, 30% of the character-HLA pairs (which are either 0 or 1) are masked, and this is used as a validation set (see Figure FIGREF9). For each character $c$, the model generates a list of the 12,815 unique HLAs ranked similarly to BIBREF29 hu2008collaborative for $c$. We look at the recall of our CSM model, which measures the percentage of total ground truth HLAs (over all characters $c$) present within the top N ranked HLAs for all $c$ by our model. That is:
where $HLA_{c}^{gt}$ are the ground truth HLAs for $c$, and $HLA_{c}^{tN}$ are the top N ranked HLAs by the model for $c$. We use $N = 100$, and our model achieves 25.08% recall.
To inspect the CSM performance, we use the T-distributed Stochastic Neighbor Embedding (t-SNE) BIBREF35 to reduce each high-dimensionality data point to two-dimensions via Kullback-Leibler Divergence BIBREF36. This allows us to map our character space into two-dimensions, where similar characters from our embedding space have higher probability of being mapped close by. We sampled characters from four different groups or regions. As seen in Figure FIGREF4, our learned character space effectively groups these characters, as similar characters are adjacent to one another in four regions.
Evaluation ::: Automatic Evaluation Setup ::: Five-Fold Cross Validation
is used for training and testing of the Uniform Model and LSRM. The folds are divided randomly by the TV shows in our dialogue data. We use the dialogue data for 80% of these shows as the four-folds for training, and the dialogue data for the remaining 20% as the fifth-fold for validation/testing. The dialogue data used is discussed in Section SECREF15. This ensures no matter how our data is distributed, each part of it is tested, allowing our evaluation to be more robust to different characters. See Appendix C for five-fold cross validation details and statistics.
Evaluation ::: Automatic Evaluation Setup ::: Five Evaluation Characters
are chosen, one from each of the five testing sets above. Each is a well-known character from a separate TV show, and acts as a target character $c_t$ for evaluation of every model. We choose Sheldon Cooper from The Big Bang Theory, Jean-Luc Picard from Star Trek, Monica Geller from Friends, Gil Grissom from CSI, and Marge Simpson from The Simpsons. We choose characters of significantly different identities and profiles (intelligent scientist, ship captain, outgoing friend, police leader, and responsible mother, respectively) from shows of a variety of genres to ensure that we can successfully recover the language styles of various types of characters. We choose well-known characters because humans require knowledge on the characters they are evaluating (see Section SECREF40).
For each of these five evaluation characters, all the dialogue lines from the character act as the ground truth responses. The initial dialogue lines are the corresponding dialogue lines to which these ground truth responses are responding. For each initial dialogue line, we randomly sample 19 other candidate responses from the associated testing set using uniform character sampling. Note that this is for evaluation, and hence we use the same uniform character sampling method for all models including ALOHA. The use of negative character sampling is only in ALOHA's training.
Evaluation ::: Baselines
We compare against four dialogue system baselines: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset using the same training hyperparameters (including learning rate scheduler and length capping settings) described in Section SECREF20. For the first three models, we use the provided pretrained (on Persona-Chat) models. We evaluate all four on our five evaluation characters discussed in Section SECREF28.
Evaluation ::: Key Evaluation Metrics ::: Hits@n/N
is the accuracy of the correct ground truth response being within the top $n$ ranked candidate responses out of $N$ total candidates. We measure Hits@1/20, Hits@5/20, and Hits@10/20.
Evaluation ::: Key Evaluation Metrics ::: Mean Rank
is the average rank that a model assigns the ground truth response among the 20 total candidates.
Evaluation ::: Key Evaluation Metrics ::: Mean Reciprocal Rank (MRR)
BIBREF37 looks at the mean of the multiplicative inverses of the rank of each correct answer out of a sample of queries $Q$:
where $rank_i$ refers to the rank position of the correct response for the $i$-th query, and $|Q|$ refers to the total number of queries in $Q$.
Evaluation ::: Key Evaluation Metrics ::: @!START@$F_1$@!END@-score
equals $2 * \frac{precision*recall}{precision+recall}$. For dialogue, precision is the fraction of words in the chosen response contained in the ground truth response, and recall is the fraction of words in the ground truth response contained in the chosen response.
Evaluation ::: Key Evaluation Metrics ::: BLEU
BIBREF38 generally indicates how close two pieces of text are in content and structure, with higher values indicating greater similarity. We report our final BLEU scores as the average scores of 1 to 4-grams.
Evaluation ::: Human Evaluation Setup
We conduct a human evaluation with 12 participants, 8 male and 4 female, who are affiliated project researchers aged 20-39 at the University of [ANON]. We choose the same five evaluation characters as in Section SECREF28. To control bias, each participant evaluates one or two characters. For each character, we randomly select 10 testing samples (each includes an initial line of dialogue along with 20 candidate responses, one of which is the ground truth) from the same testing data for the automatic evaluation discussed in Section SECREF28.
These ten samples make up a single questionnaire presented in full to each participant evaluating the corresponding character, and the participant is asked to select the single top response they think the character would most likely respond with for each of the ten initial dialogue lines. See Figure FIGREF41 for an example. We mask any character names within the candidate responses to prevent human participants from using names to identify which show the response is from.
Each candidate is prescreened to ensure they have sufficient knowledge of the character to be a participant. We ask three prescreening questions where the participant has to identify an image, relationship, and occupation of the character. All 12 of our participants passed the the prescreening.
Results and Analysis ::: Evaluation Results
Table TABREF44 shows average results of our automatic and human evaluations. Table TABREF45 shows average Hits@1/20 scores by evaluation character. See Appendix F for detailed evaluation results. ALOHA is the model with HLA-OG during training and testing, and ALOHA (No HLA-OG) is the model with HLA-OG during training but tested with the four HLAs in the OBS marked as `none' (see Section SECREF17). See Appendix G for demo interactions between a human, BERT bi-ranker baseline, and ALOHA for all five evaluation characters.
Results and Analysis ::: Evaluation Challenges
The evaluation of our task (retrieving the language style of a specific character) is challenging and hence the five-fold cross validation is necessary for the following reasons:
The ability to choose a context correct response without attributes of specific characters may be hard to separate from our target metric, which is the ability to retrieve the correct response of a target character by its HLAs. However, from manual observation, we noticed that in the 20 chosen candidate responses, there are typically numerous context correct responses, but only one ground truth for the target character (for an example, see Figure FIGREF41). Hence, a model that only chooses dialogue based on context is distinguishable from one that learns HLAs.
Retrieving responses for the target character depends on the other candidate responses. For example, dialogue retrieval performance for Grissom from CSI, which is a crime/police context, is higher than other evaluation characters (see Table TABREF45), potentially due to other candidate responses not falling within the same crime/police context.
Results and Analysis ::: Performance: ALOHA vs. Humans
As observed from Table TABREF44, ALOHA has a performance relatively close to humans. Human Hits@1/20 scores have a mean of 40.67% and a median over characters of 40%. The limited human evaluation sample size limits what can be inferred, but it indicates that the problem is solved to the extent that ALOHA is able perform relatively close to humans on average. Notice that even humans do not perform extremely well, demonstrating that this task of character based dialogue retrieval is more difficult than typical dialogue retrieval tasks BIBREF19, BIBREF12.
Looking more closely at each character from Table TABREF45, we can see that human evaluation scores are higher for Sheldon and Grissom. This may be due to these characters having more distinct personalities, making them more memorable.
We also look at Pearson correlation values of the Hits@1/20 scores across the five evaluation characters. For human versus Uniform Model, this is -0.4694, demonstrating that the Uniform Model, without knowledge of HLAs, fails to imitate human impressions. For human versus ALOHA, this is 0.4250, demonstrating that our system is able to retrieve character responses somewhat similarly to human impressions. Lastly, for human versus the difference in scores between ALOHA and Uniform Model, this is 0.7815. The difference between ALOHA and the Uniform Model, which is based on the additional knowledge of the HLAs, is hence shown to improve upon the Uniform Model similarly to human impressions. This demonstrates that HLAs are indeed an accurate method of modeling human impressions of character attributes, and also demonstrates that our system, ALOHA, is able to effectively use these HLAs to improve upon dialogue retrieval performance.
Results and Analysis ::: Performance: ALOHA vs. Baselines
ALOHA, combined with the HLAs and dialogue dataset, achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models. As observed from Table TABREF44, ALOHA achieves a significant boost in Hits@n/N accuracy and other metrics for retrieving the correct response of five diverse characters with different identities (see Section SECREF28).
Results and Analysis ::: Performance: ALOHA vs. Uniform Model
We observe a noticeable improvement in performance between ALOHA and the Uniform Model in recovering the language styles of specific characters that is consistent across all five folds (see Tables TABREF44 and TABREF45), indicating that lack of knowledge of HLAs limits the ability of the model to successfully recover the language style of specific characters. We claim that, to the best of our knowledge, we have made the first step in using HLA-based character dialogue clustering to improve upon personality learning for chatbots.
ALOHA demonstrates an accuracy boost for all five evaluation characters, showing that the system is robust and stable and has the ability to recover the dialogue styles of fictional characters regardless of the character's profile and identity, genre of the show, and context of the dialogue.
Results and Analysis ::: Performance: HLA-OG
As observed from Table TABREF44, ALOHA performs slightly better overall compared to ALOHA (No HLA-OG). Table TABREF45 shows that this slight performance increase is consistent across four of the five evaluation characters. In the case of Sheldon, the HLA-OG model performs a bit worse. This is possibly due to the large number of Sheldon's HLAs (217) compared to the other four evaluation characters (average of 93.75), along with the limited amount of HLAs we are using for guidance due to the models' limited memory. In general, HLA Observation Guidance during testing appears to improve upon the performance of ALOHA, but this improvement is minimal.
Conclusion and Future Work
We proposed Human Level Attributes (HLAs) as a novel approach to model human-like attributes of characters, and collected a large volume of dialogue data for various characters with complete and robust profiles. We also proposed and evaluated a system, ALOHA, that uses HLAs to recommend tailored responses traceable to specific characters, and demonstrated its outperformance of the baselines and ability to effectively recover language styles of various characters, showing promise for learning character or personality styles. ALOHA was also shown to be stable regardless of the character's identity, genre of show, and context of dialogue.
Potential directions for future work include training ALOHA with a multi-turn response approach BIBREF0 that tracks dialogue over multiple responses, as we could not acquire multi-turn dialogue data for TV shows. Another potential is the modeling of the dialog counterpart (e.g. the dialogue of other characters speaking to the target character). Further, performing semantic text exchange on the chosen response with a model such as SMERTI BIBREF39 may improve the ability of ALOHA to converse with humans. This is because the response may be context and HLA correct, but incorrect semantically (e.g. the response may say the weather is sunny when it is actually rainy). HLA-aligned generative models is another area of exploration. Typically, generative models produce text that is less fluent, but further work in this area may lead to better results. Lastly, a more diverse and larger participant pool is required due to the limited size of our human evaluation.",['Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)'],5151,qasper,en,,c3dc6e575eeee19145547b339ae1c7af914dfde59a652000,Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393),192
What approaches do they use towards text analysis?,"Introduction
In June 2015, the operators of the online discussion site Reddit banned several communities under new anti-harassment rules. BIBREF0 used this opportunity to combine rich online data with computational methods to study a current question: Does eliminating these “echo chambers” diminish the amount of hate speech overall? Exciting opportunities like these, at the intersection of “thick” cultural and societal questions on the one hand, and the computational analysis of rich textual data on larger-than-human scales on the other, are becoming increasingly common.
Indeed, computational analysis is opening new possibilities for exploring challenging questions at the heart of some of the most pressing contemporary cultural and social issues. While a human reader is better equipped to make logical inferences, resolve ambiguities, and apply cultural knowledge than a computer, human time and attention are limited. Moreover, many patterns are not obvious in any specific context, but only stand out in the aggregate. For example, in a landmark study, BIBREF1 analyzed the authorship of The Federalist Papers using a statistical text analysis by focusing on style, based on the distribution of function words, rather than content. As another example, BIBREF2 studied what defines English haiku and showed how computational analysis and close reading can complement each other. Computational approaches are valuable precisely because they help us identify patterns that would not otherwise be discernible.
Yet these approaches are not a panacea. Examining thick social and cultural questions using computational text analysis carries significant challenges. For one, texts are culturally and socially situated. They reflect the ideas, values and beliefs of both their authors and their target audiences, and such subtleties of meaning and interpretation are difficult to incorporate in computational approaches. For another, many of the social and cultural concepts we seek to examine are highly contested — hate speech is just one such example. Choices regarding how to operationalize and analyze these concepts can raise serious concerns about conceptual validity and may lead to shallow or obvious conclusions, rather than findings that reflect the depth of the questions we seek to address.
These are just a small sample of the many opportunities and challenges faced in computational analyses of textual data. New possibilities and frustrating obstacles emerge at every stage of research, from identification of the research question to interpretation of the results. In this article, we take the reader through a typical research process that involves measuring social or cultural concepts using computational methods, discussing both the opportunities and complications that often arise. In the Reddit case, for example, hate speech is measured, however imperfectly, by the presence of particular words semi-automatically extracted from a machine learning algorithm. Operationalizations are never perfect translations, and are often refined over the course of an investigation, but they are crucial.
We begin our exploration with the identification of research questions, proceed through data selection, conceptualization, and operationalization, and end with analysis and the interpretation of results. The research process sounds more or less linear this way, but each of these phases overlaps, and in some instances turns back upon itself. The analysis phase, for example, often feeds back into the original research questions, which may continue to evolve for much of the project. At each stage, our discussion is critically informed by insights from the humanities and social sciences, fields that have focused on, and worked to tackle, the challenges of textual analysis—albeit at smaller scales—since their inception.
In describing our experiences with computational text analysis, we hope to achieve three primary goals. First, we aim to shed light on thorny issues not always at the forefront of discussions about computational text analysis methods. Second, we hope to provide a set of best practices for working with thick social and cultural concepts. Our guidance is based on our own experiences and is therefore inherently imperfect. Still, given our diversity of disciplinary backgrounds and research practices, we hope to capture a range of ideas and identify commonalities that will resonate for many. And this leads to our final goal: to help promote interdisciplinary collaborations. Interdisciplinary insights and partnerships are essential for realizing the full potential of any computational text analysis that involves social and cultural concepts, and the more we are able to bridge these divides, the more fruitful we believe our work will be.
Research questions
We typically start by identifying the questions we wish to explore. Can text analysis provide a new perspective on a “big question” that has been attracting interest for years? Or can we raise new questions that have only recently emerged, for example about social media? For social scientists working in computational analysis, the questions are often grounded in theory, asking: How can we explain what we observe? These questions are also influenced by the availability and accessibility of data sources. For example, the choice to work with data from a particular social media platform may be partly determined by the fact that it is freely available, and this will in turn shape the kinds of questions that can be asked. A key output of this phase are the concepts to measure, for example: influence; copying and reproduction; the creation of patterns of language use; hate speech. Computational analysis of text motivated by these questions is insight driven: we aim to describe a phenomenon or explain how it came about. For example, what can we learn about how and why hate speech is used or how this changes over time? Is hate speech one thing, or does it comprise multiple forms of expression? Is there a clear boundary between hate speech and other types of speech, and what features make it more or less ambiguous? In these cases, it is critical to communicate high-level patterns in terms that are recognizable.
This contrasts with much of the work in computational text analysis, which tends to focus on automating tasks that humans perform inefficiently. These tasks range from core linguistically motivated tasks that constitute the backbone of natural language processing, such as part-of-speech tagging and parsing, to filtering spam and detecting sentiment. Many tasks are motivated by applications, for example to automatically block online trolls. Success, then, is often measured by performance, and communicating why a certain prediction was made—for example, why a document was labeled as positive sentiment, or why a word was classified as a noun—is less important than the accuracy of the prediction itself. The approaches we use and what we mean by `success' are thus guided by our research questions.
Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them. For example, they may say “we already think we know that”, “that's too naïve”, “that doesn't reflect social reality” (negative); “two major camps in the field would give different answers to that question” (neutral); “we tried to look at that back in the 1960s, but we didn't have the technology” (positive); and “that sounds like something that people who made that archive would love”, “that's a really fundamental question” (very positive).
Sometimes we also hope to connect to multiple disciplines. For example, while focusing on the humanistic concerns of an archive, we could also ask social questions such as “is this archive more about collaborative processes, culture-building or norm creation?” or “how well does this archive reflect the society in which it is embedded?"" BIBREF3 used quantitative methods to tell a story about Darwin's intellectual development—an essential biographical question for a key figure in the history of science. At the same time, their methods connected Darwin's development to the changing landscape of Victorian scientific culture, allowing them to contrast Darwin's “foraging” in the scientific literature of his time to the ways in which that literature was itself produced. Finally, their methods provided a case study, and validation of technical approaches, for cognitive scientists who are interested in how people explore and exploit sources of knowledge.
Questions about potential “dual use” may also arise. Returning to our introductory example, BIBREF0 started with a deceptively simple question: if an internet platform eliminates forums for hate speech, does this impact hate speech in other forums? The research was motivated by the belief that a rising tide of online hate speech was (and is) making the internet increasingly unfriendly for disempowered groups, including minorities, women, and LBGTQ individuals. Yet the possibility of dual use troubled the researchers from the onset. Could the methodology be adopted to target the speech of groups like Black Lives Matter? Could it be adopted by repressive governments to minimize online dissent? While these concerns remained, they concluded that hypothetical dual use scenarios did not outweigh the tangible contribution this research could offer towards making the online environment more equal and just.
Data
The next step involves deciding on the data sources, collecting and compiling the dataset, and inspecting its metadata.
Data acquisition
Many scholars in the humanities and the social sciences work with sources that are not available in digital form, and indeed may never be digitized. Others work with both analogue and digitized materials, and the increasing digitization of archives has opened opportunities to study these archives in new ways. We can go to the canonical archive or open up something that nobody has studied before. For example, we might focus on major historical moments (French Revolution, post-Milosevic Serbia) or critical epochs (Britain entering the Victorian era, the transition from Latin to proto-Romance). Or, we could look for records of how people conducted science, wrote and consumed literature, and worked out their philosophies.
A growing number of researchers work with born-digital sources or data. Born-digital data, e.g., from social media, generally do not involve direct elicitation from participants and therefore enable unobtrusive measurements BIBREF5 , BIBREF6 . In contrast, methods like surveys sometimes elicit altered responses from participants, who might adapt their responses to what they think is expected. Moreover, born-digital data is often massive, enabling large-scale studies of language and behavior in a variety of social contexts.
Still, many scholars in the social sciences and humanities work with multiple data sources. The variety of sources typically used means that more than one data collection method is often required. For example, a project examining coverage of a UK General Election, could draw data from traditional media, web archives, Twitter and Facebook, campaign manifestos, etc. and might combine textual analysis of these materials with surveys, laboratory experiments, or field observations offline. In contrast, many computational studies based on born-digital data have focused on one specific source, such as Twitter.
The use of born-digital data raises ethical concerns. Although early studies often treated privacy as a binary construct, many now acknowledge its complexity BIBREF7 . Conversations on private matters can be posted online, visible for all, but social norms regarding what should be considered public information may differ from the data's explicit visibility settings. Often no informed consent has been obtained, raising concerns and challenges regarding publishing content and potentially harmful secondary uses BIBREF8 , BIBREF4 .
Recently, concerns about potential harms stemming from secondary uses have led a number of digital service providers to restrict access to born-digital data. Facebook and Twitter, for example, have reduced or eliminated public access to their application programming interfaces (APIs) and expressed hesitation about allowing academic researchers to use data from their platforms to examine certain sensitive or controversial topics. Despite the seeming abundance of born-digital data, we therefore cannot take its availability for granted.
Working with data that someone else has acquired presents additional problems related to provenance and contextualisation. It may not always be possible to determine the criteria applied during the creation process. For example, why were certain newspapers digitized but not others, and what does this say about the collection? Similar questions arise with the use of born-digital data. For instance, when using the Internet Archive’s Wayback Machine to gather data from archived web pages, we need to consider what pages were captured, which are likely missing, and why.
We must often repurpose born-digital data (e.g., Twitter was not designed to measure public opinion), but data biases may lead to spurious results and limit justification for generalization. In particular, data collected via black box APIs designed for commercial, not research, purposes are likely to introduce biases into the inferences we draw, and the closed nature of these APIs means we rarely know what biases are introduced, let alone how severely they might impact our research BIBREF10 . These, however, are not new problems. Historians, for example, have always understood that their sources were produced within particular contexts and for particular purposes, which are not always apparent to us.
Non-representative data can still be useful for making comparisons within a sample. In the introductory example on hate speech BIBREF0 , the Reddit forums do not present a comprehensive or balanced picture of hate speech: the writing is almost exclusively in English, the targets of hate speech are mainly restricted (e.g., to black people, or women), and the population of writers is shaped by Reddit's demographics, which skew towards young white men. These biases limit the generalizability of the findings, which cannot be extrapolated to other languages, other types of hate speech, and other demographic groups. However, because the findings are based on measurements on the same sort of hate speech and the same population of writers, as long as the collected data are representative of this specific population, these biases do not pose an intractable validity problem if claims are properly restricted.
The size of many newly available datasets is one of their most appealing characteristics. Bigger datasets often make statistics more robust. The size needed for a computational text analysis depends on the research goal: When it involves studying rare events, bigger datasets are needed. However, larger is not always better. Some very large archives are “secretly” collections of multiple and distinct processes that no in-field scholar would consider related. For example, Google Books is frequently used to study cultural patterns, but the over-representation of scientific articles in Google books can be problematic BIBREF11 . Even very large born-digital datasets usually cover limited timespans compared to, e.g., the Gutenberg archive of British novels.
This stage of the research also raises important questions about fairness. Are marginalized groups, for example, represented in the tweets we have collected? If not, what types of biases might result from analyses relying on those tweets?
Local experts and “informants” can help navigate the data. They can help understand the role an archive plays in the time and place. They might tell us: Is this the central archive, or a peripheral one? What makes it unusual? Or they might tell us how certain underrepresented communities use a social media platform and advise us on strategies for ensuring our data collection includes their perspectives.
However, when it is practically infeasible to navigate the data in this way—for instance, when we cannot determine what is missing from Twitter's Streaming API or what webpages are left out of the Internet Archive—we should be open about the limitations of our analyses, acknowledging the flaws in our data and drawing cautious and reasonable conclusions from them. In all cases, we should report the choices we have made when creating or re-using any dataset.
Compiling data
After identifying the data source(s), the next step is compiling the data. This step is fundamental: if the sources cannot support a convincing result, no result will be convincing. In many cases, this involves defining a “core"" set of documents and a “comparison"" set. We often have a specific set of documents in mind: an author's work, a particular journal, a time period. But if we want to say that this “core"" set has some distinctive property, we need a “comparison"" set. Expanding the collection beyond the documents that we would immediately think of has the beneficial effect of increasing our sample size. Having more sources increases the chance that we will notice something consistent across many individually varying contexts.
Comparing sets of documents can sometimes support causal inference, presented as a contrast between a treatment group and a control. In BIBREF0 , the treatment consisted of the text written in the two forums that were eventually closed by Reddit. However, identifying a control group required a considerable amount of time and effort. Reddit is a diverse platform, with a wide variety of interactional and linguistic styles; it would be pointless to compare hate speech forums against forums dedicated to, say, pictures of wrecked bicycles. Chandrasekharan et al. used a matching design, populating the control group with forums that were as similar as possible to the treatment group, but were not banned from Reddit. The goal is to estimate the counterfactual scenario: in this case, what would have happened had the site not taken action against these specific forums? An ideal control would make it possible to distinguish the effect of the treatment — closing the forums — from other idiosyncratic properties of texts that were treated.
We also look for categories of documents that might not be useful. We might remove documents that are meta-discourse, like introductions and notes, or documents that are in a language that is not the primary language of the collection, or duplicates when we are working with archived web pages. However, we need to carefully consider the potential consequences of information we remove. Does its removal alter the data, or the interpretation of the data, we are analyzing? Are we losing anything that might be valuable at a later stage?
Labels and metadata
Sometimes all we have is documents, but often we want to look at documents in the context of some additional information, or metadata. This additional information could tell us about the creation of documents (date, author, forum), or about the reception of documents (flagged as hate speech, helpful review). Information about text segments can be extremely valuable, but it is also prone to errors, inconsistencies, bias, and missing information. Examining metadata is a good way to check a collection's balance and representativeness. Are sources disproportionately of one form? Is the collection missing a specific time window? This type of curation can be extremely time consuming as it may require expert labeling, but it often leads to the most compelling results. Sometimes metadata are also used as target labels to develop machine learning models. But using them as a “ground truth” requires caution. Labels sometimes mean something different than we expect. For example, a down vote for a social media post could indicate that the content is offensive, or that the voter simply disagreed with the expressed view.
Conceptualization
A core step in many analyses is translating social and cultural concepts (such as hate speech, rumor, or conversion) into measurable quantities. Before we can develop measurements for these concepts (the operationalization step, or the “implementation” step as denoted by BIBREF12 ), we need to define them. In the conceptualization phase we often start with questions such as: who are the domain experts, and how have they approached the topic? We are looking for a definition of the concept that is flexible enough to apply on our dataset, yet formal enough for computational research. For example, our introductory study on hate speech BIBREF0 used a statement on hate speech produced by the European Union Court of Human Rights. The goal was not to implement this definition directly in software but to use it as a reference point to anchor subsequent analyses.
If we want to move beyond the use of ad hoc definitions, it can be useful to distinguish between what political scientists Adcock and Collier call the “background concept” and the “systematized concept” BIBREF13 . The background concept comprises the full and diverse set of meanings that might be associated with a particular term. This involves delving into theoretical, conceptual, and empirical studies to assess how a concept has been defined by other scholars and, most importantly, to determine which definition is most appropriate for the particular research question and the theoretical framework in which it is situated. That definition, in turn, represents the systematized concept: the formulation that is adopted for the study.
It is important to consider that for social and cultural concepts there is no absolute ground truth. There are often multiple valid definitions for a concept (the “background” concept in the terms of Adcock and Collier), and definitions might be contested over time. This may be uncomfortable for computer scientists, whose primary measure of success is often based on comparing a model's output against “ground truth” or a “gold standard”, e.g., by comparing a sentiment classifier's output against manual annotations. However, the notion of ground truth is uncommon in the humanities and the social sciences and it is often taken too far in machine learning. BIBREF14 notes that in literary criticism and the digital humanities more broadly “interpretation, ambiguity, and argumentation are prized far above ground truth and definitive conclusions"". BIBREF15 draw attention to the different attitudes of literary scholars and computational linguists towards ambiguity, stating that “In Computational Linguistics [..] ambiguity is almost uniformly treated as a problem to be solved; the focus is on disambiguation, with the assumption that one true, correct interpretation exists."" The latter is probably true for tasks such as spam filtering, but in the social sciences and the humanities many relevant concepts are fundamentally unobservable, such as latent traits of political actors BIBREF16 or cultural fit in organizations BIBREF17 , leading to validation challenges. Moreover, when the ground truth comes from people, it may be influenced by ideological priors, priming, simple differences of opinion or perspective, and many other factors BIBREF18 . We return to this issue in our discussions on validation and analysis.
Operationalization
In this phase we develop measures (or, “operationalizations”, or “indicators”) for the concepts of interest, a process called “operationalization”. Regardless of whether we are working with computers, the output produced coincides with Adcock and Collier's “scores”—the concrete translation and output of the systematized concept into numbers or labels BIBREF13 . Choices made during this phase are always tied to the question “Are we measuring what we intend to measure?” Does our operationalization match our conceptual definition? To ensure validity we must recognize gaps between what is important and what is easy to measure. We first discuss modeling considerations. Next, we describe several frequently used computational approaches and their limitations and strengths.
Modeling considerations
The variables (both predictors and outcomes) are rarely simply binary or categorical. For example, a study on language use and age could focus on chronological age (instead of, e.g., social age BIBREF19 ). However, even then, age can be modeled in different ways. Discretization can make the modeling easier and various NLP studies have modeled age as a categorical variable BIBREF20 . But any discretization raises questions: How many categories? Where to place the boundaries? Fine distinctions might not always be meaningful for the analysis we are interested in, but categories that are too broad can threaten validity. Other interesting variables include time, space, and even the social network position of the author. It is often preferable to keep the variable in its most precise form. For example, BIBREF21 perform exploration in the context of hypothesis testing by using latitude and longitude coordinates — the original metadata attached to geotagged social media such as tweets — rather than aggregating into administrative units such as counties or cities. This is necessary when such administrative units are unlikely to be related to the target concept, as is the case in their analysis of dialect differences. Focusing on precise geographical coordinates also makes it possible to recognize fine-grained effects, such as language variation across the geography of a city.
Using a particular classification scheme means deciding which variations are visible, and which ones are hidden BIBREF22 . We are looking for a categorization scheme for which it is feasible to collect a large enough labeled document collection (e.g., to train supervised models), but which is also fine-grained enough for our purposes. Classification schemes rarely exhibit the ideal properties, i.e., that they are consistent, their categories are mutually exclusive, and that the system is complete BIBREF22 . Borderline cases are challenging, especially with social and cultural concepts, where the boundaries are often not clear-cut. The choice of scheme can also have ethical implications BIBREF22 . For example, gender is usually represented as a binary variable in NLP and computational models tend to learn gender-stereotypical patterns. The operationalization of gender in NLP has been challenged only recently BIBREF23 , BIBREF24 , BIBREF25 .
Supervised and unsupervised learning are the most common approaches to learning from data. With supervised learning, a model learns from labeled data (e.g., social media messages labeled by sentiment) to infer (or predict) these labels from unlabeled texts. In contrast, unsupervised learning uses unlabeled data. Supervised approaches are especially suitable when we have a clear definition of the concept of interest and when labels are available (either annotated or native to the data). Unsupervised approaches, such as topic models, are especially useful for exploration. In this setting, conceptualization and operationalization may occur simultaneously, with theory emerging from the data BIBREF26 . Unsupervised approaches are also used when there is a clear way of measuring a concept, often based on strong assumptions. For example, BIBREF3 measure “surprise” in an analysis of Darwin's reading decisions based on the divergence between two probability distributions.
From an analysis perspective, the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis. For example, if in a study on media frames in news stories, the theoretical framework and research question point toward frames at the story level (e.g., what is the overall causal analysis of the news article?), the story must be the unit of analysis. Yet it is often difficult to validly and reliably code a single frame at the story level. Multiple perspectives are likely to sit side-by-side in a story. Thus, an article on income inequality might point to multiple causes, such as globalization, education, and tax policies. Coding at the sentence level would detect each of these causal explanations individually, but this information would need to be somehow aggregated to determine the overall story-level frame. Sometimes scholars solve this problem by only examining headlines and lead paragraphs, arguing that based on journalistic convention, the most important information can be found at the beginning of a story. However, this leads to a return to a shorter, less nuanced analysis.
From a computational perspective, the unit of text can also make a huge difference, especially when we are using bag-of-words models, where word order within a unit does not matter. Small segments, like tweets, sometimes do not have enough information to make their semantic context clear. In contrast, larger segments, like novels, have too much variation, making it difficult to train focused models. Finding a good segmentation sometimes means combining short documents and subdividing long documents. The word “document"" can therefore be misleading. But it is so ingrained in the common NLP lexicon that we use it anyway in this article.
For insight-driven text analysis, it is often critical that high-level patterns can be communicated. Furthermore, interpretable models make it easier to find spurious features, to do error analysis, and to support interpretation of results. Some approaches are effective for prediction, but harder to interpret. The value we place on interpretability can therefore influence the approach we choose. There is an increasing interest in developing interpretable or transparent models in the NLP and machine learning communities.
Annotation
Many studies involve human coders. Sometimes the goal is to fully code the data, but in a computational analysis we often use the labels (or annotations) to train machine learning models to automatically recognize them, and to identify language patterns that are associated with these labels. For example, for a project analyzing rumors online BIBREF27 , conversation threads were annotated along different dimensions, including rumor versus non-rumor and stance towards a rumor.
The collection of annotation choices make up an annotation scheme (or “codebook”). Existing schemes and annotations can be useful as starting points. Usually settling on an annotation scheme requires several iterations, in which the guidelines are updated and annotation examples are added. For example, a political scientist could use a mixed deductive-inductive strategy for developing a codebook. She starts by laying out a set of theory-driven deductive coding rules, which means that the broad principles of the coding rules are laid out without examining examples first. These are then tested (and possibly adjusted) based on a sample of the data. In line with Adcock and Collier's notion of “content validity” BIBREF13 , the goal is to assess whether the codebook adequately captures the systematized concept. By looking at the data themselves, she gains a better sense of whether some things have been left out of the coding rules and whether anything is superfluous, misleading, or confusing. Adjustments are made and the process is repeated, often with another researcher involved.
The final annotations can be collected using a crowdsourcing platform, a smaller number of highly-trained annotators, or a group of experts. Which type of annotator to use should be informed by the complexity and specificity of the concept. For more complex concepts, highly-trained or expert annotators tend to produce more reliable results. However, complex concepts can sometimes be broken down into micro-tasks that can be performed independently in parallel by crowdsourced annotators. Concepts from highly specialized domains may require expert annotators. In all cases, however, some training will be required, and the training phase should involve continual checks of inter-annotator agreement (i.e. intercoder reliability) or checks against a gold standard (e.g. quizzes in crowdsourcing platforms).
We also need to decide how inter-annotator agreement will be measured and what an acceptable level of agreement would be. Krippendorff's alpha is frequently used in the social sciences, but the right measure depends on the type of data and task. For manual coding, we can continually check inter-annotator agreement and begin introducing checks of intra-annotator agreement, too. For most communication scholars using only manual content analysis, an acceptable rate of agreement is achieved when Krippendorf's alpha reaches 0.80 or above. When human-coded data are used to validate machine learning algorithms, the reliability of the human-coded data is even more important. Disagreement between annotators can signal weaknesses of the annotation scheme, or highlight the inherent ambiguity in what we are trying to measure. Disagreement itself can be meaningful and can be integrated in subsequent analyses BIBREF28 , BIBREF29 .
Data pre-processing
Preparing the data can be a complex and time-consuming process, often involving working with partially or wholly unstructured data. The pre-processing steps have a big impact on the operationalizations, subsequent analyses and reproducibility efforts BIBREF30 , and they are usually tightly linked to what we intend to measure. Unfortunately, these steps tend to be underreported, but documenting the pre-processing choices made is essential and is analogous to recording the decisions taken during the production of a scholarly edition or protocols in biomedical research. Data may also vary enormously in quality, depending on how it has been generated. Many historians, for example, work with text produced from an analogue original using Optical Character Recognition (OCR). Often, there will be limited information available regarding the accuracy of the OCR, and the degree of accuracy may even vary within a single corpus (e.g. where digitized text has been produced over a period of years, and the software has gradually improved). The first step, then, is to try to correct for common OCR errors. These will vary depending on the type of text, the date at which the `original' was produced, and the nature of the font and typesetting.
One step that almost everyone takes is to tokenize the original character sequence into the words and word-like units. Tokenization is a more subtle and more powerful process than people expect. It is often done using regular expressions or scripts that have been circulating within the NLP community. Tokenization heuristics, however, can be badly confused by emoticons, creative orthography (e.g., U$A, sh!t), and missing whitespace. Multi-word terms are also challenging. Treating them as a single unit can dramatically alter the patterns in text. Many words that are individually ambiguous have clear, unmistakable meanings as terms, like “black hole"" or “European Union"". However, deciding what constitutes a multi-word term is a difficult problem. In writing systems like Chinese, tokenization is a research problem in its own right.
Beyond tokenization, common steps include lowercasing, removing punctuation, stemming (removing suffixes), lemmatization (converting inflections to a base lemma), and normalization, which has never been clearly defined, but often includes grouping abbreviations like “U.S.A."" and “USA"", ordinals like “1st"" and “first"", and variant spellings like “noooooo"". The main goal of these steps is to improve the ratio of tokens (individual occurrences) to types (the distinct things in a corpus). Each step requires making additional assumptions about which distinctions are relevant: is “apple” different from “Apple”? Is “burnt” different from “burned”? Is “cool"" different from “coooool""? Sometimes these steps can actively hide useful patterns, like social meaning BIBREF32 . Some of us therefore try do as little modification as possible.
From a multilingual perspective, English and Chinese have an unusually simple inflectional system, and so it is statistically reasonable to treat each inflection as a unique word type. Romance languages have considerably more inflections than English; many indigenous North American languages have still more. For these languages, unseen data is far more likely to include previously-unseen inflections, and therefore, dealing with inflections is more important. On the other hand, the resources for handling inflections vary greatly by language, with European languages dominating the attention of the computational linguistics community thus far.
We sometimes also remove words that are not relevant to our goals, for example by calculating vocabulary frequencies. We construct a “stoplist” of words that we are not interested in. If we are looking for semantic themes we might remove function words like determiners and prepositions. If we are looking for author-specific styles, we might remove all words except function words. Some words are generally meaningful but too frequent to be useful within a specific collection. We sometimes also remove very infrequent words. Their occurrences are too low for robust patterns and removing them helps reducing the vocabulary size.
The choice of processing steps can be guided by theory or knowledge about the domain as well as experimental investigation. When we have labels, predictive accuracy of a model is a way to assess the effect of the processing steps. In unsupervised settings, it is more challenging to understand the effects of different steps. Inferences drawn from unsupervised settings can be sensitive to pre-processing choices BIBREF33 . Stemming has been found to provide little measurable benefits for topic modeling and can sometimes even be harmful BIBREF34 . All in all, this again highlights the need to document these steps.
Finally, we can also mark up the data, e.g., by identifying entities (people, places, organizations, etc.) or parts of speech. Although many NLP tools are available for such tasks, they are often challenged by linguistic variation, such as orthographic variation in historical texts BIBREF35 and social media BIBREF32 . Moreover, the performance of NLP tools often drops when applying them outside the training domain, such as applying tools developed on newswire texts to texts written by younger authors BIBREF36 . Problems (e.g., disambiguation in named entity recognition) are sometimes resolved using considerable manual intervention. This combination of the automated and the manual, however, becomes more difficult as the scale of the data increases, and the `certainty' brought by the latter may have to be abandoned.
Dictionary-based approaches
Dictionaries are frequently used to code texts in content analyses BIBREF37 . Dictionaries consist of one or more categories (i.e. word lists). Sometimes the output is simply the number of category occurrences (e.g., positive sentiment), thus weighting words within a category equally. In some other cases, words are assigned continuous scores. The high transparency of dictionaries makes them sometimes more suitable than supervised machine learning models. However, dictionaries should only be used if the scores assigned to words match how the words are used in the data (see BIBREF38 for a detailed discussion on limitations). There are many off-the-shelf dictionaries available (e.g., LIWC BIBREF39 ). These are often well-validated, but applying them on a new domain may not be appropriate without additional validation. Corpus- or domain-specific dictionaries can overcome limitations of general-purpose dictionaries.
The dictionaries are often manually compiled, but increasingly they are constructed semi-automatically (e.g., BIBREF40 ). When we semi-automatically create a word list, we use automation to identify an initial word list, and human insight to filter it. By automatically generating the initial words lists, words can be identified that human annotators might have difficulty intuiting. By manually filtering the lists, we use our theoretical understanding of the target concept to remove spurious features.
In the introduction study, SAGE BIBREF41 was used to obtain a list of words that distinguished the text in the treatment group (subreddits that were closed by Reddit) from text in the control group (similar subreddits that were not closed). The researchers then returned to the hate speech definition provided by the European Court of Human Rights, and manually filtered the top SAGE words based on this definition. Not all identified words fitted the definition. The others included: the names of the subreddits themselves, names of related subreddits, community-specific jargon that was not directly related to hate speech, and terms such as IQ and welfare, which were frequently used in discourses of hate speech, but had significant other uses. The word lists provided the measurement instrument for their main result, which is that the use of hate speech throughout Reddit declined after the two treatment subreddits were closed.
Supervised models
Supervised learning is frequently used to scale up analyses. For example, BIBREF42 wanted to analyze the motivations of Movember campaign participants. By developing a classifier based on a small set of annotations, they were able to expand the analysis to over 90k participants.
The choice of supervised learning model is often guided by the task definition and the label types. For example, to identify stance towards rumors based on sequential annotations, an algorithm for learning from sequential BIBREF43 or time series data BIBREF44 could be used. The features (sometimes called variables or predictors) are used by the model to make the predictions. They may vary from content-based features such as single words, sequences of words, or information about their syntactic structure, to meta-information such as user or network information. Deciding on the features requires experimentation and expert insight and is often called feature engineering. For insight-driven analysis, we are often interested in why a prediction has been made and features that can be interpreted by humans may be preferred. Recent neural network approaches often use simple features as input (such as word embeddings or character sequences), which requires less feature engineering but make interpretation more difficult.
Supervised models are powerful, but they can latch on to spurious features of the dataset. This is particularly true for datasets that are not well-balanced, and for annotations that are noisy. In our introductory example on hate speech in Reddit BIBREF0 , the annotations are automatically derived from the forum in which each post appears, and indeed, many of the posts in the forums (subreddits) that were banned by Reddit would be perceived by many as hate speech. But even in banned subreddits, not all of the content is hate speech (e.g., some of the top features were self-referential like the name of the subreddit) but a classifier would learn a high weight for these features.
Even when expert annotations are available on the level of individual posts, spurious features may remain. BIBREF45 produced expert annotations of hate speech on Twitter. They found that one of the strongest features for sexism is the name of an Australian TV show, because people like to post sexist comments about the contestants. If we are trying to make claims about what inhibits or encourages hate speech, we would not want those claims to be tied to the TV show's popularity. Such problems are inevitable when datasets are not well-balanced over time, across genres, topics, etc. Especially with social media data, we lack a clear and objective definition of `balance' at this time.
The risk of supervised models latching on to spurious features reinforces the need for interpretability. Although the development of supervised models is usually performance driven, placing more emphasis on interpretability could increase the adoption of these models in insight-driven analyses. One way would be to only use models that are already somewhat interpretable, for example models that use a small number of human-interpretable features. Rather than imposing such restrictions, there is also work on generating post-hoc explanations for individual predictions (e.g., BIBREF46 ), even when the underlying model itself is very complex.
Topic modeling
Topic models (e.g., LDA BIBREF47 ) are usually unsupervised and therefore less biased towards human-defined categories. They are especially suited for insight-driven analysis, because they are constrained in ways that make their output interpretable. Although there is no guarantee that a “topic” will correspond to a recognizable theme or event or discourse, they often do so in ways that other methods do not. Their easy applicability without supervision and ready interpretability make topic models good for exploration. Topic models are less successful for many performance-driven applications. Raw word features are almost always better than topics for search and document classification. LSTMs and other neural network models are better as language models. Continuous word embeddings have more expressive power to represent fine-grained semantic similarities between words.
A topic model provides a different perspective on a collection. It creates a set of probability distributions over the vocabulary of the collection, which, when combined together in different proportions, best match the content of the collection. We can sort the words in each of these distributions in descending order by probability, take some arbitrary number of most-probable words, and get a sense of what (if anything) the topic is “about”. Each of the text segments also has its own distribution over the topics, and we can sort these segments by their probability within a given topic to get a sense of how that topic is used.
One of the most common questions about topic models is how many topics to use, usually with the implicit assumption that there is a “right” number that is inherent in the collection. We prefer to think of this parameter as more like the scale of a map or the magnification of a microscope. The “right” number is determined by the needs of the user, not by the collection. If the analyst is looking for a broad overview, a relatively small number of topics may be best. If the analyst is looking for fine-grained phenomena, a larger number is better.
After fitting the model, it may be necessary to circle back to an earlier phase. Topic models find consistent patterns. When authors repeatedly use a particular theme or discourse, that repetition creates a consistent pattern. But other factors can also create similar patterns, which look as good to the algorithm. We might notice a topic that has highest probability on French stopwords, indicating that we need to do a better job of filtering by language. We might notice a topic of word fragments, such as “ing”, “tion”, “inter”, indicating that we are not handling end-of-line hyphenation correctly. We may need to add to our stoplist or change how we curate multi-word terms.
Validation
The output of our measurement procedures (in the social sciences often called the “scores”) must now be assessed in terms of their reliability and validity with regard to the (systemized) concept. Reliability aims to capture repeatability, i.e. the extent to which a given tool provides consistent results.
Validity assesses the extent to which a given measurement tool measures what it is supposed to measure. In NLP and machine learning, most models are primarily evaluated by comparing the machine-generated labels against an annotated sample. This approach presumes that the human output is the “gold standard"" against which performance should be tested. In contrast, when the reliability is measured based on the output of different annotators, no coder is taken as the standard and the likelihood of coders reaching agreement by chance (rather than because they are “correct"") is factored into the resulting statistic. Comparing against a “gold standard” suggests that the threshold for human inter- and intra-coder reliability should be particularly high.
Accuracy, as well as other measures such as precision, recall and F-score, are sometimes presented as a measure of validity, but if we do not have a genuinely objective determination of what something is supposed measure—as is often the case in text analysis—then accuracy is perhaps a better indication of reliability than of validity. In that case, validity needs to be assessed based on other techniques like those we discuss later in this section. It is also worth asking what level of accuracy is sufficient for our analysis and to what extent there may be an upper bound, especially when the labels are native to the data or when the notion of a “gold standard” is not appropriate.
For some in the humanities, validation takes the form of close reading, not designed to confirm whether the model output is correct, but to present what BIBREF48 refers to as a form of “further discovery in two directions”. Model outputs tell us something about the texts, while a close reading of the texts alongside those outputs tells us something about the models that can be used for more effective model building. Applying this circular, iterative process to 450 18th-century novels written in three languages, Piper was able to uncover a new form of “conversional novel” that was not previously captured in “literary history's received critical categories” BIBREF48 .
Along similar lines, we can subject both the machine-generated output and the human annotations to another round of content validation. That is, take a stratified random sample, selecting observations from the full range of scores, and ask: Do these make sense in light of the systematized concept? If not, what seems to be missing? Or is something extraneous being captured? This is primarily a qualitative process that requires returning to theory and interrogating the systematized concept, indicators, and scores together. This type of validation is rarely done in NLP, but it is especially important when it is difficult to assess what drives a given machine learning model. If there is a mismatch between the scores and systematized concept at this stage, the codebook may need to be adjusted, human coders retrained, more training data prepared, algorithms adjusted, or in some instances, even a new analytical method adopted.
Other types of validation are also possible, such as comparing with other approaches that aim to capture the same concept, or comparing the output with external measures (e.g., public opinion polls, the occurrence of future events). We can also go beyond only evaluating the labels (or point estimates). BIBREF16 used human judgments to not only assess the positional estimates from a scaling method of latent political traits but also to assess uncertainty intervals. Using different types of validation can increase our confidence in the approach, especially when there is no clear notion of ground truth.
Besides focusing on rather abstract evaluation measures, we could also assess the models in task-based settings using human experts. Furthermore, for insight-driven analyses, it can be more useful to focus on improving explanatory power than making small improvements in predictive performance.
Analysis
In this phase, we use our models to explore or answer our research questions. For example, given a topic model we can look at the connection between topics and metadata elements. Tags such as “hate speech"" or metadata information imply a certain way of organizing the collection. Computational models provide another organization, which may differ in ways that provide more insight into how these categories manifest themselves, or fail to do so.
Moreover, when using a supervised approach, the “errors”, i.e. disagreement between the system output and human-provided labels, can point towards interesting cases for closer analysis and help us reflect on our conceptualizations. In the words of BIBREF2 , they can be “opportunities for interpretation”. Other types of “failures” can be insightful as well. Sometimes there is a “dog that didn't bark” BIBREF49 –i.e., something that everyone thinks we should have found, but we did not. Or, sometimes the failures are telling us about the existence of something in the data that nobody noticed, or thought important, until then (e.g., the large number of travel journals in Darwin's reading lists).
Computational text analysis is not a replacement for but rather an addition to the approaches one can take to analyze social and cultural phenomena using textual data. By moving back and forth between large-scale computational analyses and small-scale qualitative analyses, we can combine their strengths so that we can identify large-scale and long-term trends, but also tell individual stories. For example, the Reddit study on hate speech BIBREF0 raised various follow-up questions: Can we distinguish hate speech from people talking about hate speech? Did people find new ways to express hate speech? If so, did the total amount of online hate speech decrease after all? As possible next steps, a qualitative discourse analyst might examine a smaller corpus to investigate whether commenters were indeed expressing hate speech in new ways; a specialist in interview methodologies might reach out to commenters to better understand the role of online hate speech in their lives. Computational text analysis represents a step towards better understanding social and cultural phenomena, and it is in many cases better suited towards opening questions rather than closing them.
Conclusion
Insight-driven computational analysis of text is becoming increasingly common. It not only helps us see more broadly, it helps us see subtle patterns more clearly and allows us to explore radical new questions about culture and society. In this article we have consolidated our experiences, as scholars from very different disciplines, in analyzing text as social and cultural data and described how the research process often unfolds. Each of the steps in the process is time-consuming and labor-intensive. Each presents challenges. And especially when working across disciplines, the research often involves a fair amount of discussion—even negotiation—about what means of operationalization and approaches to analysis are appropriate and feasible. And yet, with a bit of perseverance and mutual understanding, conceptually sound and meaningful work results so that we can truly make use of the exciting opportunities rich textual data offers.
Acknowledgements
This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. Dong Nguyen is supported with an Alan Turing Institute Fellowship (TU/A/000006). Maria Liakata is a Turing fellow at 40%. We would also like to thank the participants of the “Bridging disciplines in analysing text as social and cultural data” workshop held at the Turing Institute (2017) for insightful discussions. The workshop was funded by a Turing Institute seed funding award to Nguyen and Liakata.","['Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use', ""Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis.""]",8530,qasper_e,en,,6bc78370c92181718d2f91cc1cdb36a8937d8f1de01618ba,"Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use', ""Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis.",607
what evaluation protocols are provided?,"Introduction
Nowadays deep learning techniques outperform the other conventional methods in most of the speech-related tasks. Training robust deep neural networks for each task depends on the availability of powerful processing GPUs, as well as standard and large scale datasets. In text-independent speaker verification, large-scale datasets are available, thanks to the NIST SRE evaluations and other data collection projects such as VoxCeleb BIBREF0.
In text-dependent speaker recognition, experiments with end-to-end architectures conducted on large proprietary databases have demonstrated their superiority over traditional approaches BIBREF1. Yet, contrary to text-independent speaker recognition, text-dependent speaker recognition lacks large-scale publicly available databases. The two most well-known datasets are probably RSR2015 BIBREF2 and RedDots BIBREF3. The former contains speech data collected from 300 individuals in a controlled manner, while the latter is used primarily for evaluation rather than training, due to its small number of speakers (only 64). Motivated by this lack of large-scale dataset for text-dependent speaker verification, we chose to proceed with the collection of the DeepMine dataset, which we expect to become a standard benchmark for the task.
Apart from speaker recognition, large amounts of training data are required also for training automatic speech recognition (ASR) systems. Such datasets should not only be large in size, they should also be characterized by high variability with respect to speakers, age and dialects. While several datasets with these properties are available for languages like English, Mandarin, French, this is not the case for several other languages, such as Persian. To this end, we proceeded with collecting a large-scale dataset, suitable for building robust ASR models in Persian.
The main goal of the DeepMine project was to collect speech from at least a few thousand speakers, enabling research and development of deep learning methods. The project started at the beginning of 2017, and after designing the database and the developing Android and server applications, the data collection began in the middle of 2017. The project finished at the end of 2018 and the cleaned-up and final version of the database was released at the beginning of 2019. In BIBREF4, the running project and its data collection scenarios were described, alongside with some preliminary results and statistics. In this paper, we announce the final and cleaned-up version of the database, describe its different parts and provide various evaluation setups for each part. Finally, since the database was designed mainly for text-dependent speaker verification purposes, some baseline results are reported for this task on the official evaluation setups. Additional baseline results are also reported for Persian speech recognition. However, due to the space limitation in this paper, the baseline results are not reported for all the database parts and conditions. They will be defined and reported in the database technical documentation and in a future journal paper.
Data Collection
DeepMine is publicly available for everybody with a variety of licenses for different users. It was collected using crowdsourcing BIBREF4. The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions. The Android application did various checks on each utterance and if it passed all of them, the respondent was directed to the next phrase. For more information about data collection scenario, please refer to BIBREF4.
Data Collection ::: Post-Processing
In order to clean-up the database, the main post-processing step was to filter out problematic utterances. Possible problems include speaker word insertions (e.g. repeating some part of a phrase), deletions, substitutions, and involuntary disfluencies. To detect these, we implemented an alignment stage, similar to the second alignment stage in the LibriSpeech project BIBREF5. In this method, a custom decoding graph was generated for each phrase. The decoding graph allows for word skipping and word insertion in the phrase.
For text-dependent and text-prompted parts of the database, such errors are not allowed. Hence, any utterances with errors were removed from the enrollment and test lists. For the speech recognition part, a sub-part of the utterance which is correctly aligned to the corresponding transcription is kept. After the cleaning step, around 190 thousand utterances with full transcription and 10 thousand with sub-part alignment have remained in the database.
Data Collection ::: Statistics
After processing the database and removing problematic respondents and utterances, 1969 respondents remained in the database, with 1149 of them being male and 820 female. 297 of the respondents could not read English and have therefore read only the Persian prompts. About 13200 sessions were recorded by females and similarly, about 9500 sessions by males, i.e. women are over-represented in terms of sessions, even though their number is 17% smaller than that of males. Other useful statistics related to the database are shown in Table TABREF4.
The last status of the database, as well as other related and useful information about its availability can be found on its website, together with a limited number of samples.
DeepMine Database Parts
The DeepMine database consists of three parts. The first one contains fixed common phrases to perform text-dependent speaker verification. The second part consists of random sequences of words useful for text-prompted speaker verification, and the last part includes phrases with word- and phoneme-level transcription, useful for text-independent speaker verification using a random phrase (similar to Part4 of RedDots). This part can also serve for Persian ASR training. Each part is described in more details below. Table TABREF11 shows the number of unique phrases in each part of the database. For the English text-dependent part, the following phrases were selected from part1 of the RedDots database, hence the RedDots can be used as an additional training set for this part:
“My voice is my password.”
“OK Google.”
“Artificial intelligence is for real.”
“Actions speak louder than words.”
“There is no such thing as a free lunch.”
DeepMine Database Parts ::: Part1 - Text-dependent (TD)
This part contains a set of fixed phrases which are used to verify speakers in text-dependent mode. Each speaker utters 5 Persian phrases, and if the speaker can read English, 5 phrases selected from Part1 of the RedDots database are also recorded.
We have created three experimental setups with different numbers of speakers in the evaluation set. For each setup, speakers with more recording sessions are included in the evaluation set and the rest of the speakers are used for training in the background set (in the database, all background sets are basically training data). The rows in Table TABREF13 corresponds to the different experimental setups and shows the numbers of speakers in each set. Note that, for English, we have filtered the (Persian native) speakers by the ability to read English. Therefore, there are fewer speakers in each set for English than for Persian. There is a small “dev” set in each setup which can be used for parameter tuning to prevent over-tuning on the evaluation set.
For each experimental setup, we have defined several official trial lists with different numbers of enrollment utterances per trial in order to investigate the effects of having different amounts of enrollment data. All trials in one trial list have the same number of enrollment utterances (3 to 6) and only one test utterance. All enrollment utterances in a trial are taken from different consecutive sessions and the test utterance is taken from yet another session. From all the setups and conditions, the 100-spk with 3-session enrollment (3-sess) is considered as the main evaluation condition. In Table TABREF14, the number of trials for Persian 3-sess are shown for the different types of trial in the text-dependent speaker verification (SV). Note that for Imposter-Wrong (IW) trials (i.e. imposter speaker pronouncing wrong phrase), we merely create one wrong trial for each Imposter-Correct (IC) trial to limit the huge number of possible trials for this case. So, the number of trials for IC and IW cases are the same.
DeepMine Database Parts ::: Part2 - Text-prompted (TP)
For this part, in each session, 3 random sequences of Persian month names are shown to the respondent in two modes: In the first mode, the sequence consists of all 12 months, which will be used for speaker enrollment. The second mode contains a sequence of 3 month names that will be used as a test utterance. In each 8 sessions received by a respondent from the server, there are 3 enrollment phrases of all 12 months (all in just one session), and $7 \times 3$ other test phrases, containing fewer words. For a respondent who can read English, 3 random sequences of English digits are also recorded in each session. In one of the sessions, these sequences contain all digits and the remaining ones contain only 4 digits.
Similar to the text-dependent case, three experimental setups with different number of speaker in the evaluation set are defined (corresponding to the rows in Table TABREF16). However, different strategy is used for defining trials: Depending on the enrollment condition (1- to 3-sess), trials are enrolled on utterances of all words from 1 to 3 different sessions (i.e. 3 to 9 utterances). Further, we consider two conditions for test utterances: seq test utterance with only 3 or 4 words and full test utterances with all words (i.e. same words as in enrollment but in different order). From all setups an all conditions, the 100-spk with 1-session enrolment (1-sess) is considered as the main evaluation condition for the text-prompted case. In Table TABREF16, the numbers of trials (sum for both seq and full conditions) for Persian 1-sess are shown for the different types of trials in the text-prompted SV. Again, we just create one IW trial for each IC trial.
DeepMine Database Parts ::: Part3 - Text-independent (TI)
In this part, 8 Persian phrases that have already been transcribed on the phone level are displayed to the respondent. These phrases are chosen mostly from news and Persian Wikipedia. If the respondent is unable to read English, instead of 5 fixed phrases and 3 random digit strings, 8 other Persian phrases are also prompted to the respondent to have exactly 24 phrases in each recording session.
This part can be useful at least for three potential applications. First, it can be used for text-independent speaker verification. The second application of this part (same as Part4 of RedDots) is text-prompted speaker verification using random text (instead of a random sequence of words). Finally, the third application is large vocabulary speech recognition in Persian (explained in the next sub-section).
Based on the recording sessions, we created two experimental setups for speaker verification. In the first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set (can be used as training data). In the second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set. Table TABREF18 shows numbers of speakers in each set of the database for text-independent SV case.
For text-independent SV, we have considered 4 scenarios for enrollment and 4 scenarios for test. The speaker can be enrolled using utterances from 1, 2 or 3 consecutive sessions (1sess to 3sess) or using 8 utterances from 8 different sessions. The test speech can be one utterance (1utt) for short duration scenario or all utterances in one session (1sess) for long duration case. In addition, test speech can be selected from 5 English phrases for cross-language testing (enrollment using Persian utterances and test using English utterances). From all setups, 1sess-1utt and 1sess-1sess for 438-spk set are considered as the main evaluation setups for text-independent case. Table TABREF19 shows number of trials for these setups.
For text-prompted SV with random text, the same setup as text-independent case together with corresponding utterance transcriptions can be used.
DeepMine Database Parts ::: Part3 - Speech Recognition
As explained before, Part3 of the DeepMine database can be used for Persian read speech recognition. There are only a few databases for speech recognition in Persian BIBREF6, BIBREF7. Hence, this part can at least partly address this problem and enable robust speech recognition applications in Persian. Additionally, it can be used for speaker recognition applications, such as training deep neural networks (DNNs) for extracting bottleneck features BIBREF8, or for collecting sufficient statistics using DNNs for i-vector training.
We have randomly selected 50 speakers (25 for each gender) from the all speakers in the database which have net speech (without silence parts) between 25 minutes to 50 minutes as test speakers. For each speaker, the utterances in the first 5 sessions are included to (small) test-set and the other utterances of test speakers are considered as a large-test-set. The remaining utterances of the other speakers are included in the training set. The test-set, large-test-set and train-set contain 5.9, 28.5 and 450 hours of speech respectively.
There are about 8300 utterances in Part3 which contain only Persian full names (i.e. first and family name pairs). Each phrase consists of several full names and their phoneme transcriptions were extracted automatically using a trained Grapheme-to-Phoneme (G2P). These utterances can be used to evaluate the performance of a systems for name recognition, which is usually more difficult than the normal speech recognition because of the lack of a reliable language model.
Experiments and Results
Due to the space limitation, we present results only for the Persian text-dependent speaker verification and speech recognition.
Experiments and Results ::: Speaker Verification Experiments
We conducted an experiment on text-dependent speaker verification part of the database, using the i-vector based method proposed in BIBREF9, BIBREF10 and applied it to the Persian portion of Part1. In this experiment, 20-dimensional MFCC features along with first and second derivatives are extracted from 16 kHz signals using HTK BIBREF11 with 25 ms Hamming windowed frames with 15 ms overlap.
The reported results are obtained with a 400-dimensional gender independent i-vector based system. The i-vectors are first length-normalized and are further normalized using phrase- and gender-dependent Regularized Within-Class Covariance Normalization (RWCCN) BIBREF10. Cosine distance is used to obtain speaker verification scores and phrase- and gender-dependent s-norm is used for normalizing the scores. For aligning speech frames to Gaussian components, monophone HMMs with 3 states and 8 Gaussian components in each state are used BIBREF10. We only model the phonemes which appear in the 5 Persian text-dependent phrases.
For speaker verification experiments, the results were reported in terms of Equal Error Rate (EER) and Normalized Detection Cost Function as defined for NIST SRE08 ($\mathrm {NDCF_{0.01}^{min}}$) and NIST SRE10 ($\mathrm {NDCF_{0.001}^{min}}$). As shown in Table TABREF22, in text-dependent SV there are 4 types of trials: Target-Correct and Imposter-Correct refer to trials when the pass-phrase is uttered correctly by target and imposter speakers respectively, and in same manner, Target-Wrong and Imposter-Wrong refer to trials when speakers uttered a wrong pass-phrase. In this paper, only the correct trials (i.e. Target-Correct as target trials vs Imposter-Correct as non-target trials) are considered for evaluating systems as it has been proved that these are the most challenging trials in text-dependent SV BIBREF8, BIBREF12.
Table TABREF23 shows the results of text-dependent experiments using Persian 100-spk and 3-sess setup. For filtering trials, the respondents' mobile brand and model were used in this experiment. In the table, the first two letters in the filter notation relate to the target trials and the second two letters (i.e. right side of the colon) relate for non-target trials. For target trials, the first Y means the enrolment and test utterances were recorded using a device with the same brand by the target speaker. The second Y letter means both recordings were done using exactly the same device model. Similarly, the first Y for non-target trials means that the devices of target and imposter speakers are from the same brand (i.e. manufacturer). The second Y means that, in addition to the same brand, both devices have the same model. So, the most difficult target trials are “NN”, where the speaker has used different a device at the test time. In the same manner, the most difficult non-target trials which should be rejected by the system are “YY” where the imposter speaker has used the same device model as the target speaker (note that it does not mean physically the same device because each speaker participated in the project using a personal mobile device). Hence, the similarity in the recording channel makes rejection more difficult.
The first row in Table TABREF23 shows the results for all trials. By comparing the results with the best published results on RSR2015 and RedDots BIBREF10, BIBREF8, BIBREF12, it is clear that the DeepMine database is more challenging than both RSR2015 and RedDots databases. For RSR2015, the same i-vector/HMM-based method with both RWCCN and s-norm has achieved EER less than 0.3% for both genders (Table VI in BIBREF10). The conventional Relevance MAP adaptation with HMM alignment without applying any channel-compensation techniques (i.e. without applying RWCCN and s-norm due to the lack of suitable training data) on RedDots Part1 for the male has achieved EER around 1.5% (Table XI in BIBREF10). It is worth noting that EERs for DeepMine database without any channel-compensation techniques are 2.1 and 3.7% for males and females respectively.
One interesting advantage of the DeepMine database compared to both RSR2015 and RedDots is having several target speakers with more than one mobile device. This is allows us to analyse the effects of channel compensation methods. The second row in Table TABREF23 corresponds to the most difficult trials where the target trials come from mobile devices with different models while imposter trials come from the same device models. It is clear that severe degradation was caused by this kind of channel effects (i.e. decreasing within-speaker similarities while increasing between-speaker similarities), especially for females.
The results in the third row show the condition when target speakers at the test time use exactly the same device that was used for enrollment. Comparing this row with the results in the first row proves how much improvement can be achieved when exactly the same device is used by the target speaker.
The results in the fourth row show the condition when imposter speakers also use the same device model at test time to fool the system. So, in this case, there is no device mismatch in all trials. By comparing the results with the third row, we can see how much degradation is caused if we only consider the non-target trials with the same device.
The fifth row shows similar results when the imposter speakers use device of the same brand as the target speaker but with a different model. Surprisingly, in this case, the degradation is negligible and it means that mobiles from a specific brand (manufacturer) have different recording channel properties.
The degraded female results in the sixth row as compared to the third row show the effect of using a different device model from the same brand for target trials. For males, the filters brings almost the same subsets of trials, which explains the very similar results in this case.
Looking at the first two and the last row of Table TABREF23, one can notice the significantly worse performance obtained for the female trials as compared to males. Note that these three rows include target trials where the devices used for enrollment do not necessarily match the devices used for recording test utterances. On the other hand, in rows 3 to 6, which exclude such mismatched trials, the performance for males and females is comparable. This suggest that the degraded results for females are caused by some problematic trials with device mismatch. The exact reason for this degradation is so far unclear and needs a further investigation.
In the last row of the table, the condition of the second row is relaxed: the target device should have different model possibly from the same brand and imposter device only needs to be from the same brand. In this case, as was expected, the performance degradation is smaller than in the second row.
Experiments and Results ::: Speech Recognition Experiments
In addition to speaker verification, we present several speech recognition experiments on Part3. The experiments were performed with the Kaldi toolkit BIBREF13. For training HMM-based MonoPhone model, only 20 thousands of shortest utterances are used and for other models the whole training data is used. The DNN based acoustic model is a time-delay DNN with low-rank factorized layers and skip connections without i-vector adaptation (a modified network from one of the best performing LibriSpeech recipes). The network is shown in Table TABREF25: there are 16 F-TDNN layers, with dimension 1536 and linear bottleneck layers of dimension 256. The acoustic model is trained for 10 epochs using lattice-free maximum mutual information (LF-MMI) with cross-entropy regularization BIBREF14. Re-scoring is done using a pruned trigram language model and the size of the dictionary is around 90,000 words.
Table TABREF26 shows the results in terms of word error rate (WER) for different evaluated methods. As can be seen, the created database can be used to train well performing and practically usable Persian ASR models.
Conclusions
In this paper, we have described the final version of a large speech corpus, the DeepMine database. It has been collected using crowdsourcing and, according to the best of our knowledge, it is the largest public text-dependent and text-prompted speaker verification database in two languages: Persian and English. In addition, it is the largest text-independent speaker verification evaluation database, making it suitable to robustly evaluate state-of-the-art methods on different conditions. Alongside these appealing properties, it comes with phone-level transcription, making it suitable to train deep neural network models for Persian speech recognition.
We provided several evaluation protocols for each part of the database. The protocols allow researchers to investigate the performance of different methods in various scenarios and study the effects of channels, duration and phrase text on the performance. We also provide two test sets for speech recognition: One normal test set with a few minutes of speech for each speaker and one large test set with more (30 minutes on average) speech that can be used for any speaker adaptation method.
As baseline results, we reported the performance of an i-vector/HMM based method on Persian text-dependent part. Moreover, we conducted speech recognition experiments using conventional HMM-based methods, as well as state-of-the-art deep neural network based method using Kaldi toolkit with promising performance. Text-dependent results have shown that the DeepMine database is more challenging than RSR2015 and RedDots databases.
Acknowledgments
The data collection project was mainly supported by Sharif DeepMine company. The work on the paper was supported by Czech National Science Foundation (GACR) project ""NEUREM3"" No. 19-26934X and the National Programme of Sustainability (NPU II) project ""IT4Innovations excellence in science - LQ1602"".","['three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set']",3880,qasper_e,en,,b66b3c973729802d35f81fa66a7dbb1b846445856909a378,"three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set",561
What is the additive modification to the objective function?,"Introduction
Distributed word representations, commonly referred to as word embeddings BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , serve as elementary building blocks in the course of algorithm design for an expanding range of applications in natural language processing (NLP), including named entity recognition BIBREF4 , BIBREF5 , parsing BIBREF6 , sentiment analysis BIBREF7 , BIBREF8 , and word-sense disambiguation BIBREF9 . Although the empirical utility of word embeddings as an unsupervised method for capturing the semantic or syntactic features of a certain word as it is used in a given lexical resource is well-established BIBREF10 , BIBREF11 , BIBREF12 , an understanding of what these features mean remains an open problem BIBREF13 , BIBREF14 and as such word embeddings mostly remain a black box. It is desirable to be able to develop insight into this black box and be able to interpret what it means, while retaining the utility of word embeddings as semantically-rich intermediate representations. Other than the intrinsic value of this insight, this would not only allow us to explain and understand how algorithms work BIBREF15 , but also set a ground that would facilitate the design of new algorithms in a more deliberate way.
Recent approaches to generating word embeddings (e.g. BIBREF0 , BIBREF2 ) are rooted linguistically in the field of distributed semantics BIBREF16 , where words are taken to assume meaning mainly by their degree of interaction (or lack thereof) with other words in the lexicon BIBREF17 , BIBREF18 . Under this paradigm, dense, continuous vector representations are learned in an unsupervised manner from a large corpus, using the word cooccurrence statistics directly or indirectly, and such an approach is shown to result in vector representations that mathematically capture various semantic and syntactic relations between words BIBREF0 , BIBREF2 , BIBREF3 . However, the dense nature of the learned embeddings obfuscate the distinct concepts encoded in the different dimensions, which renders the resulting vectors virtually uninterpretable. The learned embeddings make sense only in relation to each other and their specific dimensions do not carry explicit information that can be interpreted. However, being able to interpret a word embedding would illuminate the semantic concepts implicitly represented along the various dimensions of the embedding, and reveal its hidden semantic structures.
In the literature, researchers tackled interpretability problem of the word embeddings using different approaches. Several researchers BIBREF19 , BIBREF20 , BIBREF21 proposed algorithms based on non-negative matrix factorization (NMF) applied to cooccurrence variant matrices. Other researchers suggested to obtain interpretable word vectors from existing uninterpretable word vectors by applying sparse coding BIBREF22 , BIBREF23 , by training a sparse auto-encoder to transform the embedding space BIBREF24 , by rotating the original embeddings BIBREF25 , BIBREF26 or by applying transformations based on external semantic datasets BIBREF27 .
Although the above-mentioned approaches provide better interpretability that is measured using a particular method such as word intrusion test, usually the improved interpretability comes with a cost of performance in the benchmark tests such as word similarity or word analogy. One possible explanation for this performance decrease is that the proposed transformations from the original embedding space distort the underlying semantic structure constructed by the original embedding algorithm. Therefore, it can be claimed that a method that learns dense and interpretable word embeddings without inflicting any damage to the underlying semantic learning mechanism is the key to achieve both high performing and interpretable word embeddings.
Especially after the introduction of the word2vec algorithm by Mikolov BIBREF0 , BIBREF1 , there has been a growing interest in algorithms that generate improved word representations under some performance metric. Significant effort is spent on appropriately modifying the objective functions of the algorithms in order to incorporate knowledge from external resources, with the purpose of increasing the performance of the resulting word representations BIBREF28 , BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 , BIBREF35 , BIBREF36 , BIBREF37 . Inspired by the line of work reported in these studies, we propose to use modified objective functions for a different purpose: learning more interpretable dense word embeddings. By doing this, we aim to incorporate semantic information from an external lexical resource into the word embedding so that the embedding dimensions are aligned along predefined concepts. This alignment is achieved by introducing a modification to the embedding learning process. In our proposed method, which is built on top of the GloVe algorithm BIBREF2 , the cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to. For words that do not belong to any one of the word-groups, the cost term is left untouched. Specifically, Roget's Thesaurus BIBREF38 , BIBREF39 is used to derive the concepts and concept word-groups to be used as the external lexical resource for our proposed method. We quantitatively demonstrate the increase in interpretability by using the measure given in BIBREF27 , BIBREF40 as well as demonstrating qualitative results. We also show that the semantic structure of the original embedding has not been harmed in the process since there is no performance loss with standard word-similarity or word-analogy tests.
The paper is organized as follows. In Section SECREF2 , we discuss previous studies related to our work under two main categories: interpretability of word embeddings and joint-learning frameworks where the objective function is modified. In Section SECREF3 , we present the problem framework and provide the formulation within the GloVe BIBREF2 algorithm setting. In Section SECREF4 where our approach is proposed, we motivate and develop a modification to the original objective function with the aim of increasing representation interpretability. In Section SECREF5 , experimental results are provided and the proposed method is quantitatively and qualitatively evaluated. Additionally, in Section SECREF5 , results demonstrating the extent to which the original semantic structure of the embedding space is affected are presented by using word-analogy and word-similarity tests. We conclude the paper in Section SECREF6 .
Related Work
Methodologically, our work is related to prior studies that aim to obtain “improved” word embeddings using external lexical resources, under some performance metric. Previous work in this area can be divided into two main categories: works that i) modify the word embedding learning algorithm to incorporate lexical information, ii) operate on pre-trained embeddings with a post-processing step.
Among works that follow the first approach, BIBREF28 extend the Skip-Gram model by incorporating the word similarity relations extracted from the Paraphrase Database (PPDB) and WordNet BIBREF29 , into the Skip-Gram predictive model as an additional cost term. In BIBREF30 , the authors extend the CBOW model by considering two types of semantic information, termed relational and categorical, to be incorporated into the embeddings during training. For the former type of semantic information, the authors propose the learning of explicit vectors for the different relations extracted from a semantic lexicon such that the word pairs that satisfy the same relation are distributed more homogeneously. For the latter, the authors modify the learning objective such that some weighted average distance is minimized for words under the same semantic category. In BIBREF31 , the authors represent the synonymy and hypernymy-hyponymy relations in terms of inequality constraints, where the pairwise similarity rankings over word triplets are forced to follow an order extracted from a lexical resource. Following their extraction from WordNet, the authors impose these constraints in the form of an additive cost term to the Skip-Gram formulation. Finally, BIBREF32 builds on top of the GloVe algorithm by introducing a regularization term to the objective function that encourages the vector representations of similar words as dictated by WordNet to be similar as well.
Turning our attention to the post-processing approach for enriching word embeddings with external lexical knowledge, BIBREF33 has introduced the retrofitting algorithm that acts on pre-trained embeddings such as Skip-Gram or GloVe. The authors propose an objective function that aims to balance out the semantic information captured in the pre-trained embeddings with the constraints derived from lexical resources such as WordNet, PPDB and FrameNet. One of the models proposed in BIBREF34 extends the retrofitting approach to incorporate the word sense information from WordNet. Similarly, BIBREF35 creates multi-sense embeddings by gathering the word sense information from a lexical resource and learning to decompose the pre-trained embeddings into a convex combination of sense embeddings. In BIBREF36 , the authors focus on improving word embeddings for capturing word similarity, as opposed to mere relatedness. To this end, they introduce the counter-fitting technique which acts on the input word vectors such that synonymous words are attracted to one another whereas antonymous words are repelled, where the synonymy-antonymy relations are extracted from a lexical resource. More recently, the ATTRACT-REPEL algorithm proposed by BIBREF37 improves on counter-fitting by a formulation which imparts the word vectors with external lexical information in mini-batches.
Most of the studies discussed above ( BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 , BIBREF36 , BIBREF37 ) report performance improvements in benchmark tests such as word similarity or word analogy, while BIBREF29 uses a different analysis method (mean reciprocal rank). In sum, the literature is rich with studies aiming to obtain word embeddings that perform better under specific performance metrics. However, less attention has been directed to the issue of interpretability of the word embeddings. In the literature, the problem of interpretability has been tackled using different approaches. BIBREF19 proposed non-negative matrix factorization (NMF) for learning sparse, interpretable word vectors from co-occurrence variant matrices where the resulting vector space is called non-negative sparse embeddigns (NNSE). However, since NMF methods require maintaining a global matrix for learning, they suffer from memory and scale issue. This problem has been addressed in BIBREF20 where an online method of learning interpretable word embeddings from corpora using a modified version of skip-gram model BIBREF0 is proposed. As a different approach, BIBREF21 combined text-based similarity information among words with brain activity based similarity information to improve interpretability using joint non-negative sparse embedding (JNNSE).
A common alternative approach for learning interpretable embeddings is to learn transformations that map pre-trained state-of-the-art embeddings to new interpretable semantic spaces. To obtain sparse, higher dimensional and more interpretable vector spaces, BIBREF22 and BIBREF23 use sparse coding on conventional dense word embeddings. However, these methods learn the projection vectors that are used for the transformation from the word embeddings without supervision. For this reason, labels describing the corresponding semantic categories cannot be provided. An alternative approach was proposed in BIBREF25 , where orthogonal transformations were utilized to increase interpretability while preserving the performance of the underlying embedding. However, BIBREF25 has also shown that total interpretability of an embedding is kept constant under any orthogonal transformation and it can only be redistributed across the dimensions. Rotation algorithms based on exploratory factor analysis (EFA) to preserve the performance of the original word embeddings while improving their interpretability was proposed in BIBREF26 . BIBREF24 proposed to deploy a sparse auto-encoder using pre-trained dense word embeddings to improve interpretability. More detailed investigation of semantic structure and interpretability of word embeddings can be found in BIBREF27 , where a metric was proposed to quantitatively measure the degree of interpretability already present in the embedding vector spaces.
Previous works on interpretability mentioned above, except BIBREF21 , BIBREF27 and our proposed method, do not need external resources, utilization of which has both advantages and disadvantages. Methods that do not use external resources require fewer resources but they also lack the aid of information extracted from these resources.
Problem Description
For the task of unsupervised word embedding extraction, we operate on a discrete collection of lexical units (words) INLINEFORM0 that is part of an input corpus INLINEFORM1 , with number of tokens INLINEFORM2 , sourced from a vocabulary INLINEFORM3 of size INLINEFORM4 . In the setting of distributional semantics, the objective of a word embedding algorithm is to maximize some aggregate utility over the entire corpus so that some measure of “closeness” is maximized for pairs of vector representations INLINEFORM14 for words which, on the average, appear in proximity to one another. In the GloVe algorithm BIBREF2 , which we base our improvements upon, the following objective function is considered: DISPLAYFORM0
In ( EQREF6 ), INLINEFORM0 and INLINEFORM1 stand for word and context vector representations, respectively, for words INLINEFORM2 and INLINEFORM3 , while INLINEFORM4 represents the (possibly weighted) cooccurrence count for the word pair INLINEFORM5 . Intuitively, ( EQREF6 ) represents the requirement that if some word INLINEFORM6 occurs often enough in the context (or vicinity) of another word INLINEFORM7 , then the corresponding word representations should have a large enough inner product in keeping with their large INLINEFORM8 value, up to some bias terms INLINEFORM9 ; and vice versa. INLINEFORM10 in ( EQREF6 ) is used as a discounting factor that prohibits rare cooccurrences from disproportionately influencing the resulting embeddings.
The objective ( EQREF6 ) is minimized using stochastic gradient descent by iterating over the matrix of cooccurrence records INLINEFORM0 . In the GloVe algorithm, for a given word INLINEFORM1 , the final word representation is taken to be the average of the two intermediate vector representations obtained from ( EQREF6 ); i.e, INLINEFORM2 . In the next section, we detail the enhancements made to ( EQREF6 ) for the purposes of enhanced interpretability, using the aforementioned framework as our basis.
Imparting Interpretability
Our approach falls into a joint-learning framework where the distributional information extracted from the corpus is allowed to fuse with the external lexicon-based information. Word-groups extracted from Roget's Thesaurus are directly mapped to individual dimensions of word embeddings. Specifically, the vector representations of words that belong to a particular group are encouraged to have deliberately increased values in a particular dimension that corresponds to the word-group under consideration. This can be achieved by modifying the objective function of the embedding algorithm to partially influence vector representation distributions across their dimensions over an input vocabulary. To do this, we propose the following modification to the GloVe objective in ( EQREF6 ): rCl J = i,j=1V f(Xij)[ (wiTwj + bi + bj -Xij)2
+ k(l=1D INLINEFORM0 iFl g(wi,l) + l=1D INLINEFORM1 j Fl g(wj,l) ) ]. In ( SECREF4 ), INLINEFORM2 denotes the indices for the elements of the INLINEFORM3 th concept word-group which we wish to assign in the vector dimension INLINEFORM4 . The objective ( SECREF4 ) is designed as a mixture of two individual cost terms: the original GloVe cost term along with a second term that encourages embedding vectors of a given concept word-group to achieve deliberately increased values along an associated dimension INLINEFORM5 . The relative weight of the second term is controlled by the parameter INLINEFORM6 . The simultaneous minimization of both objectives ensures that words that are similar to, but not included in, one of these concept word-groups are also “nudged” towards the associated dimension INLINEFORM7 . The trained word vectors are thus encouraged to form a distribution where the individual vector dimensions align with certain semantic concepts represented by a collection of concept word-groups, one assigned to each vector dimension. To facilitate this behaviour, ( SECREF4 ) introduces a monotone decreasing function INLINEFORM8 defined as INLINEFORM9
which serves to increase the total cost incurred if the value of the INLINEFORM0 th dimension for the two vector representations INLINEFORM1 and INLINEFORM2 for a concept word INLINEFORM3 with INLINEFORM4 fails to be large enough. INLINEFORM5 is also shown in Fig. FIGREF7 .
The objective ( SECREF4 ) is minimized using stochastic gradient descent over the cooccurrence records INLINEFORM0 . Intuitively, the terms added to ( SECREF4 ) in comparison with ( EQREF6 ) introduce the effect of selectively applying a positive step-type input to the original descent updates of ( EQREF6 ) for concept words along their respective vector dimensions, which influences the dimension value in the positive direction. The parameter INLINEFORM1 in ( SECREF4 ) allows for the adjustment of the magnitude of this influence as needed.
In the next section, we demonstrate the feasibility of this approach by experiments with an example collection of concept word-groups extracted from Roget's Thesaurus.
Experiments and Results
We first identified 300 concepts, one for each dimension of the 300-dimensional vector representation, by employing Roget's Thesaurus. This thesaurus follows a tree structure which starts with a Root node that contains all the words and phrases in the thesaurus. The root node is successively split into Classes and Sections, which are then (optionally) split into Subsections of various depths, finally ending in Categories, which constitute the smallest unit of word/phrase collections in the structure. The actual words and phrases descend from these Categories, and make up the leaves of the tree structure. We note that a given word typically appears in multiple categories corresponding to the different senses of the word. We constructed concept word-groups from Roget's Thesaurus as follows: We first filtered out the multi-word phrases and the relatively obscure terms from the thesaurus. The obscure terms were identified by checking them against a vocabulary extracted from Wikipedia. We then obtained 300 word-groups as the result of a partitioning operation applied to the subtree that ends with categories as its leaves. The partition boundaries, hence the resulting word-groups, can be chosen in many different ways. In our proposed approach, we have chosen to determine this partitioning by traversing this tree structure from the root node in breadth-first order, and by employing a parameter INLINEFORM0 for the maximum size of a node. Here, the size of a node is defined as the number of unique words that ever-descend from that node. During the traversal, if the size of a given node is less than this threshold, we designate the words that ultimately descend from that node as a concept word-group. Otherwise, if the node has children, we discard the node, and queue up all its children for further consideration. If this node does not have any children, on the other hand, the node is truncated to INLINEFORM1 elements with the highest frequency-ranks, and the resulting words are designated as a concept word-group. We note that the choice of INLINEFORM2 greatly affects the resulting collection of word-groups: Excessively large values result in few word-groups that greatly overlap with one another, while overly small values result in numerous tiny word-groups that fail to adequately represent a concept. We experimentally determined that a INLINEFORM3 value of 452 results in the most healthy number of relatively large word-groups (113 groups with size INLINEFORM4 100), while yielding a preferably small overlap amongst the resulting word-groups (with average overlap size not exceeding 3 words). A total of 566 word-groups were thus obtained. 259 smallest word-groups (with size INLINEFORM5 38) were discarded to bring down the number of word-groups to 307. Out of these, 7 groups with the lowest median frequency-rank were further discarded, which yields the final 300 concept word-groups used in the experiments. We present some of the resulting word-groups in Table TABREF9 .
By using the concept word-groups, we have trained the GloVe algorithm with the proposed modification given in Section SECREF4 on a snapshot of English Wikipedia measuring 8GB in size, with the stop-words filtered out. Using the parameters given in Table TABREF10 , this resulted in a vocabulary size of 287,847. For the weighting parameter in Eq. SECREF4 , we used a value of INLINEFORM0 . The algorithm was trained over 20 iterations. The GloVe algorithm without any modifications was also trained as a baseline with the same parameters. In addition to the original GloVe algorithm, we compare our proposed method with previous studies that aim to obtain interpretable word vectors. We train the improved projected gradient model proposed in BIBREF20 to obtain word vectors (called OIWE-IPG) using the same corpus we use to train GloVe and our proposed method. Using the methods proposed in BIBREF23 , BIBREF26 , BIBREF24 on our baseline GloVe embeddings, we obtain SOV, SPINE and Parsimax (orthogonal) word representations, respectively. We train all the models with the proposed parameters. However, in BIBREF26 , the authors show results for a relatively small vocabulary of 15,000 words. When we trained their model on our baseline GloVe embeddings with a large vocabulary of size 287,847, the resulting vectors performed significantly poor on word similarity tasks compared to the results presented in their paper. In addition, Parsimax (orthogonal) word vectors obtained using method in BIBREF26 are nearly identical to the baseline vectors (i.e. learned orthogonal transformation matrix is very close to identity). Therefore, Parsimax (orthogonal) yields almost same results with baseline vectors in all evaluations. We evaluate the interpretability of the resulting embeddings qualitatively and quantitatively. We also test the performance of the embeddings on word similarity and word analogy tests.
In our experiments, vocabulary size is close to 300,000 while only 16,242 unique words of the vocabulary are present in the concept groups. Furthermore, only dimensions that correspond to the concept group of the word will be updated due to the additional cost term. Given that these concept words can belong to multiple concept groups (2 on average), only 33,319 parameters are updated. There are 90 million individual parameters present for the 300,000 word vectors of size 300. Of these parameters, only approximately 33,000 are updated by the additional cost term.
Qualitative Evaluation for Interpretability
In Fig. FIGREF13 , we demonstrate the particular way in which the proposed algorithm ( SECREF4 ) influences the vector representation distributions. Specifically, we consider, for illustration, the 32nd dimension values for the original GloVe algorithm and our modified version, restricting the plots to the top-1000 words with respect to their frequency ranks for clarity of presentation. In Fig. FIGREF13 , the words in the horizontal axis are sorted in descending order with respect to the values at the 32nd dimension of their word embedding vectors coming from the original GloVe algorithm. The dimension values are denoted with blue and red/green markers for the original and the proposed algorithms, respectively. Additionally, the top-50 words that achieve the greatest 32nd dimension values among the considered 1000 words are emphasized with enlarged markers, along with text annotations. In the presented simulation of the proposed algorithm, the 32nd dimension values are encoded with the concept JUDGMENT, which is reflected as an increase in the dimension values for words such as committee, academy, and article. We note that these words (red) are not part of the pre-determined word-group for the concept JUDGMENT, in contrast to words such as award, review and account (green) which are. This implies that the increase in the corresponding dimension values seen for these words is attributable to the joint effect of the first term in ( SECREF4 ) which is inherited from the original GloVe algorithm, in conjunction with the remaining terms in the proposed objective expression ( SECREF4 ). This experiment illustrates that the proposed algorithm is able to impart the concept of JUDGMENT on its designated vector dimension above and beyond the supplied list of words belonging to the concept word-group for that dimension. We also present the list of words with the greatest dimension value for the dimensions 11, 13, 16, 31, 36, 39, 41, 43 and 79 in Table TABREF11 . These dimensions are aligned/imparted with the concepts that are given in the column headers. In Table TABREF11 , the words that are highlighted with green denote the words that exist in the corresponding word-group obtained from Roget's Thesaurus (and are thus explicitly forced to achieve increased dimension values), while the red words denote the words that achieve increased dimension values by virtue of their cooccurrence statistics with the thesaurus-based words (indirectly, without being explicitly forced). This again illustrates that a semantic concept can indeed be coded to a vector dimension provided that a sensible lexical resource is used to guide semantically related words to the desired vector dimension via the proposed objective function in ( SECREF4 ). Even the words that do not appear in, but are semantically related to, the word-groups that we formed using Roget's Thesaurus, are indirectly affected by the proposed algorithm. They also reflect the associated concepts at their respective dimensions even though the objective functions for their particular vectors are not modified. This point cannot be overemphasized. Although the word-groups extracted from Roget's Thesaurus impose a degree of supervision to the process, the fact that the remaining words in the entire vocabulary are also indirectly affected makes the proposed method a semi-supervised approach that can handle words that are not in these chosen word-groups. A qualitative example of this result can be seen in the last column of Table TABREF11 . It is interesting to note the appearance of words such as guerilla, insurgency, mujahideen, Wehrmacht and Luftwaffe in addition to the more obvious and straightforward army, soldiers and troops, all of which are not present in the associated word-group WARFARE.
Most of the dimensions we investigated exhibit similar behaviour to the ones presented in Table TABREF11 . Thus generally speaking, we can say that the entries in Table TABREF11 are representative of the great majority. However, we have also specifically looked for dimensions that make less sense and determined a few such dimensions which are relatively less satisfactory. These less satisfactory examples are given in Table TABREF14 . These examples are also interesting in that they shed insight into the limitations posed by polysemy and existence of very rare outlier words.
Quantitative Evaluation for Interpretability
One of the main goals of this study is to improve the interpretability of dense word embeddings by aligning the dimensions with predefined concepts from a suitable lexicon. A quantitative measure is required to reliably evaluate the achieved improvement. One of the methods proposed to measure the interpretability is the word intrusion test BIBREF41 . But, this method is expensive to apply since it requires evaluations from multiple human evaluators for each embedding dimension. In this study, we use a semantic category-based approach based on the method and category dataset (SEMCAT) introduced in BIBREF27 to quantify interpretability. Specifically, we apply a modified version of the approach presented in BIBREF40 in order to consider possible sub-groupings within the categories in SEMCAT. Interpretability scores are calculated using Interpretability Score (IS) as given below:
DISPLAYFORM0
In ( EQREF17 ), INLINEFORM0 and INLINEFORM1 represents the interpretability scores in the positive and negative directions of the INLINEFORM2 dimension ( INLINEFORM3 , INLINEFORM4 number of dimensions in the embedding space) of word embedding space for the INLINEFORM5 category ( INLINEFORM6 , INLINEFORM7 is number of categories in SEMCAT, INLINEFORM8 ) in SEMCAT respectively. INLINEFORM9 is the set of words in the INLINEFORM10 category in SEMCAT and INLINEFORM11 is the number of words in INLINEFORM12 . INLINEFORM13 corresponds to the minimum number of words required to construct a semantic category (i.e. represent a concept). INLINEFORM14 represents the set of INLINEFORM15 words that have the highest ( INLINEFORM16 ) and lowest ( INLINEFORM17 ) values in INLINEFORM18 dimension of the embedding space. INLINEFORM19 is the intersection operator and INLINEFORM20 is the cardinality operator (number of elements) for the intersecting set. In ( EQREF17 ), INLINEFORM21 gives the interpretability score for the INLINEFORM22 dimension and INLINEFORM23 gives the average interpretability score of the embedding space.
Fig. FIGREF18 presents the measured average interpretability scores across dimensions for original GloVe embeddings, for the proposed method and for the other four methods we compare, along with a randomly generated embedding. Results are calculated for the parameters INLINEFORM0 and INLINEFORM1 . Our proposed method significantly improves the interpretability for all INLINEFORM2 compared to the original GloVe approach. Our proposed method is second to only SPINE in increasing interpretability. However, as we will experimentally demonstrate in the next subsection, in doing this, SPINE almost entirely destroys the underlying semantic structure of the word embeddings, which is the primary function of a word embedding.
The proposed method and interpretability measurements are both based on utilizing concepts represented by word-groups. Therefore it is expected that there will be higher interpretability scores for some of the dimensions for which the imparted concepts are also contained in SEMCAT. However, by design, word groups that they use are formed by using different sources and are independent. Interpretability measurements use SEMCAT while our proposed method utilizes Roget's Thesaurus.
Intrinsic Evaluation of the Embeddings
It is necessary to show that the semantic structure of the original embedding has not been damaged or distorted as a result of aligning the dimensions with given concepts, and that there is no substantial sacrifice involved from the performance that can be obtained with the original GloVe. To check this, we evaluate performances of the proposed embeddings on word similarity BIBREF42 and word analogy BIBREF0 tests. We compare the results with the original embeddings and the three alternatives excluding Parsimax BIBREF26 since orthogonal transformations will not affect the performance of the original embeddings on these tests.
Word similarity test measures the correlation between word similarity scores obtained from human evaluation (i.e. true similarities) and from word embeddings (usually using cosine similarity). In other words, this test quantifies how well the embedding space reflects human judgements in terms of similarities between different words. The correlation scores for 13 different similarity test sets are reported in Table TABREF20 . We observe that, let alone a reduction in performance, the obtained scores indicate an almost uniform improvement in the correlation values for the proposed algorithm, outperforming all the alternatives in almost all test sets. Categories from Roget's thesaurus are groupings of words that are similar in some sense which the original embedding algorithm may fail to capture. These test results signify that the semantic information injected into the algorithm by the additional cost term is significant enough to result in a measurable improvement. It should also be noted that scores obtained by SPINE is unacceptably low on almost all tests indicating that it has achieved its interpretability performance at the cost of losing its semantic functions.
Word analogy test is introduced in BIBREF1 and looks for the answers of the questions that are in the form of ""X is to Y, what Z is to ?"" by applying simple arithmetic operations to vectors of words X, Y and Z. We present precision scores for the word analogy tests in Table TABREF21 . It can be seen that the alternative approaches that aim to improve interpretability, have poor performance on the word analogy tests. However, our proposed method has comparable performance with the original GloVe embeddings. Our method outperforms GloVe in semantic analogy test set and in overall results, while GloVe performs slightly better in syntactic test set. This comparable performance is mainly due to the cost function of our proposed method that includes the original objective of the GloVe.
To investigate the effect of the additional cost term on the performance improvement in the semantic analogy test, we present Table TABREF22 . In particular, we present results for the cases where i) all questions in the dataset are considered, ii) only the questions that contains at least one concept word are considered, iii) only the questions that consist entirely of concept words are considered. We note specifically that for the last case, only a subset of the questions under the semantic category family.txt ended up being included. We observe that for all three scenarios, our proposed algorithm results in an improvement in the precision scores. However, the greatest performance increase is seen for the last scenario, which underscores the extent to which the semantic features captured by embeddings can be improved with a reasonable selection of the lexical resource from which the concept word-groups were derived.
Conclusion
We presented a novel approach to impart interpretability into word embeddings. We achieved this by encouraging different dimensions of the vector representation to align with predefined concepts, through the addition of an additional cost term in the optimization objective of the GloVe algorithm that favors a selective increase for a pre-specified input of concept words along each dimension.
We demonstrated the efficacy of this approach by applying qualitative and quantitative evaluations for interpretability. We also showed via standard word-analogy and word-similarity tests that the semantic coherence of the original vector space is preserved, even slightly improved. We have also performed and reported quantitative comparisons with several other methods for both interpretabilty increase and preservation of semantic coherence. Upon inspection of Fig. FIGREF18 and Tables TABREF20 , TABREF21 , and TABREF22 altogether, it should be noted that our proposed method achieves both of the objectives simultaneously, increased interpretability and preservation of the intrinsic semantic structure.
An important point was that, while it is expected for words that are already included in the concept word-groups to be aligned together since their dimensions are directly updated with the proposed cost term, it was also observed that words not in these groups also aligned in a meaningful manner without any direct modification to their cost function. This indicates that the cost term we added works productively with the original cost function of GloVe to handle words that are not included in the original concept word-groups, but are semantically related to those word-groups. The underlying mechanism can be explained as follows. While the outside lexical resource we introduce contains a relatively small number of words compared to the total number of words, these words and the categories they represent have been carefully chosen and in a sense, ""densely span"" all the words in the language. By saying ""span"", we mean they cover most of the concepts and ideas in the language without leaving too many uncovered areas. With ""densely"" we mean all areas are covered with sufficient strength. In other words, this subset of words is able to constitute a sufficiently strong skeleton, or scaffold. Now remember that GloVe works to align or bring closer related groups of words, which will include words from the lexical source. So the joint action of aligning the words with the predefined categories (introduced by us) and aligning related words (handled by GloVe) allows words not in the lexical groups to also be aligned meaningfully. We may say that the non-included words are ""pulled along"" with the included words by virtue of the ""strings"" or ""glue"" that is provided by GloVe. In numbers, the desired effect is achieved by manipulating less than only 0.05% of parameters of the entire word vectors. Thus, while there is a degree of supervision coming from the external lexical resource, the rest of the vocabulary is also aligned indirectly in an unsupervised way. This may be the reason why, unlike earlier proposed approaches, our method is able to achieve increasing interpretability without destroying underlying semantic structure, and consequently without sacrificing performance in benchmark tests.
Upon inspecting the 2nd column of Table TABREF14 , where qualitative results for concept TASTE are presented, another insight regarding the learning mechanism of our proposed approach can be made. Here it seems understandable that our proposed approach, along with GloVe, brought together the words taste and polish, and then the words Polish and, for instance, Warsaw are brought together by GloVe. These examples are interesting in that they shed insight into how GloVe works and the limitations posed by polysemy. It should be underlined that the present approach is not totally incapable of handling polysemy, but cannot do so perfectly. Since related words are being clustered, sufficiently well-connected words that do not meaningfully belong along with others will be appropriately ""pulled away"" from that group by several words, against the less effective, inappropriate pull of a particular word. Even though polish with lowercase ""p"" belongs where it is, it is attracting Warsaw to itself through polysemy and this is not meaningful. Perhaps because Warsaw is not a sufficiently well-connected word, it ends being dragged along, although words with greater connectedness to a concept group might have better resisted such inappropriate attractions.
In this study, we used the GloVe algorithm as the underlying dense word embedding scheme to demonstrate our approach. However, we stress that it is possible for our approach to be extended to other word embedding algorithms which have a learning routine consisting of iterations over cooccurrence records, by making suitable adjustments in the objective function. Since word2vec model is also based on the coocurrences of words in a sliding window through a large corpus, we expect that our approach can also be applied to word2vec after making suitable adjustments, which can be considered as an immediate future work for our approach. Although the semantic concepts are encoded in only one direction (positive) within the embedding dimensions, it might be beneficial to pursue future work that also encodes opposite concepts, such as good and bad, in two opposite directions of the same dimension.
The proposed methodology can also be helpful in computational cross-lingual studies, where the similarities are explored across the vector spaces of different languages BIBREF43 , BIBREF44 .","[""The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,"", 'An additive term added to the cost function for any one of the words of concept word-groups']",6244,qasper_e,en,,14a3bbb82b8513481e056adb121e5b37e648bb44e97406f2,"The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,"", 'An additive term added to the cost function for any one of the words of concept word-groups",548
How do they evaluate their resulting word embeddings?,"Introduction
Low dimensional word representations (embeddings) have become a key component in modern NLP systems for language modeling, parsing, sentiment classification, and many others. These embeddings are usually derived by employing the distributional hypothesis: that similar words appear in similar contexts BIBREF0 .
The models that perform the word embedding can be divided into two classes: predictive, which learn a target or context word distribution, and counting, which use a raw, weighted, or factored word-context co-occurrence matrix BIBREF1 . The most well-known predictive model, which has become eponymous with word embedding, is word2vec BIBREF2 . Popular counting models include PPMI-SVD BIBREF3 , GloVe BIBREF4 , and LexVec BIBREF5 .
These models all learn word-level representations, which presents two main problems: 1) Learned information is not explicitly shared among the representations as each word has an independent vector. 2) There is no clear way to represent out-of-vocabulary (OOV) words.
fastText BIBREF6 addresses these issues in the Skip-gram word2vec model by representing a word by the sum of a unique vector and a set of shared character n-grams (from hereon simply referred to as n-grams) vectors. This addresses both issues above as learned information is shared through the n-gram vectors and from these OOV word representations can be constructed.
In this paper we propose incorporating subword information into counting models using a strategy similar to fastText.
We use LexVec as the counting model as it generally outperforms PPMI-SVD and GloVe on intrinsic and extrinsic evaluations BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , but the method proposed here should transfer to GloVe unchanged.
The LexVec objective is modified such that a word's vector is the sum of all its subword vectors.
We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords offer any advantage over simple n-grams.
To evaluate the impact subword information has on in-vocabulary (IV) word representations, we run intrinsic evaluations consisting of word similarity and word analogy tasks. The incorporation of subword information results in similar gains (and losses) to that of fastText over Skip-gram. Whereas incorporating n-gram subwords tends to capture more syntactic information, unsupervised morphemes better preserve semantics while also improving syntactic results. Given that intrinsic performance can correlate poorly with performance on downstream tasks BIBREF12 , we also conduct evaluation using the VecEval suite of tasks BIBREF13 , in which
all subword models, including fastText, show no significant improvement over word-level models.
We verify the model's ability to represent OOV words by quantitatively evaluating nearest-neighbors. Results show that, like fastText, both LexVec n-gram and (to a lesser degree) unsupervised morpheme models give coherent answers.
This paper discusses related word ( $§$ ""Related Work"" ), introduces the subword LexVec model ( $§$ ""Subword LexVec"" ), describes experiments ( $§$ ""Materials"" ), analyzes results ( $§$ ""Results"" ), and concludes with ideas for future works ( $§$ ""Conclusion and Future Work"" ).
Related Work
Word embeddings that leverage subword information were first introduced by BIBREF14 which represented a word of as the sum of four-gram vectors obtained running an SVD of a four-gram to four-gram co-occurrence matrix. Our model differs by learning the subword vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed.
There are many models that use character-level subword information to form word representations BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , as well as fastText (the model on which we base our work). Closely related are models that use morphological segmentation in learning word representations BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 , BIBREF25 . Our model also uses n-grams and morphological segmentation, but it performs explicit matrix factorization to learn subword and word representations, unlike these related models which mostly use neural networks.
Finally, BIBREF26 and BIBREF27 retrofit morphological information onto pre-trained models. These differ from our work in that we incorporate morphological information at training time, and that only BIBREF26 is able to generate embeddings for OOV words.
Subword LexVec
The LexVec BIBREF7 model factorizes the PPMI-weighted word-context co-occurrence matrix using stochastic gradient descent.
$$PPMI_{wc} = max(0, \log \frac{M_{wc} \; M_{**}}{ M_{w*} \; M_{*c} })$$   (Eq. 3)
where $M$ is the word-context co-occurrence matrix constructed by sliding a window of fixed size centered over every target word
$w$ in the subsampled BIBREF2 training corpus and incrementing cell $M_{wc}$ for every context word $c$ appearing within this window (forming a $(w,c)$ pair). LexVec adjusts the PPMI matrix using context distribution smoothing BIBREF3 .
With the PPMI matrix calculated, the sliding window process is repeated and the following loss functions are minimized for every observed $(w,c)$ pair and target word $w$ :
$$L_{wc} &= \frac{1}{2} (u_w^\top v_c - PPMI_{wc})^2 \\  L_{w} &= \frac{1}{2} \sum \limits _{i=1}^k{\mathbf {E}_{c_i \sim P_n(c)} (u_w^\top v_{c_i} - PPMI_{wc_i})^2 }$$   (Eq. 4)
where $u_w$ and $v_c$ are $d$ -dimensional word and context vectors. The second loss function describes how, for each target word, $k$ negative samples BIBREF2 are drawn from the smoothed context unigram distribution.
Given a set of subwords $S_w$ for a word $w$ , we follow fastText and replace $u_w$ in eq:lexvec2,eq:lexvec3 by $u^{\prime }_w$ such that:
$$u^{\prime }_w = \frac{1}{|S_w| + 1} (u_w + \sum _{s \in S_w} q_{hash(s)})$$   (Eq. 5)
such that a word is the sum of its word vector and its $d$ -dimensional subword vectors $q_x$ . The number of possible subwords is very large so the function $hash(s)$ hashes a subword to the interval $[1, buckets]$ . For OOV words,
$$u^{\prime }_w = \frac{1}{|S_w|} \sum _{s \in S_w} q_{hash(s)}$$   (Eq. 7)
We compare two types of subwords: simple n-grams (like fastText) and unsupervised morphemes. For example, given the word “cat”, we mark beginning and end with angled brackets and use all n-grams of length 3 to 6 as subwords, yielding $S_{\textnormal {cat}} = \lbrace  \textnormal {$ $ ca, at$ $, cat} \rbrace $ . Morfessor BIBREF11 is used to probabilistically segment words into morphemes. The Morfessor model is trained using raw text so it is entirely unsupervised. For the word “subsequent”, we get $S_{\textnormal {subsequent}} = \lbrace  \textnormal {$ $ sub, sequent$ $} \rbrace $ .
Materials
Our experiments aim to measure if the incorporation of subword information into LexVec results in similar improvements as observed in moving from Skip-gram to fastText, and whether unsupervised morphemes offer any advantage over n-grams. For IV words, we perform intrinsic evaluation via word similarity and word analogy tasks, as well as downstream tasks. OOV word representation is tested through qualitative nearest-neighbor analysis.
All models are trained using a 2015 dump of Wikipedia, lowercased and using only alphanumeric characters. Vocabulary is limited to words that appear at least 100 times for a total of 303517 words. Morfessor is trained on this vocabulary list.
We train the standard LexVec (LV), LexVec using n-grams (LV-N), and LexVec using unsupervised morphemes (LV-M) using the same hyper-parameters as BIBREF7 ( $\textnormal {window} = 2$ , $\textnormal {initial learning rate} = .025$ , $\textnormal {subsampling} = 10^{-5}$ , $\textnormal {negative samples} = 5$ , $\textnormal {context distribution smoothing} = .75$ , $\textnormal {positional contexts} = \textnormal {True}$ ).
Both Skip-gram (SG) and fastText (FT) are trained using the reference implementation of fastText with the hyper-parameters given by BIBREF6 ( $\textnormal {window} = 5$ , $\textnormal {initial learning rate} = .025$ , $\textnormal {subsampling} = 10^{-4}$ , $\textnormal {negative samples} = 5$ ).
All five models are run for 5 iterations over the training corpus and generate 300 dimensional word representations. LV-N, LV-M, and FT use 2000000 buckets when hashing subwords.
For word similarity evaluations, we use the WordSim-353 Similarity (WS-Sim) and Relatedness (WS-Rel) BIBREF28 and SimLex-999 (SimLex) BIBREF29 datasets, and the Rare Word (RW) BIBREF20 dataset to verify if subword information improves rare word representation. Relationships are measured using the Google semantic (GSem) and syntactic (GSyn) analogies BIBREF2 and the Microsoft syntactic analogies (MSR) dataset BIBREF30 .
We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings.
Finally, we use LV-N, LV-M, and FT to generate OOV word representations for the following words: 1) “hellooo”: a greeting commonly used in instant messaging which emphasizes a syllable. 2) “marvelicious”: a made-up word obtained by merging “marvelous” and “delicious”. 3) “louisana”: a misspelling of the proper name “Louisiana”. 4) “rereread”: recursive use of prefix “re”. 5) “tuzread”: made-up prefix “tuz”.
Results
Results for IV evaluation are shown in tab:intrinsic, and for OOV in tab:oov.
Like in FT, the use of subword information in both LV-N and LV-M results in 1) better representation of rare words, as evidenced by the increase in RW correlation, and 2) significant improvement on the GSyn and MSR tasks, in evidence of subwords encoding information about a word's syntactic function (the suffix “ly”, for example, suggests an adverb).
There seems to a trade-off between capturing semantics and syntax as in both LV-N and FT there is an accompanying decrease on the GSem tasks in exchange for gains on the GSyn and MSR tasks. Morphological segmentation in LV-M appears to favor syntax less strongly than do simple n-grams.
On the downstream tasks, we only observe statistically significant ( $p < .05$ under a random permutation test) improvement on the chunking task, and it is a very small gain. We attribute this to both regular and subword models having very similar quality on frequent IV word representation. Statistically, these are the words are that are most likely to appear in the downstream task instances, and so the superior representation of rare words
has, due to their nature, little impact on overall accuracy. Because in all tasks OOV words are mapped to the “ $\langle $ unk $\rangle $ ” token, the subword models are not being used to the fullest, and in future work we will investigate whether generating representations for all words improves task performance.
In OOV representation (tab:oov), LV-N and FT work almost identically, as is to be expected. Both find highly coherent neighbors for the words “hellooo”, “marvelicious”, and “rereread”. Interestingly, the misspelling of “louisana” leads to coherent name-like neighbors, although none is the expected correct spelling “louisiana”. All models stumble on the made-up prefix “tuz”. A possible fix would be to down-weigh very rare subwords in the vector summation. LV-M is less robust than LV-N and FT on this task as it is highly sensitive to incorrect segmentation, exemplified in the “hellooo” example.
Finally, we see that nearest-neighbors are a mixture of similarly pre/suffixed words. If these pre/suffixes are semantic, the neighbors are semantically related, else if syntactic they have similar syntactic function. This suggests that it should be possible to get tunable representations which are more driven by semantics or syntax by a weighted summation of subword vectors, given we can identify whether a pre/suffix is semantic or syntactic in nature and weigh them accordingly. This might be possible without supervision using corpus statistics as syntactic subwords are likely to be more frequent, and so could be down-weighted for more semantic representations. This is something we will pursue in future work.
Conclusion and Future Work
In this paper, we incorporated subword information (simple n-grams and unsupervised morphemes) into the LexVec word embedding model and evaluated its impact on the resulting IV and OOV word vectors. Like fastText, subword LexVec learns better representations for rare words than its word-level counterpart. All models generated coherent representations for OOV words, with simple n-grams demonstrating more robustness than unsupervised morphemes. In future work, we will verify whether using OOV representations in downstream tasks improves performance. We will also explore the trade-off between semantics and syntax when subword information is used.","['We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings.']",2083,qasper_e,en,,1443430475309d05eb23043251943ee0b3c3ade6f0730fdc,"We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings.",489
How were the datasets annotated?,"Introduction
The lack of annotated training and evaluation data for many tasks and domains hinders the development of computational models for the majority of the world's languages BIBREF0, BIBREF1, BIBREF2. The necessity to guide and advance multilingual and cross-lingual NLP through annotation efforts that follow cross-lingually consistent guidelines has been recently recognized by collaborative initiatives such as the Universal Dependency (UD) project BIBREF3. The latest version of UD (as of March 2020) covers more than 70 languages. Crucially, this resource continues to steadily grow and evolve through the contributions of annotators from across the world, extending the UD's reach to a wide array of typologically diverse languages. Besides steering research in multilingual parsing BIBREF4, BIBREF5, BIBREF6 and cross-lingual parser transfer BIBREF7, BIBREF8, BIBREF9, the consistent annotations and guidelines have also enabled a range of insightful comparative studies focused on the languages' syntactic (dis)similarities BIBREF10, BIBREF11, BIBREF12.
Inspired by the UD work and its substantial impact on research in (multilingual) syntax, in this article we introduce Multi-SimLex, a suite of manually and consistently annotated semantic datasets for 12 different languages, focused on the fundamental lexical relation of semantic similarity BIBREF13, BIBREF14. For any pair of words, this relation measures whether their referents share the same (functional) features, as opposed to general cognitive association captured by co-occurrence patterns in texts (i.e., the distributional information). Datasets that quantify the strength of true semantic similarity between concept pairs such as SimLex-999 BIBREF14 or SimVerb-3500 BIBREF15 have been instrumental in improving models for distributional semantics and representation learning. Discerning between semantic similarity and relatedness/association is not only crucial for theoretical studies on lexical semantics (see §SECREF2), but has also been shown to benefit a range of language understanding tasks in NLP. Examples include dialog state tracking BIBREF16, BIBREF17, spoken language understanding BIBREF18, BIBREF19, text simplification BIBREF20, BIBREF21, BIBREF22, dictionary and thesaurus construction BIBREF23, BIBREF24.
Despite the proven usefulness of semantic similarity datasets, they are available only for a small and typologically narrow sample of resource-rich languages such as German, Italian, and Russian BIBREF25, whereas some language types and low-resource languages typically lack similar evaluation data. Even if some resources do exist, they are limited in their size (e.g., 500 pairs in Turkish BIBREF26, 500 in Farsi BIBREF27, or 300 in Finnish BIBREF28) and coverage (e.g., all datasets which originated from the original English SimLex-999 contain only high-frequent concepts, and are dominated by nouns). This is why, as our departure point, we introduce a larger and more comprehensive English word similarity dataset spanning 1,888 concept pairs (see §SECREF4).
Most importantly, semantic similarity datasets in different languages have been created using heterogeneous construction procedures with different guidelines for translation and annotation, as well as different rating scales. For instance, some datasets were obtained by directly translating the English SimLex-999 in its entirety BIBREF25, BIBREF16 or in part BIBREF28. Other datasets were created from scratch BIBREF26 and yet others sampled English concept pairs differently from SimLex-999 and then translated and reannotated them in target languages BIBREF27. This heterogeneity makes these datasets incomparable and precludes systematic cross-linguistic analyses. In this article, consolidating the lessons learned from previous dataset construction paradigms, we propose a carefully designed translation and annotation protocol for developing monolingual Multi-SimLex datasets with aligned concept pairs for typologically diverse languages. We apply this protocol to a set of 12 languages, including a mixture of major languages (e.g., Mandarin, Russian, and French) as well as several low-resource ones (e.g., Kiswahili, Welsh, and Yue Chinese). We demonstrate that our proposed dataset creation procedure yields data with high inter-annotator agreement rates (e.g., the average mean inter-annotator agreement for Welsh is 0.742).
The unified construction protocol and alignment between concept pairs enables a series of quantitative analyses. Preliminary studies on the influence that polysemy and cross-lingual variation in lexical categories (see §SECREF6) have on similarity judgments are provided in §SECREF5. Data created according to Multi-SimLex protocol also allow for probing into whether similarity judgments are universal across languages, or rather depend on linguistic affinity (in terms of linguistic features, phylogeny, and geographical location). We investigate this question in §SECREF25. Naturally, Multi-SimLex datasets can be used as an intrinsic evaluation benchmark to assess the quality of lexical representations based on monolingual, joint multilingual, and transfer learning paradigms. We conduct a systematic evaluation of several state-of-the-art representation models in §SECREF7, showing that there are large gaps between human and system performance in all languages. The proposed construction paradigm also supports the automatic creation of 66 cross-lingual Multi-SimLex datasets by interleaving the monolingual ones. We outline the construction of the cross-lingual datasets in §SECREF6, and then present a quantitative evaluation of a series of cutting-edge cross-lingual representation models on this benchmark in §SECREF8.
Contributions. We now summarize the main contributions of this work:
1) Building on lessons learned from prior work, we create a more comprehensive lexical semantic similarity dataset for the English language spanning a total of 1,888 concept pairs balanced with respect to similarity, frequency, and concreteness, and covering four word classes: nouns, verbs, adjectives and, for the first time, adverbs. This dataset serves as the main source for the creation of equivalent datasets in several other languages.
2) We present a carefully designed and rigorous language-agnostic translation and annotation protocol. These well-defined guidelines will facilitate the development of future Multi-SimLex datasets for other languages. The proposed protocol eliminates some crucial issues with prior efforts focused on the creation of multi-lingual semantic resources, namely: i) limited coverage; ii) heterogeneous annotation guidelines; and iii) concept pairs which are semantically incomparable across different languages.
3) We offer to the community manually annotated evaluation sets of 1,888 concept pairs across 12 typologically diverse languages, and 66 large cross-lingual evaluation sets. To the best of our knowledge, Multi-SimLex is the most comprehensive evaluation resource to date focused on the relation of semantic similarity.
4) We benchmark a wide array of recent state-of-the-art monolingual and cross-lingual word representation models across our sample of languages. The results can serve as strong baselines that lay the foundation for future improvements.
5) We present a first large-scale evaluation study on the ability of encoders pretrained on language modeling (such as bert BIBREF29 and xlm BIBREF30) to reason over word-level semantic similarity in different languages. To our own surprise, the results show that monolingual pretrained encoders, even when presented with word types out of context, are sometimes competitive with static word embedding models such as fastText BIBREF31 or word2vec BIBREF32. The results also reveal a huge gap in performance between massively multilingual pretrained encoders and language-specific encoders in favor of the latter: our findings support other recent empirical evidence related to the “curse of multilinguality” BIBREF33, BIBREF34 in representation learning.
6) We make all of these resources available on a website which facilitates easy creation, submission and sharing of Multi-Simlex-style datasets for a larger number of languages. We hope that this will yield an even larger repository of semantic resources that inspire future advances in NLP within and across languages.
In light of the success of Universal Dependencies BIBREF3, we hope that our initiative will instigate a collaborative public effort with established and clear-cut guidelines that will result in additional Multi-SimLex datasets in a large number of languages in the near future. Moreover, we hope that it will provide means to advance our understanding of distributional and lexical semantics across a large number of languages. All monolingual and cross-lingual Multi-SimLex datasets–along with detailed translation and annotation guidelines–are available online at: https://multisimlex.com/.
Lexical Semantic Similarity ::: Similarity and Association
The focus of the Multi-SimLex initiative is on the lexical relation of pure semantic similarity. For any pair of words, this relation measures whether their referents share the same features. For instance, graffiti and frescos are similar to the extent that they are both forms of painting and appear on walls. This relation can be contrasted with the cognitive association between two words, which often depends on how much their referents interact in the real world, or are found in the same situations. For instance, a painter is easily associated with frescos, although they lack any physical commonalities. Association is also known in the literature under other names: relatedness BIBREF13, topical similarity BIBREF35, and domain similarity BIBREF36.
Semantic similarity and association overlap to some degree, but do not coincide BIBREF37, BIBREF38. In fact, there exist plenty of pairs that are intuitively associated but not similar. Pairs where the converse is true can also be encountered, although more rarely. An example are synonyms where a word is common and the other infrequent, such as to seize and to commandeer. BIBREF14 revealed that while similarity measures based on the WordNet graph BIBREF39 and human judgments of association in the University of South Florida Free Association Database BIBREF40 do correlate, a number of pairs follow opposite trends. Several studies on human cognition also point in the same direction. For instance, semantic priming can be triggered by similar words without association BIBREF41. On the other hand, a connection with cue words is established more quickly for topically related words rather than for similar words in free association tasks BIBREF42.
A key property of semantic similarity is its gradience: pairs of words can be similar to a different degree. On the other hand, the relation of synonymy is binary: pairs of words are synonyms if they can be substituted in all contexts (or most contexts, in a looser sense), otherwise they are not. While synonyms can be conceived as lying on one extreme of the semantic similarity continuum, it is crucial to note that their definition is stated in purely relational terms, rather than invoking their referential properties BIBREF43, BIBREF44, BIBREF45. This makes behavioral studies on semantic similarity fundamentally different from lexical resources like WordNet BIBREF46, which include paradigmatic relations (such as synonymy).
Lexical Semantic Similarity ::: Similarity for NLP: Intrinsic Evaluation and Semantic Specialization
The ramifications of the distinction between similarity and association are profound for distributional semantics. This paradigm of lexical semantics is grounded in the distributional hypothesis, formulated by BIBREF47 and BIBREF48. According to this hypothesis, the meaning of a word can be recovered empirically from the contexts in which it occurs within a collection of texts. Since both pairs of topically related words and pairs of purely similar words tend to appear in the same contexts, their associated meaning confounds the two distinct relations BIBREF14, BIBREF49, BIBREF50. As a result, distributional methods obscure a crucial facet of lexical meaning. This limitation also reflects onto word embeddings (WEs), representations of words as low-dimensional vectors that have become indispensable for a wide range of NLP applications BIBREF51, BIBREF52, BIBREF53. In particular, it involves both static WEs learned from co-occurrence patterns BIBREF32, BIBREF54, BIBREF31 and contextualized WEs learned from modeling word sequences BIBREF55, BIBREF29. As a result, in the induced representations, geometrical closeness (measured e.g. through cosine distance) conflates genuine similarity with broad relatedness. For instance, the vectors for antonyms such as sober and drunk, by definition dissimilar, might be neighbors in the semantic space under the distributional hypothesis. BIBREF36, BIBREF56, and BIBREF53 demonstrated that different choices of hyper-parameters in WE algorithms (such as context window) emphasize different relations in the resulting representations. Likewise, BIBREF57 and BIBREF54 discovered that WEs learned from texts annotated with syntactic information mirror similarity better than simple local bag-of-words neighborhoods.
The failure of WEs to capture semantic similarity, in turn, affects model performance in several NLP applications where such knowledge is crucial. In particular, Natural Language Understanding tasks such as statistical dialog modeling, text simplification, or semantic text similarity BIBREF58, BIBREF18, BIBREF59, among others, suffer the most. As a consequence, resources providing clean information on semantic similarity are key in mitigating the side effects of the distributional signal. In particular, such databases can be employed for the intrinsic evaluations of specific WE models as a proxy of their reliability for downstream applications BIBREF60, BIBREF61, BIBREF14; intuitively, the more WEs are misaligned with human judgments of similarity, the more their performance on actual tasks is expected to be degraded. Moreover, word representations can be specialized (a.k.a. retrofitted) by disentangling word relations of similarity and association. In particular, linguistic constraints sourced from external databases (such as synonyms from WordNet) can be injected into WEs BIBREF62, BIBREF63, BIBREF16, BIBREF22, BIBREF64 in order to enforce a particular relation in a distributional semantic space while preserving the original adjacency properties.
Lexical Semantic Similarity ::: Similarity and Language Variation: Semantic Typology
In this work, we tackle the concept of (true) semantic similarity from a multilingual perspective. While the same meaning representations may be shared by all human speakers at a deep cognitive level, there is no one-to-one mapping between the words in the lexicons of different languages. This makes the comparison of similarity judgments across languages difficult, since the meaning overlap of translationally equivalent words is sometimes far less than exact. This results from the fact that the way languages `partition' semantic fields is partially arbitrary BIBREF65, although constrained cross-lingually by common cognitive biases BIBREF66. For instance, consider the field of colors: English distinguishes between green and blue, whereas Murle (South Sudan) has a single word for both BIBREF67.
In general, semantic typology studies the variation in lexical semantics across the world's languages. According to BIBREF68, the ways languages categorize concepts into the lexicon follow three main axes: 1) granularity: what is the number of categories in a specific domain?; 2) boundary location: where do the lines marking different categories lie?; 3) grouping and dissection: what are the membership criteria of a category; which instances are considered to be more prototypical? Different choices with respect to these axes lead to different lexicalization patterns. For instance, distinct senses in a polysemous word in English, such as skin (referring to both the body and fruit), may be assigned separate words in other languages such as Italian pelle and buccia, respectively BIBREF70. We later analyze whether similarity scores obtained from native speakers also loosely follow the patterns described by semantic typology.
Previous Work and Evaluation Data
Word Pair Datasets. Rich expert-created resources such as WordNet BIBREF46, BIBREF71, VerbNet BIBREF72, BIBREF73, or FrameNet BIBREF74 encode a wealth of semantic and syntactic information, but are expensive and time-consuming to create. The scale of this problem gets multiplied by the number of languages in consideration. Therefore, crowd-sourcing with non-expert annotators has been adopted as a quicker alternative to produce smaller and more focused semantic resources and evaluation benchmarks. This alternative practice has had a profound impact on distributional semantics and representation learning BIBREF14. While some prominent English word pair datasets such as WordSim-353 BIBREF75, MEN BIBREF76, or Stanford Rare Words BIBREF77 did not discriminate between similarity and relatedness, the importance of this distinction was established by BIBREF14 through the creation of SimLex-999. This inspired other similar datasets which focused on different lexical properties. For instance, SimVerb-3500 BIBREF15 provided similarity ratings for 3,500 English verbs, whereas CARD-660 BIBREF78 aimed at measuring the semantic similarity of infrequent concepts.
Semantic Similarity Datasets in Other Languages. Motivated by the impact of datasets such as SimLex-999 and SimVerb-3500 on representation learning in English, a line of related work focused on creating similar resources in other languages. The dominant approach is translating and reannotating the entire original English SimLex-999 dataset, as done previously for German, Italian, and Russian BIBREF25, Hebrew and Croatian BIBREF16, and Polish BIBREF79. Venekoski:2017nodalida apply this process only to a subset of 300 concept pairs from the English SimLex-999. On the other hand, BIBREF27 sampled a new set of 500 English concept pairs to ensure wider topical coverage and balance across similarity spectra, and then translated those pairs to German, Italian, Spanish, and Farsi (SEMEVAL-500). A similar approach was followed by BIBREF26 for Turkish, by BIBREF80 for Mandarin Chinese, and by BIBREF81 for Japanese. BIBREF82 translated the concatenation of SimLex-999, WordSim-353, and the English SEMEVAL-500 into Thai and then reannotated it. Finally, BIBREF83 translated English SimLex-999 and WordSim-353 to 11 resource-rich target languages (German, French, Russian, Italian, Dutch, Chinese, Portuguese, Swedish, Spanish, Arabic, Farsi), but they did not provide details concerning the translation process and the resolution of translation disagreements. More importantly, they also did not reannotate the translated pairs in the target languages. As we discussed in § SECREF6 and reiterate later in §SECREF5, semantic differences among languages can have a profound impact on the annotation scores; particulary, we show in §SECREF25 that these differences even roughly define language clusters based on language affinity.
A core issue with the current datasets concerns a lack of one unified procedure that ensures the comparability of resources in different languages. Further, concept pairs for different languages are sourced from different corpora (e.g., direct translation of the English data versus sampling from scratch in the target language). Moreover, the previous SimLex-based multilingual datasets inherit the main deficiencies of the English original version, such as the focus on nouns and highly frequent concepts. Finally, prior work mostly focused on languages that are widely spoken and do not account for the variety of the world's languages. Our long-term goal is devising a standardized methodology to extend the coverage also to languages that are resource-lean and/or typologically diverse (e.g., Welsh, Kiswahili as in this work).
Multilingual Datasets for Natural Language Understanding. The Multi-SimLex initiative and corresponding datasets are also aligned with the recent efforts on procuring multilingual benchmarks that can help advance computational modeling of natural language understanding across different languages. For instance, pretrained multilingual language models such as multilingual bert BIBREF29 or xlm BIBREF30 are typically probed on XNLI test data BIBREF84 for cross-lingual natural language inference. XNLI was created by translating examples from the English MultiNLI dataset, and projecting its sentence labels BIBREF85. Other recent multilingual datasets target the task of question answering based on reading comprehension: i) MLQA BIBREF86 includes 7 languages ii) XQuAD BIBREF87 10 languages; iii) TyDiQA BIBREF88 9 widely spoken typologically diverse languages. While MLQA and XQuAD result from the translation from an English dataset, TyDiQA was built independently in each language. Another multilingual dataset, PAWS-X BIBREF89, focused on the paraphrase identification task and was created translating the original English PAWS BIBREF90 into 6 languages. We believe that Multi-SimLex can substantially contribute to this endeavor by offering a comprehensive multilingual benchmark for the fundamental lexical level relation of semantic similarity. In future work, Multi-SimLex also offers an opportunity to investigate the correlations between word-level semantic similarity and performance in downstream tasks such as QA and NLI across different languages.
The Base for Multi-SimLex: Extending English SimLex-999
In this section, we discuss the design principles behind the English (eng) Multi-SimLex dataset, which is the basis for all the Multi-SimLex datasets in other languages, as detailed in §SECREF5. We first argue that a new, more balanced, and more comprehensive evaluation resource for lexical semantic similarity in English is necessary. We then describe how the 1,888 word pairs contained in the eng Multi-SimLex were selected in such a way as to represent various linguistic phenomena within a single integrated resource.
Construction Criteria. The following criteria have to be satisfied by any high-quality semantic evaluation resource, as argued by previous studies focused on the creation of such resources BIBREF14, BIBREF15, BIBREF91, BIBREF27:
(C1) Representative and diverse. The resource must cover the full range of diverse concepts occurring in natural language, including different word classes (e.g., nouns, verbs, adjectives, adverbs), concrete and abstract concepts, a variety of lexical fields, and different frequency ranges.
(C2) Clearly defined. The resource must provide a clear understanding of which semantic relation exactly is annotated and measured, possibly contrasting it with other relations. For instance, the original SimLex-999 and SimVerb-3500 explicitly focus on true semantic similarity and distinguish it from broader relatedness captured by datasets such as MEN BIBREF76 or WordSim-353 BIBREF75.
(C3) Consistent and reliable. The resource must ensure consistent annotations obtained from non-expert native speakers following simple and precise annotation guidelines.
In choosing the word pairs and constructing eng Multi-SimLex, we adhere to these requirements. Moreover, we follow good practices established by the research on related resources. In particular, since the introduction of the original SimLex-999 dataset BIBREF14, follow-up works have improved its construction protocol across several aspects, including: 1) coverage of more lexical fields, e.g., by relying on a diverse set of Wikipedia categories BIBREF27, 2) infrequent/rare words BIBREF78, 3) focus on particular word classes, e.g., verbs BIBREF15, 4) annotation quality control BIBREF78. Our goal is to make use of these improvements towards a larger, more representative, and more reliable lexical similarity dataset in English and, consequently, in all other languages.
The Final Output: English Multi-SimLex. In order to ensure that the criterion C1 is satisfied, we consolidate and integrate the data already carefully sampled in prior work into a single, comprehensive, and representative dataset. This way, we can control for diversity, frequency, and other properties while avoiding to perform this time-consuming selection process from scratch. Note that, on the other hand, the word pairs chosen for English are scored from scratch as part of the entire Multi-SimLex annotation process, introduced later in §SECREF5. We now describe the external data sources for the final set of word pairs:
1) Source: SimLex-999. BIBREF14. The English Multi-SimLex has been initially conceived as an extension of the original SimLex-999 dataset. Therefore, we include all 999 word pairs from SimLex, which span 666 noun pairs, 222 verb pairs, and 111 adjective pairs. While SimLex-999 already provides examples representing different POS classes, it does not have a sufficient coverage of different linguistic phenomena: for instance, it contains only very frequent concepts, and it does not provide a representative set of verbs BIBREF15.
2) Source: SemEval-17: Task 2 BIBREF92. We start from the full dataset of 500 concept pairs to extract a total of 334 concept pairs for English Multi-SimLex a) which contain only single-word concepts, b) which are not named entities, c) where POS tags of the two concepts are the same, d) where both concepts occur in the top 250K most frequent word types in the English Wikipedia, and e) do not already occur in SimLex-999. The original concepts were sampled as to span all the 34 domains available as part of BabelDomains BIBREF93, which roughly correspond to the main high-level Wikipedia categories. This ensures topical diversity in our sub-sample.
3) Source: CARD-660 BIBREF78. 67 word pairs are taken from this dataset focused on rare word similarity, applying the same selection criteria a) to e) employed for SEMEVAL-500. Words are controlled for frequency based on their occurrence counts from the Google News data and the ukWaC corpus BIBREF94. CARD-660 contains some words that are very rare (logboat), domain-specific (erythroleukemia) and slang (2mrw), which might be difficult to translate and annotate across a wide array of languages. Hence, we opt for retaining only the concept pairs above the threshold of top 250K most frequent Wikipedia concepts, as above.
4) Source: SimVerb-3500 BIBREF15 Since both CARD-660 and SEMEVAL-500 are heavily skewed towards noun pairs, and nouns also dominate the original SimLex-999, we also extract additional verb pairs from the verb-specific similarity dataset SimVerb-3500. We randomly sample 244 verb pairs from SimVerb-3500 that represent all similarity spectra. In particular, we add 61 verb pairs for each of the similarity intervals: $[0,1.5), [1.5,3), [3,4.5), [4.5, 6]$. Since verbs in SimVerb-3500 were originally chosen from VerbNet BIBREF95, BIBREF73, they cover a wide range of verb classes and their related linguistic phenomena.
5) Source: University of South Florida BIBREF96 norms, the largest database of free association for English. In order to improve the representation of different POS classes, we sample additional adjectives and adverbs from the USF norms following the procedure established by BIBREF14, BIBREF15. This yields additional 122 adjective pairs, but only a limited number of adverb pairs (e.g., later – never, now – here, once – twice). Therefore, we also create a set of adverb pairs semi-automatically by sampling adjectives that can be derivationally transformed into adverbs (e.g. adding the suffix -ly) from the USF, and assessing the correctness of such derivation in WordNet. The resulting pairs include, for instance, primarily – mainly, softly – firmly, roughly – reliably, etc. We include a total of 123 adverb pairs into the final English Multi-SimLex. Note that this is the first time adverbs are included into any semantic similarity dataset.
Fulfillment of Construction Criteria. The final eng Multi-SimLex dataset spans 1,051 noun pairs, 469 verb pairs, 245 adjective pairs, and 123 adverb pairs. As mentioned above, the criterion C1 has been fulfilled by relying only on word pairs that already underwent meticulous sampling processes in prior work, integrating them into a single resource. As a consequence, Multi-SimLex allows for fine-grained analyses over different POS classes, concreteness levels, similarity spectra, frequency intervals, relation types, morphology, lexical fields, and it also includes some challenging orthographically similar examples (e.g., infection – inflection). We ensure that the criteria C2 and C3 are satisfied by using similar annotation guidelines as Simlex-999, SimVerb-3500, and SEMEVAL-500 that explicitly target semantic similarity. In what follows, we outline the carefully tailored process of translating and annotating Multi-SimLex datasets in all target languages.
Multi-SimLex: Translation and Annotation
We now detail the development of the final Multi-SimLex resource, describing our language selection process, as well as translation and annotation of the resource, including the steps taken to ensure and measure the quality of this resource. We also provide key data statistics and preliminary cross-lingual comparative analyses.
Language Selection. Multi-SimLex comprises eleven languages in addition to English. The main objective for our inclusion criteria has been to balance language prominence (by number of speakers of the language) for maximum impact of the resource, while simultaneously having a diverse suite of languages based on their typological features (such as morphological type and language family). Table TABREF10 summarizes key information about the languages currently included in Multi-SimLex. We have included a mixture of fusional, agglutinative, isolating, and introflexive languages that come from eight different language families. This includes languages that are very widely used such as Chinese Mandarin and Spanish, and low-resource languages such as Welsh and Kiswahili. We hope to further include additional languages and inspire other researchers to contribute to the effort over the lifetime of this project.
The work on data collection can be divided into two crucial phases: 1) a translation phase where the extended English language dataset with 1,888 pairs (described in §SECREF4) is translated into eleven target languages, and 2) an annotation phase where human raters scored each pair in the translated set as well as the English set. Detailed guidelines for both phases are available online at: https://multisimlex.com.
Multi-SimLex: Translation and Annotation ::: Word Pair Translation
Translators for each target language were instructed to find direct or approximate translations for the 1,888 word pairs that satisfy the following rules. (1) All pairs in the translated set must be unique (i.e., no duplicate pairs); (2) Translating two words from the same English pair into the same word in the target language is not allowed (e.g., it is not allowed to translate car and automobile to the same Spanish word coche). (3) The translated pairs must preserve the semantic relations between the two words when possible. This means that, when multiple translations are possible, the translation that best conveys the semantic relation between the two words found in the original English pair is selected. (4) If it is not possible to use a single-word translation in the target language, then a multi-word expression (MWE) can be used to convey the nearest possible semantics given the above points (e.g., the English word homework is translated into the Polish MWE praca domowa).
Satisfying the above rules when finding appropriate translations for each pair–while keeping to the spirit of the intended semantic relation in the English version–is not always straightforward. For instance, kinship terminology in Sinitic languages (Mandarin and Yue) uses different terms depending on whether the family member is older or younger, and whether the family member comes from the mother’s side or the father’s side. UTF8gbsn In Mandarin, brother has no direct translation and can be translated as either: 哥哥 (older brother) or 弟弟 (younger brother). Therefore, in such cases, the translators are asked to choose the best option given the semantic context (relation) expressed by the pair in English, otherwise select one of the translations arbitrarily. This is also used to remove duplicate pairs in the translated set, by differentiating the duplicates using a variant at each instance. Further, many translation instances were resolved using near-synonymous terms in the translation. For example, the words in the pair: wood – timber can only be directly translated in Estonian to puit, and are not distinguishable. Therefore, the translators approximated the translation for timber to the compound noun puitmaterjal (literally: wood material) in order to produce a valid pair in the target language. In some cases, a direct transliteration from English is used. For example, the pair: physician and doctor both translate to the same word in Estonian (arst); the less formal word doktor is used as a translation of doctor to generate a valid pair.
We measure the quality of the translated pairs by using a random sample set of 100 pairs (from the 1,888 pairs) to be translated by an independent translator for each target language. The sample is proportionally stratified according to the part-of-speech categories. The independent translator is given identical instructions to the main translator; we then measure the percentage of matched translated words between the two translations of the sample set. Table TABREF12 summarizes the inter-translator agreement results for all languages and by part-of-speech subsets. Overall across all languages, the agreement is 84.8%, which is similar to prior work BIBREF27, BIBREF97.
Multi-SimLex: Translation and Annotation ::: Guidelines and Word Pair Scoring
Across all languages, 145 human annotators were asked to score all 1,888 pairs (in their given language). We finally collect at least ten valid annotations for each word pair in each language. All annotators were required to abide by the following instructions:
1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity.
2. Each annotator must score the entire set of 1,888 pairs in the dataset. The pairs must not be shared between different annotators.
3. Annotators are able to break the workload over a period of approximately 2-3 weeks, and are able to use external sources (e.g. dictionaries, thesauri, WordNet) if required.
4. Annotators are kept anonymous, and are not able to communicate with each other during the annotation process.
The selection criteria for the annotators required that all annotators must be native speakers of the target language. Preference to annotators with university education was given, but not required. Annotators were asked to complete a spreadsheet containing the translated pairs of words, as well as the part-of-speech, and a column to enter the score. The annotators did not have access to the original pairs in English.
To ensure the quality of the collected ratings, we have employed an adjudication protocol similar to the one proposed and validated by Pilehvar:2018emnlp. It consists of the following three rounds:
Round 1: All annotators are asked to follow the instructions outlined above, and to rate all 1,888 pairs with integer scores between 0 and 6.
Round 2: We compare the scores of all annotators and identify the pairs for each annotator that have shown the most disagreement. We ask the annotators to reconsider the assigned scores for those pairs only. The annotators may chose to either change or keep the scores. As in the case with Round 1, the annotators have no access to the scores of the other annotators, and the process is anonymous. This process gives a chance for annotators to correct errors or reconsider their judgments, and has been shown to be very effective in reaching consensus, as reported by BIBREF78. We used a very similar procedure as BIBREF78 to identify the pairs with the most disagreement; for each annotator, we marked the $i$th pair if the rated score $s_i$ falls within: $s_i \ge \mu _i + 1.5$ or $s_i \le \mu _i - 1.5$, where $\mu _i$ is the mean of the other annotators' scores.
Round 3: We compute the average agreement for each annotator (with the other annotators), by measuring the average Spearman's correlation against all other annotators. We discard the scores of annotators that have shown the least average agreement with all other annotators, while we maintain at least ten annotators per language by the end of this round. The actual process is done in multiple iterations: (S1) we measure the average agreement for each annotator with every other annotator (this corresponds to the APIAA measure, see later); (S2) if we still have more than 10 valid annotators and the lowest average score is higher than in the previous iteration, we remove the lowest one, and rerun S1. Table TABREF14 shows the number of annotators at both the start (Round 1) and end (Round 3) of our process for each language.
We measure the agreement between annotators using two metrics, average pairwise inter-annotator agreement (APIAA), and average mean inter-annotator agreement (AMIAA). Both of these use Spearman's correlation ($\rho $) between annotators scores, the only difference is how they are averaged. They are computed as follows:
where $\rho (s_i,s_j)$ is the Spearman's correlation between annotators $i$ and $j$'s scores ($s_i$,$s_j$) for all pairs in the dataset, and $N$ is the number of annotators. APIAA has been used widely as the standard measure for inter-annotator agreement, including in the original SimLex paper BIBREF14. It simply averages the pairwise Spearman's correlation between all annotators. On the other hand, AMIAA compares the average Spearman's correlation of one held-out annotator with the average of all the other $N-1$ annotators, and then averages across all $N$ `held-out' annotators. It smooths individual annotator effects and arguably serves as a better upper bound than APIAA BIBREF15, BIBREF91, BIBREF78.
We present the respective APIAA and AMIAA scores in Table TABREF16 and Table TABREF17 for all part-of-speech subsets, as well as the agreement for the full datasets. As reported in prior work BIBREF15, BIBREF91, AMIAA scores are typically higher than APIAA scores. Crucially, the results indicate `strong agreement' (across all languages) using both measurements. The languages with the highest annotator agreement were French (fra) and Yue Chinese (yue), while Russian (rus) had the lowest overall IAA scores. These scores, however, are still considered to be `moderately strong agreement'.
Multi-SimLex: Translation and Annotation ::: Data Analysis
Similarity Score Distributions. Across all languages, the average score (mean $=1.61$, median$=1.1$) is on the lower side of the similarity scale. However, looking closer at the scores of each language in Table TABREF19, we indicate notable differences in both the averages and the spread of scores. Notably, French has the highest average of similarity scores (mean$=2.61$, median$=2.5$), while Kiswahili has the lowest average (mean$=1.28$, median$=0.5$). Russian has the lowest spread ($\sigma =1.37$), while Polish has the largest ($\sigma =1.62$). All of the languages are strongly correlated with each other, as shown in Figure FIGREF20, where all of the Spearman's correlation coefficients are greater than 0.6 for all language pairs. Languages that share the same language family are highly correlated (e.g, cmn-yue, rus-pol, est-fin). In addition, we observe high correlations between English and most other languages, as expected. This is due to the effect of using English as the base/anchor language to create the dataset. In simple words, if one translates to two languages $L_1$ and $L_2$ starting from the same set of pairs in English, it is higly likely that $L_1$ and $L_2$ will diverge from English in different ways. Therefore, the similarity between $L_1$-eng and $L_2$-eng is expected to be higher than between $L_1$-$L_2$, especially if $L_1$ and $L_2$ are typologically dissimilar languages (e.g., heb-cmn, see Figure FIGREF20). This phenomenon is well documented in related prior work BIBREF25, BIBREF27, BIBREF16, BIBREF97. While we acknowledge this as a slight artifact of the dataset design, it would otherwise be impossible to construct a semantically aligned and comprehensive dataset across a large number of languages.
We also report differences in the distribution of the frequency of words among the languages in Multi-SimLex. Figure FIGREF22 shows six example languages, where each bar segment shows the proportion of words in each language that occur in the given frequency range. For example, the 10K-20K segment of the bars represents the proportion of words in the dataset that occur in the list of most frequent words between the frequency rank of 10,000 and 20,000 in that language; likewise with other intervals. Frequency lists for the presented languages are derived from Wikipedia and Common Crawl corpora. While many concept pairs are direct or approximate translations of English pairs, we can see that the frequency distribution does vary across different languages, and is also related to inherent language properties. For instance, in Finnish and Russian, while we use infinitive forms of all verbs, conjugated verb inflections are often more frequent in raw corpora than the corresponding infinitive forms. The variance can also be partially explained by the difference in monolingual corpora size used to derive the frequency rankings in the first place: absolute vocabulary sizes are expected to fluctuate across different languages. However, it is also important to note that the datasets also contain subsets of lower-frequency and rare words, which can be used for rare word evaluations in multiple languages, in the spirit of Pilehvar:2018emnlp's English rare word dataset.
Cross-Linguistic Differences. Table TABREF23 shows some examples of average similarity scores of English, Spanish, Kiswahili and Welsh concept pairs. Remember that the scores range from 0 to 6: the higher the score, the more similar the participants found the concepts in the pair. The examples from Table TABREF23 show evidence of both the stability of average similarity scores across languages (unlikely – friendly, book – literature, and vanish – disappear), as well as language-specific differences (care – caution). Some differences in similarity scores seem to group languages into clusters. For example, the word pair regular – average has an average similarity score of 4.0 and 4.1 in English and Spanish, respectively, whereas in Kiswahili and Welsh the average similarity score of this pair is 0.5 and 0.8. We analyze this phenomenon in more detail in §SECREF25.
There are also examples for each of the four languages having a notably higher or lower similarity score for the same concept pair than the three other languages. For example, large – big in English has an average similarity score of 5.9, whereas Spanish, Kiswahili and Welsh speakers rate the closest concept pair in their native language to have a similarity score between 2.7 and 3.8. What is more, woman – wife receives an average similarity of 0.9 in English, 2.9 in Spanish, and greater than 4.0 in Kiswahili and Welsh. The examples from Spanish include banco – asiento (bank – seat) which receives an average similarity score 5.1, while in the other three languages the similarity score for this word pair does not exceed 0.1. At the same time, the average similarity score of espantosamente – fantásticamente (amazingly – fantastically) is much lower in Spanish (0.4) than in other languages (4.1 – 5.1). In Kiswahili, an example of a word pair with a higher similarity score than the rest would be machweo – jioni (sunset – evening), having an average score of 5.5, while the other languages receive 2.8 or less, and a notably lower similarity score is given to wa ajabu - mkubwa sana (wonderful – terrific), getting 0.9, while the other languages receive 5.3 or more. Welsh examples include yn llwyr - yn gyfan gwbl (purely – completely), which scores 5.4 among Welsh speakers but 2.3 or less in other languages, while addo – tyngu (promise – swear) is rated as 0 by all Welsh annotators, but in the other three languages 4.3 or more on average.
There can be several explanations for the differences in similarity scores across languages, including but not limited to cultural context, polysemy, metonymy, translation, regional and generational differences, and most commonly, the fact that words and meanings do not exactly map onto each other across languages. For example, it is likely that the other three languages do not have two separate words for describing the concepts in the concept pair: big – large, and the translators had to opt for similar lexical items that were more distant in meaning, explaining why in English the concept pair received a much higher average similarity score than in other languages. A similar issue related to the mapping problem across languages arose in the Welsh concept pair yn llwye – yn gyfan gwbl, where Welsh speakers agreed that the two concepts are very similar. When asked, bilingual speakers considered the two Welsh concepts more similar than English equivalents purely – completely, potentially explaining why a higher average similarity score was reached in Welsh. The example of woman – wife can illustrate cultural differences or another translation-related issue where the word `wife' did not exist in some languages (for example, Estonian), and therefore had to be described using other words, affecting the comparability of the similarity scores. This was also the case with the football – soccer concept pair. The pair bank – seat demonstrates the effect of the polysemy mismatch across languages: while `bank' has two different meanings in English, neither of them is similar to the word `seat', but in Spanish, `banco' can mean `bank', but it can also mean `bench'. Quite naturally, Spanish speakers gave the pair banco – asiento a higher similarity score than the speakers of languages where this polysemy did not occur.
An example of metonymy affecting the average similarity score can be seen in the Kiswahili version of the word pair: sunset – evening (machweo – jioni). The average similarity score for this pair is much higher in Kiswahili, likely because the word `sunset' can act as a metonym of `evening'. The low similarity score of wonderful – terrific in Kiswahili (wa ajabu - mkubwa sana) can be explained by the fact that while `mkubwa sana' can be used as `terrific' in Kiswahili, it technically means `very big', adding to the examples of translation- and mapping-related effects. The word pair amazingly – fantastically (espantosamente – fantásticamente) brings out another translation-related problem: the accuracy of the translation. While `espantosamente' could arguably be translated to `amazingly', more common meanings include: `frightfully', `terrifyingly', and `shockingly', explaining why the average similarity score differs from the rest of the languages. Another problem was brought out by addo – tyngu (promise – swear) in Welsh, where the `tyngu' may not have been a commonly used or even a known word choice for annotators, pointing out potential regional or generational differences in language use.
Table TABREF24 presents examples of concept pairs from English, Spanish, Kiswahili, and Welsh on which the participants agreed the most. For example, in English all participants rated the similarity of trial – test to be 4 or 5. In Spanish and Welsh, all participants rated start – begin to correspond to a score of 5 or 6. In Kiswahili, money – cash received a similarity rating of 6 from every participant. While there are numerous examples of concept pairs in these languages where the participants agreed on a similarity score of 4 or higher, it is worth noting that none of these languages had a single pair where all participants agreed on either 1-2, 2-3, or 3-4 similarity rating. Interestingly, in English all pairs where all the participants agreed on a 5-6 similarity score were adjectives.
Multi-SimLex: Translation and Annotation ::: Effect of Language Affinity on Similarity Scores
Based on the analysis in Figure FIGREF20 and inspecting the anecdotal examples in the previous section, it is evident that the correlation between similarity scores across languages is not random. To corroborate this intuition, we visualize the vectors of similarity scores for each single language by reducing their dimensionality to 2 via Principal Component Analysis BIBREF98. The resulting scatter plot in Figure FIGREF26 reveals that languages from the same family or branch have similar patterns in the scores. In particular, Russian and Polish (both Slavic), Finnish and Estonian (both Uralic), Cantonese and Mandarin Chinese (both Sinitic), and Spanish and French (both Romance) are all neighbors.
In order to quantify exactly the effect of language affinity on the similarity scores, we run correlation analyses between these and language features. In particular, we extract feature vectors from URIEL BIBREF99, a massively multilingual typological database that collects and normalizes information compiled by grammarians and field linguists about the world's languages. In particular, we focus on information about geography (the areas where the language speakers are concentrated), family (the phylogenetic tree each language belongs to), and typology (including syntax, phonological inventory, and phonology). Moreover, we consider typological representations of languages that are not manually crafted by experts, but rather learned from texts. BIBREF100 proposed to construct such representations by training language-identifying vectors end-to-end as part of neural machine translation models.
The vector for similarity judgments and the vector of linguistic features for a given language have different dimensionality. Hence, we first construct a distance matrix for each vector space, such that both columns and rows are language indices, and each cell value is the cosine distance between the vectors of the corresponding language pair. Given a set of L languages, each resulting matrix $S$ has dimensionality of $\mathbb {R}^{|L| \times |L|}$ and is symmetrical. To estimate the correlation between the matrix for similarity judgments and each of the matrices for linguistic features, we run a Mantel test BIBREF101, a non-parametric statistical test based on matrix permutations that takes into account inter-dependencies among pairwise distances.
The results of the Mantel test reported in Table FIGREF26 show that there exist statistically significant correlations between similarity judgments and geography, family, and syntax, given that $p < 0.05$ and $z > 1.96$. The correlation coefficient is particularly strong for geography ($r = 0.647$) and syntax ($r = 0.649$). The former result is intuitive, because languages in contact easily borrow and loan lexical units, and cultural interactions may result in similar cognitive categorizations. The result for syntax, instead, cannot be explained so easily, as formal properties of language do not affect lexical semantics. Instead, we conjecture that, while no causal relation is present, both syntactic features and similarity judgments might be linked to a common explanatory variable (such as geography). In fact, several syntactic properties are not uniformly spread across the globe. For instance, verbs with Verb–Object–Subject word order are mostly concentrated in Oceania BIBREF102. In turn, geographical proximity leads to similar judgment patterns, as mentioned above. On the other hand, we find no correlation with phonology and inventory, as expected, nor with the bottom-up typological features from BIBREF100.
Cross-Lingual Multi-SimLex Datasets
A crucial advantage of having semantically aligned monolingual datasets across different languages is the potential to create cross-lingual semantic similarity datasets. Such datasets allow for probing the quality of cross-lingual representation learning algorithms BIBREF27, BIBREF103, BIBREF104, BIBREF105, BIBREF106, BIBREF30, BIBREF107 as an intrinsic evaluation task. However, the cross-lingual datasets previous work relied upon BIBREF27 were limited to a homogeneous set of high-resource languages (e.g., English, German, Italian, Spanish) and a small number of concept pairs (all less than 1K pairs). We address both problems by 1) using a typologically more diverse language sample, and 2) relying on a substantially larger English dataset as a source for the cross-lingual datasets: 1,888 pairs in this work versus 500 pairs in the work of Camacho:2017semeval. As a result, each of our cross-lingual datasets contains a substantially larger number of concept pairs, as shown in Table TABREF30. The cross-lingual Multi-Simlex datasets are constructed automatically, leveraging word pair translations and annotations collected in all 12 languages. This yields a total of 66 cross-lingual datasets, one for each possible combination of languages. Table TABREF30 provides the final number of concept pairs, which lie between 2,031 and 3,480 pairs for each cross-lingual dataset, whereas Table TABREF29 shows some sample pairs with their corresponding similarity scores.
The automatic creation and verification of cross-lingual datasets closely follows the procedure first outlined by Camacho:2015acl and later adopted by Camacho:2017semeval (for semantic similarity) and Vulic:2019acl (for graded lexical entailment). First, given two languages, we intersect their aligned concept pairs obtained through translation. For instance, starting from the aligned pairs attroupement – foule in French and rahvasumm – rahvahulk in Estonian, we construct two cross-lingual pairs attroupement – rahvaluk and rahvasumm – foule. The scores of cross-lingual pairs are then computed as averages of the two corresponding monolingual scores. Finally, in order to filter out concept pairs whose semantic meaning was not preserved during this operation, we retain only cross-lingual pairs for which the corresponding monolingual scores $(s_s, s_t)$ differ at most by one fifth of the full scale (i.e., $\mid s_s - s_t \mid \le 1.2$). This heuristic mitigates the noise due to cross-lingual semantic shifts BIBREF27, BIBREF97. We refer the reader to the work of Camacho:2015acl for a detailed technical description of the procedure. UTF8gbsn
To assess the quality of the resulting cross-lingual datasets, we have conducted a verification experiment similar to Vulic:2019acl. We randomly sampled 300 concept pairs in the English-Spanish, English-French, and English-Mandarin cross-lingual datasets. Subsequently, we asked bilingual native speakers to provide similarity judgments of each pair. The Spearman's correlation score $\rho $ between automatically induced and manually collected ratings achieves $\rho \ge 0.90$ on all samples, which confirms the viability of the automatic construction procedure.
Score and Class Distributions. The summary of score and class distributions across all 66 cross-lingual datasets are provided in Figure FIGREF31 and Figure FIGREF31, respectively. First, it is obvious that the distribution over the four POS classes largely adheres to that of the original monolingual Multi-SimLex datasets, and that the variance is quite low: e.g., the eng-fra dataset contains the lowest proportion of nouns (49.21%) and the highest proportion of verbs (27.1%), adjectives (15.28%), and adverbs (8.41%). On the other hand, the distribution over similarity intervals in Figure FIGREF31 shows a much greater variance. This is again expected as this pattern resurfaces in monolingual datasets (see Table TABREF19). It is also evident that the data are skewed towards lower-similarity concept pairs. However, due to the joint size of all cross-lingual datasets (see Table TABREF30), even the least represented intervals contain a substantial number of concept pairs. For instance, the rus-yue dataset contains the least highly similar concept pairs (in the interval $[4,6]$) of all 66 cross-lingual datasets. Nonetheless, the absolute number of pairs (138) in that interval for rus-yue is still substantial. If needed, this makes it possible to create smaller datasets which are balanced across the similarity spectra through sub-sampling.
Monolingual Evaluation of Representation Learning Models
After the numerical and qualitative analyses of the Multi-SimLex datasets provided in §§ SECREF18–SECREF25, we now benchmark a series of representation learning models on the new evaluation data. We evaluate standard static word embedding algorithms such as fastText BIBREF31, as well as a range of more recent text encoders pretrained on language modeling such as multilingual BERT BIBREF29. These experiments provide strong baseline scores on the new Multi-SimLex datasets and offer a first large-scale analysis of pretrained encoders on word-level semantic similarity across diverse languages. In addition, the experiments now enabled by Multi-SimLex aim to answer several important questions. (Q1) Is it viable to extract high-quality word-level representations from pretrained encoders receiving subword-level tokens as input? Are such representations competitive with standard static word-level embeddings? (Q2) What are the implications of monolingual pretraining versus (massively) multilingual pretraining for performance? (Q3) Do lightweight unsupervised post-processing techniques improve word representations consistently across different languages? (Q4) Can we effectively transfer available external lexical knowledge from resource-rich languages to resource-lean languages in order to learn word representations that distinguish between true similarity and conceptual relatedness (see the discussion in §SECREF6)?
Monolingual Evaluation of Representation Learning Models ::: Models in Comparison
Static Word Embeddings in Different Languages. First, we evaluate a standard method for inducing non-contextualized (i.e., static) word embeddings across a plethora of different languages: fastText (ft) vectors BIBREF31 are currently the most popular and robust choice given 1) the availability of pretrained vectors in a large number of languages BIBREF108 trained on large Common Crawl (CC) plus Wikipedia (Wiki) data, and 2) their superior performance across a range of NLP tasks BIBREF109. In fact, fastText is an extension of the standard word-level CBOW and skip-gram word2vec models BIBREF32 that takes into account subword-level information, i.e. the contituent character n-grams of each word BIBREF110. For this reason, fastText is also more suited for modeling rare words and morphologically rich languages.
We rely on 300-dimensional ft word vectors trained on CC+Wiki and available online for 157 languages. The word vectors for all languages are obtained by CBOW with position-weights, with character n-grams of length 5, a window of size 5, 10 negative examples, and 10 training epochs. We also probe another (older) collection of ft vectors, pretrained on full Wikipedia dumps of each language.. The vectors are 300-dimensional, trained with the skip-gram objective for 5 epochs, with 5 negative examples, a window size set to 5, and relying on all character n-grams from length 3 to 6. Following prior work, we trim the vocabularies for all languages to the 200K most frequent words and compute representations for multi-word expressions by averaging the vectors of their constituent words.
Unsupervised Post-Processing. Further, we consider a variety of unsupervised post-processing steps that can be applied post-training on top of any pretrained input word embedding space without any external lexical semantic resource. So far, the usefulness of such methods has been verified only on the English language through benchmarks for lexical semantics and sentence-level tasks BIBREF113. In this paper, we assess if unsupervised post-processing is beneficial also in other languages. To this end, we apply the following post-hoc transformations on the initial word embeddings:
1) Mean centering (mc) is applied after unit length normalization to ensure that all vectors have a zero mean, and is commonly applied in data mining and analysis BIBREF114, BIBREF115.
2) All-but-the-top (abtt) BIBREF113, BIBREF116 eliminates the common mean vector and a few top dominating directions (according to PCA) from the input distributional word vectors, since they do not contribute towards distinguishing the actual semantic meaning of different words. The method contains a single (tunable) hyper-parameter $dd_{A}$ which denotes the number of the dominating directions to remove from the initial representations. Previous work has verified the usefulness of abtt in several English lexical semantic tasks such as semantic similarity, word analogies, and concept categorization, as well as in sentence-level text classification tasks BIBREF113.
3) uncovec BIBREF117 adjusts the similarity order of an arbitrary input word embedding space, and can emphasize either syntactic or semantic information in the transformed vectors. In short, it transforms the input space $\mathbf {X}$ into an adjusted space $\mathbf {X}\mathbf {W}_{\alpha }$ through a linear map $\mathbf {W}_{\alpha }$ controlled by a single hyper-parameter $\alpha $. The $n^{\text{th}}$-order similarity transformation of the input word vector space $\mathbf {X}$ (for which $n=1$) can be obtained as $\mathbf {M}_{n}(\mathbf {X}) = \mathbf {M}_1(\mathbf {X}\mathbf {W}_{(n-1)/2})$, with $\mathbf {W}_{\alpha }=\mathbf {Q}\mathbf {\Gamma }^{\alpha }$, where $\mathbf {Q}$ and $\mathbf {\Gamma }$ are the matrices obtained via eigendecomposition of $\mathbf {X}^T\mathbf {X}=\mathbf {Q}\mathbf {\Gamma }\mathbf {Q}^T$. $\mathbf {\Gamma }$ is a diagonal matrix containing eigenvalues of $\mathbf {X}^T\mathbf {X}$; $\mathbf {Q}$ is an orthogonal matrix with eigenvectors of $\mathbf {X}^T\mathbf {X}$ as columns. While the motivation for the uncovec methods does originate from adjusting discrete similarity orders, note that $\alpha $ is in fact a continuous real-valued hyper-parameter which can be carefully tuned. For more technical details we refer the reader to the original work of BIBREF117.
As mentioned, all post-processing methods can be seen as unsupervised retrofitting methods that, given an arbitrary input vector space $\mathbf {X}$, produce a perturbed/transformed output vector space $\mathbf {X}^{\prime }$, but unlike common retrofitting methods BIBREF62, BIBREF16, the perturbation is completely unsupervised (i.e., self-contained) and does not inject any external (semantic similarity oriented) knowledge into the vector space. Note that different perturbations can also be stacked: e.g., we can apply uncovec and then use abtt on top the output uncovec vectors. When using uncovec and abtt we always length-normalize and mean-center the data first (i.e., we apply the simple mc normalization). Finally, we tune the two hyper-parameters $d_A$ (for abtt) and $\alpha $ (uncovec) on the English Multi-SimLex and use the same values on the datasets of all other languages; we report results with $dd_A = 3$ or $dd_A = 10$, and $\alpha =-0.3$.
Contextualized Word Embeddings. We also evaluate the capacity of unsupervised pretraining architectures based on language modeling objectives to reason over lexical semantic similarity. To the best of our knowledge, our article is the first study performing such analyses. State-of-the-art models such as bert BIBREF29, xlm BIBREF30, or roberta BIBREF118 are typically very deep neural networks based on the Transformer architecture BIBREF119. They receive subword-level tokens as inputs (such as WordPieces BIBREF120) to tackle data sparsity. In output, they return contextualized embeddings, dynamic representations for words in context.
To represent words or multi-word expressions through a pretrained model, we follow prior work BIBREF121 and compute an input item's representation by 1) feeding it to a pretrained model in isolation; then 2) averaging the $H$ last hidden representations for each of the item’s constituent subwords; and then finally 3) averaging the resulting subword representations to produce the final $d$-dimensional representation, where $d$ is the embedding and hidden-layer dimensionality (e.g., $d=768$ with bert). We opt for this approach due to its proven viability and simplicity BIBREF121, as it does not require any additional corpora to condition the induction of contextualized embeddings. Other ways to extract the representations from pretrained models BIBREF122, BIBREF123, BIBREF124 are beyond the scope of this work, and we will experiment with them in the future.
In other words, we treat each pretrained encoder enc as a black-box function to encode a single word or a multi-word expression $x$ in each language into a $d$-dimensional contextualized representation $\mathbf {x}_{\textsc {enc}} \in \mathbb {R}^d = \textsc {enc}(x)$ (e.g., $d=768$ with bert). As multilingual pretrained encoders, we experiment with the multilingual bert model (m-bert) BIBREF29 and xlm BIBREF30. m-bert is pretrained on monolingual Wikipedia corpora of 102 languages (comprising all Multi-SimLex languages) with a 12-layer Transformer network, and yields 768-dimensional representations. Since the concept pairs in Multi-SimLex are lowercased, we use the uncased version of m-bert. m-bert comprises all Multi-SimLex languages, and its evident ability to perform cross-lingual transfer BIBREF12, BIBREF125, BIBREF126 also makes it a convenient baseline model for cross-lingual experiments later in §SECREF8. The second multilingual model we consider, xlm-100, is pretrained on Wikipedia dumps of 100 languages, and encodes each concept into a $1,280$-dimensional representation. In contrast to m-bert, xlm-100 drops the next-sentence prediction objective and adds a cross-lingual masked language modeling objective. For both encoders, the representations of each concept are computed as averages over the last $H=4$ hidden layers in all experiments, as suggested by Wu:2019arxiv.
Besides m-bert and xlm, covering multiple languages, we also analyze the performance of “language-specific” bert and xlm models for the languages where they are available: Finnish, Spanish, English, Mandarin Chinese, and French. The main goal of this comparison is to study the differences in performance between multilingual “one-size-fits-all” encoders and language-specific encoders. For all experiments, we rely on the pretrained models released in the Transformers repository BIBREF127.
Unsupervised post-processing steps devised for static word embeddings (i.e., mean-centering, abtt, uncovec) can also be applied on top of contextualized embeddings if we predefine a vocabulary of word types $V$ that will be represented in a word vector space $\mathbf {X}$. We construct such $V$ for each language as the intersection of word types covered by the corresponding CC+Wiki fastText vectors and the (single-word or multi-word) expressions appearing in the corresponding Multi-SimLex dataset.
Finally, note that it is not feasible to evaluate a full range of available pretrained encoders within the scope of this work. Our main intention is to provide the first set of baseline results on Multi-SimLex by benchmarking a sample of most popular encoders, at the same time also investigating other important questions such as performance of static versus contextualized word embeddings, or multilingual versus language-specific pretraining. Another purpose of the experiments is to outline the wide potential and applicability of the Multi-SimLex datasets for multilingual and cross-lingual representation learning evaluation.
Monolingual Evaluation of Representation Learning Models ::: Results and Discussion
The results we report are Spearman's $\rho $ coefficients of the correlation between the ranks derived from the scores of the evaluated models and the human scores provided in each Multi-SimLex dataset. The main results with static and contextualized word vectors for all test languages are summarized in Table TABREF43. The scores reveal several interesting patterns, and also pinpoint the main challenges for future work.
State-of-the-Art Representation Models. The absolute scores of CC+Wiki ft, Wiki ft, and m-bert are not directly comparable, because these models have different coverage. In particular, Multi-SimLex contains some out-of-vocabulary (OOV) words whose static ft embeddings are not available. On the other hand, m-bert has perfect coverage. A general comparison between CC+Wiki and Wiki ft vectors, however, supports the intuition that larger corpora (such as CC+Wiki) yield higher correlations. Another finding is that a single massively multilingual model such as m-bert cannot produce semantically rich word-level representations. Whether this actually happens because the training objective is different—or because the need to represent 100+ languages reduces its language-specific capacity—is investigated further below.
The overall results also clearly indicate that (i) there are differences in performance across different monolingual Multi-SimLex datasets, and (ii) unsupervised post-processing is universally useful, and can lead to huge improvements in correlation scores for many languages. In what follows, we also delve deeper into these analyses.
Impact of Unsupervised Post-Processing. First, the results in Table TABREF43 suggest that applying dimension-wise mean centering to the initial vector spaces has positive impact on word similarity scores in all test languages and for all models, both static and contextualized (see the +mc rows in Table TABREF43). BIBREF128 show that distributional word vectors have a tendency towards narrow clusters in the vector space (i.e., they occupy a narrow cone in the vector space and are therefore anisotropic BIBREF113, BIBREF129), and are prone to the undesired effect of hubness BIBREF130, BIBREF131. Applying dimension-wise mean centering has the effect of spreading the vectors across the hyper-plane and mitigating the hubness issue, which consequently improves word-level similarity, as it emerges from the reported results. Previous work has already validated the importance of mean centering for clustering-based tasks BIBREF132, bilingual lexicon induction with cross-lingual word embeddings BIBREF133, BIBREF134, BIBREF135, and for modeling lexical semantic change BIBREF136. However, to the best of our knowledge, the results summarized in Table TABREF43 are the first evidence that also confirms its importance for semantic similarity in a wide array of languages. In sum, as a general rule of thumb, we suggest to always mean-center representations for semantic tasks.
The results further indicate that additional post-processing methods such as abtt and uncovec on top of mean-centered vector spaces can lead to further gains in most languages. The gains are even visible for languages which start from high correlation scores: for instance., cmn with CC+Wiki ft increases from 0.534 to 0.583, from 0.315 to 0.526 with Wiki ft, and from 0.408 to 0.487 with m-bert. Similarly, for rus with CC+Wiki ft we can improve from 0.422 to 0.500, and for fra the scores improve from 0.578 to 0.613. There are additional similar cases reported in Table TABREF43.
Overall, the unsupervised post-processing techniques seem universally useful across languages, but their efficacy and relative performance does vary across different languages. Note that we have not carefully fine-tuned the hyper-parameters of the evaluated post-processing methods, so additional small improvements can be expected for some languages. The main finding, however, is that these post-processing techniques are robust to semantic similarity computations beyond English, and are truly language independent. For instance, removing dominant latent (PCA-based) components from word vectors emphasizes semantic differences between different concepts, as only shared non-informative latent semantic knowledge is removed from the representations.
In summary, pretrained word embeddings do contain more information pertaining to semantic similarity than revealed in the initial vectors. This way, we have corroborated the hypotheses from prior work BIBREF113, BIBREF117 which were not previously empirically verified on other languages due to a shortage of evaluation data; this gap has now been filled with the introduction of the Multi-SimLex datasets. In all follow-up experiments, we always explicitly denote which post-processing configuration is used in evaluation.
POS-Specific Subsets. We present the results for subsets of word pairs grouped by POS class in Table TABREF46. Prior work based on English data showed that representations for nouns are typically of higher quality than those for the other POS classes BIBREF49, BIBREF137, BIBREF50. We observe a similar trend in other languages as well. This pattern is consistent across different representation models and can be attributed to several reasons. First, verb representations need to express a rich range of syntactic and semantic behaviors rather than purely referential features BIBREF138, BIBREF139, BIBREF73. Second, low correlation scores on the adjective and adverb subsets in some languages (e.g., pol, cym, swa) might be due to their low frequency in monolingual texts, which yields unreliable representations. In general, the variance in performance across different word classes warrants further research in class-specific representation learning BIBREF140, BIBREF50. The scores further attest the usefulness of unsupervised post-processing as almost all class-specific correlation scores are improved by applying mean-centering and abtt. Finally, the results for m-bert and xlm-100 in Table TABREF46 further confirm that massively multilingual pretraining cannot yield reasonable semantic representations for many languages: in fact, for some classes they display no correlation with human ratings at all.
Differences across Languages. Naturally, the results from Tables TABREF43 and TABREF46 also reveal that there is variation in performance of both static word embeddings and pretrained encoders across different languages. Among other causes, the lowest absolute scores with ft are reported for languages with least resources available to train monolingual word embeddings, such as Kiswahili, Welsh, and Estonian. The low performance on Welsh is especially indicative: Figure FIGREF20 shows that the ratings in the Welsh dataset match up very well with the English ratings, but we cannot achieve the same level of correlation in Welsh with Welsh ft word embeddings. Difference in performance between two closely related languages, est (low-resource) and fin (high-resource), provides additional evidence in this respect.
The highest reported scores with m-bert and xlm-100 are obtained for Mandarin Chinese and Yue Chinese: this effectively points to the weaknesses of massively multilingual training with a joint subword vocabulary spanning 102 and 100 languages. Due to the difference in scripts, “language-specific” subwords for yue and cmn do not need to be shared across a vast amount of languages and the quality of their representation remains unscathed. This effectively means that m-bert's subword vocabulary contains plenty of cmn-specific and yue-specific subwords which are exploited by the encoder when producing m-bert-based representations. Simultaneously, higher scores with m-bert (and xlm in Table TABREF46) are reported for resource-rich languages such as French, Spanish, and English, which are better represented in m-bert's training data. We also observe lower absolute scores (and a larger number of OOVs) for languages with very rich and productive morphological systems such as the two Slavic languages (Polish and Russian) and Finnish. Since Polish and Russian are known to have large Wikipedias and Common Crawl data BIBREF33 (e.g., their Wikipedias are in the top 10 largest Wikipedias worldwide), the problem with coverage can be attributed exactly to the proliferation of morphological forms in those languages.
Finally, while Table TABREF43 does reveal that unsupervised post-processing is useful for all languages, it also demonstrates that peak scores are achieved with different post-processing configurations. This finding suggests that a more careful language-specific fine-tuning is indeed needed to refine word embeddings towards semantic similarity. We plan to inspect the relationship between post-processing techniques and linguistic properties in more depth in future work.
Multilingual vs. Language-Specific Contextualized Embeddings. Recent work has shown that—despite the usefulness of massively multilingual models such as m-bert and xlm-100 for zero-shot cross-lingual transfer BIBREF12, BIBREF125—stronger results in downstream tasks for a particular language can be achieved by pretraining language-specific models on language-specific data. In this experiment, motivated by the low results of m-bert and xlm-100 (see again Table TABREF46), we assess if monolingual pretrained encoders can produce higher-quality word-level representations than multilingual models. Therefore, we evaluate language-specific bert and xlm models for a subset of the Multi-SimLex languages for which such models are currently available: Finnish BIBREF141 (bert-base architecture, uncased), French BIBREF142 (the FlauBERT model based on xlm), English (bert-base, uncased), Mandarin Chinese (bert-base) BIBREF29 and Spanish (bert-base, uncased). In addition, we also evaluate a series of pretrained encoders available for English: (i) bert-base, bert-large, and bert-large with whole word masking (wwm) from the original work on BERT BIBREF29, (ii) monolingual “English-specific” xlm BIBREF30, and (iii) two models which employ parameter reduction techniques to build more compact encoders: albert-b uses a configuration similar to bert-base, while albert-l is similar to bert-large, but with an $18\times $ reduction in the number of parameters BIBREF143.
From the results in Table FIGREF49, it is clear that monolingual pretrained encoders yield much more reliable word-level representations. The gains are visible even for languages such as cmn which showed reasonable performance with m-bert and are substantial on all test languages. This further confirms the validity of language-specific pretraining in lieu of multilingual training, if sufficient monolingual data are available. Moreover, a comparison of pretrained English encoders in Figure FIGREF49 largely follows the intuition: the larger bert-large model yields slight improvements over bert-base, and we can improve a bit more by relying on word-level (i.e., lexical-level) masking.Finally, light-weight albert model variants are quite competitive with the original bert models, with only modest drops reported, and albert-l again outperforms albert-b. Overall, it is interesting to note that the scores obtained with monolingual pretrained encoders are on a par with or even outperform static ft word embeddings: this is a very intriguing finding per se as it shows that such subword-level models trained on large corpora can implicitly capture rich lexical semantic knowledge.
Similarity-Specialized Word Embeddings. Conflating distinct lexico-semantic relations is a well-known property of distributional representations BIBREF144, BIBREF53. Semantic specialization fine-tunes distributional spaces to emphasize a particular lexico-semantic relation in the transformed space by injecting external lexical knowledge BIBREF145. Explicitly discerning between true semantic similarity (as captured in Multi-SimLex) and broad conceptual relatedness benefits a number of tasks, as discussed in §SECREF4. Since most languages lack dedicated lexical resources, however, one viable strategy to steer monolingual word vector spaces to emphasize semantic similarity is through cross-lingual transfer of lexical knowledge, usually through a shared cross-lingual word vector space BIBREF106. Therefore, we evaluate the effectiveness of specialization transfer methods using Multi-SimLex as our multilingual test bed.
We evaluate a current state-of-the-art cross-lingual specialization transfer method with minimal requirements, put forth recently by Ponti:2019emnlp. In a nutshell, their li-postspec method is a multi-step procedure that operates as follows. First, the knowledge about semantic similarity is extracted from WordNet in the form of triplets, that is, linguistic constraints $(w_1, w_2, r)$, where $w_1$ and $w_2$ are two concepts, and $r$ is a relation between them obtained from WordNet (e.g., synonymy or antonymy). The goal is to “attract” synonyms closer to each other in the transformed vector space as they reflect true semantic similarity, and “repel” antonyms further apart. In the second step, the linguistic constraints are translated from English to the target language via a shared cross-lingual word vector space. To this end, following Ponti:2019emnlp we rely on cross-lingual word embeddings (CLWEs) BIBREF146 available online, which are based on Wiki ft vectors. Following that, a constraint refinement step is applied in the target language which aims to eliminate the noise inserted during the translation process. This is done by training a relation classification tool: it is trained again on the English linguistic constraints and then used on the translated target language constraints, where the transfer is again enabled via a shared cross-lingual word vector space. Finally, a state-of-the-art monolingual specialization procedure from Ponti:2018emnlp injects the (now target language) linguistic constraints into the target language distributional space.
The scores are summarized in Table TABREF56. Semantic specialization with li-postspec leads to substantial improvements in correlation scores for the majority of the target languages, demonstrating the importance of external semantic similarity knowledge for semantic similarity reasoning. However, we also observe deteriorated performance for the three target languages which can be considered the lowest-resource ones in our set: cym, swa, yue. We hypothesize that this occurs due to the inferior quality of the underlying monolingual Wikipedia word embeddings, which generates a chain of error accumulations. In particular, poor distributional word estimates compromise the alignment of the embedding spaces, which in turn results in increased translation noise, and reduced refinement ability of the relation classifier. On a high level, this “poor get poorer” observation again points to the fact that one of the primary causes of low performance of resource-low languages in semantic tasks is the sheer lack of even unlabeled data for distributional training. On the other hand, as we see from Table TABREF46, typological dissimilarity between the source and the target does not deteriorate the effectiveness of semantic specialization. In fact, li-postspec does yield substantial gains also for the typologically distant targets such as heb, cmn, and est. The critical problem indeed seems to be insufficient raw data for monolingual distributional training.
Cross-Lingual Evaluation
Similar to monolingual evaluation in §SECREF7, we now evaluate several state-of-the-art cross-lingual representation models on the suite of 66 automatically constructed cross-lingual Multi-SimLex datasets. Again, note that evaluating a full range of cross-lingual models available in the rich prior work on cross-lingual representation learning is well beyond the scope of this article. We therefore focus our cross-lingual analyses on several well-established and indicative state-of-the-art cross-lingual models, again spanning both static and contextualized cross-lingual word embeddings.
Cross-Lingual Evaluation ::: Models in Comparison
Static Word Embeddings. We rely on a state-of-the-art mapping-based method for the induction of cross-lingual word embeddings (CLWEs): vecmap BIBREF148. The core idea behind such mapping-based or projection-based approaches is to learn a post-hoc alignment of independently trained monolingual word embeddings BIBREF106. Such methods have gained popularity due to their conceptual simplicity and competitive performance coupled with reduced bilingual supervision requirements: they support CLWE induction with only as much as a few thousand word translation pairs as the bilingual supervision BIBREF149, BIBREF150, BIBREF151, BIBREF107. More recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs BIBREF152, BIBREF135, identical strings BIBREF153, or even only shared numerals BIBREF154. In the extreme, fully unsupervised projection-based CLWEs extract such seed bilingual lexicons from scratch on the basis of monolingual data only BIBREF103, BIBREF148, BIBREF155, BIBREF156, BIBREF104, BIBREF157.
Recent empirical studies BIBREF158, BIBREF135, BIBREF159 have compared a variety of unsupervised and weakly supervised mapping-based CLWE methods, and vecmap emerged as the most robust and very competitive choice. Therefore, we focus on 1) its fully unsupervised variant (unsuper) in our comparisons. For several language pairs, we also report scores with two other vecmap model variants: 2) a supervised variant which learns a mapping based on an available seed lexicon (super), and 3) a supervised variant with self-learning (super+sl) which iteratively increases the seed lexicon and improves the mapping gradually. For a detailed description of these variants, we refer the reader to recent work BIBREF148, BIBREF135. We again use CC+Wiki ft vectors as initial monolingual word vectors, except for yue where Wiki ft is used. The seed dictionaries of two different sizes (1k and 5k translation pairs) are based on PanLex BIBREF160, and are taken directly from prior work BIBREF135, or extracted from PanLex following the same procedure as in the prior work.
Contextualized Cross-Lingual Word Embeddings. We again evaluate the capacity of (massively) multilingual pretrained language models, m-bert and xlm-100, to reason over cross-lingual lexical similarity. Implicitly, such an evaluation also evaluates “the intrinsic quality” of shared cross-lingual word-level vector spaces induced by these methods, and their ability to boost cross-lingual transfer between different language pairs. We rely on the same procedure of aggregating the models' subword-level parameters into word-level representations, already described in §SECREF34.
As in monolingual settings, we can apply unsupervised post-processing steps such as abtt to both static and contextualized cross-lingual word embeddings.
Cross-Lingual Evaluation ::: Results and Discussion
Main Results and Differences across Language Pairs. A summary of the results on the 66 cross-lingual Multi-SimLex datasets are provided in Table TABREF59 and Figure FIGREF60. The findings confirm several interesting findings from our previous monolingual experiments (§SECREF44), and also corroborate several hypotheses and findings from prior work, now on a large sample of language pairs and for the task of cross-lingual semantic similarity.
First, we observe that the fully unsupervised vecmap model, despite being the most robust fully unsupervised method at present, fails to produce a meaningful cross-lingual word vector space for a large number of language pairs (see the bottom triangle of Table TABREF59): many correlation scores are in fact no-correlation results, accentuating the problem of fully unsupervised cross-lingual learning for typologically diverse languages and with fewer amounts of monolingual data BIBREF135. The scores are particularly low across the board for lower-resource languages such as Welsh and Kiswahili. It also seems that the lack of monolingual data is a larger problem than typological dissimilarity between language pairs, as we do observe reasonably high correlation scores with vecmap for language pairs such as cmn-spa, heb-est, and rus-fin. However, typological differences (e.g., morphological richness) still play an important role as we observe very low scores when pairing cmn with morphologically rich languages such fin, est, pol, and rus. Similar to prior work of Vulic:2019we and doval2019onthe, given the fact that unsupervised vecmap is the most robust unsupervised CLWE method at present BIBREF158, our results again question the usefulness of fully unsupervised approaches for a large number of languages, and call for further developments in the area of unsupervised and weakly supervised cross-lingual representation learning.
The scores of m-bert and xlm-100 lead to similar conclusions as in the monolingual settings. Reasonable correlation scores are achieved only for a small subset of resource-rich language pairs (e.g., eng, fra, spa, cmn) which dominate the multilingual m-bert training. Interestingly, the scores indicate a much higher performance of language pairs where yue is one of the languages when we use m-bert instead of vecmap. This boils down again to the fact that yue, due to its specific language script, has a good representation of its words and subwords in the shared m-bert vocabulary. At the same time, a reliable vecmap mapping between yue and other languages cannot be found due to a small monolingual yue corpus. In cases when vecmap does not yield a degenerate cross-lingual vector space starting from two monolingual ones, the final correlation scores seem substantially higher than the ones obtained by the single massively multilingual m-bert model.
Finally, the results in Figure FIGREF60 again verify the usefulness of unsupervised post-processing also in cross-lingual settings. We observe improved performance with both m-bert and xlm-100 when mean centering (+mc) is applied, and further gains can be achieved by using abtt on the mean-centered vector spaces. A similar finding also holds for static cross-lingual word embeddings, where applying abbt (-10) yields higher scores on 61/66 language pairs.
Fully Unsupervised vs. Weakly Supervised Cross-Lingual Embeddings. The results in Table TABREF59 indicate that fully unsupervised cross-lingual learning fails for a large number of language pairs. However, recent work BIBREF135 has noted that these sub-optimal non-alignment solutions with the unsuper model can be avoided by relying on (weak) cross-lingual supervision spanning only several thousands or even hundreds of word translation pairs. Therefore, we examine 1) if we can further improve the results on cross-lingual Multi-SimLex resorting to (at least some) cross-lingual supervision for resource-rich language pairs; and 2) if such available word-level supervision can also be useful for a range of languages which displayed near-zero performance in Table TABREF59. In other words, we test if recent “tricks of the trade” used in the rich literature on CLWE learning reflect in gains on cross-lingual Multi-SimLex datasets.
First, we reassess the findings established on the bilingual lexicon induction task BIBREF161, BIBREF135: using at least some cross-lingual supervision is always beneficial compared to using no supervision at all. We report improvements over the unsuper model for all 10 language pairs in Table TABREF66, even though the unsuper method initially produced strong correlation scores. The importance of self-learning increases with decreasing available seed dictionary size, and the +sl model always outperforms unsuper with 1k seed pairs; we observe the same patterns also with even smaller dictionary sizes than reported in Table TABREF66 (250 and 500 seed pairs). Along the same line, the results in Table TABREF67 indicate that at least some supervision is crucial for the success of static CLWEs on resource-leaner language pairs. We note substantial improvements on all language pairs; in fact, the vecmap model is able to learn a more reliable mapping starting from clean supervision. We again note large gains with self-learning.
Multilingual vs. Bilingual Contextualized Embeddings. Similar to the monolingual settings, we also inspect if massively multilingual training in fact dilutes the knowledge necessary for cross-lingual reasoning on a particular language pair. Therefore, we compare the 100-language xlm-100 model with i) a variant of the same model trained on a smaller set of 17 languages (xlm-17); ii) a variant of the same model trained specifically for the particular language pair (xlm-2); and iii) a variant of the bilingual xlm-2 model that also leverages bilingual knowledge from parallel data during joint training (xlm-2++). We again use the pretrained models made available by Conneau:2019nips, and we refer to the original work for further technical details.
The results are summarized in Figure FIGREF60, and they confirm the intuition that massively multilingual pretraining can damage performance even on resource-rich languages and language pairs. We observe a steep rise in performance when the multilingual model is trained on a much smaller set of languages (17 versus 100), and further improvements can be achieved by training a dedicated bilingual model. Finally, leveraging bilingual parallel data seems to offer additional slight gains, but a tiny difference between xlm-2 and xlm-2++ also suggests that this rich bilingual information is not used in the optimal way within the xlm architecture for semantic similarity.
In summary, these results indicate that, in order to improve performance in cross-lingual transfer tasks, more work should be invested into 1) pretraining dedicated language pair-specific models, and 2) creative ways of leveraging available cross-lingual supervision (e.g., word translation pairs, parallel or comparable corpora) BIBREF121, BIBREF123, BIBREF124 with pretraining paradigms such as bert and xlm. Using such cross-lingual supervision could lead to similar benefits as indicated by the results obtained with static cross-lingual word embeddings (see Table TABREF66 and Table TABREF67). We believe that Multi-SimLex can serve as a valuable means to track and guide future progress in this research area.
Conclusion and Future Work
We have presented Multi-SimLex, a resource containing human judgments on the semantic similarity of word pairs for 12 monolingual and 66 cross-lingual datasets. The languages covered are typologically diverse and include also under-resourced ones, such as Welsh and Kiswahili. The resource covers an unprecedented amount of 1,888 word pairs, carefully balanced according to their similarity score, frequency, concreteness, part-of-speech class, and lexical field. In addition to Multi-Simlex, we release the detailed protocol we followed to create this resource. We hope that our consistent guidelines will encourage researchers to translate and annotate Multi-Simlex -style datasets for additional languages. This can help and create a hugely valuable, large-scale semantic resource for multilingual NLP research.
The core Multi-SimLex we release with this paper already enables researchers to carry out novel linguistic analysis as well as establishes a benchmark for evaluating representation learning models. Based on our preliminary analyses, we found that speakers of closely related languages tend to express equivalent similarity judgments. In particular, geographical proximity seems to play a greater role than family membership in determining the similarity of judgments across languages. Moreover, we tested several state-of-the-art word embedding models, both static and contextualized representations, as well as several (supervised and unsupervised) post-processing techniques, on the newly released Multi-SimLex. This enables future endeavors to improve multilingual representation learning with challenging baselines. In addition, our results provide several important insights for research on both monolingual and cross-lingual word representations:
1) Unsupervised post-processing techniques (mean centering, elimination of top principal components, adjusting similarity orders) are always beneficial independently of the language, although the combination leading to the best scores is language-specific and hence needs to be tuned.
2) Similarity rankings obtained from word embeddings for nouns are better aligned with human judgments than all the other part-of-speech classes considered here (verbs, adjectives, and, for the first time, adverbs). This confirms previous generalizations based on experiments on English.
3) The factor having the greatest impact on the quality of word representations is the availability of raw texts to train them in the first place, rather than language properties (such as family, geographical area, typological features).
4) Massively multilingual pretrained encoders such as m-bert BIBREF29 and xlm-100 BIBREF30 fare quite poorly on our benchmark, whereas pretrained encoders dedicated to a single language are more competitive with static word embeddings such as fastText BIBREF31. Moreover, for language-specific encoders, parameter reduction techniques reduce performance only marginally.
5) Techniques to inject clean lexical semantic knowledge from external resources into distributional word representations were proven to be effective in emphasizing the relation of semantic similarity. In particular, methods capable of transferring such knowledge from resource-rich to resource-lean languages BIBREF59 increased the correlation with human judgments for most languages, except for those with limited unlabelled data.
Future work can expand our preliminary, yet large-scale study on the ability of pretrained encoders to reason over word-level semantic similarity in different languages. For instance, we have highlighted how sharing the same encoder parameters across multiple languages may harm performance. However, it remains unclear if, and to what extent, the input language embeddings present in xlm-100 but absent in m-bert help mitigate this issue. In addition, pretrained language embeddings can be obtained both from typological databases BIBREF99 and from neural architectures BIBREF100. Plugging these embeddings into the encoders in lieu of embeddings trained end-to-end as suggested by prior work BIBREF162, BIBREF163, BIBREF164 might extend the coverage to more resource-lean languages.
Another important follow-up analysis might involve the comparison of the performance of representation learning models on multilingual datasets for both word-level semantic similarity and sentence-level Natural Language Understanding. In particular, Multi-SimLex fills a gap in available resources for multilingual NLP and might help understand how lexical and compositional semantics interact if put alongside existing resources such as XNLI BIBREF84 for natural language inference or PAWS-X BIBREF89 for cross-lingual paraphrase identification. Finally, the Multi-SimLex annotation could turn out to be a unique source of evidence to study the effects of polysemy in human judgments on semantic similarity: for equivalent word pairs in multiple languages, are the similarity scores affected by how many senses the two words (or multi-word expressions) incorporate?
In light of the success of initiatives like Universal Dependencies for multilingual treebanks, we hope that making Multi-SimLex and its guidelines available will encourage other researchers to expand our current sample of languages. We particularly encourage creation and submission of comparable Multi-SimLex datasets for under-resourced and typologically diverse languages in future work. In particular, we have made a Multi-Simlex community website available to facilitate easy creation, gathering, dissemination, and use of annotated datasets: https://multisimlex.com/.
This work is supported by the ERC Consolidator Grant LEXICAL: Lexical Acquisition Across Languages (no 648909). Thierry Poibeau is partly supported by a PRAIRIE 3IA Institute fellowship (""Investissements d'avenir"" program, reference ANR-19-P3IA-0001).","['1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process']",14722,qasper_e,en,,1bb7cd04a893fe2a5497bdf7d0dcb064da2ef56144b44bdd,"1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process",480
What labels for antisocial events are available in datasets?,"Introduction
“Ché saetta previsa vien più lenta.”
– Dante Alighieri, Divina Commedia, Paradiso
Antisocial behavior is a persistent problem plaguing online conversation platforms; it is both widespread BIBREF0 and potentially damaging to mental and emotional health BIBREF1, BIBREF2. The strain this phenomenon puts on community maintainers has sparked recent interest in computational approaches for assisting human moderators.
Prior work in this direction has largely focused on post-hoc identification of various kinds of antisocial behavior, including hate speech BIBREF3, BIBREF4, harassment BIBREF5, personal attacks BIBREF6, and general toxicity BIBREF7. The fact that these approaches only identify antisocial content after the fact limits their practicality as tools for assisting pre-emptive moderation in conversational domains.
Addressing this limitation requires forecasting the future derailment of a conversation based on early warning signs, giving the moderators time to potentially intervene before any harm is done (BIBREF8 BIBREF8, BIBREF9 BIBREF9, see BIBREF10 BIBREF10 for a discussion). Such a goal recognizes derailment as emerging from the development of the conversation, and belongs to the broader area of conversational forecasting, which includes future-prediction tasks such as predicting the eventual length of a conversation BIBREF11, whether a persuasion attempt will eventually succeed BIBREF12, BIBREF13, BIBREF14, whether team discussions will eventually lead to an increase in performance BIBREF15, or whether ongoing counseling conversations will eventually be perceived as helpful BIBREF16.
Approaching such conversational forecasting problems, however, requires overcoming several inherent modeling challenges. First, conversations are dynamic and their outcome might depend on how subsequent comments interact with each other. Consider the example in Figure FIGREF2: while no individual comment is outright offensive, a human reader can sense a tension emerging from their succession (e.g., dismissive answers to repeated questioning). Thus a forecasting model needs to capture not only the content of each individual comment, but also the relations between comments. Previous work has largely relied on hand-crafted features to capture such relations—e.g., similarity between comments BIBREF16, BIBREF12 or conversation structure BIBREF17, BIBREF18—, though neural attention architectures have also recently shown promise BIBREF19.
The second modeling challenge stems from the fact that conversations have an unknown horizon: they can be of varying lengths, and the to-be-forecasted event can occur at any time. So when is it a good time to make a forecast? Prior work has largely proposed two solutions, both resulting in important practical limitations. One solution is to assume (unrealistic) prior knowledge of when the to-be-forecasted event takes place and extract features up to that point BIBREF20, BIBREF8. Another compromising solution is to extract features from a fixed-length window, often at the start of the conversation BIBREF21, BIBREF15, BIBREF16, BIBREF9. Choosing a catch-all window-size is however impractical: short windows will miss information in comments they do not encompass (e.g., a window of only two comments would miss the chain of repeated questioning in comments 3 through 6 of Figure FIGREF2), while longer windows risk missing the to-be-forecasted event altogether if it occurs before the end of the window, which would prevent early detection.
In this work we introduce a model for forecasting conversational events that overcomes both these inherent challenges by processing comments, and their relations, as they happen (i.e., in an online fashion). Our main insight is that models with these properties already exist, albeit geared toward generation rather than prediction: recent work in context-aware dialog generation (or “chatbots”) has proposed sequential neural models that make effective use of the intra-conversational dynamics BIBREF22, BIBREF23, BIBREF24, while concomitantly being able to process the conversation as it develops (see BIBREF25 for a survey).
In order for these systems to perform well in the generative domain they need to be trained on massive amounts of (unlabeled) conversational data. The main difficulty in directly adapting these models to the supervised domain of conversational forecasting is the relative scarcity of labeled data: for most forecasting tasks, at most a few thousands labeled examples are available, insufficient for the notoriously data-hungry sequential neural models.
To overcome this difficulty, we propose to decouple the objective of learning a neural representation of conversational dynamics from the objective of predicting future events. The former can be pre-trained on large amounts of unsupervised data, similarly to how chatbots are trained. The latter can piggy-back on the resulting representation after fine-tuning it for classification using relatively small labeled data. While similar pre-train-then-fine-tune approaches have recently achieved state-of-the-art performance in a number of NLP tasks—including natural language inference, question answering, and commonsense reasoning (discussed in Section SECREF2)—to the best of our knowledge this is the first attempt at applying this paradigm to conversational forecasting.
To test the effectiveness of this new architecture in forecasting derailment of online conversations, we develop and distribute two new datasets. The first triples in size the highly curated `Conversations Gone Awry' dataset BIBREF9, where civil-starting Wikipedia Talk Page conversations are crowd-labeled according to whether they eventually lead to personal attacks; the second relies on in-the-wild moderation of the popular subreddit ChangeMyView, where the aim is to forecast whether a discussion will later be subject to moderator action due to “rude or hostile” behavior. In both datasets, our model outperforms existing fixed-window approaches, as well as simpler sequential baselines that cannot account for inter-comment relations. Furthermore, by virtue of its online processing of the conversation, our system can provide substantial prior notice of upcoming derailment, triggering on average 3 comments (or 3 hours) before an overtly toxic comment is posted.
To summarize, in this work we:
introduce the first model for forecasting conversational events that can capture the dynamics of a conversation as it develops;
build two diverse datasets (one entirely new, one extending prior work) for the task of forecasting derailment of online conversations;
compare the performance of our model against the current state-of-the-art, and evaluate its ability to provide early warning signs.
Our work is motivated by the goal of assisting human moderators of online communities by preemptively signaling at-risk conversations that might deserve their attention. However, we caution that any automated systems might encode or even amplify the biases existing in the training data BIBREF26, BIBREF27, BIBREF28, so a public-facing implementation would need to be exhaustively scrutinized for such biases BIBREF29.
Further Related Work
Antisocial behavior. Antisocial behavior online comes in many forms, including harassment BIBREF30, cyberbullying BIBREF31, and general aggression BIBREF32. Prior work has sought to understand different aspects of such behavior, including its effect on the communities where it happens BIBREF33, BIBREF34, the actors involved BIBREF35, BIBREF36, BIBREF37, BIBREF38 and connections to the outside world BIBREF39.
Post-hoc classification of conversations. There is a rich body of prior work on classifying the outcome of a conversation after it has concluded, or classifying conversational events after they happened. Many examples exist, but some more closely related to our present work include identifying the winner of a debate BIBREF40, BIBREF41, BIBREF42, identifying successful negotiations BIBREF21, BIBREF43, as well as detecting whether deception BIBREF44, BIBREF45, BIBREF46 or disagreement BIBREF47, BIBREF48, BIBREF49, BIBREF50, BIBREF51 has occurred.
Our goal is different because we wish to forecast conversational events before they happen and while the conversation is still ongoing (potentially allowing for interventions). Note that some post-hoc tasks can also be re-framed as forecasting tasks (assuming the existence of necessary labels); for instance, predicting whether an ongoing conversation will eventually spark disagreement BIBREF18, rather than detecting already-existing disagreement.
Conversational forecasting. As described in Section SECREF1, prior work on forecasting conversational outcomes and events has largely relied on hand-crafted features to capture aspects of conversational dynamics. Example feature sets include statistical measures based on similarity between utterances BIBREF16, sentiment imbalance BIBREF20, flow of ideas BIBREF20, increase in hostility BIBREF8, reply rate BIBREF11 and graph representations of conversations BIBREF52, BIBREF17. By contrast, we aim to automatically learn neural representations of conversational dynamics through pre-training.
Such hand-crafted features are typically extracted from fixed-length windows of the conversation, leaving unaddressed the problem of unknown horizon. While some work has trained multiple models for different window-lengths BIBREF8, BIBREF18, they consider these models to be independent and, as such, do not address the issue of aggregating them into a single forecast (i.e., deciding at what point to make a prediction). We implement a simple sliding windows solution as a baseline (Section SECREF5).
Pre-training for NLP. The use of pre-training for natural language tasks has been growing in popularity after recent breakthroughs demonstrating improved performance on a wide array of benchmark tasks BIBREF53, BIBREF54. Existing work has generally used a language modeling objective as the pre-training objective; examples include next-word prediction BIBREF55, sentence autoencoding, BIBREF56, and machine translation BIBREF57. BERT BIBREF58 introduces a variation on this in which the goal is to predict the next sentence in a document given the current sentence. Our pre-training objective is similar in spirit, but operates at a conversation level, rather than a document level. We hence view our objective as conversational modeling rather than (only) language modeling. Furthermore, while BERT's sentence prediction objective is framed as a multiple-choice task, our objective is framed as a generative task.
Derailment Datasets
We consider two datasets, representing related but slightly different forecasting tasks. The first dataset is an expanded version of the annotated Wikipedia conversations dataset from BIBREF9. This dataset uses carefully-controlled crowdsourced labels, strictly filtered to ensure the conversations are civil up to the moment of a personal attack. This is a useful property for the purposes of model analysis, and hence we focus on this as our primary dataset. However, we are conscious of the possibility that these strict labels may not fully capture the kind of behavior that moderators care about in practice. We therefore introduce a secondary dataset, constructed from the subreddit ChangeMyView (CMV) that does not use post-hoc annotations. Instead, the prediction task is to forecast whether the conversation will be subject to moderator action in the future.
Wikipedia data. BIBREF9's `Conversations Gone Awry' dataset consists of 1,270 conversations that took place between Wikipedia editors on publicly accessible talk pages. The conversations are sourced from the WikiConv dataset BIBREF59 and labeled by crowdworkers as either containing a personal attack from within (i.e., hostile behavior by one user in the conversation directed towards another) or remaining civil throughout.
A series of controls are implemented to prevent models from picking up on trivial correlations. To prevent models from capturing topic-specific information (e.g., political conversations are more likely to derail), each attack-containing conversation is paired with a clean conversation from the same talk page, where the talk page serves as a proxy for topic. To force models to actually capture conversational dynamics rather than detecting already-existing toxicity, human annotations are used to ensure that all comments preceding a personal attack are civil.
To the ends of more effective model training, we elected to expand the `Conversations Gone Awry' dataset, using the original annotation procedure. Since we found that the original data skewed towards shorter conversations, we focused this crowdsourcing run on longer conversations: ones with 4 or more comments preceding the attack. Through this additional crowdsourcing, we expand the dataset to 4,188 conversations, which we are publicly releasing as part of the Cornell Conversational Analysis Toolkit (ConvoKit).
We perform an 80-20-20 train/dev/test split, ensuring that paired conversations end up in the same split in order to preserve the topic control. Finally, we randomly sample another 1 million conversations from WikiConv to use for the unsupervised pre-training of the generative component.
Reddit CMV data. The CMV dataset is constructed from conversations collected via the Reddit API. In contrast to the Wikipedia-based dataset, we explicitly avoid the use of post-hoc annotation. Instead, we use as our label whether a conversation eventually had a comment removed by a moderator for violation of Rule 2: “Don't be rude or hostile to other users”.
Though the lack of post-hoc annotation limits the degree to which we can impose controls on the data (e.g., some conversations may contain toxic comments not flagged by the moderators) we do reproduce as many of the Wikipedia data's controls as we can. Namely, we replicate the topic control pairing by choosing pairs of positive and negative examples that belong to the same top-level post, following BIBREF12; and enforce that the removed comment was made by a user who was previously involved in the conversation. This process results in 6,842 conversations, to which we again apply a pair-preserving 80-20-20 split. Finally, we gather over 600,000 conversations that do not include any removed comment, for unsupervised pre-training.
Conversational Forecasting Model
We now describe our general model for forecasting future conversational events. Our model integrates two components: (a) a generative dialog model that learns to represent conversational dynamics in an unsupervised fashion; and (b) a supervised component that fine-tunes this representation to forecast future events. Figure FIGREF13 provides an overview of the proposed architecture, henceforth CRAFT (Conversational Recurrent Architecture for ForecasTing).
Terminology. For modeling purposes, we treat a conversation as a sequence of $N$ comments $C = \lbrace c_1,\dots ,c_N\rbrace $. Each comment, in turn, is a sequence of tokens, where the number of tokens may vary from comment to comment. For the $n$-th comment ($1 \le n \le N)$, we let $M_n$ denote the number of tokens. Then, a comment $c_n$ can be represented as a sequence of $M_n$ tokens: $c_n = \lbrace w_1,\dots ,w_{M_n}\rbrace $.
Generative component. For the generative component of our model, we use a hierarchical recurrent encoder-decoder (HRED) architecture BIBREF60, a modified version of the popular sequence-to-sequence (seq2seq) architecture BIBREF61 designed to account for dependencies between consecutive inputs. BIBREF23 showed that HRED can successfully model conversational context by encoding the temporal structure of previously seen comments, making it an ideal fit for our use case. Here, we provide a high-level summary of the HRED architecture, deferring deeper technical discussion to BIBREF60 and BIBREF23.
An HRED dialog model consists of three components: an utterance encoder, a context encoder, and a decoder. The utterance encoder is responsible for generating semantic vector representations of comments. It consists of a recurrent neural network (RNN) that reads a comment token-by-token, and on each token $w_m$ updates a hidden state $h^{\text{enc}}$ based on the current token and the previous hidden state:
where $f^{\text{RNN}}$ is a nonlinear gating function (our implementation uses GRU BIBREF62). The final hidden state $h^{\text{enc}}_M$ can be viewed as a vector encoding of the entire comment.
Running the encoder on each comment $c_n$ results in a sequence of $N$ vector encodings. A second encoder, the context encoder, is then run over this sequence:
Each hidden state $h^{\text{con}}_n$ can then be viewed as an encoding of the full conversational context up to and including the $n$-th comment. To generate a response to comment $n$, the context encoding $h^{\text{con}}_n$ is used to initialize the hidden state $h^{\text{dec}}_{0}$ of a decoder RNN. The decoder produces a response token by token using the following recurrence:
where $f^{\text{out}}$ is some function that outputs a probability distribution over words; we implement this using a simple feedforward layer. In our implementation, we further augment the decoder with attention BIBREF63, BIBREF64 over context encoder states to help capture long-term inter-comment dependencies. This generative component can be pre-trained using unlabeled conversational data.
Prediction component. Given a pre-trained HRED dialog model, we aim to extend the model to predict from the conversational context whether the to-be-forecasted event will occur. Our predictor consists of a multilayer perceptron (MLP) with 3 fully-connected layers, leaky ReLU activations between layers, and sigmoid activation for output. For each comment $c_n$, the predictor takes as input the context encoding $h^{\text{con}}_n$ and forwards it through the MLP layers, resulting in an output score that is interpreted as a probability $p_{\text{event}}(c_{n+1})$ that the to-be-forecasted event will happen (e.g., that the conversation will derail).
Training the predictive component starts by initializing the weights of the encoders to the values learned in pre-training. The main training loop then works as follows: for each positive sample—i.e., a conversation containing an instance of the to-be-forecasted event (e.g., derailment) at comment $c_e$—we feed the context $c_1,\dots ,c_{e-1}$ through the encoder and classifier, and compute cross-entropy loss between the classifier output and expected output of 1. Similarly, for each negative sample—i.e., a conversation where none of the comments exhibit the to-be-forecasted event and that ends with $c_N$—we feed the context $c_1,\dots ,c_{N-1}$ through the model and compute loss against an expected output of 0.
Note that the parameters of the generative component are not held fixed during this process; instead, backpropagation is allowed to go all the way through the encoder layers. This process, known as fine-tuning, reshapes the representation learned during pre-training to be more directly useful to prediction BIBREF55.
We implement the model and training code using PyTorch, and we are publicly releasing our implementation and the trained models together with the data as part of ConvoKit.
Forecasting Derailment
We evaluate the performance of CRAFT in the task of forecasting conversational derailment in both the Wikipedia and CMV scenarios. To this end, for each of these datasets we pre-train the generative component on the unlabeled portion of the data and fine-tune it on the labeled training split (data size detailed in Section SECREF3).
In order to evaluate our sequential system against conversational-level ground truth, we need to aggregate comment level predictions. If any comment in the conversation triggers a positive prediction—i.e., $p_{\text{event}}(c_{n+1})$ is greater than a threshold learned on the development split—then the respective conversation is predicted to derail. If this forecast is triggered in a conversation that actually derails, but before the derailment actually happens, then the conversation is counted as a true positive; otherwise it is a false positive. If no positive predictions are triggered for a conversation, but it actually derails then it counts as a false negative; if it does not derail then it is a true negative.
Fixed-length window baselines. We first seek to compare CRAFT to existing, fixed-length window approaches to forecasting. To this end, we implement two such baselines: Awry, which is the state-of-the-art method proposed in BIBREF9 based on pragmatic features in the first comment-reply pair, and BoW, a simple bag-of-words baseline that makes a prediction using TF-IDF weighted bag-of-words features extracted from the first comment-reply pair.
Online forecasting baselines. Next, we consider simpler approaches for making forecasts as the conversations happen (i.e., in an online fashion). First, we propose Cumulative BoW, a model that recomputes bag-of-words features on all comments seen thus far every time a new comment arrives. While this approach does exhibit the desired behavior of producing updated predictions for each new comment, it fails to account for relationships between comments.
This simple cumulative approach cannot be directly extended to models whose features are strictly based on a fixed number of comments, like Awry. An alternative is to use a sliding window: for a feature set based on a window of $W$ comments, upon each new comment we can extract features from a window containing that comment and the $W-1$ comments preceding it. We apply this to the Awry method and call this model Sliding Awry. For both these baselines, we aggregate comment-level predictions in the same way as in our main model.
CRAFT ablations. Finally, we consider two modified versions of the CRAFT model in order to evaluate the impact of two of its key components: (1) the pre-training step, and (2) its ability to capture inter-comment dependencies through its hierarchical memory.
To evaluate the impact of pre-training, we train the prediction component of CRAFT on only the labeled training data, without first pre-training the encoder layers with the unlabeled data. We find that given the relatively small size of labeled data, this baseline fails to successfully learn, and ends up performing at the level of random guessing. This result underscores the need for the pre-training step that can make use of unlabeled data.
To evaluate the impact of the hierarchical memory, we implement a simplified version of CRAFT where the memory size of the context encoder is zero (CRAFT $-$ CE), thus effectively acting as if the pre-training component is a vanilla seq2seq model. In other words, this model cannot capture inter-comment dependencies, and instead at each step makes a prediction based only on the utterance encoding of the latest comment.
Results. Table TABREF17 compares CRAFT to the baselines on the test splits (random baseline is 50%) and illustrates several key findings. First, we find that unsurprisingly, accounting for full conversational context is indeed helpful, with even the simple online baselines outperforming the fixed-window baselines. On both datasets, CRAFT outperforms all baselines (including the other online models) in terms of accuracy and F1. Furthermore, although it loses on precision (to CRAFT $-$ CE) and recall (to Cumulative BoW) individually on the Wikipedia data, CRAFT has the superior balance between the two, having both a visibly higher precision-recall curve and larger area under the curve (AUPR) than the baselines (Figure FIGREF20). This latter property is particularly useful in a practical setting, as it allows moderators to tune model performance to some desired precision without having to sacrifice as much in the way of recall (or vice versa) compared to the baselines and pre-existing solutions.
Analysis
We now examine the behavior of CRAFT in greater detail, to better understand its benefits and limitations. We specifically address the following questions: (1) How much early warning does the the model provide? (2) Does the model actually learn an order-sensitive representation of conversational context?
Early warning, but how early? The recent interest in forecasting antisocial behavior has been driven by a desire to provide pre-emptive, actionable warning to moderators. But does our model trigger early enough for any such practical goals?
For each personal attack correctly forecasted by our model, we count the number of comments elapsed between the time the model is first triggered and the attack. Figure FIGREF22 shows the distribution of these counts: on average, the model warns of an attack 3 comments before it actually happens (4 comments for CMV). To further evaluate how much time this early warning would give to the moderator, we also consider the difference in timestamps between the comment where the model first triggers and the comment containing the actual attack. Over 50% of conversations get at least 3 hours of advance warning (2 hours for CMV). Moreover, 39% of conversations get at least 12 hours of early warning before they derail.
Does order matter? One motivation behind the design of our model was the intuition that comments in a conversation are not independent events; rather, the order in which they appear matters (e.g., a blunt comment followed by a polite one feels intuitively different from a polite comment followed by a blunt one). By design, CRAFT has the capacity to learn an order-sensitive representation of conversational context, but how can we know that this capacity is actually used? It is conceivable that the model is simply computing an order-insensitive “bag-of-features”. Neural network models are notorious for their lack of transparency, precluding an analysis of how exactly CRAFT models conversational context. Nevertheless, through two simple exploratory experiments, we seek to show that it does not completely ignore comment order.
The first experiment for testing whether the model accounts for comment order is a prefix-shuffling experiment, visualized in Figure FIGREF23. For each conversation that the model predicts will derail, let $t$ denote the index of the triggering comment, i.e., the index where the model first made a derailment forecast. We then construct synthetic conversations by taking the first $t-1$ comments (henceforth referred to as the prefix) and randomizing their order. Finally, we count how often the model no longer predicts derailment at index $t$ in the synthetic conversations. If the model were ignoring comment order, its prediction should remain unchanged (as it remains for the Cumulative BoW baseline), since the actual content of the first $t$ comments has not changed (and CRAFT inference is deterministic). We instead find that in roughly one fifth of cases (12% for CMV) the model changes its prediction on the synthetic conversations. This suggests that CRAFT learns an order-sensitive representation of context, not a mere “bag-of-features”.
To more concretely quantify how much this order-sensitive context modeling helps with prediction, we can actively prevent the model from learning and exploiting any order-related dynamics. We achieve this through another type of shuffling experiment, where we go back even further and shuffle the comment order in the conversations used for pre-training, fine-tuning and testing. This procedure preserves the model's ability to capture signals present within the individual comments processed so far, as the utterance encoder is unaffected, but inhibits it from capturing any meaningful order-sensitive dynamics. We find that this hurts the model's performance (65% accuracy for Wikipedia, 59.5% for CMV), lowering it to a level similar to that of the version where we completely disable the context encoder.
Taken together, these experiments provide evidence that CRAFT uses its capacity to model conversational context in an order-sensitive fashion, and that it makes effective use of the dynamics within. An important avenue for future work would be developing more transparent models that can shed light on exactly what kinds of order-related features are being extracted and how they are used in prediction.
Conclusions and Future Work
In this work, we introduced a model for forecasting conversational events that processes comments as they happen and takes the full conversational context into account to make an updated prediction at each step. This model fills a void in the existing literature on conversational forecasting, simultaneously addressing the dual challenges of capturing inter-comment dynamics and dealing with an unknown horizon. We find that our model achieves state-of-the-art performance on the task of forecasting derailment in two different datasets that we release publicly. We further show that the resulting system can provide substantial prior notice of derailment, opening up the potential for preemptive interventions by human moderators BIBREF65.
While we have focused specifically on the task of forecasting derailment, we view this work as a step towards a more general model for real-time forecasting of other types of emergent properties of conversations. Follow-up work could adapt the CRAFT architecture to address other forecasting tasks mentioned in Section SECREF2—including those for which the outcome is extraneous to the conversation. We expect different tasks to be informed by different types of inter-comment dynamics, and further architecture extensions could add additional supervised fine-tuning in order to direct it to focus on specific dynamics that might be relevant to the task (e.g., exchange of ideas between interlocutors or stonewalling).
With respect to forecasting derailment, there remain open questions regarding what human moderators actually desire from an early-warning system, which would affect the design of a practical system based on this work. For instance, how early does a warning need to be in order for moderators to find it useful? What is the optimal balance between precision, recall, and false positive rate at which such a system is truly improving moderator productivity rather than wasting their time through false positives? What are the ethical implications of such a system? Follow-up work could run a user study of a prototype system with actual moderators to address these questions.
A practical limitation of the current analysis is that it relies on balanced datasets, while derailment is a relatively rare event for which a more restrictive trigger threshold would be appropriate. While our analysis of the precision-recall curve suggests the system is robust across multiple thresholds ($AUPR=0.7$), additional work is needed to establish whether the recall tradeoff would be acceptable in practice.
Finally, one major limitation of the present work is that it assigns a single label to each conversation: does it derail or not? In reality, derailment need not spell the end of a conversation; it is possible that a conversation could get back on track, suffer a repeat occurrence of antisocial behavior, or any number of other trajectories. It would be exciting to consider finer-grained forecasting of conversational trajectories, accounting for the natural—and sometimes chaotic—ebb-and-flow of human interactions.
Acknowledgements. We thank Caleb Chiam, Liye Fu, Lillian Lee, Alexandru Niculescu-Mizil, Andrew Wang and Justine Zhang for insightful conversations (with unknown horizon), Aditya Jha for his great help with implementing and running the crowd-sourcing tasks, Thomas Davidson and Claire Liang for exploratory data annotation, as well as the anonymous reviewers for their helpful comments. This work is supported in part by the NSF CAREER award IIS-1750615 and by the NSF Grant SES-1741441.","['The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: ""Don\'t be rude or hostile to others users.""']",4779,qasper_e,en,,10f5ce4c9054e6a2508cc67328ca303d3b5ee57bae2b7fd9,"The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: ""Don\'t be rude or hostile to others users.""",409
How do profile changes vary for influential leads and their followers over the social movement?,"Introduction
User profiles on social media platforms serve as a virtual introduction of the users. People often maintain their online profile space to reflect their likes and values. Further, how users maintain their profile helps them develop relationship with coveted audience BIBREF0. A user profile on Twitter is composed of several attributes with some of the most prominent ones being the profile name, screen name, profile image, location, description, followers count, and friend count. While the screen name, display name, profile image, and description identify the user, the follower and friend counts represent the user's social connectivity. Profile changes might represent identity choice at a small level. However, previous studies have shown that on a broader level, profile changes may be an indication of a rise in a social movement BIBREF1, BIBREF2.
In this work, we conduct a large-scale study of the profile change behavior of users on Twitter during the 2019 general elections in India. These elections are of interest from the perspective of social media analysis for many reasons. Firstly, the general elections in India were held in 7 phases from 11th April, 2019 to 19th May, 2019. Thus, the elections serve as a rich source of data for social movements and changing political alignments throughout the two months. Secondly, the increase in the Internet user-base and wider adoption of smartphones has made social media outreach an essential part of the broader election campaign. Thus, there exists a considerable volume of political discourse data that can be mined to gain interesting insights. Twitter was widely used for political discourse even during the 2014 general elections BIBREF3 and the number was bound to increase this year. A piece of evidence in favor of this argument is the enormous 49 million followers that Narendra Modi, the Prime Minister of India has on Twitter.
We believe profile attributes have significant potential to understand how social media played a vital role in the election campaign by political parties as well as the supporters during the #LokSabhaElections2019. A prominent example of the use of profile changes for political campaigning is the #MainBhiChowkidar campaign launched by the Bhartiya Janta Party (BJP) during the #LokSabhaElections2019. In Figure FIGREF2, we show the first occurrence of #MainBhiChowkidar campaign on Twitter which was launched in response to an opposition campaign called “Chowkidar chor hai” (The watchman is a thief). On 17th March 2019, the Indian Prime Minister Narendra Modi stepped up this campaign and added Chowkidar (Watchman) to his display name. This spurred off a social movement on Twitter with users from across the nation and political spectrum updating their profiles on Twitter by prefixing “Chowkidar” to their display name BIBREF4. We are thus interested in studying the different facets of this campaign, in particular, the name changes in the display name of accounts, verified, and others.
In Figure FIGREF3, we compare the Twitter profiles of the two leaders of national parties during Lok Sabha Election, 2019. The top profile belongs to @narendramodi (Narendra Modi), PM of India and member of the ruling party Bharatiya Janata Party (BJP), while the bottom profile belongs to @RahulGandhi (Rahul Gandhi), president of Indian National Congress (INC). Figure FIGREF3 shows a snapshot of both leaders before, during, and after the Lok Sabha Elections in 2019. The ruling leader showed brevity in the use of his profile information for political campaigning and changed his profile information during and after the elections, which was not the case with opposition.
Intending to understand the political discourse from the lens of profile attributes on social media, we address the following questions.
Introduction ::: Research Questions
To analyze the importance of user-profiles in elections, we need to distinguish between the profile change behavior of political accounts and follower accounts. This brings us to the first research question:
RQ1 [Comparison]: How often do political accounts and common user accounts change their profile information and which profile attributes are changed the most.
The political inclination of Twitter users can be classified based on the content of their past tweets and community detection methods BIBREF5. However, the same analysis has not been extended to the study of user-profiles in detail. We thus, study the following:
RQ2 [Political Inclination]: Do users reveal their political inclination on Twitter directly/indirectly using their user profiles only?
Behavior contagion is a phenomenon where individuals adopt a behavior if one of their opinion leaders adopts it BIBREF6. We analyze the profile change behavior of accounts based on the “Chowkidar” movement with the intent to find behavior contagion of an opinion leader. Thus, our third research question is:
RQ3 [Behavior Contagion]: Can profile change behavior be catered to behavior contagion effect where followers tend to adopt an opinion as their opinion leader adopts it?
We are interested in knowing the common attributes of the people who took part in the chowkidar movement. To characterize these users, we need to analyze the major topic patterns that these users engage in. Thus, we ask the question:
RQ4 [Topic Modelling]: What are the most discussed topics amongst the users that were part of the Chowkidar campaign?
Introduction ::: Contributions
In summary, our main contributions are:
We collect daily and weekly profile snapshots of $1,293$ political handles on Twitter and over 55 million of their followers respectively over a period of two months during and after the #LokSabhaElections2019. We will make the data and code available on our website.
We analyze the political and follower handles for profile changes over the snapshots and show that the political handles engage in more profile change behavior. We also show that certain profile attributes are changed more frequently than others with subtle differences in the behavior of political and follower accounts.
We show users display support for a political party on Twitter through both organic means like party name mentions on their profile, and inorganic means such as showing support for a movement like #MainBhiChowkidar. We analyze the Chowkidar movement in detail and demonstrate that it illustrates a contagion behavior.
We perform topic modelling of the users who took part in the Chowkidar movement and show that they were most likely Narendra Modi supporters and mostly discussed political topics on Twitter.
Dataset Collection and Description
Our data collection process is divided into two stages. We first manually curate a set of political accounts on Twitter. We use this set of users as a seed set and then collect the profile information of all the followers of these handles.
Seed set: To answer the RQ1, we first manually curate a set of $2,577$ Twitter handles of politicians and official/unofficial party handles involved in Lok Sabha Elections 2019. Let this set of users be called $\mathbf {U}$. Of the $2,577$ curated handles, only $1,293$ were verified at the beginning of April. Let this set of verified handles be called $\mathbf {P}$. We collect a snapshot of the profile information of $\mathbf {P}$ daily for two months from 5th April - 5th June 2019.
Tracked set: We use the set $P$ of verified Twitter handles as a seed set and collect the profile information of all their followers. We hypothesize that the users who follow one or more political handles are more likely to express their political orientation on Twitter. Thus, we consider only the followers of all the political handles in our work. The total number of followers of all the handles in set $\mathbf {P}$ are 600 million, of which only $55,830,844$ users are unique. Owing to Twitter API constraints, we collect snapshots of user-profiles for these handles only once a week over the two months of April 5 - June 5, 2019. There were a total of 9 snapshots, of which 7 coincided with the phases of the election. The exact dates on which the snapshots were taken has been mentioned in Table TABREF10. As we collect snapshots over the of two months, the number of followers of political handles in set $P$ can increase significantly. Similarly, a small subset of followers of handles in $P$ could also have been suspended/deleted. To maintain consistency in our analysis, we only use those handles for our analysis that are present in all the 9 snapshots. We call this set of users as $\mathbf {S}$. The set $\mathbf {S}$ consists of 49,433,640 unique accounts.
Dataset: Our dataset consists of daily and weekly snapshots of profile information of users in set $\mathbf {S}$ and $\mathbf {P}$ respectively. We utilize the Twitter API to collect the profile information of users in both the above sets. Twitter API returns a user object that consists of a total of 34 attributes like name, screen name, description, profile image, location, and followers. We further pre-process the dataset to remove all the handles that were inactive for a long time before proceeding with further analysis.
In set $\mathbf {S}$, more than 15 million out of the total 49 million users have made no tweets and have marked no favourites as well. We call the remaining 34 million users as Active users. Of the Active users, only 5 million users have made a tweet in year 2019. The distribution of the last tweet time of all users in set $S$ is shown in Figure FIGREF14. The 5 million twitter users since the start of 2019 fall in favor of the argument that a minority of users are only responsible for most of the political opinions BIBREF7. We believe there is a correlation between Active users and users who make changes in profile attribute. We, therefore use the set of 34 million users(Active Users of set $\mathbf {S}$ ) for our further analysis.
Profile Attribute Analysis
In this section we characterize the profile change behavior of political accounts (Set $P$) and follower accounts (Set $S$).
Profile Attribute Analysis ::: Political Handles
Increase in the Internet user-base and widespread adoption of smartphones in the country has led to an increase in the importance of social media campaigns during the elections. This is evident by the fact that in our dataset (Set $U$), we found that a total of 54 handles got verified throughout 61 snapshots. Moreover the average number of followers of handles in set $P$ increased from $3,08,716.84$ to $3,19,881.09$ over the 61 snapshots. Given that the average followers of set $P$ increased by approximately $3\%$ and 54 profile handles got verified in the span of 61 days, we believe that efforts of leaders on social media campaign can't be denied.
We consider 5 major profile attributes for further analysis. These attributes are username, display name, profile image, location and description respectively. The username is a unique handle or screen name associated with each twitter user that appears on the profile URL and is used to communicate with each other on Twitter. Some username examples are $@narendramodi$, $@RahulGandhi$ etc. The display name on the other hand, is merely a personal identifier that is displayed on the profile page of a user, e.g. `Narendra Modi', `Rahul Gandhi', etc. The description is a string to describe the account, while the location is user-defined profile location. We considered the 5 profile attributes as stated above since these attributes are key elements of a users identity and saliently define users likes and values BIBREF2.
For the set of users in $P$, the total number of changes in these 5 profile attributes over 61 snapshots is 974. Figure FIGREF12 shows the distribution of profile changes for specific attributes over all the 61 snapshots. The most changed attribute among the political handles is the profile image followed by display name. In the same Figure, there is a sharp increase in the number of changes to the display name attribute on 23rd May, 2019. The increase is because @narendramodi removed Chowkidar from the display name of his Twitter handle which resulted in many other political handles doing the same. Another interesting trend is that none of the handles changed their username over all the snapshots. This could be attributed to the fact that Twitter discourages a change of usernames for verified accounts as the verified status might not remain intact after the change BIBREF8.
Profile Attribute Analysis ::: Follower Handles
To characterize the profile changing behavior amongst the followers , we study the profile snapshots of all the users in set $S$ over 9 snapshots.
Of all the active user accounts, $1,363,499$ users made at least one change to their profile attributes over all the snapshots. Thus, over $3\%$ of all the active users engaged in some form of profile change activity over the 9 snapshots. Of these $1,363,499$ users, more than $95\%$ of the users made only 1 change to their profile, and around 275 users made more than 20 changes. On an average, each one of these $1,363,499$ users made about 2 changes to their profiles with a standard deviation of $1.42$.
In order to compare the profile change behavior of political accounts and follower accounts, we analyzed the profile changes of set $\mathbf {P}$ over the 9 snapshots only. For the same 9 snapshots, $54.9\%$ of all users in set $\mathbf {P}$ made at least one change to their profile attributes. This is in sharp contrast to the the users of set $\mathbf {S}$, where only $3\%$ of users made any changes to their profile attributes. Similarly, users of set $\mathbf {P}$ made $75.32\%$ changes to their profiles on an average, which is 25 times more than the set $\mathbf {S}$.
INSIGHT 1: Political handles are more likely to engage in profile changing behavior as compared to their followers.
In Figure FIGREF15, we plot the number of changes in given attributes over all the snapshots for the users in set $S$. From this plot, we find that not all the attributes are modified at an equal rate. Profile image and Location are the most changed profile attributes and account for nearly $34\%$ and $25\%$ respectively of the total profile changes in our dataset. We analyze the trends in Figure FIGREF12 and find that the political handles do not change their usernames at all. This is in contrast to the trend in Figure FIGREF15 where we see that there are a lot of handles that change their usernames multiple times. The most likely reason for the same is that most of the follower handles are not verified and would not loose their verified status on changing their username.
The follower handles also use their usernames to show support to a movement or person and revert back to their original username later. Out of the $79,991$ username changes, $1,711$ users of set ${S}$ went back and forth to the same name which included adding of Election related keywords. Examples of few such cases in our dataset are shown in Table TABREF16.
INSIGHT 2: Users do not change all profile attributes equally, and the profile attributes that political and follower handles focus on are different.
Profile Attribute Analysis ::: Similarity analysis
Change in an attribute in the profile involves replacing an existing attribute with a new one. We analyze the similarity between the current, new display names and usernames used by users in set $\mathbf {P}$ and $\mathbf {S}$ respectively. For this analysis, we consider only the users who have changed their username or display name at least once in all 9 snapshots. We use the Longest Common Subsequence (LCS) to compute the similarity between two strings. Thus, for each user, we compute the average LCS score between all possible pairs of unique display names and usernames adopted by them. A high average LCS score indicates that the newer username or display name were not significantly different from the previous ones.
The graph in Figure FIGREF18 shows that nearly $80\%$ of the users in set $S$ made more significant changes to their Display Names as compared to the political handles. (This can be seen by considering a horizontal line on the graph. For any given fraction of users, the average LCS score of set $S$ (Followers) is lesser than set $P$ (Politicians) for approximately $80\%$ percent of users). For around $10\%$ of the users in set $P$, the new and old profile attributes were unrelated (LCS score < $0.5$) and for users in set $S$, this value went up to $30\%$.
INSIGHT 3: Political handles tend to make new changes related to previous attribute values. However, the followers make comparatively less related changes to previous attribute values.
Political support through profile meta data
In this section, we first describe the phenomenon of mention of political parties names in the profile attributes of users. This is followed by the analysis of profiles that make specific mentions of political handles in their profile attributes. Both of these constitute an organic way of showing support to a party and does not involve any direct campaigning by the parties. We also study in detail the #MainBhiChowkidar campaign and analyze the corresponding change in profile attributes associated with it.
Political support through profile meta data ::: Party mention in the profile attributes
To sustain our hypothesis of party mention being an organic behavior, we analyzed party name mentions in the profiles of users in set $\mathbf {S}$. For this analysis, we focused on two of the major national parties in India, namely Indian National Congress (INC) and Bhartiya Janta Party (BJP). We specifically search for mentions of the party names and a few of its variants in the display name, username and description of the user profiles. We find that $50,432$ users have mentioned “BJP” (or its variants) on their description in the first snapshot of our data itself. This constitutes approximately $1\%$ of the total number of active users in our dataset, implying a substantial presence of the party on Twitter. Moreover, $2,638$ users added “BJP” (or its variants) to their description over the course of data collection and $1,638$ users removed “BJP” from their description. Table TABREF20 shows the number of mentions of both parties in the different profile attributes.
Political support through profile meta data ::: Who follows who
We observe that a lot of users in our dataset wrote that they are proud to be followed by a famous political handle in their description. We show example of such cases in Table TABREF22.
We perform an exhaustive search of all such mentions in the description attribute of the users and find $1,164$ instances of this phenomenon. This analysis and the party mention analysis in Section SECREF19, both testify that people often display their political inclination on Twitter via their profile attributes itself.
INSIGHT 4: Twitter users often display their political inclinations in their profile attributes itself and are pretty open about it.
Political support through profile meta data ::: The Chowkidar movement
As discussed is Section SECREF1, @narendramodi added “Chowkidar” to his display name in response to an opposition campaign called “Chowkidar chor hai”. In a coordinated campaign, several other leaders and ministers of BJP also changed their Twitter profiles by adding the prefix Chowkidar to their display names. The movement, however, did not remain confined amongst the members of the party themselves and soon, several Twitter users updated their profiles as well. We opine that the whole campaign of adding Chowkidar to the profile attributes show an inorganic behavior, with political leaders acting as the catalyst. An interesting aspect of this campaign was the fact that the users used several different variants of the word Chowkidar while adding it to their Twitter profiles. Some of the most common variants of Chowkidar that were present in our dataset along with its frequency of use is shown in Figure FIGREF24.
We utilize a regular expression query to exhaustively search all the variants of the word Chowkidar in our dataset. We found $2,60,607$ instances of users in set $S$ added Chowkidar (or its variants) to their display names. These $2,60,607$ instances comprised a total of 241 unique chowkidar variants of which 225 have been used lesser than 500 times. We also perform the same analysis for the other profile attributes like description and username as well. The number of users who added Chowkidar to their description and username are $14,409$ and $12,651$ respectively. The union and intersection of all these users are $270,945$ and 727 respectively, implying that most users added Chowkidar to only one of their profile attributes.
We believe, the effect of changing the profile attribute in accordance with Prime Minister's campaign is an example of inorganic behavior contagion BIBREF6, BIBREF9. The authors in BIBREF6 argue that opinion diffuses easily in a network if it comes from opinion leaders who are considered to be users with a very high number of followers. We see a similar behavior contagion in our dataset with respect to the Chowkidar movement.
We analyze the similarity of the display names with respect to the behavior contagion of Chowkidar movement. In the CDF plot of Figure FIGREF18, a significant spike is observed in the region of LCS values between $0.6$-$0.8$. This spike is caused mostly due to the political handles, who added Chowkidar to their display names which accounted for $95.7\%$ of the users in this region. In set $P$, a total of 373 people added the specific keyword Chowkidar to their display names of which 315 lie in the normalized LCS range of $0.6$ to $0.8$. We perform similar analysis on the users of set $S$ and find that around $57\%$ of the users within the region of $0.6$-$0.8$ LCS added specific keyword Chowkidar to their display names. We perform similar analysis on other campaign keywords like Ab Hoga Nyay as well but none of them have the same level of influence or popularity like that of the “Chowkidar” and hence we call it as the “Chowkidar Movement”.
On 23rd May, Narendra Modi removed Chowkidar from his Twitter profile and urged others to do the same as shown in Figure FIGREF25. While most users followed Narendra Modi's instructions and removed Chowkidar from their profiles, some users still continued to add chowkidar to their names. The weekly addition and removal of Chowkidar from user profiles is presented in detail in Figure FIGREF26. Thus, the behavior contagion is evident from the fact that after Narendra Modi removed Chowkidar on 23rd May, majority of the population in set $\mathbf {S}$ removed it too.
INSIGHT 5: The Chowkidar movement was very widespread and illustrated contagion behavior.
Topic modelling of users
A large number of people were part of the Chowkidar movement. Hence, it is important to gauge their level of interest/support to the party in order to accurately judge the impact of the campaign. We thus perform topic modelling of the description attribute of these users. We utilize the LDA Mallet model BIBREF10 for topic modelling after performing the relevant preprocessing steps on the data. We set the number of topics to 10 as it gave the highest coherence score in our experiments. The 10 topics and the 5 most important keywords belonging to these topics are shown below in Figure FIGREF27. These topics reveal several interesting insights. The most discussed topic amongst these users involves Narendra Modi and includes words like “namo” (slang for Narendra Modi), supporter, nationalist and “bhakt” (devout believer). A few other topics also involve politics like topic 2 and topic 5. Topic 2, in particular contains the words chowkidar, student, and study, indicating that a significant set of users who were part of this campaign were actually students. Topic number 0 accounts for more than $40\%$ of the total descriptions, which essentially means that most of the people involved in this movement were supporting BJP or Narendra Modi. We also plot the topics associated with each user's description on a 2D plane by performing tSNE BIBREF11 on the 10 dimensional data to ascertain the variation in the type of topics. The topics are pretty distinct and well separated on the 2D plane as can be seen in Figure FIGREF30.
INSIGHT 6: The users who were part of the chowkidar movement were mostly Narendra Modi supporters and engaged in political topics
Related Work
Social Media profiles are a way to present oneself and to cultivate relationship with like minded or desired audience. BIBREF0, BIBREF12. With change in time and interest, users on Twitter change their profile names, description, screen names, location or language BIBREF2. Some of these changes are organic BIBREF13, BIBREF2, while other changes can be triggered by certain events BIBREF14, BIBREF15. In this section, we look into the previous literature which are related to user profile attribute's change behavior. We further look into how Twitter has been used to initiate or sustain movements BIBREF14, BIBREF15, BIBREF1. Finally we investigate how the contagion behavior adaptation spread through opinion leaders BIBREF6, BIBREF9 and conclude with our work's contribution to the literature.
Related Work ::: Profile attribute change behavior
Previous studies have focused on change in username attribute extensively, as usernames in Twitter helps in linkability, mentions and becomes part of the profile's URL BIBREF16, BIBREF13, BIBREF17. While the focus of few works is to understand the reason the users change their profile names BIBREF16, BIBREF2, others strongly argue username change is a bad idea BIBREF17, BIBREF13. Jain et al. argue that the main reason people opt to change their username is to leverage more tweet space. Similarly, the work by Mariconti et al. suggests that the change in username may be a result of name squatting, where a person may take away the username of a popular handle for malicious reasons. More recent studies in the area have focused on several profile attributes changes BIBREF2. They suggest that profile attributes can be used to represent oneself at micro-level as well as represent social movement at macro-level.
Related Work ::: Social Media for online movements
The role of Social Media during social movements have been studied very extensively BIBREF14, BIBREF18, BIBREF19, BIBREF15. With the help of Social Media, participation and ease to express oneself becomes easy BIBREF1. There have been many studies where Twitter has been used to gather the insight of online users in the form of tweets BIBREF1, BIBREF14, BIBREF15. The authors in BIBREF14 study the tweets to understand how people reacted to decriminalization of LGBT in India. The study of Gezi Park protest shows how the protest took a political turn, as people changed their usernames to show support to their political leaders BIBREF1.
The success of the party “Alternative for Deutschland(AFD)” in German Federal Elections, 2017 can be catered to its large online presence BIBREF18. The authors in BIBREF18 study the tweets and retweets networks during the German elections and analyze how users are organized into communities and how information flows between them. The role of influential leaders during elections has also been studied in detail in studies like BIBREF20. This brings us to the study of how influential leaders contribute to spread and adaptation of events.
Related Work ::: Adoption of behavior from leaders
The real and virtual world are intertwined in a way that any speech, event, or action of leaders may trigger the changes is behavior of people online BIBREF1, BIBREF21. Opinion leaders have been defined as `the individuals who were likely to influence other persons in their immediate environment.' BIBREF22 Opinion leaders on social media platforms help in adoption of behavior by followers easily BIBREF9, BIBREF6. The performance and effectiveness of the opinion leader directly affect the contagion of the leader BIBREF9. On Twitter, opinion leaders show high motivation for information seeking and public expression. Moreover, they make a significant contribution to people's political involvement BIBREF21.
Our work contributes to this literature in two major ways. First, we investigate how profile attribute changes are similar or different from the influential leaders in the country and how much of these changes are the result of the same. Then we see how the users change their profile information in the midst of election in the largest democracy in the world.
Discussion
In this paper, we study the use of profile attributes on Twitter as the major endorsement during #LokSabhaElectios2019. We identify $1,293$ verified political leader's profile and extract $49,433,640$ unique followers. We collect the daily and weekly snapshots of the political leaders and the follower handles from 5th April, 2019 to 5th June, 2019. With the constraint of Twitter API, we collected a total of 9 snapshots of the follower's profile and 61 snapshots of Political handles. We consider the 5 major attributes for the analysis, including, Profile Name, Username, Description, Profile Image, and Location.
Firstly, we analyze the most changed attributes among the 5 major attributes. We find that most of the users made at-most 4 changes during the election campaign, with profile image being the most changed attribute for both the political as well as follower handles. We also show that political handles made more changes to profile attributes than the follower handles. About $80\%$ changes made by the followers in the period of data collection was related to #LokSabhaElectios2019. While the profile handles made changes, the changes mostly included going back and forth to same profile attribute, with the change in attribute being active support to a political party or political leader.
We argue that we can predict the political inclination of a user using just the profile attribute of the users. We further show that the presence of party name in the profile attribute can be considered as an organic behavior and signals support to a party. However, we argue that the addition of election campaign related keywords to the profile is a form of inorganic behavior. The inorganic behavior analysis falls inline with behavior contagion, where the followers tend to adapt to the behavior of their opinion leaders. The “Chowkidar movement” showed a similar effect in the #LokSabhaElectios2019, which was evident by how the other political leaders and followers of BJP party added chowkidar to their profile attributes after @narendramodi did. We thus, argue that people don't shy away from showing support to political parties through profile information.
We believe our analysis will be helpful to understand how social movement like election are perceived and sustained through profile attributes on social media. Our findings provide baseline data for study in the direction of election campaign and to further analyze people's perspective on elections.","['Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values.']",5092,qasper_e,en,,d8fa56cc40d195ace5ae35d187db59d7480cf969938ad3db,"Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values.",353
how was the data collected?,"Introduction
Ambiguity and implicitness are inherent properties of natural language that cause challenges for computational models of language understanding. In everyday communication, people assume a shared common ground which forms a basis for efficiently resolving ambiguities and for inferring implicit information. Thus, recoverable information is often left unmentioned or underspecified. Such information may include encyclopedic and commonsense knowledge. This work focuses on commonsense knowledge about everyday activities, so-called scripts.
This paper introduces a dataset to evaluate natural language understanding approaches with a focus on interpretation processes requiring inference based on commonsense knowledge. In particular, we present MCScript, a dataset for assessing the contribution of script knowledge to machine comprehension. Scripts are sequences of events describing stereotypical human activities (also called scenarios), for example baking a cake or taking a bus BIBREF0 . To illustrate the importance of script knowledge, consider Example ( SECREF1 ):
Without using commonsense knowledge, it may be difficult to tell who ate the food: Rachel or the waitress. In contrast, if we utilize commonsense knowledge, in particular, script knowledge about the eating in a restaurant scenario, we can make the following inferences: Rachel is most likely a customer, since she received an order. It is usually the customer, and not the waitress, who eats the ordered food. So She most likely refers to Rachel.
Various approaches for script knowledge extraction and processing have been proposed in recent years. However, systems have been evaluated for specific aspects of script knowledge only, such as event ordering BIBREF1 , BIBREF2 , event paraphrasing BIBREF3 , BIBREF4 or event prediction (namely, the narrative cloze task BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 ). These evaluation methods lack a clear connection to real-world tasks. Our MCScript dataset provides an extrinsic evaluation framework, based on text comprehension involving commonsense knowledge. This framework makes it possible to assess system performance in a multiple-choice question answering setting, without imposing any specific structural or methodical requirements.
MCScript is a collection of (1) narrative texts, (2) questions of various types referring to these texts, and (3) pairs of answer candidates for each question. It comprises approx. 2,100 texts and a total of approx. 14,000 questions. Answering a substantial subset of questions requires knowledge beyond the facts mentioned in the text, i.e. it requires inference using commonsense knowledge about everyday activities. An example is given in Figure FIGREF2 . For both questions, the correct choice for an answer requires commonsense knowledge about the activity of planting a tree, which goes beyond what is mentioned in the text. Texts, questions, and answers were obtained through crowdsourcing. In order to ensure high quality, we manually validated and filtered the dataset. Due to our design of the data acquisition process, we ended up with a substantial subset of questions that require commonsense inference (27.4%).
Corpus
Machine comprehension datasets consist of three main components: texts, questions and answers. In this section, we describe our data collection for these 3 components. We first describe a series of pilot studies that we conducted in order to collect commonsense inference questions (Section SECREF4 ). In Section SECREF5 , we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk (henceforth MTurk). Section SECREF17 gives information about some necessary postprocessing steps and the dataset validation. Lastly, Section SECREF19 gives statistics about the final dataset.
Pilot Study
As a starting point for our pilots, we made use of texts from the InScript corpus BIBREF10 , which provides stories centered around everyday situations (see Section SECREF7 ). We conducted three different pilot studies to determine the best way of collecting questions that require inference over commonsense knowledge:
The most intuitive way of collecting reading comprehension questions is to show texts to workers and let them formulate questions and answers on the texts, which is what we tried internally in a first pilot. Since our focus is to provide an evaluation framework for inference over commonsense knowledge, we manually assessed the number of questions that indeed require common sense knowledge. We found too many questions and answers collected in this manner to be lexically close to the text.
In a second pilot, we investigated the option to take the questions collected for one text and show them as questions for another text of the same scenario. While this method resulted in a larger number of questions that required inference, we found the majority of questions to not make sense at all when paired with another text. Many questions were specific to a text (and not to a scenario), requiring details that could not be answered from other texts. Since the two previous pilot setups resulted in questions that centered around the texts themselves, we decided for a third pilot to not show workers any specific texts at all. Instead, we asked for questions that centered around a specific script scenario (e.g. eating in a restaurant). We found this mode of collection to result in questions that have the right level of specificity for our purposes: namely, questions that are related to a scenario and that can be answered from different texts (about that scenario), but for which a text does not need to provide the answer explicitly.
The next section will describe the mode of collection chosen for the final dataset, based on the third pilot, in more detail.
Data Collection
As mentioned in the previous section, we decided to base the question collection on script scenarios rather than specific texts. As a starting point for our data collection, we use scenarios from three script data collections BIBREF3 , BIBREF11 , BIBREF12 . Together, these resources contain more than 200 scenarios. To make sure that scenarios have different complexity and content, we selected 80 of them and came up with 20 new scenarios. Together with the 10 scenarios from InScript, we end up with a total of 110 scenarios.
For the collection of texts, we followed modiinscript, where workers were asked to write a story about a given activity “as if explaining it to a child”. This results in elaborate and explicit texts that are centered around a single scenario. Consequently, the texts are syntactically simple, facilitating machine comprehension models to focus on semantic challenges and inference. We collected 20 texts for each scenario. Each participant was allowed to write only one story per scenario, but work on as many scenarios as they liked. For each of the 10 scenarios from InScript, we randomly selected 20 existing texts from that resource.
For collecting questions, workers were instructed to “imagine they told a story about a certain scenario to a child and want to test if the child understood everything correctly”. This instruction also ensured that questions are linguistically simple, elaborate and explicit. Workers were asked to formulate questions about details of such a situation, i.e. independent of a concrete narrative. This resulted in questions, the answer to which is not literally mentioned in the text. To cover a broad range of question types, we asked participants to write 3 temporal questions (asking about time points and event order), 3 content questions (asking about persons or details in the scenario) and 3 reasoning questions (asking how or why something happened). They were also asked to formulate 6 free questions, which resulted in a total of 15 questions. Asking each worker for a high number of questions enforced that more creative questions were formulated, which go beyond obvious questions for a scenario. Since participants were not shown a concrete story, we asked them to use the neutral pronoun “they” to address the protagonist of the story. We permitted participants to work on as many scenarios as desired and we collected questions from 10 participants per scenario.
Our mode of question collection results in questions that are not associated with specific texts. For each text, we collected answers for 15 questions that were randomly selected from the same scenario. Since questions and texts were collected independently, answering a random question is not always possible for a given text. Therefore, we carried out answer collection in two steps. In the first step, we asked participants to assign a category to each text–question pair.
We distinguish two categories of answerable questions: The category text-based was assigned to questions that can be answered from the text directly. If the answer could only be inferred by using commonsense knowledge, the category script-based was assigned. Making this distinction is interesting for evaluation purposes, since it enables us to estimate the number of commonsense inference questions. For questions that did not make sense at all given a text, unfitting was assigned. If a question made sense for a text, but it was impossible to find an answer, the label unknown was used.
In a second step, we told participants to formulate a plausible correct and a plausible incorrect answer candidate to answerable questions (text-based or script-based). To level out the effort between answerable and non-answerable questions, participants had to write a new question when selecting unknown or unfitting. In order to get reliable judgments about whether or not a question can be answered, we collected data from 5 participants for each question and decided on the final category via majority vote (at least 3 out of 5). Consequently, for each question with a majority vote on either text-based or script-based, there are 3 to 5 correct and incorrect answer candidates, one from each participant who agreed on the category. Questions without a clear majority vote or with ties were not included in the dataset.
We performed four post-processing steps on the collected data.
We manually filtered out texts that were instructional rather than narrative.
All texts, questions and answers were spellchecked by running aSpell and manually inspecting all corrections proposed by the spellchecker.
We found that some participants did not use “they” when referring to the protagonist. We identified “I”, “you”, “he”, “she”, “my”, “your”, “his”, “her” and “the person” as most common alternatives and replaced each appearance manually with “they” or “their”, if appropriate.
We manually filtered out invalid questions, e.g. questions that are suggestive (“Should you ask an adult before using a knife?”) or that ask for the personal opinion of the reader (“Do you think going to the museum was a good idea?”).
Answer Selection and Validation
We finalized the dataset by selecting one correct and one incorrect answer for each question–text pair. To increase the proportion of non-trivial inference cases, we chose the candidate with the lowest lexical overlap with the text from the set of correct answer candidates as correct answer. Using this principle also for incorrect answers leads to problems. We found that many incorrect candidates were not plausible answers to a given question. Instead of selecting a candidate based on overlap, we hence decided to rely on majority vote and selected the candidate from the set of incorrect answers that was most often mentioned.
For this step, we normalized each candidate by lowercasing, deleting punctuation and stop words (articles, and, to and or), and transforming all number words into digits, using text2num. We merged all answers that were string-identical, contained another answer, or had a Levenshtein distance BIBREF13 of 3 or less to another answer. The “most frequent answer” was then selected based on how many other answers it was merged with. Only if there was no majority, we selected the candidate with the highest overlap with the text as a fallback. Due to annotation mistakes, we found a small number of chosen correct and incorrect answers to be inappropriate, that is, some “correct” answers were actually incorrect and vice versa. Therefore, we manually validated the complete dataset in a final step. We asked annotators to read all texts, questions, and answers, and to mark for each question whether the correct and incorrect answers were appropriate. If an answer was inappropriate or contained any errors, they selected a different answer from the set of collected candidates. For approximately 11.5% of the questions, at least one answer was replaced. 135 questions (approx. 1%) were excluded from the dataset because no appropriate correct or incorrect answer could be found.
Data Statistics
For all experiments, we admitted only experienced MTurk workers who are based in the US. One HIT consisted of writing one text for the text collection, formulating 15 questions for the question collection, or finding 15 pairs of answers for the answer collection. We paid $0.50 per HIT for the text and question collection, and $0.60 per HIT for the answer collection.
More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. For 13% of the questions, the workers did not agree on one of the 4 categories with a 3 out of 5 majority, so we did not include these questions in our dataset.
The distribution of category labels on the remaining 87% is shown in Table TABREF10 . 14,074 (52%) questions could be answered. Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based). After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). This ratio was manually verified based on a random sample of questions.
We split the dataset into training (9,731 questions on 1,470 texts), development (1,411 questions on 219 texts), and test set (2,797 questions on 430 texts). Each text appears only in one of the three sets. The complete set of texts for 5 scenarios was held out for the test set. The average text, question, and answer length is 196.0 words, 7.8 words, and 3.6 words, respectively. On average, there are 6.7 questions per text.
Figure FIGREF21 shows the distribution of question types in the dataset, which we identified using simple heuristics based on the first words of a question: Yes/no questions were identified as questions starting with an auxiliary or modal verb, all other question types were determined based on the question word.
We found that 29% of all questions are yes/no questions. Questions about details of a situation (such as what/ which and who) form the second most frequent question category. Temporal questions (when and how long/often) form approx. 11% of all questions. We leave a more detailed analysis of question types for future work.
Data Analysis
As can be seen from the data statistics, our mode of collection leads to a substantial proportion of questions that require inference using commonsense knowledge. Still, the dataset contains a large number of questions in which the answer is explicitly contained or implied by the text: Figure FIGREF22 shows passages from an example text of the dataset together with two such questions. For question Q1, the answer is given literally in the text. Answering question Q2 is not as simple; it can be solved, however, via standard semantic relatedness information (chicken and hotdogs are meat; water, soda and juice are drinks).
The following cases require commonsense inference to be decided. In all these cases, the answers are not overtly contained nor easily derivable from the respective texts. We do not show the full texts, but only the scenario names for each question.
Example UID23 refers to a library setting. Script knowledge helps in assessing that usually, paying is not an event when borrowing a book, which answers the question. Similarly, event information helps in answering the questions in Examples UID24 and UID25 . In Example UID26 , knowledge about the typical role of parents in the preparation of a picnic will enable a plausibility decision. Similarly, in Example UID27 , it is commonsense knowledge that showers usually take a few minutes rather than hours.
There are also cases in which the answer can be inferred from the text, but where commonsense knowledge is still beneficial: The text for example UID28 does not contain the information that breakfast is eaten in the morning, but it could still be inferred from many pointers in the text (e.g. phrases such as I woke up), or from commonsense knowledge.
These few examples illustrate that our dataset covers questions with a wide spectrum of difficulty, from rather simple questions that can be answered from the text to challenging inference problems.
Experiments
In this section, we assess the performance of baseline models on MCScript, using accuracy as the evaluation measure. We employ models of differing complexity: two unsupervised models using only word information and distributional information, respectively, and two supervised neural models. We assess performance on two dimensions: One, we show how well the models perform on text-based questions as compared to questions that require common sense for finding the correct answer. Two, we evaluate each model for each different question type.
Models
We first use a simple word matching baseline, by selecting the answer that has the highest literal overlap with the text. In case of a tie, we randomly select one of the answers.
The second baseline is a sliding window approach that looks at windows of INLINEFORM0 tokens on the text. Each text and each answer are represented as a sequence of word embeddings. The embeddings for each window of size INLINEFORM1 and each answer are then averaged to derive window and answer representations, respectively. The answer with the lowest cosine distance to one of the windows of the text is then selected as correct.
We employ a simple neural model as a third baseline. In this model, each text, question, and answer is represented by a vector. For a given sequence of words INLINEFORM0 , we compute this representation by averaging over the components of the word embeddings INLINEFORM1 that correspond to a word INLINEFORM2 , and then apply a linear transformation using a weight matrix. This procedure is applied to each answer INLINEFORM3 to derive an answer representation INLINEFORM4 . The representation of a text INLINEFORM5 and of a question INLINEFORM6 are computed in the same way. We use different weight matrices for INLINEFORM7 , INLINEFORM8 and INLINEFORM9 , respectively. A combined representation INLINEFORM10 for the text–question pair is then constructed using a bilinear transformation matrix INLINEFORM11 : DISPLAYFORM0
We compute a score for each answer by using the dot product and pass the scores for both answers through a softmax layer for prediction. The probability INLINEFORM0 for an answer INLINEFORM1 to be correct is thus defined as: DISPLAYFORM0
The attentive reader is a well-established machine comprehension model that reaches good performance e.g. on the CNN/Daily Mail corpus BIBREF14 , BIBREF15 . We use the model formulation by chen2016thorough and lai2017race, who employ bilinear weight functions to compute both attention and answer-text fit. Bi-directional GRUs are used to encode questions, texts and answers into hidden representations. For a question INLINEFORM0 and an answer INLINEFORM1 , the last state of the GRUs, INLINEFORM2 and INLINEFORM3 , are used as representations, while the text is encoded as a sequence of hidden states INLINEFORM4 . We then compute an attention score INLINEFORM5 for each hidden state INLINEFORM6 using the question representation INLINEFORM7 , a weight matrix INLINEFORM8 , and an attention bias INLINEFORM9 . Last, a text representation INLINEFORM10 is computed as a weighted average of the hidden representations: DISPLAYFORM0
The probability INLINEFORM0 of answer INLINEFORM1 being correct is then predicted using another bilinear weight matrix INLINEFORM2 , followed by an application of the softmax function over both answer options for the question: DISPLAYFORM0
Implementation Details
Texts, questions and answers were tokenized using NLTK and lowercased. We used 100-dimensional GloVe vectors BIBREF16 to embed each token. For the neural models, the embeddings are used to initialize the token representations, and are refined during training. For the sliding similarity window approach, we set INLINEFORM0 .
The vocabulary of the neural models was extracted from training and development data. For optimizing the bilinear model and the attentive reader, we used vanilla stochastic gradient descent with gradient clipping, if the norm of gradients exceeds 10. The size of the hidden layers was tuned to 64, with a learning rate of INLINEFORM0 , for both models. We apply a dropout of INLINEFORM1 to the word embeddings. Batch size was set to 25 and all models were trained for 150 epochs. During training, we measured performance on the development set, and we selected the model from the best performing epoch for testing.
Results and Evaluation
As an upper bound for model performance, we assess how well humans can solve our task. Two trained annotators labeled the correct answer on all instances of the test set. They agreed with the gold standard in 98.2 % of cases. This result shows that humans have no difficulty in finding the correct answer, irrespective of the question type.
Table TABREF37 shows the performance of the baseline models as compared to the human upper bound and a random baseline. As can be seen, neural models have a clear advantage over the pure word overlap baseline, which performs worst, with an accuracy of INLINEFORM0 .
The low accuracy is mostly due to the nature of correct answers in our data: Each correct answer has a low overlap with the text by design. Since the overlap model selects the answer with a high overlap to the text, it does not perform well. In particular, this also explains the very bad result on text-based questions. The sliding similarity window model does not outperform the simple word overlap model by a large margin: Distributional information alone is insufficient to handle complex questions in the dataset.
Both neural models outperform the unsupervised baselines by a large margin. When comparing the two models, the attentive reader is able to beat the bilinear model by only INLINEFORM0 . A possible explanation for this is that the attentive reader only attends to the text. Since many questions cannot be directly answered from the text, the attentive reader is not able to perform significantly better than a simpler neural model.
What is surprising is that the attentive reader works better on commonsense-based questions than on text questions. This can be explained by the fact that many commonsense questions do have prototypical answers within a scenario, irrespective of the text. The attentive reader is apparently able to just memorize these prototypical answers, thus achieving higher accuracy.
Inspecting attention values of the attentive reader, we found that in most cases, the model is unable to properly attend to the relevant parts of the text, even when the answer is literally given in the text. A possible explanation is that the model is confused by the large amount of questions that cannot be answered from the text directly, which might confound the computation of attention values.
Also, the attentive reader was originally constructed for reconstructing literal text spans as answers. Our mode of answer collection, however, results in many correct answers that cannot be found verbatim in the text. This presents difficulties for the attention mechanism.
The fact that an attention model outperforms a simple bilinear baseline only marginally shows that MCScript poses a new challenge to machine comprehension systems. Models concentrating solely on the text are insufficient to perform well on the data.
Figure FIGREF39 gives accuracy values of all baseline systems on the most frequent question types (appearing >25 times in the test data), as determined based on the question words (see Section SECREF19 ). The numbers depicted on the left-hand side of the y-axis represent model accuracy. The right-hand side of the y-axis indicates the number of times a question type appears in the test data.
The neural models unsurprisingly outperform the other models in most cases, and the difference for who questions is largest. A large number of these questions ask for the narrator of the story, who is usually not mentioned literally in the text, since most stories are written in the first person.
It is also apparent that all models perform rather badly on yes/no questions. Each model basically compares the answer to some representation of the text. For yes/no questions, this makes sense for less than half of all cases. For the majority of yes/no questions, however, answers consist only of yes or no, without further content words.
Related Work
In recent years, a number of reading comprehension datasets have been proposed, including MCTest BIBREF17 , BAbI BIBREF18 , the Children's Book Test (CBT, hill2015goldilocks), CNN/Daily Mail BIBREF14 , the Stanford Question Answering Dataset (SQuAD, rajpurkar2016squad), and RACE BIBREF19 . These datasets differ with respect to text type (Wikipedia texts, examination texts, etc.), mode of answer selection (span-based, multiple choice, etc.) and test systems regarding different aspects of language understanding, but they do not explicitly address commonsense knowledge.
Two notable exceptions are the NewsQA and TriviaQA datasets. NewsQA BIBREF20 is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. NewsQA closely resembles our own data collection with respect to the method of data acquisition. As for our data collection, full texts were not shown to workers as a basis for question formulation, but only the text's title and a short summary, to avoid literal repetitions and support the generation of non-trivial questions requiring background knowledge. The NewsQA text collection differs from ours in domain and genre (newswire texts vs. narrative stories about everyday events). Knowledge required to answer the questions is mostly factual knowledge and script knowledge is only marginally relevant. Also, the task is not exactly question answering, but identification of document passages containing the answer.
TriviaQA BIBREF21 is a corpus that contains automatically collected question-answer pairs from 14 trivia and quiz-league websites, together with web-crawled evidence documents from Wikipedia and Bing. While a majority of questions require world knowledge for finding the correct answer, it is mostly factual knowledge.
Summary
We present a new dataset for the task of machine comprehension focussing on commonsense knowledge. Questions were collected based on script scenarios, rather than individual texts, which resulted in question–answer pairs that explicitly involve commonsense knowledge. In contrast to previous evaluation tasks, this setup allows us for the first time to assess the contribution of script knowledge for computational models of language understanding in a real-world evaluation scenario.
We expect our dataset to become a standard benchmark for testing models of commonsense and script knowledge. Human performance shows that the dataset is highly reliable. The results of several baselines, in contrast, illustrate that our task provides challenging test cases for the broader natural language processing community. MCScript forms the basis of a shared task organized at SemEval 2018. The dataset is available at http://www.sfb1102.uni-saarland.de/?page_id=2582.
Acknowledgements
We thank the reviewers for their helpful comments. We also thank Florian Pusse for the help with the MTurk experiments and our student assistants Christine SchÃ¤fer, Damyana Gateva, Leonie Harter, Sarah Mameche, Stefan GrÃ¼newald and Tatiana Anikina for help with the annotations. This research was funded by the German Research Foundation (DFG) as part of SFB 1102 `Information Density and Linguistic Encoding' and EXC 284 `Multimodal Computing and Interaction'.","['The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation.']",4536,qasper_e,en,,1d74cf15feae9302442102605c3838418f99b746b992ae00,"The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation.",352
What are the global network features which quantify different aspects of the sharing process?,"Introduction and related work
In recent years there has been increasing interest on the issue of disinformation spreading on online social media. Global concern over false (or ""fake"") news as a threat to modern democracies has been frequently raised–ever since 2016 US Presidential elections–in correspondence of events of political relevance, where the proliferation of manipulated and low-credibility content attempts to drive and influence people opinions BIBREF0BIBREF1BIBREF2BIBREF3.
Researchers have highlighted several drivers for the diffusion of such malicious phenomenon, which include human factors (confirmation bias BIBREF4, naive realism BIBREF5), algorithmic biases (filter bubble effect BIBREF0), the presence of deceptive agents on social platforms (bots and trolls BIBREF6) and, lastly, the formation of echo chambers BIBREF7 where people polarize their opinions as they are insulated from contrary perspectives.
The problem of automatically detecting online disinformation news has been typically formulated as a binary classification task (i.e. credible vs non-credible articles), and tackled with a variety of different techniques, based on traditional machine learning and/or deep learning, which mainly differ in the dataset and the features they employ to perform the classification. We may distinguish three approaches: those built on content-based features, those based on features extracted from the social context, and those which combine both aspects. A few main challenges hinder the task, namely the impossibility to manually verify all news items, the lack of gold-standard datasets and the adversarial setting in which malicious content is created BIBREF3BIBREF6.
In this work we follow the direction pointed out in a few recent contributions on the diffusion of disinformation compared to traditional and objective information. These have shown that false news spread faster and deeper than true news BIBREF8, and that social bots and echo chambers play an important role in the diffusion of malicious content BIBREF6, BIBREF7. Therefore we focus on the analysis of spreading patterns which naturally arise on social platforms as a consequence of multiple interactions between users, due to the increasing trend in online sharing of news BIBREF0.
A deep learning framework for detection of fake news cascades is provided in BIBREF9, where the authors refer to BIBREF8 in order to collect Twitter cascades pertaining to verified false and true rumors. They employ geometric deep learning, a novel paradigm for graph-based structures, to classify cascades based on four categories of features, such as user profile, user activity, network and spreading, and content. They also observe that a few hours of propagation are sufficient to distinguish false news from true news with high accuracy. Diffusion cascades on Weibo and Twitter are analyzed in BIBREF10, where authors focus on highlighting different topological properties, such as the number of hops from the source or the heterogeneity of the network, to show that fake news shape diffusion networks which are highly different from credible news, even at early stages of propagation.
In this work, we consider the results of BIBREF11 as our baseline. The authors use off-the-shelf machine learning classifiers to accurately classify news articles leveraging Twitter diffusion networks. To this aim, they consider a set of basic features which can be qualitatively interpreted w.r.t to the social behavior of users sharing credible vs non-credible information. Their methodology is overall in accordance with BIBREF12, where authors successfully detect Twitter astroturfing content, i.e. political campaigns disguised as spontaneous grassroots, with a machine learning framework based on network features.
In this paper, we propose a classification framework based on a multi-layer formulation of Twitter diffusion networks. For each article we disentangle different social interactions on Twitter, namely tweets, retweets, mentions, replies and quotes, to accordingly build a diffusion network composed of multiple layers (on for each type of interaction), and we compute structural features separately for each layer. We pick a set of global network properties from the network science toolbox which can be qualitatively explained in terms of social dimensions and allow us to encode different networks with a tuple of features. These include traditional indicators, e.g. network density, number of strong/weak connected components and diameter, and more elaborated ones such as main K-core number BIBREF13 and structural virality BIBREF14. Our main research question is whether the use of a multi-layer, disentangled network yields a significant advance in terms of classification accuracy over a conventional single-layer diffusion network. Additionally, we are interested in understanding which of the above features, and in which layer, are most effective in the classification task.
We perform classification experiments with an off-the-shelf Logistic Regression model on two different datasets of mainstream and disinformation news shared on Twitter respectively in the United States and in Italy during 2019. In the former case we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries.
The outline of this paper is the following: we first formulate the problem and describe data collection, network representation and structural properties employed for the classification; then we provide experimental results–classification performances, layer and feature importance analyses and a temporal classification evaluation–and finally we draw conclusions and future directions.
Methodology ::: Disinformation and mainstream news
In this work we formulate our classification problem as follows: given two classes of news articles, respectively $D$ (disinformation) and $M$ (mainstream), a set of news articles $A_i$ and associated class labels $C_i \in \lbrace D,M\rbrace $, and a set of tweets $\Pi _i=\lbrace T_i^1, T_i^2, ...\rbrace $ each of which contains an Uniform Resource Locator (URL) pointing explicitly to article $A_i$, predict the class $C_i$ of each article $A_i$. There is huge debate and controversy on a proper taxonomy of malicious and deceptive information BIBREF1BIBREF2BIBREF15BIBREF16BIBREF17BIBREF3BIBREF11. In this work we prefer the term disinformation to the more specific fake news to refer to a variety of misleading and harmful information. Therefore, we follow a source-based approach, a consolidated strategy also adopted by BIBREF6BIBREF16BIBREF2BIBREF1, in order to obtain relevant data for our analysis. We collected:
Disinformation articles, published by websites which are well-known for producing low-credibility content, false and misleading news reports as well as extreme propaganda and hoaxes and flagged as such by reputable journalists and fact-checkers;
Mainstream news, referring to traditional news outlets which deliver factual and credible information.
We believe that this is currently the most reliable classification approach, but it entails obvious limitations, as disinformation outlets may also publish true stories and likewise misinformation is sometimes reported on mainstream media. Also, given the choice of news sources, we cannot test whether our methodology is able to classify disinformation vs factual but not mainstream news which are published on niche, non-disinformation outlets.
Methodology ::: US dataset
We collected tweets associated to a dozen US mainstream news websites, i.e. most trusted sources described in BIBREF18, with the Streaming API, and we referred to Hoaxy API BIBREF16 for what concerns tweets containing links to 100+ US disinformation outlets. We filtered out articles associated to less than 50 tweets. The resulting dataset contains overall $\sim $1.7 million tweets for mainstream news, collected in a period of three weeks (February 25th, 2019-March 18th, 2019), which are associated to 6,978 news articles, and $\sim $1.6 million tweets for disinformation, collected in a period of three months (January 1st, 2019-March 18th, 2019) for sake of balance of the two classes, which hold 5,775 distinct articles. Diffusion censoring effects BIBREF14 were correctly taken into account in both collection procedures. We provide in Figure FIGREF4 the distribution of articles by source and political bias for both news domains.
As it is reported that conservatives and liberals exhibit different behaviors on online social platforms BIBREF19BIBREF20BIBREF21, we further assigned a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2. In order to assess the robustness of our method, we performed classification experiments by training only on left-biased (or right-biased) outlets of both disinformation and mainstream domains and testing on the entire set of sources, as well as excluding particular sources that outweigh the others in terms of samples to avoid over-fitting.
Methodology ::: Italian dataset
For what concerns the Italian scenario we first collected tweets with the Streaming API in a 3-week period (April 19th, 2019-May 5th, 2019), filtering those containing URLs pointing to Italian official newspapers websites as described in BIBREF22; these correspond to the list provided by the association for the verification of newspaper circulation in Italy (Accertamenti Diffusione Stampa). We instead referred to the dataset provided by BIBREF23 to obtain a set of tweets, collected continuously since January 2019 using the same Twitter endpoint, which contain URLs to 60+ Italian disinformation websites. In order to get balanced classes (April 5th, 2019-May 5th, 2019), we retained data collected in a longer period w.r.t to mainstream news. In both cases we filtered out articles with less than 50 tweets; overall this dataset contains $\sim $160k mainstream tweets, corresponding to 227 news articles, and $\sim $100k disinformation tweets, corresponding to 237 news articles. We provide in Figure FIGREF5 the distribution of articles according to distinct sources for both news domains. As in the US dataset, we took into account censoring effects BIBREF14 by excluding tweets published before (left-censoring) or after two weeks (right-censoring) from the beginning of the collection process.
The different volumes of news shared on Twitter in the two countries are due both to the different population size of US and Italy (320 vs 60 millions) but also to the different usage of Twitter platform (and social media in general) for news consumption BIBREF24. Both datasets analyzed in this work are available from the authors on request.
A crucial aspect in our approach is the capability to fully capturing sharing cascades on Twitter associated to news articles. It has been reported BIBREF25 that the Twitter streaming endpoint filters out tweets matching a given query if they exceed 1% of the global daily volume of shared tweets, which nowadays is approximately $5\cdot 10^8$; however, as we always collected less than $10^6$ tweets per day, we did not incur in this issue and we thus gathered 100% of tweets matching our query.
Methodology ::: Building diffusion networks
We built Twitter diffusion networks following an approach widely adopted in the literature BIBREF6BIBREF17BIBREF2. We remark that there is an unavoidable limitation in Twitter Streaming API, which does not allow to retrieve true re-tweeting cascades because re-tweets always point to the original source and not to intermediate re-tweeting users BIBREF8BIBREF14; thus we adopt the only viable approach based on Twitter's public availability of data. Besides, by disentangling different interactions with multiple layers we potentially reduce the impact of this limitation on the global network properties compared to the single-layer approach used in our baseline.
Using the notation described in BIBREF26. we employ a multi-layer representation for Twitter diffusion networks. Sociologists have indeed recognized decades ago that it is crucial to study social systems by constructing multiple social networks where different types of ties among same individuals are used BIBREF27. Therefore, for each news article we built a multi-layer diffusion network composed of four different layers, one for each type of social interaction on Twitter platform, namely retweet (RT), reply (R), quote (Q) and mention (M), as shown in Figure FIGREF11. These networks are not necessarily node-aligned, i.e. users might be missing in some layers. We do not insert ""dummy"" nodes to represent all users as it would have severe impact on the global network properties (e.g. number of weakly connected components). Alternatively one may look at each multi-layer diffusion network as an ensemble of individual graphs BIBREF26; since global network properties are computed separately for each layer, they are not affected by the presence of any inter-layer edges.
In our multi-layer representation, each layer is a directed graph where we add edges and nodes for each tweet of the layer type, e.g. for the RT layer: whenever user $a$ retweets account $b$ we first add nodes $a$ and $b$ if not already present in the RT layer, then we build an edge that goes from $b$ to $a$ if it does not exists or we increment the weight by 1. Similarly for the other layers: for the R layer edges go from user $a$ (who replies) to user $b$, for the Q layer edges go from user $b$ (who is quoted by) to user $a$ and for the M layer edges go from user $a$ (who mentions) to user $b$.
Note that, by construction, our layers do not include isolated nodes; they correspond to ""pure tweets"", i.e. tweets which have not originated any interactions with other users. However, they are present in our dataset, and their number is exploited for classification, as described below.
Methodology ::: Global network properties
We used a set of global network indicators which allow us to encode each network layer by a tuple of features. Then we simply concatenated tuples as to represent each multi-layer network with a single feature vector. We used the following global network properties:
Number of Strongly Connected Components (SCC): a Strongly Connected Component of a directed graph is a maximal (sub)graph where for each pair of vertices $u,v$ there is a path in each direction ($u\rightarrow v$, $v\rightarrow u$).
Size of the Largest Strongly Connected Component (LSCC): the number of nodes in the largest strongly connected component of a given graph.
Number of Weakly Connected Components (WCC): a Weakly Connected Component of a directed graph is a maximal (sub)graph where for each pair of vertices $(u, v)$ there is a path $u \leftrightarrow v$ ignoring edge directions.
Size of the Largest Weakly Connected Component (LWCC): the number of nodes in the largest weakly connected component of a given graph.
Diameter of the Largest Weakly Connected Component (DWCC): the largest distance (length of the shortest path) between two nodes in the (undirected version of) largest weakly connected component of a graph.
Average Clustering Coefficient (CC): the average of the local clustering coefficients of all nodes in a graph; the local clustering coefficient of a node quantifies how close its neighbours are to being a complete graph (or a clique). It is computed according to BIBREF28.
Main K-core Number (KC): a K-core BIBREF13 of a graph is a maximal sub-graph that contains nodes of internal degree $k$ or more; the main K-core number is the highest value of $k$ (in directed graphs the total degree is considered).
Density (d): the density for directed graphs is $d=\frac{|E|}{|V||V-1|}$, where $|E|$ is the number of edges and $|N|$ is the number of vertices in the graph; the density equals 0 for a graph without edges and 1 for a complete graph.
Structural virality of the largest weakly connected component (SV): this measure is defined in BIBREF14 as the average distance between all pairs of nodes in a cascade tree or, equivalently, as the average depth of nodes, averaged over all nodes in turn acting as a root; for $|V| > 1$ vertices, $SV=\frac{1}{|V||V-1|}\sum _i\sum _j d_{ij}$ where $d_{ij}$ denotes the length of the shortest path between nodes $i$ and $j$. This is equivalent to compute the Wiener's index BIBREF29 of the graph and multiply it by a factor $\frac{1}{|V||V-1|}$. In our case we computed it for the undirected equivalent graph of the largest weakly connected component, setting it to 0 whenever $V=1$.
We used networkx Python package BIBREF30 to compute all features. Whenever a layer is empty. we simply set to 0 all its features. In addition to computing the above nine features for each layer, we added two indicators for encoding information about pure tweets, namely the number T of pure tweets (containing URLs to a given news article) and the number U of unique users authoring those tweets. Therefore, a single diffusion network is represented by a vector with $9\cdot 4+2=38$ entries.
Methodology ::: Interpretation of network features and layers
Aforementioned network properties can be qualitatively explained in terms of social footprints as follows: SCC correlates with the size of the diffusion network, as the propagation of news occurs in a broadcast manner most of the time, i.e. re-tweets dominate on other interactions, while LSCC allows to distinguish cases where such mono-directionality is somehow broken. WCC equals (approximately) the number of distinct diffusion cascades pertaining to each news article, with exceptions corresponding to those cases where some cascades merge together via Twitter interactions such as mentions, quotes and replies, and accordingly LWCC and DWCC equals the size and the depth of the largest cascade. CC corresponds to the level of connectedness of neighboring users in a given diffusion network whereas KC identifies the set of most influential users in a network and describes the efficiency of information spreading BIBREF17. Finally, d describes the proportions of potential connections between users which are actually activated and SV indicates whether a news item has gained popularity with a single and large broadcast or in a more viral fashion through multiple generations.
For what concerns different Twitter actions, users primarily interact with each other using retweets and mentions BIBREF20.
The former are the main engagement activity and act as a form of endorsement, allowing users to rebroadcast content generated by other users BIBREF31. Besides, when node B retweets node A we have an implicit confirmation that information from A appeared in B's Twitter feed BIBREF12. Quotes are simply a special case of retweets with comments.
Mentions usually include personal conversations as they allow someone to address a specific user or to refer to an individual in the third person; in the first case they are located at the beginning of a tweet and they are known as replies, otherwise they are put in the body of a tweet BIBREF20. The network of mentions is usually seen as a stronger version of interactions between Twitter users, compared to the traditional graph of follower/following relationships BIBREF32.
Experiments ::: Setup
We performed classification experiments using a basic off-the-shelf classifier, namely Logistic Regression (LR) with L2 penalty; this also allows us to compare results with our baseline. We applied a standardization of the features and we used the default configuration for parameters as described in scikit-learn package BIBREF33. We also tested other classifiers (such as K-Nearest Neighbors, Support Vector Machines and Random Forest) but we omit results as they give comparable performances. We remark that our goal is to show that a very simple machine learning framework, with no parameter tuning and optimization, allows for accurate results with our network-based approach.
We used the following evaluation metrics to assess the performances of different classifiers (TP=true positives, FP=false positives, FN=false negatives):
Precision = $\frac{TP}{TP+FP}$, the ability of a classifier not to label as positive a negative sample.
Recall = $\frac{TP}{TP+FN}$, the ability of a classifier to retrieve all positive samples.
F1-score = $2 \frac{\mbox{Precision} \cdot \mbox{Recall}}{\mbox{Precision} + \mbox{Recall}}$, the harmonic average of Precision and Recall.
Area Under the Receiver Operating Characteristic curve (AUROC); the Receiver Operating Characteristic (ROC) curve BIBREF34, which plots the TP rate versus the FP rate, shows the ability of a classifier to discriminate positive samples from negative ones as its threshold is varied; the AUROC value is in the range $[0, 1]$, with the random baseline classifier holding AUROC$=0.5$ and the ideal perfect classifier AUROC$=1$; thus larger AUROC values (and steeper ROCs) correspond to better classifiers.
In particular we computed so-called macro average–simple unweighted mean–of these metrics evaluated considering both labels (disinformation and mainstream). We employed stratified shuffle split cross validation (with 10 folds) to evaluate performances.
Finally, we partitioned networks according to the total number of unique users involved in the sharing, i.e. the number of nodes in the aggregated network represented with a single-layer representation considering together all layers and also pure tweets. A breakdown of both datasets according to size class (and political biases for the US scenario) is provided in Table 1 and Table 2.
Experiments ::: Classification performances
In Table 3 we first provide classification performances on the US dataset for the LR classifier evaluated on the size class described in Table 1. We can observe that in all instances our methodology performs better than a random classifier (50% AUROC), with AUROC values above 85% in all cases.
For what concerns political biases, as the classes of mainstream and disinformation networks are not balanced (e.g., 1,292 mainstream and 4,149 disinformation networks with right bias) we employ a Balanced Random Forest with default parameters (as provided in imblearn Python package BIBREF35). In order to test the robustness of our methodology, we trained only on left-biased networks or right-biased networks and tested on the entire set of sources (relative to the US dataset); we provide a comparison of AUROC values for both biases in Figure 4. We can notice that our multi-layer approach still entails significant results, thus showing that it can accurately distinguish mainstream news from disinformation regardless of the political bias. We further corroborated this result with additional classification experiments, that show similar performances, in which we excluded from the training/test set two specific sources (one at a time and both at the same time) that outweigh the others in terms of data samples–respectively ""breitbart.com"" for right-biased sources and ""politicususa.com"" for left-biased ones.
We performed classification experiments on the Italian dataset using the LR classifier and different size classes (we excluded $[1000, +\infty )$ which is empty); we show results for different evaluation metrics in Table 3. We can see that despite the limited number of samples (one order of magnitude smaller than the US dataset) the performances are overall in accordance with the US scenario. As shown in Table 4, we obtain results which are much better than our baseline in all size classes (see Table 4):
In the US dataset our multi-layer methodology performs much better in all size classes except for large networks ($[1000, +\infty )$ size class), reaching up to 13% improvement on smaller networks ($[0, 100)$ size class);
In the IT dataset our multi-layer methodology outperforms the baseline in all size classes, with the maximum performance gain (20%) on medium networks ($[100, 1000)$ size class); the baseline generally reaches bad performances compared to the US scenario.
Overall, our performances are comparable with those achieved by two state-of-the-art deep learning models for ""fake news"" detection BIBREF9BIBREF36.
Experiments ::: Layer importance analysis
In order to understand the impact of each layer on the performances of classifiers, we performed additional experiments considering separately each layer (we ignored T and U features relative to pure tweets). In Table 5 we show metrics for each layer and all size classes, computed with a 10-fold stratified shuffle split cross validation, evaluated on the US dataset; in Figure 5 we show AUROC values for each layer compared with the general multi-layer approach. We can notice that both Q and M layers alone capture adequately the discrepancies of the two distinct news domains in the United States as they obtain good results with AUROC values in the range 75%-86%; these are comparable with those of the multi-layer approach which, nevertheless, outperforms them across all size classes.
We obtained similar performances for the Italian dataset, as the M layer obtains comparable performances w.r.t multi-layer approach with AUROC values in the range 72%-82%. We do not show these results for sake of conciseness.
Experiments ::: Feature importance analysis
We further investigated the importance of each feature by performing a $\chi ^2$ test, with 10-fold stratified shuffle split cross validation, considering the entire range of network sizes $[0, +\infty )$. We show the Top-5 most discriminative features for each country in Table 6.
We can notice the exact same set of features (with different relative orderings in the Top-3) in both countries; these correspond to two global network propertie–LWCC, which indicates the size of the largest cascade in the layer, and SCC, which correlates with the size of the network–associated to the same set of layers (Quotes, Retweets and Mentions).
We further performed a $\chi ^2$ test to highlight the most discriminative features in the M layer of both countries, which performed equally well in the classification task as previously highlighted; also in this case we focused on the entire range of network sizes $[0, +\infty )$. Interestingly, we discovered exactly the same set of Top-3 features in both countries, namely LWCC, SCC and DWCC (which indicates the depth of the largest cascade in the layer).
An inspection of the distributions of all aforementioned features revealed that disinformation news exhibit on average larger values than mainstream news.
We can qualitatively sum up these results as follows:
Sharing patterns in the two news domains exhibit discrepancies which might be country-independent and due to the content that is being shared.
Interactions in disinformation sharing cascades tends to be broader and deeper than in mainstream news, as widely reported in the literature BIBREF8BIBREF2BIBREF7.
Users likely make a different usage of mentions when sharing news belonging to the two domains, consequently shaping different sharing patterns.
Experiments ::: Temporal analysis
Similar to BIBREF9, we carried out additional experiments to answer the following question: how long do we need to observe a news spreading on Twitter in order to accurately classify it as disinformation or mainstream?
With this goal, we built several versions of our original dataset of multi-layer networks by considering in turn the following lifetimes: 1 hour, 6 hours, 12 hours, 1 day, 2 days, 3 days and 7 days; for each case, we computed the global network properties of the corresponding network and evaluated the LR classifier with 10-fold cross validation, separately for each lifetime (and considering always the entire set of networks). We show corresponding AUROC values for both US and IT datasets in Figure 6.
We can see that in both countries news diffusion networks can be accurately classified after just a few hours of spreading, with AUROC values which are larger than 80% after only 6 hours of diffusion. These results are very promising and suggest that articles pertaining to the two news domains exhibit discrepancies in their sharing patterns that can be timely exploited in order to rapidly detect misleading items from factual information.
Conclusions
In this work we tackled the problem of the automatic classification of news articles in two domains, namely mainstream and disinformation news, with a language-independent approach which is based solely on the diffusion of news items on Twitter social platform. We disentangled different types of interactions on Twitter to accordingly build a multi-layer representation of news diffusion networks, and we computed a set of global network properties–separately for each layer–in order to encode each network with a tuple of features. Our goal was to investigate whether a multi-layer representation performs better than one layer BIBREF11, and to understand which of the features, observed at given layers, are most effective in the classification task.
Experiments with an off-the-shelf classifier such as Logistic Regression on datasets pertaining to two different media landscapes (US and Italy) yield very accurate classification results (AUROC up to 94%), even when accounting for the different political bias of news sources, which are far better than our baseline BIBREF11 with improvements up to 20%. Classification performances using single layers show that the layer of mentions alone entails better performance w.r.t other layers in both countries.
We also highlighted the most discriminative features across different layers in both countries; the results suggest that differences between the two news domains might be country-independent but rather due only to the typology of content shared, and that disinformation news shape broader and deeper cascades.
Additional experiments involving the temporal evolution of Twitter diffusion networks show that our methodology can accurate classify mainstream and disinformation news after a few hours of propagation on the platform.
Overall, our results prove that the topological features of multi-layer diffusion networks might be effectively exploited to detect online disinformation. We do not deny the presence of deceptive efforts to orchestrate the regular spread of information on social media via content amplification and manipulation BIBREF37BIBREF38. On the contrary, we postulate that such hidden forces might play to accentuate the discrepancies between the diffusion patterns of disinformation and mainstream news (and thus to make our methodology effective).
In the future we aim to further investigate three directions: (1) employ temporal networks to represent news diffusion and apply classification techniques that take into account the sequential aspect of data (e.g. recurrent neural networks); (2) carry out an extensive comparison of the diffusion of disinformation and mainstream news across countries to investigate deeper the presence of differences and similarities in sharing patterns; (3) leverage our network-based features in addition to state-of-the-art text-based approaches for ""fake news"" dete ction in order to deliver a real-world system to detect misleading and harmful information spreading on social media.","['Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)']",4906,qasper_e,en,,04d9e82efc74cd3ef2b635904c05a5f0b2c34ac5a4318304,"Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)",336
what keyphrase extraction models were reassessed?,"Introduction
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/
Recent years have seen a surge of interest in automatic keyphrase extraction, thanks to the availability of the SemEval-2010 benchmark dataset BIBREF0 . This dataset is composed of documents (scientific articles) that were automatically converted from PDF format to plain text. As a result, most documents contain irrelevant pieces of text (e.g. muddled sentences, tables, equations, footnotes) that require special handling, so as to not hinder the performance of keyphrase extraction systems. In previous work, these are usually removed at the preprocessing step, but using a variety of techniques ranging from simple heuristics BIBREF1 , BIBREF2 , BIBREF3 to sophisticated document logical structure detection on richly-formatted documents recovered from Google Scholar BIBREF4 . Under such conditions, it may prove difficult to draw firm conclusions about which keyphrase extraction model performs best, as the impact of preprocessing on overall performance cannot be properly quantified.
While previous work clearly states that efficient document preprocessing is a prerequisite for the extraction of high quality keyphrases, there is, to our best knowledge, no empirical evidence of how preprocessing affects keyphrase extraction performance. In this paper, we re-assess the performance of several state-of-the-art keyphrase extraction models at increasingly sophisticated levels of preprocessing. Three incremental levels of document preprocessing are experimented with: raw text, text cleaning through document logical structure detection, and removal of keyphrase sparse sections of the document. In doing so, we present the first consistent comparison of different keyphrase extraction models and study their robustness over noisy text. More precisely, our contributions are:
Dataset and Preprocessing
The SemEval-2010 benchmark dataset BIBREF0 is composed of 244 scientific articles collected from the ACM Digital Library (conference and workshop papers). The input papers ranged from 6 to 8 pages and were converted from PDF format to plain text using an off-the-shelf tool. The only preprocessing applied is a systematic dehyphenation at line breaks and removal of author-assigned keyphrases. Scientific articles were selected from four different research areas as defined in the ACM classification, and were equally distributed into training (144 articles) and test (100 articles) sets. Gold standard keyphrases are composed of both author-assigned keyphrases collected from the original PDF files and reader-assigned keyphrases provided by student annotators.
Long documents such as those in the SemEval-2010 benchmark dataset are notoriously difficult to handle due to the large number of keyphrase candidates (i.e. phrases that are eligible to be keyphrases) that the systems have to cope with BIBREF6 . Furthermore, noisy textual content, whether due to format conversion errors or to unusable elements (e.g. equations), yield many spurious keyphrase candidates that negatively affect keyphrase extraction performance. This is particularly true for systems that make use of core NLP tools to select candidates, that in turn exhibit poor performance on degraded text. Filtering out irrelevant text is therefore needed for addressing these issues.
In this study, we concentrate our effort on re-assessing keyphrase extraction performance on three increasingly sophisticated levels of document preprocessing described below.
Table shows the average number of sentences and words along with the maximum possible recall for each level of preprocessing. The maximum recall is obtained by computing the fraction of the reference keyphrases that occur in the documents. We observe that the level 2 preprocessing succeeds in eliminating irrelevant text by significantly reducing the number of words (-19%) while maintaining a high maximum recall (-2%). Level 3 preprocessing drastically reduce the number of words to less than a quarter of the original amount while interestingly still preserving high recall.
Keyphrase Extraction Models
We re-implemented five keyphrase extraction models : the first two are commonly used as baselines, the third is a resource-lean unsupervised graph-based ranking approach, and the last two were among the top performing systems in the SemEval-2010 keyphrase extraction task BIBREF0 . We note that two of the systems are supervised and rely on the training set to build their classification models. Document frequency counts are also computed on the training set. Stemming is applied to allow more robust matching. The different keyphrase extraction models are briefly described below:
Each model uses a distinct keyphrase candidate selection method that provides a trade-off between the highest attainable recall and the size of set of candidates. Table summarizes these numbers for each model. Syntax-based selection heuristics, as used by TopicRank and WINGNUS, are better suited to prune candidates that are unlikely to be keyphrases. As for KP-miner, removing infrequent candidates may seem rather blunt, but it turns out to be a simple yet effective pruning method when dealing with long documents. For details on how candidate selection methods affect keyphrase extraction, please refer to BIBREF16 .
Apart from TopicRank that groups similar candidates into topics, the other models do not have any redundancy control mechanism. Yet, recent work has shown that up to 12% of the overall error made by state-of-the-art keyphrase extraction systems were due to redundancy BIBREF6 , BIBREF17 . Therefore as a post-ranking step, we remove redundant keyphrases from the ranked lists generated by all models. A keyphrase is considered redundant if it is included in another keyphrase that is ranked higher in the list.
Experimental settings
We follow the evaluation procedure used in the SemEval-2010 competition and evaluate the performance of each model in terms of f-measure (F) at the top INLINEFORM0 keyphrases. We use the set of combined author- and reader-assigned keyphrases as reference keyphrases. Extracted and reference keyphrases are stemmed to reduce the number of mismatches.
Results
The performances of the keyphrase extraction models at each preprocessing level are shown in Table . Overall, we observe a significant increase of performance for all models at levels 3, confirming that document preprocessing plays an important role in keyphrase extraction performance. Also, the difference of f-score between the models, as measured by the standard deviation INLINEFORM0 , gradually decreases with the increasing level of preprocessing. This result strengthens the assumption made in this paper, that performance variation across models is partly a function of the effectiveness of document preprocessing.
Somewhat surprisingly, the ordering of the two best models reverses at level 3. This showcases that rankings are heavily influenced by the preprocessing stage, despite the common lack of details and analysis it suffers from in explanatory papers. We also remark that the top performing model, namely KP-Miner, is unsupervised which supports the findings of BIBREF6 indicating that recent unsupervised approaches have rivaled their supervised counterparts in performance.
In an attempt to quantify the performance variation across preprocessing levels, we compute the standard deviation INLINEFORM0 for each model. Here we see that unsupervised models are more sensitive to noisy input, as revealed by higher standard deviations. We found two main reasons for this. First, using multiple discriminative features to rank keyphrase candidates adds inherent robustness to the models. Second, the supervision signal helps models to disregard noise.
In Table , we compare the outputs of the five models by measuring the percentage of valid keyphrases that are retrieved by all models at once for each preprocessing level. By these additional results, we aim to assess whether document preprocessing smoothes differences between models. We observe that the overlap between the outputs of the different models increases along with the level of preprocessing. This suggests that document preprocessing reduces the effect that the keyphrase extraction model in itself has on overall performance. In other words, the singularity of each model fades away gradually with increase in preprocessing effort.
Reproducibility
Being able to reproduce experimental results is a central aspect of the scientific method. While assessing the importance of the preprocessing stage for five approaches, we found that several results were not reproducible, as shown in Table .
We note that the trends for baselines and high ranking systems are opposite: compared to the published results, our reproduction of top systems under-performs and our reproduction of baselines over-performs. We hypothesise that this stems from a difference in hyperparameter tuning, including the ones that the preprocessing stage makes implicit. Competitors have strong incentives to correctly optimize hyperparameters, to achieve a high ranking and get more publicity for their work while competition organizers might have the opposite incentive: too strong a baseline might not be considered a baseline anymore.
We also observe that with this leveled preprocessing, the gap between baselines and top systems is much smaller, lessening again the importance of raw scores and rankings to interpret the shared task results and emphasizing the importance of understanding correctly the preprocessing stage.
Additional experiments
In the previous sections, we provided empirical evidence that document preprocessing weighs heavily on the outcome of keyphrase extraction models. This raises the question of whether further improvement might be gained from a more aggressive preprocessing. To answer this question, we take another step beyond content filtering and further abridge the input text from level 3 preprocessed documents using an unsupervised summarization technique. More specifically, we keep the title and abstract intact, as they are the two most keyphrase dense parts within scientific articles BIBREF4 , and select only the most content bearing sentences from the remaining contents. To do this, sentences are ordered using TextRank BIBREF14 and the less informative ones, as determined by their TextRank scores normalized by their lengths in words, are filtered out.
Finding the optimal subset of sentences from already shortened documents is however no trivial task as maximum recall linearly decreases with the number of sentences discarded. Here, we simply set the reduction ratio to 0.865 so that the average maximum recall on the training set does not lose more than 5%. Table shows the reduction in the average number of sentences and words compared to level 3 preprocessing.
The performances of the keyphrase extraction models at level 4 preprocessing are shown in Table . We note that two models, namely TopicRank and TF INLINEFORM0 IDF, lose on performance. These two models mainly rely on frequency counts to rank keyphrase candidates, which in turn become less reliable at level 4 because of the very short length of the documents. Other models however have their f-scores once again increased, thus indicating that further improvement is possible from more reductive document preprocessing strategies.
Conclusion
In this study, we re-assessed the performance of several keyphrase extraction models and showed that performance variation across models is partly a function of the effectiveness of the document preprocessing. Our results also suggest that supervised keyphrase extraction models are more robust to noisy input.
Given our findings, we recommend that future works use a common preprocessing to evaluate the interest of keyphrase extraction approaches. For this reason we make the four levels of preprocessing used in this study available for the community.","['Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion.']",1822,qasper_e,en,,867b7b94ee2ae6051131ca90ebc66df6ba3084f4bb3bdda8,"Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion.",293
